<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.CL](#cs.CL) [Total: 86]
- [cs.DL](#cs.DL) [Total: 3]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.RO](#cs.RO) [Total: 43]
- [cs.SI](#cs.SI) [Total: 6]
- [cs.CR](#cs.CR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 18]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.SE](#cs.SE) [Total: 37]
- [cs.DS](#cs.DS) [Total: 15]
- [cs.HC](#cs.HC) [Total: 36]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.SD](#cs.SD) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 本文比较了用于3D材料图像镶嵌模型的算法，评估了不同优化方法的优缺点，并为选择合适的方法提供了指导。


<details>
  <summary>Details</summary>
Motivation: 本文旨在对用于拟合材料（如多晶体和泡沫）3D 图像数据的镶嵌模型的算法策略进行比较分析。

Method: 本研究回顾并评估了基于优化的方法，包括线性规划、非线性规划、通过交叉熵方法进行的随机优化和梯度下降，用于生成逼近体素基晶粒结构的 Voronoi 图、Laguerre 图和广义平衡幂图 (GBPD)。

Result: 使用差异度量在真实数据集上评估拟合质量，这些度量量化了晶粒体积、表面积和拓扑结构之间的差异。

Conclusion: 本研究结果强调了模型复杂性、优化算法复杂性以及近似质量之间的权衡，为根据数据特性和应用需求选择合适的方法提供了指导。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 该研究提出并评估了几种用于自动驾驶汽车场景理解的深度学习模型，重点是语义分割。通过在BDD100k数据集上使用不同的骨干网络，研究证明了骨干网络选择对模型性能的重要性，并实现了性能的提升。


<details>
  <summary>Details</summary>
Motivation: 为了在自动驾驶汽车领域更有效地利用深度学习技术，解决场景理解和语义分割的挑战。

Method: 提出并研究了几种有效的模型，并使用了几种骨干网络作为编码器。

Result: 所提出的模型在BDD100k数据集上进行了研究，实验结果表明，选择合适的骨干网络对模型的语义分割性能有显著影响，并且所提出的模型在准确率、平均交并比和损失函数方面均有所提升。

Conclusion: 所提出的模型在准确率、平均交并比和损失函数方面得到改进，提高了场景语义分割的性能。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [3] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一个上下文感知的框架，通过激活一组轻量级的低秩适配器（LoRA）来实现嵌入式设备上的高效多标签视频分类，该框架具有较低的能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备上的实时多标签视频分类受到计算和能源预算的限制。视频流表现出标签稀疏性、时间连续性和标签共现等结构特性，可以用于更高效的推理。

Method: Polymorph框架激活一个最小集的光谱低秩适配器（LoRA），每个适配器专注于由共现模式派生的子集。在运行时，Polymorph动态选择和组合所需的适配器以覆盖活动标签，避免了完整的模型切换和权重合并。

Result: Polymorph在TAO数据集上实现了40%的能耗降低和9个点的mAP提升，优于强基线。

Conclusion: Polymorph框架通过动态选择和组合所需的适配器来提高可扩展性，同时降低延迟和能耗。在TAO数据集上，与强基线相比，Polymorph实现了40%的能耗降低和9个点的mAP提升。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [4] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA是一种新的TTA方法，通过匹配CLIP的预训练目标来改进其在分布变化下的性能，并在开放集设置中通过OCE损失提高了异常检测能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决像CLIP这样的视觉语言模型（VLMs）在分布变化下泛化能力不足的问题，并改进现有的测试时间适应（TTA）方法（如熵最小化）与CLIP的对比学习目标不一致所带来的局限性。

Method: 提出了一种名为CLIPTTA的新型梯度下降测试时间适应（TTA）方法，该方法利用软对比损失，该损失与CLIP的预训练目标保持一致。CLIPTTA 的梯度具有批次感知设计，可以减轻模型崩溃的风险。此外，还提出了一种新颖的异常对比暴露（OCE）损失，用于在开放集设置中提高异常检测能力。

Result: CLIPTTA在各种分布变化下表现出比基于熵的TTA方法更强的性能和更稳定的适应性，并且在许多情况下优于现有的最先进TTA方法。

Conclusion: CLIPTTA在75个跨越不同分布变化的数据集上进行了评估，其表现始终优于基于熵的目标，并且与最先进的测试时间适应（TTA）方法相比具有高度竞争力，在大量数据集上表现更优，并在各种分布变化中展现出更稳定的性能。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [5] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: AF 通过 TIME 和 TAP 机制，解决 GCD 中因注意力分散导致的性能下降问题，可即插即用，提升现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 GCD 方法在处理无标签数据时，模型倾向于关注任务不相关的背景区域，导致特征提取不佳，即注意力分散问题。

Method: AF 包含两个组件：Token Importance Measurement (TIME) 和 Token Adaptive Pruning (TAP)。TIME 对多尺度 token 重要性进行量化，TAP 利用 TIME 提供的多尺度重要性分数来修剪信息量低的 token。AF 是一个轻量级的、即插即用的模块，可以无缝集成到现有的 GCD 方法中，且计算开销极小。

Result: 当 AF 集成到 SimGCD 方法中时，在保持极低计算开销的同时，性能提升高达 15.4%。

Conclusion: Attention Focusing (AF) 是一个新提出的自适应机制，通过修剪信息量低的 token 来锐化模型的焦点，以解决广义类别发现（GCD）中的注意力分散问题。AF 包含两个组件：Token Importance Measurement (TIME) 和 Token Adaptive Pruning (TAP)。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [6] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 生成式超分辨率（GSR）模型会产生“幻觉”（生成的细节与原图不匹配）。本研究利用大语言模型提出“幻觉分数”（HS）来量化幻觉，并使用深度特征距离作为奖励来优化GSR模型，以减少幻觉，提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成式超分辨率（GSR）模型虽然在感知图像质量上表现优异，但未能达到人类期望的质量和保真度之间的最佳平衡。模型生成的细节与低分辨率图像（LRI）或真实图像（GTI）在感知上不匹配的“幻觉”伪影是一个关键但研究不足的问题，限制了GSR的实际应用。

Method: 提出了一种利用多模态大语言模型（MLLM）构建提示来评估幻觉视觉元素并生成“幻觉分数”（HS）的方法。同时，提出了一种利用深度特征距离作为可微分奖励函数来减轻幻觉的GSR模型对齐方法。

Result: 所提出的“幻觉分数”（HS）与人类评估高度一致，并提供了优于现有图像指标的补充见解。研究发现，某些深度特征距离与HS具有很强的相关性，并成功地将GSR模型与这些特征对齐，以减轻幻觉。

Conclusion: 本研究提出了一种基于多模态大语言模型（MLLM）的“幻觉分数”（HS）来衡量和减轻生成式超分辨率（GSR）中的幻觉伪影。HS与人类评估高度一致，并能为超分辨率（SR）模型提供补充见解。此外，研究发现某些深度特征距离与HS强相关，可用于指导GSR模型减轻幻觉。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [7] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一个结合了深度学习和光流的超声图像追踪工具，可用于心脏、肌肉和关节等多种应用，追踪准确且易于使用。


<details>
  <summary>Details</summary>
Motivation: 准确追踪B模式超声图像中的组织运动对于量化组织动力学至关重要，但存在散斑噪声、低边缘对比度和平面外运动等挑战。

Method: DUSTrack结合了深度学习和光流技术，并包含一个用户界面，用于生成训练数据和模型优化，同时采用一种新颖的光流滤波技术来减少噪声。

Result: DUSTrack在零样本点追踪方面表现出优越的准确性，并且在追踪性能上可与专用方法相媲美。

Conclusion: DUSTrack是一个强大的、灵活的开源解决方案，用于从超声视频中量化组织运动，并在心脏、肌肉和踝关节等应用中展示了其通用性。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [8] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一个神经符号框架，用于可解释的习得基础，它结合了常识先验、语言模型和视觉证据，通过迭代推理来识别场景中能够执行特定动作的对象。


<details>
  <summary>Details</summary>
Motivation: 介绍CRAFT，一个用于可解释的习得基础的神经符号框架，它识别场景中能够进行给定动作（例如，“切割”）的对象。

Method: CRAFT集成了来自ConceptNet的结构化常识先验和语言模型，以及来自CLIP的视觉证据，并使用基于能量的推理循环来迭代地优化预测。

Result: 实验表明，CRAFT在多对象、无标签设置中提高了准确性，同时提高了可解释性。

Conclusion: CRAFT在多对象、无标签设置中提高了准确性，同时提高了可解释性，为实现鲁棒和可信赖的场景理解迈出了一步。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [9] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 提出了一种创新的三维高斯喷溅视频流化框架，通过基于高斯形变场的方法和混合显著性切片、差异化质量模型，实现了高效压缩和传输，并在视频质量、压缩效率和传输速率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决三维高斯喷溅视频数据量大、压缩和传输复杂等问题，提出了一种创新的三维高斯喷溅视频流化框架。

Method: 提出了一种基于高斯形变场的三维高斯喷溅视频构建方法，并结合混合显著性切片和差异化质量模型，实现了高效的数据压缩和带宽波动适应。

Result: 实验评估验证了所构建的三维高斯喷溅视频流化系统的传输性能。

Conclusion: 该方法在视频质量、压缩效率和传输速率等方面优于现有方法。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [10] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是第一个针对真实世界红外图像的多模态大型语言模型，它使用包含超过26万个真实图像文本对的IR-TD数据集，并通过一种双交叉模态课程迁移学习策略来提高性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决真实世界红外图像在文本数据稀疏性和领域特定特性方面给视觉语言模型带来的挑战，以及现有方法依赖于从可见光图像进行风格转换生成的合成红外图像，从而限制了它们捕捉红外模态独特特性的能力。

Method: 提出了一种名为IRGPT的多模态大型语言模型，该模型建立在一个大规模的红外文本数据集（IR-TD）之上，包含超过26万个真实的图像文本对。该数据集包含真实的红外图像以及精心制作的文本。此外，还引入了一种双交叉模态课程迁移学习策略，通过考虑红外-可见光和红外-文本的难度分数，系统地将知识从可见光领域迁移到红外领域。

Result: IRGPT在9项任务（例如，识别、接地）的基准测试中取得了最先进的性能，即使与更大规模的模型相比也是如此。

Conclusion: IRGPT在9项任务的基准测试中实现了最先进的性能，即使与更大规模的模型相比也是如此。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [11] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 提出了一种名为 GPI-Net 的新网络，利用 Gestalt 原理和混合注意力机制，有效解决了点云配准中融合局部和全局特征的挑战，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 点云配准中的高层特征匹配是准确识别高质量对应关系的关键前提，但由于特征冗余和复杂空间关系，融合局部和全局特征极具挑战性。Gestalt 原理在分析局部和全局关系方面具有优势，因此需要一种能有效融合这些信息的方法。

Method: 提出了一种名为 GPI-Net 的新型 Gestalt 引导并行交互网络，该网络通过正交几何一致性来优化点云配准中的高层特征匹配。该方法利用 Gestalt 原理促进局部和全局信息的互补通信，通过正交集成策略减少冗余信息并生成更紧凑的全局结构。通过 Gestalt 特征注意力（GFA）块和混合注意力机制捕捉几何特征，并通过双路径多粒度并行交互聚合（DMG）块促进不同粒度信息的融合。

Result: 通过大量的实验证明，GPI-Net 在点云配准任务上相比现有方法具有更优越的性能。

Conclusion: 所提出的 GPI-Net 在各种挑战性任务上均表现出优于现有方法的性能。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [12] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本研究提出了一套全面的解决方案，以应对3DGS视频流传输中的切片、质量评估和比特率适应等挑战，并在实验中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 3DGS视频流传输是学术界和工业界的研究热点，但仍处于早期阶段，面临切片、质量评估和比特率适应等挑战。

Method: 提出了一种基于显著性分析的自适应3DGS切片技术，该技术整合了空间和时间特征，为每个切片编码了具有专门的变形场和多质量级别的版本以进行自适应选择。此外，还引入了一个新颖的3DGS视频质量评估框架，该框架联合评估了流传输过程中3DGS表示中的空间域退化以及生成的2D渲染图像的质量。最后，开发了一种专门针对3DGS视频流传输的、基于元学习的自适应比特率算法。

Result: 实验证明，所提出的方法在3DGS视频流传输的切片、质量评估和比特率适应方面显著优于现有技术。

Conclusion: 该研究提出的自适应3DGS切片技术、3DGS视频质量评估框架和基于元学习的自适应比特率算法，在3DGS视频流传输方面显著优于现有方法。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [13] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS是一个创新的混合专家自动驾驶框架，通过场景自适应和智能路由，显著提升了自动驾驶在复杂交通环境中的鲁棒性和适应性，并在基准测试中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的单模态规划方法在学习整体策略时，难以获得处理多样化场景所需的多元化驾驶技能。

Method: 提出了一种名为GEMINUS的混合专家端到端自动驾驶框架，该框架包含一个全局专家、一个场景自适应专家组和一个双感知路由器。全局专家在整个数据集上训练，场景自适应专家在相应的场景子集上训练，双感知路由器同时考虑场景特征和路由不确定性来动态激活专家模块。

Result: GEMINUS在Bench2Drive闭环基准测试中表现优于现有方法，达到了最高的驾驶得分和成功率，并且仅使用了单目视觉输入。消融研究显示，与原始单专家基线相比，GEMINUS在驾驶得分、成功率和多能力平均值方面分别提高了7.67%、22.06%和19.41%。

Conclusion: GEMINUS框架通过结合全局专家、场景自适应专家组和双感知路由器，实现了在多样化场景下的自适应和鲁棒性表现，在Bench2Drive闭环基准测试中超越了现有方法，并在仅使用单目视觉输入的情况下达到了最高的驾驶得分和成功率。消融研究也证明了其相对于原始单专家基线的显著改进。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [14] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard是一种防篡改的可视化图像数据检索框架，能够将元数据链接嵌入图像中，即使在图像被篡改后也能恢复，支持多种应用。


<details>
  <summary>Details</summary>
Motivation: 现有的可视化图像在传播过程中主要以栅格图片形式存在，导致源代、交互特性和元数据等关键信息的丢失。虽然已有方法尝试将元数据嵌入图像以支持可视化图像数据检索（VIDR），但这些方法在面对在线传播中常见的图像篡改（如裁剪和编辑）时，其嵌入的元数据往往容易丢失，缺乏实用性。

Method: 提出了一种名为VisGuard的防篡改可视化图像数据检索（VIDR）框架，该框架通过重复数据平铺、可逆信息广播和基于锚点的裁剪定位等技术，将元数据链接嵌入到可视化图像中，以抵抗常见的图像篡改（如裁剪和编辑），并确保链接的可恢复性。

Result: VisGuard在数据检索准确性、嵌入容量以及抵抗篡改和隐写分析的安全性方面表现优越，能够支持交互式图表重建、篡改检测和版权保护等多种应用。

Conclusion: VisGuard框架能够有效嵌入元数据链接到可视化图像中，即使在图像被裁剪和编辑等常见篡改后，嵌入的数据链接仍然可以恢复，从而实现了可视化图像数据的检索（VIDR）。该框架通过重复数据平铺、可逆信息广播和基于锚点的裁剪定位等技术提高了鲁棒性，能够支持交互式图表重建、篡改检测和版权保护等多种应用。实验证明，VisGuard在数据检索准确性、嵌入容量以及抵抗篡改和隐写分析的安全性方面表现优越，能够有效地促进和保护可视化信息的传播和传递。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [15] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet通过结合空间特征提取和时间差分（DSD模块），并使用四元组损失，实现了优于现有方法的序列级视觉地点识别，尤其在季节和视点变化条件下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的视觉地点识别（VPR）方法主要关注单帧嵌入，忽略了图像序列中的时间相干性，这使得在动态和感知上混淆的环境中进行长期定位成为一个挑战。

Method: 提出了一种名为OptiCorNet的新型序列建模框架，该框架包含一个轻量级的一维卷积编码器和一个可学习的差分时间算子（DSD），用于联合捕获短期空间上下文和长期时间转换。DSD模块通过固定的差分核来模拟序列间的方向差异，并使用基于LSTM的优化和可选的残差投影来生成紧凑、具有区分性的描述符。此外，还采用了四元组损失来优化批次内的正对齐和多负发散。

Result: 在多个公开基准测试中，OptiCorNet在季节和视点变化等挑战性条件下，其性能优于最先进的方法。

Conclusion: OptiCorNet通过将空间特征提取和时间差分结合到可微分的端到端可训练模块中，能够有效地进行序列建模，从而在具有挑战性的季节和视点变化下，实现比现有方法更好的视觉地点识别效果。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [16] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT是一种用于视觉Transformer（ViT）的无数据量化方法。它通过生成难度递增的合成样本和使用激活校正矩阵来提高量化性能，解决了现有方法的不足。实验表明，DFQ-ViT性能优越，无需微调，符合绿色学习原则。


<details>
  <summary>Details</summary>
Motivation: 现有的无数据量化（DFQ）方法在为视觉Transformer（ViT）生成量化模型时，依赖的合成样本未能充分捕捉和平衡全局与局部特征，导致合成数据质量不高。此外，量化模型在推理时，其中间层激活分布与全精度模型存在显著差异。这些问题共同导致了量化模型性能的严重下降。

Method: 提出了一种名为DFQ-ViT的无数据量化方法。该方法包括两个关键部分：1. 合成样本：按照由易到难的顺序生成合成样本，以提高合成数据的质量。2. 激活校正：在量化模型的校准和推理阶段，引入激活校正矩阵，以使量化模型的中间层激活与全精度模型对齐。

Result: DFQ-ViT在大量实验中表现出显著优于现有DFQ方法的性能，并且其性能与使用真实数据量化的模型相当。具体而言，在3位权重量化DeiT-T模型时，DFQ-ViT的性能比最先进的方法高出4.29%。该方法无需微调，降低了计算开销和部署门槛，提高了能效，适用于资源受限的边缘设备。

Conclusion: DFQ-ViT方法通过生成逐步难化的合成样本并引入激活校正矩阵，有效解决了现有无数据量化方法在处理ViT时合成数据质量不高以及量化后模型激活分布不匹配的问题。实验证明，DFQ-ViT在性能上显著优于现有DFQ方法，并且能与使用真实数据量化的模型相媲美，同时无需微调，符合绿色学习理念。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [17] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的检索增强点云补全框架，通过跨模态检索学习相似参考样本的结构先验信息，以解决现有方法在处理不完整或缺乏典型结构特征的点云时的局限性。该框架通过SSFE和PRAG模块，有效提升了点云补全的质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于跨模态学习的方法虽然引入实例图像来辅助学习结构特征，但它们仍然专注于特定的输入类别，限制了其生成能力。为了解决这个问题，本研究旨在提升点云补全任务的性能，尤其是在输入点云缺乏典型结构特征的情况下。

Method: 提出了一种新颖的检索增强点云补全框架，包括结构共享特征编码器（SSFE）和渐进检索增强生成器（PRAG）。SSFE用于联合提取跨模态特征并重建参考特征作为先验，其双通道控制门可以增强相关结构特征并抑制无关信息。PRAG采用分层特征融合机制，将参考先验信息与从全局到局部的输入特征进行整合。

Result: 所提出的方法在生成细粒度点云方面表现出有效性，并且在处理稀疏数据和未见类别方面具有良好的泛化能力。

Conclusion: 该方法在多个数据集和真实场景的广泛评估中，证明了其在生成细粒度点云方面的有效性，以及在处理稀疏数据和未见类别时的泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [18] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA 是一种新的 MLLM 架构，通过令牌压缩技术解决了处理全切片病理图像（WSI）的挑战，实现了高效的视觉问答，并在准确性和资源消耗方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLM 方法在处理长上下文和高计算需求的 WSI 时存在局限性，例如缺乏生成能力或过度消耗资源。

Method: TCP-LLaVA 引入了一组可训练的压缩令牌，通过受 BERT 中 [CLS] 令牌机制启发的模态压缩模块来聚合视觉和文本信息。只有压缩后的令牌被转发到 LLM 以生成答案，从而显著减少了输入长度和计算成本。

Result: TCP-LLaVA 在 VQA 准确性方面优于现有 MLLM 基线，同时将训练资源消耗降低了可观的幅度。

Conclusion: TCP-LLaVA 是首个通过令牌压缩执行 WSI VQA 的 MLLM 架构。实验表明，TCP-LLaVA 在 VQA 准确性方面优于现有的 MLLM 基线，同时大幅降低了训练资源消耗。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [19] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 一种利用事件数据的运动分割和自身运动估计新方法，无需光流，效果好，适用于机器人和导航。


<details>
  <summary>Details</summary>
Motivation: 为了在神经形态视觉传感器领域实现运动分割和自身运动估计，克服传统方法对光流或深度估计的依赖。

Method: 提出了一种基于事件的法向流的鲁棒框架，用于运动分割和自身运动估计。该框架利用事件数据和几何约束，通过迭代事件过分割、残差分析和基于聚类的细化来分离运动物体。

Result: 实验结果表明，该方法能够准确地分割场景，并估计相机的平移运动，尤其在物体边界处表现优越，且无需计算密集的光流。

Conclusion: 该方法在EVIMO2v2数据集上实现了准确的分割和运动估计，无需计算光流。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [20] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 这是一篇关于3D重建和视图合成领域的前馈方法的综述，重点介绍了其在计算机视觉、图形学和沉浸式技术中的应用和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 为了应对传统3D重建和视图合成方法计算密集、迭代优化过程复杂的问题，推动了对快速、可泛化的前馈方法的研究。

Method: 文章采用分类方法，根据底层表示架构（如点云、3D高斯泼散、神经辐射场等）对前馈技术进行分类，并审查了关键任务（如无姿态重建、动态3D重建、3D感知图像和视频合成）及其应用。

Result: 文章全面回顾了前馈技术在3D重建和视图合成中的应用，涵盖了其在数字人、SLAM、机器人等领域的应用，并对常用数据集和评估协议进行了审查。

Conclusion: 文章总结了前馈方法在3D重建和视图合成领域的进展，并讨论了开放的研究挑战和未来的发展方向。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [21] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 提出DCHM框架，通过超像素级高斯泼溅实现多视角深度一致性，用于行人检测，解决了现有方法的噪声和泛化能力问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人类建模方法引入噪声且精度低，依赖昂贵的多视角3D注释会降低泛化能力，因此需要一种无需人类标注即可准确建模人类的方法。

Method: 提出了一种名为深度一致性人类建模（DCHM）的框架，该框架采用超像素级高斯泼溅技术，实现了稀疏视角、大规模和拥挤场景下的多视角深度一致性，以生成精确的点云用于行人本地化。

Result: DCHM方法在人类建模过程中显著降低了噪声，并且在行人本地化方面优于现有方法。

Conclusion: DCHM框架在稀疏视角、大规模和拥挤场景下实现了多视角深度一致性，产生了用于行人本地化的精确点云，在人类建模过程中显著降低了噪声，并且在这样的挑战性环境中重建行人并执行多视角分割是第一个。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [22] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: 提出了一种名为 ArtiMuse 的新型 MLLM 基础 IAA 模型，并发布了 ArtiMuse-10K 数据集，旨在提高图像美学评估的准确性和深度。


<details>
  <summary>Details</summary>
Motivation: 为了满足教育应用、艺术创作和 AIGC 技术领域对全面图像美学评估 (IAA) 的日益增长的需求，特别是需要能够提供量化评分和专业见解的方法。

Method: 提出了一种名为 ArtiMuse 的新型 MLLM 基础 IAA 模型，该模型具有联合评分和专家级理解能力。

Result: 所提出的 ArtiMuse 模型和 ArtiMuse-10K 数据集旨在通过提供联合评分和专家级理解能力来推进 IAA 领域。

Conclusion: ArtiMuse-10K 是首个包含 10,000 张图像的专家策展图像美学数据集，涵盖 5 个主要类别和 15 个子类别，并附有专业评分和 8 维属性分析。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [23] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: A browser extension translates sign language to subtitles during video calls to help deaf-mute people communicate better communicate.


<details>
  <summary>Details</summary>
Motivation: To improve communication accessibility for people with hearing impairments, especially in the context of increased reliance on video meetings.

Method: Using computer vision techniques to recognize sign language, and leveraging a large-scale dataset (over 2000 Word-Level ASL videos from 100+ signers) for model training and translation.

Result: The browser extension will automatically translate sign language into subtitles, facilitating communication between deaf-mute and non-deaf-mute individuals during video calls.

Conclusion: We propose a browser extension that translates sign language to subtitles in real-time during video calls, addressing the communication barrier for individuals with hearing impairments.

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [24] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本研究利用Florence大型多模态模型和域特定数据增强技术，在胃肠内窥镜医学影像问答任务上取得了准确的结果，展示了其在医学VQA领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决ImageCLEFmed MEDVQA 2025挑战中的子任务1，该任务针对胃肠内窥镜的视觉问答。

Method: 本研究采用Florence模型作为VQA管线的主干，并结合了强大的视觉编码器和文本编码器来解析内窥镜图像并生成临床相关答案。为了提高泛化能力，研究应用了域特定的数据增强技术，这些技术在保持医学特征的同时增加了训练多样性。

Result: 在KASVIR数据集上的实验表明，通过微调Florence模型可以根据官方挑战指标生成准确的响应。

Conclusion: 该研究展示了大型多模态模型在医学影像问答方面的潜力，并为未来的可解释性、鲁棒性和临床集成工作提供了坚实的基础。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [25] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 该研究通过分析人脸表情的感知变异性，发现ANN的决策边界与人类感知变异性存在关联。研究者提出了一种新的采样方法和varEmotion数据集，并通过微调ANN模型，使得模型能够更好地匹配人类的感知模式，为个性化情感解读模型提供了新的思路。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决情感认知科学中一个基本挑战，即开发能够准确捕捉外部情感刺激与人类内部体验之间关系的 GAI 模型。虽然ANN在面部表情识别方面表现出色，但其模拟个体感知差异的能力仍有待探索。特别是，研究关注高感知变异性现象，即个体在观看相同刺激时，其情绪分类存在显著差异。

Method: 本研究提出了一种新颖的感知边界采样方法来生成人脸表情刺激，这些刺激位于ANN决策边界上。利用这些刺激构建了varEmotion数据集，并通过大规模人类行为实验进行了分析。此外，研究还通过使用行为数据对ANN表征进行微调，实现了ANN预测与人类感知模式的对齐。

Result: 研究结果表明，ANN难以区分的人脸表情样本同样会引起人类观察者之间不同的感知判断。这些令人困惑ANN的刺激也会激起人类参与者更高的感知不确定性，凸显了情绪感知中共享的计算原理。通过行为数据微调ANN表征后，ANN预测与人群和个体层面的感知模式实现了对齐。

Conclusion: 本研究通过微调神经网络（ANN）表征，实现了ANN预测与人群和个体层面的人类感知模式的对齐。研究结果建立了ANN决策边界与人类感知变异性之间的系统性联系，为情感解读的个性化建模提供了新的见解。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [26] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 本系统通过杂乱区分算法和基于GAN的图像修复技术，帮助摄影用户识别和移除照片中的杂乱，从而在更短时间内创作出更高质量的作品。


<details>
  <summary>Details</summary>
Motivation: 照片中的杂乱会分散观众对摄影师 intended emotions or stories 的注意力。摄影爱好者由于无意识的疏忽或在创建无杂乱、美观的拍摄场景方面的经验不足，经常在照片中包含杂乱。因此，我们致力于开发一个提供杂乱识别和移除解决方案和指导的相机引导系统。

Method: 本系统基于两个技术创新：一个具有对象美学评估的杂乱区分算法，以及一个基于生成对抗网络的迭代图像修复算法，用于重建高分辨率图像中被移除对象的缺失区域。

Result: 系统估计并可视化对象对照片整体美学和内容的贡献，用户可以基于此进行交互式杂乱识别。系统还提供去除杂乱的建议以及计算上去除杂乱对象的工具，以指导用户处理不同种类的杂乱并改进他们的摄影作品。

Conclusion: 用户研究表明，该系统提供了灵活的界面和准确的算法，使用户能够更好地识别干扰，并在更短的时间内拍摄出更高质量的图像。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [27] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D is a framework that explicitly encodes relationships between objects using natural language, improving 3D scene understanding for tasks like grounding, captioning, and question answering.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene-language models struggle with relational understanding, particularly when visual embeddings alone do not adequately convey the roles and interactions of objects.

Method: Descrip3D enhances each object with a textual description that captures both its intrinsic attributes and contextual relationships. These relational cues are incorporated into the model through a dual-level integration: embedding fusion and prompt-level injection.

Result: Descrip3D consistently outperforms strong baseline models on five benchmark datasets, including ScanRefer, Multi3DRefer, ScanQA, SQA3D, and Scan2Cap.

Conclusion: Descrip3D consistently outperforms strong baseline models on five benchmark datasets, demonstrating the effectiveness of language-guided relational representation for understanding complex indoor scenes.

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [28] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD 是一种新的方法，通过分析模型的输出（logits）来预测模型在微调过程中的表现，从而帮助我们选择最合适的预训练模型。它使用一个数学模型（常微分方程）来模拟这个过程，并且效果很好。


<details>
  <summary>Details</summary>
Motivation: 在预训练模型广泛应用的背景下，如何高效地为下游任务选择最合适的预训练模型是一个重大挑战。现有方法在模拟微调动态时，通常在特征空间中使用线性变换，这与微调目标不完全一致，也无法捕捉优化过程中的非线性特征。

Method: LEAD 提出了一种基于网络输出的logits的微调对齐方法。该方法构建了一个理论框架来模拟优化过程，并推导了一个常微分方程（ODE）来描述向最终logits状态的非线性演变。此外，还设计了一种类别感知分解方法来考虑跨类别的不同演变动态。

Result: 实验结果表明，LEAD 在 24 个监督和自监督预训练模型以及 10 个下游数据集上展现出优越的性能，证明了其广泛的适应性，即使在数据稀疏的情况下也表现良好。

Conclusion: LEAD 通过分析预训练模型的微调动态，提供了一种在微调前选择最合适模型的有效方法，并在多项下游任务和数据集上取得了优异性能，尤其在数据量少的情况下仍表现出色。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [29] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: Pix2Pix在MRI图像转换方面优于其他模型，但可能需要更多数据。


<details>
  <summary>Details</summary>
Motivation: 为了减少MRI扫描时间并降低成本，同时保持诊断质量，研究人员致力于开发计算方法，从已获取的MRI对比度合成缺失的对比度。

Method: 对生成对抗网络（GANs）、扩散模型和流匹配（FM）技术进行了基准测试，用于T1w到T2w的2D MRI图像转换。所有框架都采用可比的设置实现，并在三个公开的健康成人MRI数据集上进行了评估。

Result: 在结构保真度、图像质量和计算效率方面，基于GAN的Pix2Pix模型优于基于扩散和FM的方法。

Conclusion: GAN-based Pix2Pix模型在T1w到T2w的2D MRI图像转换任务中，在结构保真度、图像质量和计算效率方面优于扩散模型和流匹配方法。结果表明，流模型在小型数据集和简单任务上容易过拟合，可能需要更多数据才能达到或超过GAN的性能。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [30] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本研究评估了TensorFlow、PyTorch和JAX在血细胞图像分类中的性能，发现JAX和PyTorch的准确性与现有技术相当，并突出了它们在医学图像分析中的效率。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在血细胞图像分析方面显示出巨大潜力，但缺乏对特定深度学习框架性能的详细分析。

Method: 本研究比较了TensorFlow (Keras)、PyTorch和JAX这三个流行的深度学习框架在血细胞图像分类任务上的性能，主要关注推理时间差异以及不同图像尺寸下的分类性能。

Result: 研究结果表明，不同框架的性能存在差异，这受到图像分辨率和框架特定优化等因素的影响。

Conclusion: JAX和PyTorch在血细胞图像分类任务上达到了与当前基准相当的准确率，证明了这些框架在医学图像分类中的效率。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [31] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是第一个解决3D开放词汇子概念发现问题的方法，它结合了无监督分割和弱开放词汇引导，适用于机器人和自动驾驶等领域。


<details>
  <summary>Details</summary>
Motivation: 传统方法要么专门针对特定任务（开放词汇分割），要么针对场景内容（无监督语义分割）。DiSCO-3D旨在解决更广泛的3D开放词汇子概念发现问题，以适应场景和用户查询。

Method: DiSCO-3D基于神经场表示，结合了无监督分割和弱开放词汇引导。

Result: DiSCO-3D在3D开放词汇子概念发现方面实现了有效的性能。

Conclusion: DiSCO-3D在3D开放词汇子概念发现方面表现出有效性能，并在开放词汇和无监督分割的边缘情况下取得了最先进的结果。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [32] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph框架利用图神经网络结合视觉Transformer，通过建模面部属性的结构关系来提升人脸表情识别的准确性，并在多个数据集上验证了其有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别对于人机交互应用至关重要，由于面部属性结构随表情变化，将结构信息整合到面部属性中对表情识别至关重要。

Method: 提出了一种名为Exp-Graph的新框架，利用基于图的建模来表示面部属性之间的结构关系。具体地，使用面部标志作为图的顶点，通过面部标志的邻近性和基于视觉Transformer编码的面部属性的局部外观相似性来确定边。此外，利用图卷积网络捕获和整合这些结构依赖性到面部属性编码中，以提高表情识别的准确性。

Result: Exp-Graph模型在Oulu-CASIA、eNTERFACE05和AFEW数据集上分别取得了98.09%、79.01%和56.39%的识别准确率，证明了其在不同环境下的泛化能力和有效性。

Conclusion: Exp-Graph在Oulu-CASIA、eNTERFACE05和AFEW三个基准数据集上进行了全面评估，分别实现了98.09%、79.01%和56.39%的识别准确率，表明该模型在受控实验室环境和真实、无约束环境中都具有很强的泛化能力，有效提升了人脸表情识别的准确性。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [33] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: 提出DD-SAM2，一个高效的SAM2适应框架，用于医疗视频分割和跟踪。通过DD-Adapter增强特征提取并利用流式内存，在数据有限的情况下也能实现高精度。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习医学图像分割方法通常针对特定模态且难以适应动态的医疗成像场景。虽然SAM2及其变体提供了基于提示的通用解决方案，但将其应用于医疗视频场景需要大规模数据集进行重新训练或迁移学习，这会导致高昂的计算成本和灾难性遗忘的风险。本研究旨在解决这些挑战，提出一种高效的适应框架。

Method: 提出了一种名为DD-SAM2的SAM2高效适应框架，该框架包含一个深度可分离适配器（DD-Adapter），用于增强多尺度特征提取，同时尽量减少参数开销。该设计能够有效地在数据量有限的医疗视频上对SAM2进行微调，并利用SAM2的流式内存进行医疗视频对象跟踪和分割。

Result: 在TrackRad2025（肿瘤分割）和EchoNet-Dynamic（左心室跟踪）数据集上的综合评估表明，DD-SAM2取得了优越的性能，Dice得分分别为0.93和0.97。

Conclusion: DD-SAM2通过引入深度可分离适配器（DD-Adapter）并充分利用SAM2的流式内存，实现了对SAM2在医疗视频分割和跟踪任务上的高效适应，即使在训练数据有限的情况下也能取得优越性能，Dice得分分别为0.93和0.97。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [34] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++ 是一种用于检测和解释跨模态合成媒体的新框架，通过强化学习得到改进，并在新的 GenBuster++ 基准测试上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 现有的虚假内容检测方法在处理结合了多种媒体格式的合成内容时效果不佳，因为它们的设计主要基于单一模态。为了应对这一挑战，需要一种能够跨模态地检测和解释合成媒体的新方法。

Method: 本研究提出了一种名为 BusterX++ 的新颖框架，用于跨模态检测和解释合成媒体。该方法采用先进的强化学习（RL）的训练后策略来消除冷启动问题，并通过多阶段训练、思维奖励和混合推理实现性能的稳定提升。此外，研究人员还提出了 GenBuster++ 基准测试，包含 4000 个由人类专家精心挑选的图像和视频片段，以评估此类检测系统的性能。

Result: BusterX++ 在跨模态检测任务上取得了显著的性能提升，并且实验证明了其有效性和泛化能力。GenBuster++ 基准测试也为评估跨模态虚假内容检测系统提供了一个全面的评估平台。

Conclusion: BusterX++ 在跨模态虚假内容检测方面表现出色，并且 GenBuster++ 基准测试为评估此类系统提供了全面的平台。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [35] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion通過基於狀態空間模型的雙路參數交互機制，解決了多光譜目標檢測中的泛化和效率瓶頸，在多項任務中取得了最先進的性能。


<details>
  <summary>Details</summary>
Motivation: 解決了現代多光譜特徵融合在目標檢測中的兩個關鍵限制：1）過度偏好局部互補特徵而非跨模態共享語義，影響了泛化性能；2）感受野大小和計算複雜度之間的權衡，對可擴展的特徵建模構成了關鍵瓶頸。

Method: 提出了一種新穎的多光譜狀態空間特徵融合框架（MS2Fusion），該框架基於狀態空間模型（SSM），通過雙路參數交互機制實現高效融合。第一條跨參數交互分支繼承了跨注意力的優勢，通過SSM中的跨模態隱態解碼來挖掘互補信息。第二條共享參數分支通過參數共享在SSM中進行聯合嵌入，以獲得跨模態相似的語義特徵和結構，探索跨模態對齊。這兩個分支通過SSM在統一框架中進行聯合優化，以融合多光譜特徵。

Result: MS2Fusion在主流基準測試中顯著優於其他最先進的多光譜目標檢測方法，並在RGB-T語義分割和RGBT顯著目標檢測任務上取得了最先進的結果。

Conclusion: MS2Fusion在FLIR、M3FD和LLVIP等主流基準測試中顯著優於其他最先進的多光譜目標檢測方法，證明了其優越性。此外，MS2Fusion具有通用性，適用於其他多光譜感知任務，在RGB-T語義分割和RGBT顯著目標檢測上取得了最先進的結果。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [36] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai是一个利用AI提升体育裁判公正性和效率的框架，通过计算机视觉和深度学习技术，能够实时准确地识别和评分运动员动作，并可扩展至多种体育项目。


<details>
  <summary>Details</summary>
Motivation: 传统体育裁判系统存在延迟、主观性和不一致性等问题，影响了公平性和运动员的信任。AI技术，特别是FST.ai框架，旨在解决这些问题，提高判罚的准确性和效率。

Method: 利用计算机视觉、深度学习和边缘推理技术，通过姿态估计、动作分类和冲击分析来实现对跆拳道头部踢击的实时检测和评分。

Result: FST.ai系统将决策时间从几分钟缩短到几秒钟，提高了裁判判罚的一致性和透明度。该框架具有可扩展性和运动无关性，可应用于多种体育项目。

Conclusion: FST.ai框架的提出，通过结合计算机视觉、深度学习和边缘推理等技术，实现了运动项目（以跆拳道为例）中关键动作的自动识别和分类，显著提高了裁判决策的实时性、一致性和透明度，并展示了其跨运动领域的应用潜力。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [37] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 本研究提出了一种创新的计算机视觉框架，使用语义分割来估算餐盘食物浪费，并在模拟环境中实现了高精度和实时吞吐量。虽然在处理复杂菜肴时存在一些挑战，但该框架为减少食物浪费提供了可扩展且非接触式的解决方案，并为未来的研究和实际应用开辟了道路。


<details>
  <summary>Details</summary>
Motivation: 量化机构餐饮场所的餐后食物浪费对于支持数据驱动的可持续发展战略至关重要。

Method: 本研究提出了一种利用RGB图像的语义分割来估算餐盘食物浪费的成本效益计算机视觉框架，通过对五种伊朗菜肴在用餐前后进行成像。对四种全监督模型（U-Net、U-Net++及其轻量级变体）进行了训练，并使用了带上限的动态逆频率损失和AdamW优化器。

Result: 所有模型都达到了满意的性能，并且对于每种食物类型，至少有一个模型接近或超过了90%的DPA，这表明像素比例估计具有很强的一致性。较轻的模型和参数量较少的模型提供了更快的推理速度，在NVIDIA T4 GPU上实现了实时吞吐量。进一步的分析表明，对于干燥、更坚硬的食物（例如米饭和炸薯条），分割性能更优，而像炖菜这样的更复杂、碎片化或粘稠的菜肴，其分割性能较差，尤其是在餐后。

Conclusion: 该研究提出的计算机视觉框架是首创性的，代表了一种可扩展的、非接触式的解决方案，用于持续监测食物消耗。这项研究为大型餐饮服务环境中自动、实时的废物跟踪系统奠定了基础，并为旨在减少机构食物浪费的餐厅管理和政策制定者提供了可行的见解和可行的未来方向。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [38] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML通過雙路多級判別來增強組織病理學圖像和基因表達譜之間的對齊，從而提高基因表達預測的準確性和泛化能力，並在實驗中取得了最先進的性能。


<details>
  <summary>Details</summary>
Motivation: 現有方法往往未能充分利用組織病理學圖像和基因表達譜之間的多種表示級別的跨模態表示對齊，從而限制了預測性能。

Method: Gene-DML是一個統一的框架，通過雙路多級判別來構建潛在空間，以增強形態學和轉錄組學之間的對應關係。多尺度實例級別判別路徑將在局部、鄰近和全局級別提取的分層組織病理學表示與基因表達譜對齊，捕捉 ધ્યા量尺度的形態學-轉錄組學關係。同時，跨級別實例組判別路徑對個體（圖像/基因）實例和跨模態（基因/圖像）組之間的一致性結構進行強制，從而加強跨模態的對齊。

Result: 通過聯合建模細粒度和結構級別的判別，Gene-DML能夠學習魯棒的跨模態表示，從而提高預測準確性和在不同生物學背景下的泛化能力。

Conclusion: Gene-DML在公开的空間轉錄組數據集上進行了廣泛的實驗，在基因表達預測方面取得了最先進的性能。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [39] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 为了解决多模态大语言模型在理解多页文档方面的不足，研究人员提出了Doc-750K数据集和Docopilot模型。Docopilot无需检索增强生成即可处理文档依赖，并在文档理解任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决了当前多模态大语言模型（MLLMs）在复杂、多页文档理解方面能力不足的问题，以及现有检索增强生成（RAG）方法存在的碎片化检索上下文、多阶段错误累积和额外检索时间成本等问题。

Method: 提出了一种名为Docopilot的原生多模态模型，该模型无需依赖检索增强生成（RAG）即可准确处理文档级依赖关系。

Result: 构建了一个名为Doc-750K的高质量文档级数据集，该数据集包含多样化的文档结构、广泛的跨页依赖关系以及源自原始文档的真实问答对。实验证明Docopilot的优越性能。

Conclusion: Docopilot在文档理解任务和多轮交互中实现了卓越的连贯性、准确性和效率，为文档级多模态理解设定了新的基准。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [40] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents 是一个创新的协作多 Agent 系统，通过专业 Agent、智能任务分配和严格验证，提高了 WSI 分析的准确性和多任务处理能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前多模态大语言模型（MLLMs）在 WSI 分析中任务通用性与准确性难以平衡的问题，并探索协作多 Agent 系统在病理学领域的潜力。

Method: 提出 WSI-Agents，一个集成了专业功能 Agent、任务分配和验证机制的新型协作多 Agent 系统，包含任务分配模块、验证机制和总结模块。

Result: 在多模态 WSI 基准测试中的广泛实验表明，WSI-Agents 在多样化任务中优于现有的 WSI MLLMs 和医学 Agent 框架。

Conclusion: WSI-Agents 在多模态 WSI 分析任务中展现出优于当前 WSI MLLMs 和医学 Agent 框架的性能。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [41] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: 本研究提出了一种多模态交互提示蒸馏（MIPD）框架，用于开放词汇地面情境识别（Ov-GSR）。该框架将知识从大型 MLLM 教师模型转移到小型 GSR 学生模型，以提高其对未见和罕见情境的识别能力。实验结果表明，MIPD 在 Ov-SWiG 和 HICO-DET 数据集上均取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有 MLLMs 在复杂地面情境识别（GSR）方面能力不足且资源消耗大的问题，以及传统 GSR 模型泛化能力差、难以识别新颖或罕见情境的缺点。研究旨在通过知识蒸馏，提升小型 GSR 模型识别新颖和罕见情境的能力。

Method: 提出了一种名为多模态交互提示蒸馏（MIPD）的新框架，该框架利用 LLM 驱动的判断性推理生成器（JRG）来构建包含上下文语义信息的积极和消极“瞥见”和“注视”推理。然后，通过负引导多模态提示对齐（NMPA）模块，将场景感知和实例感知提示与 MLLM 教师的视觉信息对齐，以捕获整体和感知的多模态知识。最后，将对齐后的多模态知识蒸馏到学生 Ov-GSR 模型中。

Result: 在改进的 Ov-SWiG 数据集上实现了优越的性能，能够更好地识别已见、罕见和未见的情境。此外，在 HICO-DET 数据集上，所提出的方法也展示了在未见目标检测方面的改进。

Conclusion: 该研究提出的 MIPD 框架通过知识蒸馏显著提高了 Ov-GSR 模型的泛化能力和零样本能力，使其在已见、罕见和未见情境下均表现出色，并在 HICO-DET 数据集上展示了改进的未见目标检测能力。

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [42] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: 提出GTPBD数据集，这是首个包含超过20万个复杂梯田地块的精细化数据集，解决了现有研究对梯田地形表示不足的问题，并提供了多项任务的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有农业地块提取研究主要关注中等分辨率或规则平原农田，缺乏对复杂梯田地形的表示，无法满足精准农业的需求。

Method: 提出了一种名为GTPBD（全球梯田地块和边界数据集）的数据集，包含超过20万个手工标注的复杂梯田地块，并对八种语义分割方法、四种边缘提取方法、三种地块提取方法和五种无监督域自适应方法进行了基准测试。

Result: GTPBD数据集包含了47,537张高分辨率图像，具有像素级边界标签、掩码标签和地块标签三个层级的标签，涵盖了中国七个主要地理区域和全球范围内的跨大陆气候区，适用于语义分割、边缘检测、梯田地块提取和无监督域自适应等任务。

Conclusion: GTPBD填补了梯田遥感研究的关键空白，为精细化农业地形分析和跨场景知识转移提供了基础性支撑。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [43] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet通过整合眼底图像、社会经济和合并症数据，并结合对比学习的推迟系统，提高了糖尿病视网膜病变分期的准确性，尤其有助于改善服务不足人群的早期检测和医疗公平性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是可预防性失明的主要原因，尤其是在低收入社区，由于筛查机会有限，患者更容易在诊断前就进展到晚期阶段。合并症也会加速疾病进展。

Method: 提出了一种名为MultiRetNet的新型流程，结合了视网膜成像、社会经济因素和合并症信息。实验了三种多模态融合方法，确定通过全连接层进行融合是最通用的。通过合成对抗性的、低质量的图像并利用对比学习来训练推迟系统，以识别需要临床医生审查的异常样本。

Result: 通过全连接层融合的多模态方法在次优图像上保持了诊断准确性，该系统能够识别需要临床医生审查的异常样本，有望改善早期检测，尤其是在服务不足的人群中。

Conclusion: 该方法旨在通过整合视网膜成像、社会经济因素和合并症信息，提高糖尿病视网膜病变（DR）分期的准确性，并结合临床推迟系统以实现临床人工干预。实验表明，通过全连接层进行融合是最通用的方法。通过合成对抗性的低质量图像并利用对比学习来训练推迟系统，可以识别需要临床医生审查的异常样本。该系统在次优图像上保持了诊断准确性，并整合了关键健康数据，有望改善早期检测，特别是在服务不足的人群中，从而降低医疗成本，提高早期检测率，并解决医疗保健公平性问题。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [44] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 一种新颖的机器人动作预测方法，使用InstructPix2Pix模型，仅需单个图像和文本提示即可预测未来100帧，实现了高效、轻量级和多模态的预测。


<details>
  <summary>Details</summary>
Motivation: 为了实现机器人、自动驾驶系统和人类活动预测等领域更安全、更智能的决策，预测未来运动轨迹至关重要。

Method: 提出了一种新颖、高效、轻量级的机器人动作预测方法，该方法将InstructPix2Pix模型改编用于预测未来视觉帧，并接受视觉和文本输入以实现多模态未来帧预测。

Result: 与需要多个输入帧、计算量大和推理延迟高的传统视频预测模型相比，该方法仅需要单个图像和文本提示作为输入，实现了更快的推理速度、更低的GPU需求和灵活的多模态控制，在机器人和体育运动轨迹分析等领域具有优势。

Conclusion: 该方法在RoboTWin数据集上实现了优于现有技术的SSIM和PSNR，为机器人动作预测任务提供了更轻量级、更高效的解决方案。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [45] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 本研究提出了InterAct VideoQA数据集，用于改进智能交通系统中的视频问答模型。该数据集包含真实交通视频和大量的问答对，能够更好地评估和训练模型处理复杂的交通场景和时空依赖性。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA模型在处理真实交通场景的复杂性方面存在挑战，这些场景涉及多并发的时空事件。需要一个专门的数据集来评估和改进VideoQA模型在交通监控任务中的性能。

Method: 提出InterAct VideoQA数据集，包含8小时真实交通录像，划分为10秒片段，并提供超过25,000个关于时空动态、车辆交互、事件检测等交通属性的问答对。对现有先进VideoQA模型在该数据集上的表现进行了评估，并进行了微调以验证数据集的有效性。

Result: 在InterAct VideoQA数据集上评估的先进VideoQA模型暴露了在复杂交通场景中进行细粒度时空依赖性推理的挑战。通过在该数据集上进行微调，模型性能得到了显著提升，证明了领域特定数据集的必要性。

Conclusion: InterAct VideoQA数据集的出现及其在现有VideoQA模型上的评估和微调，显著提升了模型在处理复杂交通场景方面的能力，证明了领域特定数据集对于VideoQA模型发展的重要性，并为未来在智能交通系统（ITS）中部署实际的VideoQA模型铺平了道路。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [46] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA通过LLM细化查询，再用时间基础模型和MLLM处理视频和问题，在视频问答任务上取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答（VideoQA）方法在处理长视频和回答复杂问题时存在不足。具体而言，任务无关采样方法会不区分地处理所有帧，导致关键事件被无关内容淹没；而启发式检索方法只能捕捉表面模式，无法理解复杂的因果时间结构。

Method: LeAdQA通过结合因果感知查询细化和细粒度视觉基础来解决现有视频问答方法的局限性。首先利用大型语言模型（LLM）重塑问题-选项对，以消除因果歧义并聚焦时间。然后，利用细化后的查询指导时间基础模型检索关键片段，并通过自适应融合机制整合证据。最后，利用多模态大型语言模型（MLLM）处理视觉-文本线索以生成准确答案。

Result: 实验结果表明，LeAdQA通过精确的视觉基础，显著提高了对视频-问题关系的理解，在复杂推理任务上达到了最先进水平，并保持了计算效率。

Conclusion: LeAdQA在NExT-QA、IntentQA和NExT-GQA数据集上实现了最先进（SOTA）的性能，尤其在复杂推理任务上，显著提高了视频-问题关系的理解能力，同时保持了计算效率。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [47] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI是一个创新的框架，通过利用CLIP嵌入和自适应处理来解决水下图像增强中的数据集偏差和高计算成本问题，同时提高公平性和可解释性，为海洋保护提供支持。


<details>
  <summary>Details</summary>
Motivation: 水下图像增强对于海洋保护，特别是珊瑚礁监测至关重要。然而，当前基于人工智能的增强模型常常面临数据集偏差、高计算成本和缺乏透明度等问题，这可能导致错误的解释。

Method: EBA-AI是一个由伦理引导的、有偏差意识的人工智能框架，它利用CLIP嵌入来检测和减轻数据集偏差，并集成自适应处理以优化能源效率，从而显著降低GPU使用率。

Result: 实验表明，EBA-AI在LSUI400、Oceanex和UIEB100数据集上的PSNR降低了1.0 dB，但计算节省实现了大规模海洋监测的实时可行性。不确定性估计和可解释性技术增强了对人工智能驱动的环境决策的信任。

Conclusion: EBA-AI框架通过利用CLIP嵌入来检测和减轻数据集偏差，并集成自适应处理以优化能源效率，显著降低了GPU使用率，同时保持了具有竞争力的增强质量。实验表明，EBA-AI在效率、公平性和可解释性方面取得了良好的平衡，为可持续、有偏差意识和计算效率高的海洋保护工作做出了贡献。

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [48] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS是首个用于冻结ViTs的多光谱成像（HSI）空谱可解释性框架。它通过类别特定的光谱提示和[SINK]标记，解决了现有方法在捕捉光谱线索和计算效率上的问题，实现了稳定、准确且高效的可解释性，显著提升了IoU并减少了注意力崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有的显著性方法在多光谱成像（HSI）的Vision Transformers（ViTs）解释中存在两个主要挑战：1. 难以捕捉有意义的光谱线索，注意力常塌陷于类别标记。 2. 全光谱ViTs在解释性方面计算成本过高，因为HSI数据具有高维度特性。因此，需要一个能够实现可靠且高效的空谱可解释性的框架。

Method: FOCUS框架引入了两个核心组件：1. 类别特定的光谱提示：引导注意力机制关注有意义的波长组。 2. 可学习的[SINK]标记：通过吸引力损失进行训练，以吸收噪声或冗余的注意力。这两个组件协同工作，使得在单次前向传播中即可生成稳定的可解释三维显著性图和光谱重要性曲线，无需梯度反传或主干修改。

Result: FOCUS框架将波段级IoU提高了15％，将注意力崩溃减少了40％以上，并且生成的光谱显著性结果与专家标注高度一致。该方法参数开销小于1％，实现了高分辨率ViT的可解释性，使其在实际多光谱应用中可行。

Conclusion: FOCUS框架实现了对冻结的Vision Transformers（ViTs）在多光谱成像（HSI）任务中的可靠且高效的空谱可解释性。该框架通过类别特定的光谱提示引导注意力机制关注有意义的波长组，并引入一个可学习的[SINK]标记来吸收噪声注意力，从而在不进行梯度反传或修改主干网络的情况下，生成稳定且可解释的三维显著性图和光谱重要性曲线。 FOCUS在波段级IoU方面提高了15％，将注意力崩溃减少了40％以上，并且其显著性结果与专家标注高度一致。该方法参数开销不到1％，使得高分辨率ViTs的可解释性在实际多光谱应用中变得可行，弥合了黑箱模型和可信赖的多光谱决策之间的长期差距。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [49] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: 提出LASED数据集和可控CNN以解决航空视觉地点识别中的数据和旋转挑战，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决航空视觉地点识别（vPR）面临的挑战，包括大规模、高海拔数据集的可用性有限以及UAV图像固有的旋转模糊性。

Method: 提出了一种名为LASED的大规模航空数据集，包含约一百万张图像，涵盖爱沙尼亚170,000个独特地点，并结合了可控卷积神经网络（CNN）来处理旋转不变性。

Result: 在LASED上训练的模型比在较小、多样性较低的数据集上训练的模型具有更高的召回率。可控CNN在处理旋转模糊性方面优于传统CNN，平均召回率提高了12%。

Conclusion: 通过结合结构化、大规模数据集和旋转等变神经网络，该方法显著提高了航空视觉地点识别（vPR）的模型鲁棒性和泛化能力。

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [50] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: HPD是一种新的下采样方法，通过MinMaxPooling保留更多空间信息，提高了语义分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的下采样方法（如最大池化和跨行卷积）在特征聚合、感受野扩展和计算约简方面表现良好，但在语义分割任务中可能导致关键空间信息的丢失，影响像素级预测精度。

Method: 提出了一种基于信息互补的下采样方法——混合池化下采样（HPD），采用MinMaxPooling替代传统方法，通过提取局部区域的最大值信息来保留图像的明暗对比和细节特征。

Result: 在ACDC和Synapse数据集上的各种CNN架构进行的实验表明，HPD在分割性能上优于传统方法，平均DSC系数提高了0.5%。

Conclusion: HPD提供了一种用于语义分割的高效解决方案，在分割性能上优于传统方法，平均DSC系数提高了0.5%。

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [51] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: EPD是一种新的ODE求解器，通过并行梯度评估加速扩散模型采样，同时保持高质量图像。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型（DMs）在采样速度慢（高采样延迟）的问题，同时避免现有加速方法在低延迟下的图像质量下降。

Method: EPD（Ensemble Parallel Direction）求解器，通过在每个ODE步中结合多个并行梯度评估来减少截断误差。该方法通过蒸馏的方式优化一小组可学习参数，确保了最小的训练开销。

Result: 在CIFAR-10、FFHQ、ImageNet和LSUN Bedroom等多个图像合成基准测试中，EPD在相同的5 NFE延迟下，实现了卓越的图像质量（FID分别为4.47、7.97、8.17和8.26），显著优于现有的学习方法。

Conclusion: 所提出的EPD（Ensemble Parallel Direction）求解器通过引入并行梯度评估来减少截断误差，能够在低延迟的情况下生成高质量图像，并且可以作为现有ODE采样器的插件进行改进。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [52] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 一种新的LiDAR-视觉里程计框架，通过深度补全和多尺度特征提取，在KITTI Odometry数据集上实现了最先进的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了实现自主系统的自我定位和导航，里程计是一项关键任务。

Method: 提出了一种新的LiDAR-视觉里程计框架，该框架集成了LiDAR点云和图像，用于精确和鲁棒的姿态估计。该方法利用从点云和图像通过深度补全估计的密集深度图，并结合了具有注意力机制的多尺度特征提取网络，实现了自适应的深度感知表示。此外，利用密集深度信息优化了光流估计，并减少了遮挡区域的误差。其分层姿态优化模块逐步优化运动估计，确保在动态环境和尺度模糊下的鲁棒预测。

Result: 在KITTI Odometry数据集上的综合实验表明，我们的方法实现了与最先进的视觉和LiDAR里程计方法相当或更优的准确性和鲁棒性。

Conclusion: 该方法在KITTI Odometry数据集上实现了与最先进的视觉和LiDAR里程计方法相当或更优的准确性和鲁棒性。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [53] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: 评估了 DUSt3R/MASt3R/VGGT 等先进的 3D 重建模型在航空影像上的表现，发现在稀疏、低分辨率场景下效果显著，但不能完全替代传统方法。


<details>
  <summary>Details</summary>
Motivation: 评估 DUSt3R/MASt3R/VGGT 模型在航空影像上的表现，探索它们在处理极低影像重叠、立体遮挡和纹理缺失区域的潜力，以及在冗余影像集上加速 3D 重建的能力。

Method: 对预训练的 DUSt3R/MASt3R/VGGT 模型在 UseGeo 数据集的航空影像块上进行了全面的评估，用于姿态估计和密集 3D 重建。

Result: 在非常稀疏的影像集（少于 10 张影像，分辨率高达 518 像素）上实现了准确的密集点云重建，完整性比 COLMAP 提高了 50%。VGGT 在计算效率、可扩展性和相机姿态估计的可靠性方面表现更优。然而，所有方法在高分辨率影像和大影像集上均存在局限性，姿态可靠性随影像数量和几何复杂度的增加而下降。

Conclusion:  transformer-based 方法在处理低分辨率、稀疏的航空影像方面具有潜力，可作为传统 SfM 和 MVS 方法的补充，但不能完全替代它们。

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [54] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0 是一个利用大规模人类视频训练的灵巧视觉-语言-动作（VLA）模型。它通过物理指令调整范式，结合了大规模预训练、物理空间对齐和机器人任务适应，并使用零件级别的运动标记化方法实现了高精度的手部运动重建。该模型在手部运动生成和指令遵循方面表现出色，并在真实机器人操作中取得了预期效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视觉语言动作模型在复杂操作任务中依赖合成数据导致的 sim-to-real 差距以及远程操作演示数据规模和多样性不足的问题，研究者提出利用人类手部作为基础操纵器，以利用网络数据中丰富的手部灵巧性和可扩展性。

Method: 提出了一种名为“物理指令调整”的新颖训练范式，该范式结合了来自人类视频的大规模视觉语言动作（VLA）预训练、用于 3D 推理的物理空间对齐以及用于机器人任务的训练后适应。此外，还引入了一种零件级别的运动标记化方法，实现了毫米级的重建精度，以对精确的手部轨迹进行建模以用于动作学习。

Result: Being-H0 在手部运动生成和指令遵循方面表现出色，并且可以很好地随着模型和数据规模的扩展而扩展。在应用物理指令调整后，Being-H0 在现实世界的机器人操作中也显示出预期的收益。

Conclusion: Being-H0 在手部运动生成和指令遵循方面表现出色，并且可以很好地随着模型和数据规模的扩展而扩展。在应用物理指令调整后，Being-H0 在现实世界的机器人操作中也显示出预期的收益。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [55] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 提出了一种名为VPIP的视觉任务提示框架，并开发了一个名为GenLV的统一低级视觉模型。该模型在100多个低级视觉任务的基准测试中表现出色，并能有效处理零样本泛化和少样本迁移等任务。


<details>
  <summary>Details</summary>
Motivation: 为了解决跨越图像修复、增强、风格化和特征提取等多样化任务的统一建模挑战。

Method: 提出了一种视觉任务提示（VPIP）框架，该框架利用输入-目标图像对作为视觉提示来指导模型执行各种低级视觉任务。该框架包括一个端到端的图像处理骨干网、一个提示编码器和一个提示交互模块。

Result: GenLV模型在多个代表性任务上的表现以及在零样本泛化、少样本迁移和任务特定微调场景下的评估结果均表明，增加训练任务的数量可以提高泛化能力，尤其是在数据有限的任务上，模型能够通过联合训练学习可转移的表示。

Conclusion: 所提出的方法在广泛的任务中取得了可观的性能，并且在零样本泛化、少样本迁移和特定任务微调等场景下表现出很强的适应性，证实了该框架作为通用低级视觉建模的统一基础的有效性、可扩展性和潜力。

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [56] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 本研究受人类认知启发，提出了一种名为 HICOM 的新框架，用于检测多脸深度伪造视频。该框架通过识别场景运动一致性、人脸外观兼容性、人际注视对齐和人脸-身体一致性等线索，有效提高了检测准确率和泛化能力，并通过 LLM 增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法在处理多脸场景时存在不足，因为它们缺乏对关键上下文线索的识别能力。本研究旨在利用人类认知来应对这一挑战。

Method: 通过人类研究确定了四种关键线索：场景-运动一致性、人脸外观兼容性、人际注视对齐和人脸-身体一致性。基于这些线索设计了 HICOM 框架，并利用 LLM 提供可解释性。

Result: HICOM 在数据集内检测的平均准确率提高了 3.3%，在真实世界扰动下的准确率提高了 2.8%，在未见数据集上的表现优于现有方法 5.8%，证明了人类启发线索的泛化能力。

Conclusion: 该研究提出了一种受人类认知启发的框架 HICOM，用于检测多脸深度伪造视频，并在实验中证明了其有效性和泛化能力，同时通过集成 LLM 提高了可解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [57] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant通过结合段感知量化和双尺度量化，解决了扩散模型量化的挑战，提高了通用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的PTQ方法依赖于特定架构的启发式方法，泛化性有限，并且难以与工业部署流程集成。SegQuant旨在解决这些限制，提高跨模型的通用性。

Method: SegQuant包含一个段感知、基于图的量化策略（SegLinear）和一个双尺度量化方案（DualScale）。SegLinear捕获结构语义和空间异质性，而DualScale保留极性不对称激活。

Result: SegQuant在保持视觉保真度的同时，在扩散模型的量化方面取得了强大的性能。

Conclusion: SegQuant是一个统一的量化框架，它结合了互补技术，具有广泛的适用性，可以超越基于Transformer的扩散模型，同时保持与主流部署工具的兼容性。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [58] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: 金融图表理解是LVLM的一个新兴且具有挑战性的领域。FinChart-Bench基准和对25个LVLM的评估揭示了当前模型在理解复杂金融图表方面存在的关键挑战，特别是在指令遵循和空间推理方面。


<details>
  <summary>Details</summary>
Motivation: 金融图表因其复杂的时间结构和领域特定术语而未被充分探索，需要一个专门的基准来评估LVLM在该领域的表现。

Method: 提出了FinChart-Bench，一个包含1200张金融图表和7016个问题的基准数据集，并在此基准上对25个最先进的LVLM进行了全面评估。

Result: 评估结果显示，开源模型和闭源模型之间的性能差距正在缩小，但升级后的模型有时会出现性能下降。大多数模型在指令遵循方面存在困难，并且在空间推理能力方面表现出显著的局限性。

Conclusion: 当前的大型视觉语言模型（LVLM）在理解金融图表方面存在显著局限性，尤其是在指令遵循和空间推理能力方面，并且在作为自动化评估者方面尚不可靠。

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [59] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet是一种新的去雾方法，通过迁移雾气模式来适应新域，并在真实世界数据集上提高了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在处理未见过或训练数据有限的真实世界雾气图像时，性能会显著下降。为了解决这个问题，需要开发一种灵活的域自适应方法来提高测试时的去雾性能。

Method: 提出了一种名为PHATNet（Physics-guided Haze Transfer Network）的灵活域自适应方法。该方法通过将看不见的雾气模式从目标域迁移到源域无雾图像，创建特定域的微调集，以更新去雾模型以实现有效的域自适应。此外，还引入了雾气迁移一致性损失和内容泄露损失来增强PHATNet的解耦能力。

Result: 实验结果表明，PHATNet显著提升了最先进的去雾模型在基准真实世界图像去雾数据集上的性能。

Conclusion: PHATNet显著提高了最先进的去雾模型在基准真实世界图像去雾数据集上的性能。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [60] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出了一种配对图像生成方法，解决了DBT图像肿块分割中数据不足和模型难以学习高隐藏性病变特征的问题，提高了生成质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 数字乳房断层合成（DBT）图像中肿块病变的分割对于乳腺癌的早期筛查至关重要，但高密度乳腺组织导致病变隐藏性高，手动标注困难且耗时，进而缺乏用于模型训练的标注数据。现有扩散模型在处理高隐藏性病变时面临生成质量低和无法生成对应标注的挑战。

Method: 提出了一种配对图像生成方法，通过训练一个额外的扩散向导来对条件扩散模型实现配对图像的生成，无需进行外部条件约束。

Result: 生成的配对DBT切片和肿块掩模可用于肿块分割任务的监督训练，实验结果表明该方法在无需额外条件的情况下提高了生成质量，并有效缓解了标注数据短缺的问题，提升了下游分割任务的性能。

Conclusion: 该方法在无需额外条件的情况下提高了生成质量，并有助于缓解标注数据短缺的问题，从而提升了下游任务的性能。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [61] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: 提出了一种新的自监督深度补全方法，无需密集深度标签或额外图像，仅需稀疏深度测量和对应图像即可训练。


<details>
  <summary>Details</summary>
Motivation: 解决了现有监督学习方法依赖昂贵密集深度标签和自监督方法需要多帧图像的问题，使其适用于静态或单帧场景。

Method: 提出了一种新颖的自监督深度补全范式，利用深度分布特性设计损失函数，并结合分割图进行深度估计。

Result: 实验证明了该方法的有效性。

Conclusion: 该方法通过利用深度分布特性设计新的损失函数，并结合视觉基础模型生成的分割图，实现了仅使用稀疏深度测量和对应图像进行训练的自监督深度补全。

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [62] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出了一种创新的视频恢复框架，利用基础模型和自然语言关联视频帧的退化信息，实现可解释和灵活的恢复，且无需退化知识。同时，呼吁基准标准化并提出了新的基准数据集，在各项测试中均取得优异成果。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种可解释、灵活且无需预先了解退化知识的全方位视频恢复方法，并推动视频恢复基准的标准化。

Method: 提出了一种全方位视频恢复框架，利用基础模型将视频帧的退化感知语义上下文与自然语言关联，以提供可解释和灵活的指导。该方法在训练和测试时均不依赖于退化知识，通过学习近似关联知识实现基础模型的推理时解耦。此外，还提出了多退化设置下的三任务（3D）和四任务（4D）基准，以及两个时变复合退化基准（包括一个模拟不同强度降雪的新数据集）。

Result: 在提出的多退化和时变复合退化基准上均取得了最先进的性能。

Conclusion: 该研究提出了一种全方位视频恢复框架，通过基础模型将视频帧的退化感知语义上下文以自然语言形式进行关联，从而提供可解释和灵活的指导。与现有技术不同，该方法在训练或测试时均不假设退化知识，而是学习一种近似的关联知识，使得基础模型可以在推理时安全地解耦，且不增加额外成本。此外，研究者呼吁对全方位视频恢复的基准进行标准化，并提出了多退化设置下的两个基准（三任务3D和四任务4D），以及两个时变复合退化基准，其中之一是模拟了自然界中天气退化（如不同强度的降雪）对视频影响的新数据集。最后，研究者将他们的方法与现有技术进行了比较，并在所有基准上报告了最先进的性能。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [63] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: 提出了一种不确定性感知增强框架，用于DETR目标检测器，通过将边界框建模为多元高斯分布并使用Gromov-Wasserstein距离来提高定位精度和建模预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统的检测器依赖于确定性的边界回归，忽略了预测的不确定性，限制了模型的鲁棒性。

Method: 提出了一种不确定性感知增强框架，用于DETR目标检测器。该框架将边界框建模为多元高斯分布，并将Gromov-Wasserstein距离纳入损失函数，以更好地对齐预测和真实分布。此外，还推导了一种贝叶斯风险公式，用于过滤高风险信息和提高检测的可靠性。最后，提出了一种通过置信区间量化定位不确定性的简单算法。

Result: 在COCO基准测试上，该方法可以有效地整合到现有的DETR变体中，并提高其性能。在白细胞检测任务中，在LISC和WBCDD数据集上取得了最先进的成果。

Conclusion: 该方法可以有效地整合到现有的DETR变体中，并提高其性能。此外，该框架还可以扩展到白细胞检测任务，并在LISC和WBCDD数据集上取得了最先进的成果，证明了其在通用和特定领域检测任务中的可扩展性。

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [64] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于超图增强Transformer的混合监督框架，用于通过重建微手势行为模式来识别情绪状态。该框架通过自重建和监督信息进行联合训练，并在两个公开数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 微手势作为一种新兴的研究课题，能够传达人类的情感状态，在人类行为理解和情感计算领域受到越来越多的关注。然而，基于微手势的人类情感建模尚未得到充分探索。

Method: 提出了一种基于超图增强Transformer的混合监督框架，通过超图Transformer编码器和解码器来重建行为模式，以识别微手势中的情绪状态。其中，设计了超图增强自注意力机制来逐步更新骨骼关节之间的超边，以捕捉微手势的细微运动。此外，还设计了一个浅层的情绪识别头，并以监督方式进行学习。该框架通过自重建和监督信息进行联合训练。

Result: 所提出的方法在iMiGUE和SMG两个公开数据集上进行了评估，并在多个指标上取得了最佳性能，优于现有方法。

Conclusion: 该方法在两个公开数据集iMiGUE和SMG上进行了评估，在多个指标上均取得了最佳性能，优于现有方法。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [65] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: 本研究提出了一种无需训练即可将基础模型的相对深度预测转换为度量深度的非学习方法，通过利用稀疏深度测量，保留了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 基础模型在零样本单目深度估计方面取得了显著进展，但其输出通常是相对尺度而非度量尺度。为了解决这个问题，需要一种成本效益高且不损害泛化能力的方法来将相对尺度转换为度量尺度。

Method: 提出了一种非学习方法，利用稀疏深度测量来调整基础模型的相对尺度预测，使其成为度量尺度深度。

Result: 实验结果表明，该方法有效地将基础模型的相对深度预测转换为度量深度，同时保留了其泛化能力，且无需额外训练或微调。

Conclusion: 通过利用稀疏深度测量，本研究提出了一种非学习方法，将基础模型的相对深度预测转换为度量深度。该方法无需重新训练或微调，从而保留了基础模型的泛化能力，并能产生度量深度。实验结果证明了该方法的有效性，表明其有潜力在不产生额外计算成本或牺牲泛化能力的情况下，弥合相对深度和度量深度之间的差距。

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [66] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer是一种高效的光谱注意模型，用于rPPG估计，可以无需标签进行训练，并在运动场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了结合深度学习在复杂条件下提取脉动信息的优势和手工制作方法在运动等未知场景下的泛化能力及计算效率，提出了一种混合方法。

Method: BeatFormer模型集成了变焦正交复数注意力和频域能量测量，并采用了光谱对比学习（SCL）进行训练，无需PPG或HR标签。

Result: BeatFormer在PURE、UBFC-rPPG和MMPD数据集上进行了验证，在运动场景下的跨数据集评估中表现出鲁棒性和优越性能。

Conclusion: BeatFormer通过集成变焦正交复数注意力和频域能量测量，实现了高效的光谱注意模型，用于rPPG估计。此外，通过引入光谱对比学习（SCL），BeatFormer无需PPG或HR标签即可进行训练。该模型在PURE、UBFC-rPPG和MMPD数据集上得到了验证，特别是在运动场景下的跨数据集评估中，证明了其鲁棒性和性能。

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [67] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种统一的2D预训练多模态网络，用于3D视觉基础任务，能够同时处理图像、文本和点云。通过使用CLIP模型和新颖的GARF模块，该方法简化了模型架构，降低了训练效率，并显著提高了3D检测和视觉基础任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉基础方法依赖于单独的编码器来处理不同模态（如RGB图像、文本和3D点云），导致模型庞大、复杂且训练效率低下。尽管一些方法尝试使用像CLIP这样的预训练2D多模态模型，但它们在将点云数据与2D编码器对齐方面仍存在困难，并继续依赖3D编码器进行特征提取，增加了模型的复杂性和训练的低效性。

Method: 提出了一种统一的2D预训练多模态网络来处理RGB图像、文本和点云这三种模态。该网络利用了基于适配器的2D CLIP双模态模型进行微调，以适应三模态设置。通过几何感知2D-3D特征恢复和融合（GARF）模块来融合点云和图像的几何多尺度特征，并整合文本特征进行最终的模态融合。最后，使用多模态解码器促进深度的跨模态理解。

Result: 与基线方法相比，该方法可将可训练参数数量减少约58%，在3D检测任务上提高了6.52%，在3D视觉基础任务上提高了6.25%。

Conclusion: 该方法通过统一的2D预训练多模态网络实现了跨模态特征的提取和融合，能够进行端到端的3D视觉基础模型。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [68] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 提出一种语义感知表示学习（SARL）方法，通过提取语义相关特征、使用最优传输注意力机制对齐表示以及区域分数聚合进行多标签图像分类，解决了现有方法表示易含噪声和定位不精确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如注意力机制或图卷积网络）提取的图像表示可能包含噪声且定位不精确。

Method: 1. 提取与标签语义相关的特征。2. 利用基于最优传输的注意力机制获得语义对齐的图像表示。3. 采用区域分数聚合策略进行多标签预测。

Result: SARL在PASCAL VOC 2007和MS-COCO数据集上的实验结果优于现有方法。

Conclusion: SARL在PASCAL VOC 2007和MS-COCO数据集上优于现有方法。

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [69] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: \method是一个创新的3D高斯重建框架，通过解耦几何和外观预测，并使用GS-maps表示，实现了高效、无需相机参数的3D重建，降低了计算需求，提高了效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯重建方法（Generalizable 3D Gaussian Splatting reconstruction）在训练时需要大量计算资源和数据集，并且通常将3D高斯几何和外观的预测纠缠在一起，依赖于数据驱动的先验知识，导致回归速度缓慢。为了解决这些问题，需要一种更高效、更灵活的方法。

Method: 提出了一种名为\method的解耦框架，该框架首先从局部图像对中提取特征，然后通过全局注意力块融合特征。设计了专门的点和高斯预测头，生成用于几何的多视图点图和用于外观的高斯特征，并将它们组合成GS-maps来表示3DGS对象。最后，通过一个细化网络来增强GS-maps以实现高质量重建。

Result: 该方法实现了无需相机参数的姿态无关3D重建，提高了鲁棒性和实用性。通过降低资源需求并保持高质量的输出，\method为实际的3D内容生成提供了一个高效、可扩展的解决方案。

Conclusion: 该方法通过解耦3D高斯表示中的几何和外观预测，并采用新颖的GS-maps表示，实现了高效、可扩展且无需相机参数的3D重建，为3D内容生成提供了更高效、更实用的解决方案。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [70] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: 提出了一种结合多维缩放（MDS）、${ell}_1$-范数优化和迭代移位校正的冷冻电镜姿态估计新方法，以克服低信噪比的挑战，并在实验中取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 提出一种用于冷冻电镜（cryo-EM）的姿态估计方法，以解决低信噪比（SNR）下的姿态估计和移位校正挑战，这直接影响三维重建的保真度。

Method: 提出了一种利用多维缩放（MDS）技术鲁棒地估计每个粒子的三维旋转矩阵的方法，通过对（i）基于${ell}_1$-范数或类似鲁棒范数的联合优化框架进行姿态估计，同时估计旋转轴和面内向量，并精确地强制执行单位范数和正交性约束；以及（ii）通过全局最小二乘法估计一致的面内平移的迭代移位校正算法。

Result: 与以往的基于${ell}_2$-范数的目标函数的常​​规方法相比，该方法在精度和重建保真度方面表现更优。

Conclusion: 该方法在欧拉角精度和重建保真度方面持续优于现有方法，并以傅里叶壳相关性（FSC）进行衡量。

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [71] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 提出了一种新颖的概率MIL框架，通过对注意力值进行概率建模来解决现有方法的局限性，并在医学成像任务中取得了优于现有方法的性能，同时提供了可解释的不确定性图。


<details>
  <summary>Details</summary>
Motivation: 在医学成像分类任务中，数据标签稀缺，多实例学习（MIL）范式因其只需要包标签而受到关注。现有的深度MIL方法通过聚合实例表示来计算包级别预测，但它们将注意力值视为确定性的，可能忽略了单个实例贡献的不确定性。

Method: 提出了一种新颖的概率框架，通过估计注意力值上的概率分布来处理全局和局部交互，以解决现有深度多实例学习方法中注意力值确定性处理的潜在问题。

Result: 该方法在涉及十一种最先进基线和三个医学数据集的全面评估中，在不同指标上均实现了顶级的预测性能，并且其概率处理的注意力提供了可解释的疾病定位不确定性图。

Conclusion: 提出的概率框架在不同指标上均取得了优于最先进基线方法的预测性能，并且其对注意力机制的概率处理能够提供可解释的疾病定位不确定性图。

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [72] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: 本研究提出了开放集交叉模态泛化（OSCMG）任务和MICU模型（包含FCMI和CUJP），以解决现有方法在开放集环境下的局限性，并成功提高了模型在处理未知类别时的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态统一表示方法未考虑开放集环境，而OSCMG任务需要模型不仅能进行跨模态知识迁移，还能在开放集条件下对新模态中的未见类别进行鲁棒泛化，这在现实世界应用中很常见。

Method: 本研究提出MICU，包含两个关键部分：1. 细粒度-粗粒度掩码多模态InfoNCE (FCMI)，通过在整体语义和时间层面应用对比学习，并结合掩码来增强泛化能力，从而提高多模态对齐。2. 跨模态统一的拼图 (CUJP)，通过融合与模态无关的特征选择和自监督学习，来增强特征多样性和模型不确定性，从而提升模型处理开放集任务中未知类别的能力。

Result: MICU在CMG和OSCMG基准测试中均表现出有效性，能够处理开放集交叉模态泛化任务。

Conclusion: 本研究提出的MICU在CMG和新提出的OSCMG上进行了广泛的实验评估，证明了其有效性。

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [73] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的分类器，用于评估低重叠点云配准的质量，克服了传统指标的不足，并显著提升了现有配准方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的PCR评估指标（如最大内点数）在极低内点比例下失效，因此需要新的评估方法来应对低重叠点云配准的挑战。

Method: 提出了一种数据驱动的方法，首先基于3DMatch数据集构建了相应的评估数据集，然后训练了一个基于深度学习的分类器来评估配准质量。

Result: 将所提出的分类器整合到标准的PCR流程中，可以显著提升现有PCR方法的性能。例如，与GeoTransformer结合使用时，在3DLoMatch基准测试中达到了86.97%的SOTA配准召回率，并在ETH数据集上展现了良好的泛化能力。

Conclusion: 所提出的方法通过深度学习分类器解决了低重叠点云配准（PCR）结果评估的根本问题，克服了传统评估指标在极低内点比例下的局限性。该方法通过在3DMatch数据集基础上构建对应数据集并训练深度学习分类器来实现，这是首个采用深度学习框架解决此任务的全面研究。

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [74] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL是一个用于预训练视觉-语言模型的新型提示学习框架，通过双向知识流解决模态隔离和语义衰减问题，在多项任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练视觉-语言模型（VLMs）虽然泛化能力强，但在适应下游任务时，保持其泛化能力仍然是一个挑战。现有的提示学习方法存在模态隔离和层级语义衰减两大瓶颈，限制了泛化能力。

Method: HiCroPL是一个分层跨模态提示学习框架。它通过分层知识映射器实现文本和视觉模态之间的双向知识流。在早期层，文本提示通过该映射器向视觉提示注入清晰的语义，以增强低级视觉语义的表示。在后期层，包含特定任务相关对象的视觉提示会反向作用于文本提示，实现更深层次的对齐。该框架还引入了轻量级层级知识代理，以实现高效的跨模态交互。

Result: HiCroPL在四个任务上的广泛评估显示，其性能优于现有方法，并在11个基准测试中取得了最先进（state-of-the-art）的成果，性能提升显著。

Conclusion: HiCroPL框架通过建立文本和视觉模态之间的双向知识流，实现了对预训练视觉-语言模型（VLMs）的有效适应，同时保留了它们的泛化能力。该框架通过分层知识映射器实现早期层面的文本到视觉的语义注入，以及后期层面的视觉到文本的语义细化，解决了模态隔离和层级语义衰减的问题。此外，引入的轻量级层级知识代理促进了高效的跨模态交互。在四个任务上的广泛评估表明，HiCroPL取得了优于现有方法的性能，并在11个基准测试中达到了最先进水平。

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [75] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 现有的MLLMs在图像回归任务上表现不佳，因为它们未能有效利用文本语义。本文提出了RvTC方法，通过灵活的分箱策略和数据特定提示，显著提升了MLLMs在图像回归任务上的性能，并达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在图像回归任务上的应用存在局限性，它们通常使用预设的输出词汇和通用的任务提示进行微调，但这种方法并未显示出比仅使用图像训练的单一模态方法有任何优势，未能有效利用文本的语义理解能力。

Method: 提出了一种名为RvTC（Regression via Transformer-Based Classification）的方法，该方法用灵活的基于分箱（bin-based）的方法替代了受限于预设输出词汇的分类方法，并通过增加分箱数量来消除手动设计词汇的需要，实现了在四个图像评估数据集上的最先进性能。同时，实验证明使用包含图像语义信息的数据特定提示（data-specific prompts）能够显著提升MLLMs的性能。

Result: RvTC 方法能够消除手动设计词汇的需要，并通过简单的增加分箱数量即可实现最先进的性能。通过在 AVA 数据集上添加挑战标题到提示中，性能相关性从 0.83 提升到 0.90，达到了新的最先进水平。实验证明，MLLMs 从语义提示信息中获益，其表现优于仅仅利用统计偏差。

Conclusion: MLLMs 在图像回归任务中需要结合数据本身的语义信息，使用特定于数据的提示（prompt）能够极大地提升性能，甚至超越仅使用图像训练的模型，这表明理解文本上下文对于多模态回归任务至关重要。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [76] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出了一种新的文档去歪斜方法，利用轴对齐几何特性，在AAD指标上实现了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖有注释的监督回归，未能利用物理文档中的固有几何特性。

Method: 该方法在训练阶段提出轴对齐几何约束，在推理阶段提出轴对齐预处理策略，并引入新的AAD指标进行评估。

Result: 在AAD指标上实现了18.2%~34.5%的改进，并在多个现有基准测试中取得最先进的结果。

Conclusion: 通过引入轴对齐几何约束和推理阶段的预处理策略，所提出的方法在多个现有基准测试中取得了最先进的结果，并在AAD指标上实现了18.2%~34.5%的改进。

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [77] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: 本研究提出使用 B 样条曲线拟合技术优化 FastSAM 的分割边缘，解决了其锯齿状问题，在保持实时性的同时提高了精度，增强了其在工业自动化、医学成像和自动驾驶等领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 为了解决 Segment Anything Model (SAM) 内存占用高、推理时间长的问题，FastSAM 被提出以实现实时分割，但其生成的边缘存在锯齿状，偏离真实物体形状。因此，需要一种方法来提高 FastSAM 的边缘质量，以满足工业自动化、医学成像和自动驾驶等领域对精确高效边缘识别的需求。

Method: 本研究提出了一种基于 B 样条曲线拟合的四阶段细化方法，包括两轮曲线拟合，用于平滑 FastSAM 生成的分割掩模的锯齿状边缘，以提高边缘质量和分割精度。

Result: 该方法通过 B 样条曲线拟合技术有效平滑了 FastSAM 的锯齿状边缘，显著提高了物体边缘的视觉质量和分析准确性，同时保持了实时处理能力，提高了 FastSAM 的实用性。

Conclusion: 该研究提出了一种新颖的 B 样条曲线拟合技术，用于改进 FastSAM 生成的分割掩模的边缘质量，通过四阶段的细化过程有效解决了边缘锯齿问题，同时保持了实时处理能力，提高了分割精度和实际应用价值。

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [78] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: Video-TT是一个新基准，用于评估视频语言模型在理解真实世界视频方面的正确性和鲁棒性，结果显示模型表现不如人类。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型的基准未能充分反映其在视频理解的正确性和鲁棒性方面与人类智能的差距。

Method: 引入了一个名为Video-TT的新基准，该基准包含1000个YouTube Shorts视频，每个视频都配有一个开放式问题和四个用于探究视觉和叙事复杂性的对抗性问题。

Result: 评估结果显示，视频语言模型在Video-TT基准上的表现与人类相比存在显著差距，表明模型在处理真实世界视频的复杂性和鲁棒性方面仍需改进。

Conclusion: 现有的视频语言模型（video LLMs）在理解视频内容方面与人类智能仍有显著差距，尤其是在复杂视觉叙事和对抗性问题方面。提出的Video-TT基准能够有效地评估这种差距。

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [79] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 发布OpenBreastUS数据集，包含海量真实人体乳腺超声波模拟数据，并首次实现基于神经算子的人体乳腺实时成像。


<details>
  <summary>Details</summary>
Motivation: 传统的数值求解器计算成本高且不稳定，限制了其在准实时图像重建中的应用。现有的神经算子数据集过于简化，未能反映真实世界的复杂性，因此需要一个更真实、大规模的数据集来推动神经算子在医学成像领域的应用。

Method: 提出OpenBreastUS数据集，包含8000个解剖学上真实的人体乳腺模型和超过1600万个频域波模拟，并在此数据集上对流行的神经算子在正演模拟和逆成像任务上的性能、可扩展性和泛化能力进行了基准测试。

Result: OpenBreastUS数据集为评估和开发神经算子在波方程模拟和医学成像任务中的表现提供了平台。首次展示了使用神经算子求解器进行高效的原位人体乳腺成像。

Conclusion: OpenBreastUS数据集的发布以及基于其的神经算子在体超声成像中的应用，为计算波成像领域带来了突破。它不仅提供了真实的数据集以供研究和基准测试，还首次实现了神经算子在人体乳腺成像中的高效应用，为实时医学成像提供了新的可能性。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [80] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON 是一个无需训练的通用虚拟试衣框架，通过解耦服装和姿势信息，实现了高保真纹理和精确姿势对齐，适用于多种场景，包括多人物试衣。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像的虚拟试衣（VTON）技术要么依赖于确保高保真度的店内监督方法，但难以跨领域泛化；要么依赖于改进了适应性的无监督真实世界方法，但受限于数据偏差和有限的通用性。一个能够同时适用于这两种场景且无需训练的统一解决方案仍然是一个挑战。

Method: OmniVTON 通过解耦服装和姿势信息来解决现有虚拟试衣技术的局限性。它包含一个服装先验生成机制，通过连续边界缝合技术来保留服装细节；同时利用 DDIM 逆转来捕捉姿势结构信息，并抑制纹理干扰，从而实现精确的姿势对齐。

Result: 实验结果表明，OmniVTON 在不同的数据集、服装类型和应用场景中都取得了优越的性能。值得注意的是，它是首个能够实现多人物虚拟试衣的框架，可以在同一场景中实现多个人物之间的逼真服装迁移。

Conclusion: OmniVTON 框架首次实现了无需训练的通用虚拟试衣，能够跨越不同场景（如店内和真实世界）并保持服装纹理和姿势的一致性。

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [81] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: 本文提出了PanTiny框架，通过多合一训练和复合损失函数，解决了现有融合模型计算开销大、泛化能力差的问题，实现了高效且性能优越的融合。


<details>
  <summary>Details</summary>
Motivation: 当前融合领域存在模型庞大、计算开销高、泛化能力差的问题，本文旨在挑战这一范式，提出一种更高效、泛化能力更强的解决方案。

Method: 提出了一种名为PanTiny的轻量级、单步融合框架，并引入了多合一训练范式，该范式在一个紧凑的模型上同时在三个不同的卫星数据集（WV2、WV3和GF2）上进行训练。此外，还引入了一个通用的复合损失函数来提升模型性能。

Result: PanTiny模型在效率和性能上取得了优越的平衡，超越了许多更大、更专门化的模型，并显著提高了在全分辨率数据上的泛化能力。提出的多合一训练范式和复合损失函数也对其他融合模型起到了提升作用。

Conclusion: 论文提倡一种新的范式，即开发高效、可泛化且注重数据的模型，以应对当前融合领域中模型日益增大和复杂化的问题。通过提出的PanTiny框架、多合一训练范式以及通用的复合损失函数，在效率和性能之间取得了优越的平衡，超越了许多更大、更专门化的模型。

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [82] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ 是一个新颖的、保持身份一致性的视频扩散框架，通过可学习的姿态对齐和优化的面部处理技术，能够生成高质量、无需后处理的动画视频。


<details>
  <summary>Details</summary>
Motivation: 当前用于人物图像动画的扩散模型在保持身份（ID）一致性方面存在挑战，尤其是在参考图像和驱动视频在体型或位置上存在显著差异时。

Method: StableAnimator++ 采用了一种新颖的视频扩散模型框架，包含可学习的姿态对齐、SVD 引导的相似变换矩阵预测、基于现成编码器的图像和人脸嵌入、全局内容感知人脸编码器以及分布感知的 ID 适配器，还在推理阶段引入了基于 HJB 的人脸优化来增强面部保真度。

Result: 实验结果在质量和数量上都证明了 StableAnimator++ 的有效性。

Conclusion: StableAnimator++ 在保持身份（ID）一致性方面表现出色，能够生成高质量的视频，并且无需任何后处理。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [83] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 评估了先进生成模型（如Flux系列和GPT-4o）在文本图像生成和编辑方面的能力，特别是结合OCR任务，发现它们在处理此类任务时存在不足，并提出应将文本图像生成和编辑作为通用模型的基础能力。


<details>
  <summary>Details</summary>
Motivation: 鉴于Flux系列和GPT-4o等先进生成模型在图像生成方面的卓越表现，旨在评估这些模型在掌握文本图像生成和编辑的复杂性方面的能力。

Method: 评估了包括Flux系列和GPT-4o在内的六种先进生成模型在文本图像生成和编辑方面的能力，并纳入了典型的光学字符识别（OCR）任务，将任务扩展到OCR生成任务。选择了33个代表性任务，分为文档、手写文本、场景文本、艺术文本和复杂/富布局文本五类，并使用了定制的高质量图像输入和提示。

Result: 评估结果揭示了当前生成模型在OCR任务方面的不足之处。

Conclusion: 应将照片级文本图像生成和编辑作为通用生成模型的基础能力，而不是依赖专门解决方案。

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [84] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 为解决ESD术中出血定位难题，本研究提出了首个ESD出血源数据集BleedOrigin-Bench和新颖的检测-跟踪框架BleedOrigin-Net。实验证明，BleedOrigin-Net在出血检测和跟踪方面表现优异，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内镜粘膜下剥离术（ESD）中的术中出血对患者构成重大风险，需要精确的实时出血源定位和连续监测以进行有效止血。然而，内镜医生需要反复冲洗以清除血液，只有毫秒时间来识别出血源，这是一个低效的过程，延长了手术时间和增加了患者风险。现有的AI方法主要关注出血区域分割，忽略了在ESD复杂环境中（视觉障碍频繁且场景动态变化）准确检测和跟踪出血源的关键需求，并且缺乏专门的数据集阻碍了强大的AI辅助指导系统的开发。

Method: 提出了一种名为BleedOrigin-Net的新颖双阶段检测-跟踪框架，用于在ESD手术中定位出血源，以解决从出血起始检测到连续空间跟踪的完整工作流程。该方法在BleedOrigin-Bench数据集上进行了评估，该数据集包含了1771个专家标注的出血源（跨越106,222帧，来自44个手术），并辅以39,755个伪标签帧，覆盖8个解剖部位和6种临床挑战场景。通过与YOLOv11/v12、多模态大语言模型和点跟踪方法进行比较，证明了该框架的有效性。

Result: BleedOrigin-Net在BleedOrigin-Bench数据集上取得了最先进的性能，在出血起始检测方面达到了96.85%的帧级准确率（±≤8帧），在初始源检测方面达到了70.24%的像素级准确率（≤100 px），在点跟踪方面达到了96.11%的像素级准确率（≤100 px）。

Conclusion: 目前的研究集中在出血区域分割，忽略了在复杂的内镜粘膜下剥离术（ESD）环境中准确检测和跟踪出血源的实时需求。本研究提出了BleedOrigin-Bench，这是首个针对ESD出血源的综合性数据集，包含1771个专家标注的出血源（跨越106,222帧，来自44个手术），并辅以39,755个伪标签帧，覆盖8个解剖部位和6种临床挑战场景。同时，我们提出了BleedOrigin-Net，一个新颖的双阶段检测-跟踪框架，用于ESD手术中的出血源定位，解决了从出血起始检测到连续空间跟踪的完整工作流程。与广泛使用的对象检测模型（YOLOv11/v12）、多模态大语言模型和点跟踪方法相比，我们的方法在出血起始检测方面达到了96.85%的帧级准确率（±≤8帧），在初始源检测方面达到了70.24%的像素级准确率（≤100 px），在点跟踪方面达到了96.11%的像素级准确率（≤100 px），表现出最先进的性能。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [85] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet是一种用于SLAM回环检测的方法，通过ResNet变体和少样本学习提高准确性，并优化于嵌入式设备，同时发布了LoopDB数据集。


<details>
  <summary>Details</summary>
Motivation: 为了解决SLAM（同时定位与地图构建）中的回环闭合问题，特别是提高先前访问地点的识别准确性，并满足嵌入式设备对实时计算的约束。

Method: LoopNet方法基于ResNet架构的变体，并采用了多任务学习、在线少样本学习以及DISK描述子，以提高检测准确性和实时性。

Result: LoopNet方法在环路闭合检测准确性和实时计算性能上均有提升，超越了基于手工特征和传统深度学习方法，并且在新的LoopDB基准数据集上进行了验证。

Conclusion: LoopNet方法在环路闭合检测方面优于传统方法，并且在嵌入式设备上具有实时计算能力，同时引入的LoopDB数据集也为相关研究提供了新的基准。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [86] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: 《VideoPlan》通过辅助任务增强和多令牌预测，改进了基于视频的规划方法，在多个基准测试中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前多模态大语言模型（MLLMs）在视频规划任务中面临的挑战，特别是程序性注释的稀缺性和下一令牌预测目标的低效率问题。

Method: 提出了一种名为VideoPlan的方法，通过辅助任务增强（Auxiliary Task Augmentation）和多令牌预测（Multi-token Prediction）来解决视觉规划中的数据稀疏性和行动空间结构化问题。

Result: VideoPlan在COIN和CrossTask数据集上预测3个未来动作时，分别超越了先前方法7.3%和3.4%，并在Ego4D长期行动预测任务上达到了与现有技术水平相当的性能，并且未使用专门的以自我为中心的特征。

Conclusion: VideoPlan在COIN和CrossTask数据集上实现了最先进的视觉规划方法（VPA）性能，分别超越了先前方法7.3%和3.4%，并且在Ego4D长期行动预测任务上与最先进的方法相当。

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [87] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 通过新颖的时空多图表示方法，成功解决了事件基传感器数据处理中的时空动态建模问题，显著提升了目标检测的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 事件基传感器数据到密集张量的转换会削弱其固有的稀疏性和低延迟优势。虽然现有的图表示方法能够保持稀疏性并支持异步推理，但由于对时空动态的建模不佳，其在下游任务中的表现受到限制。

Method: 提出了一种新颖的时空多图表示方法，构建了两个解耦的图：一个利用B样条基函数模拟全局结构的图，以及一个利用运动矢量注意机制模拟局部动态变化的时域图。该设计允许使用高效的2D卷积核替代计算成本高昂的3D卷积核。

Result: 在Gen1汽车和eTraM数据集的事件基目标检测任务中，相较于先前基于图的方法，准确率提高了6%以上，同时实现了5倍的速度提升，并减少了参数数量，计算成本未增加。

Conclusion: 事件基传感器提供的稀疏、异步数据通过新颖的时空多图表示能够更好地捕捉时空动态，在目标检测任务中相比先前基于图的方法提高了6%的准确率，同时实现了5倍的加速，并减少了参数数量，计算成本却无明显增加。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [88] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR:  MeshMamba利用Mamba-SSMs处理大规模3D关节网格，实现高效生成与重建。其序列化技术和排序方法是关键。基于此模型，MambaDiff3D在3D人体生成方面表现优异，Mamba-HMR则在全身姿态恢复方面实现了高效且全面的处理。


<details>
  <summary>Details</summary>
Motivation: 为了能够高效、可扩展地处理大量输入的3D网格模型（超过10,000个顶点），以生成和重建包含衣物和手部几何形状的身体网格模型。

Method:  MeshMamba采用Mamba状态空间模型（Mamba-SSMs）来学习3D关节网格模型。关键技术在于将网格顶点进行序列化，并通过根据身体部位注释或模板网格的3D顶点位置对顶点进行排序，以尊重关节形状的结构。

Result:  MeshMamba能够生成和重建包含衣物和手部几何形状的身体网格模型。MambaDiff3D在3D人体形状生成任务上优于先前方法，能够生成包含衣物的密集3D人体网格，并能生成抓握的手部。Mamba-HMR能够处理包含面部和手部的全身模型，并在接近实时的情况下达到有竞争力的性能。

Conclusion: MeshMamba在处理大规模3D网格模型方面表现出高效率和可扩展性，能够生成和重建包含超过10,000个顶点、包含衣物和手部几何形状的身体网格模型。基于MeshMamba设计的MambaDiff3D在3D人体形状生成任务上优于先前方法，而Mamba-HMR则能够处理包含面部和手部的全身模型，并在接近实时的情况下达到有竞争力的性能。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [89] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: N-JEPA模型通过将扩散噪声融入掩码图像建模，并利用多层次噪声调度，有效提升了自监督学习在图像识别任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 为了进一步增强自监督学习（SSL）的表征能力，并弥补其在图像生成和细节增强方面不如生成模型的不足，研究人员旨在探索SSL与生成模型之间的联系。通过借鉴生成模型（特别是扩散模型）通过近似数据分布生成新样本的能力，以及这种能力所蕴含的语义理解，研究者提出将扩散模型的扩散噪声与SSL相结合，以构建更具竞争力的识别模型。

Method: 提出了一种名为N-JEPA（Noise-based JEPA）的模型，该模型将扩散模型的扩散噪声融入掩码图像建模（MIM）中，具体做法是在掩码令牌的位置嵌入中加入扩散噪声。此外，模型还采用了多层次噪声调度作为一系列特征增强手段，以进一步提高模型的鲁棒性。

Result: 通过全面的研究，证实了N-JEPA模型在下游分类任务中的有效性。

Conclusion: 该研究表明，将扩散模型的扩散噪声与自监督学习（SSL）相结合，通过在掩码图像建模（MIM）的位置嵌入中加入扩散噪声，并利用多层次噪声调度进行特征增强，可以有效地提升模型在下游分类任务中的表现，证明了所提出的N-JEPA模型的有效性。

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [90] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的3D血管生成框架，通过分离全局拓扑和局部细节，能够更精确地模拟复杂的血管网络，并在真实数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管3D视觉在医学应用中对血管建模的影响日益增大，但由于血管分支模式复杂、曲率和形状不规则，精确表示其复杂的几何形状和拓扑结构仍然是一个挑战。

Method: 提出了一种分层的、基于部件的框架，用于3D血管生成，该框架将全局二叉树状拓扑与局部几何细节分离开来。该方法包括三个阶段：(1) 关键图生成，用于模拟整体分层结构；(2) 基于几何属性的血管段生成；(3) 通过整合局部血管段和全局关键图来实现分层血管组装。

Result: 在真实世界数据集上验证了该框架，证明了其在模拟复杂血管网络方面优于现有方法。

Conclusion: 该研究首次成功应用了基于部件的生成方法进行3D血管建模，为血管数据生成设定了新基准。

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [91] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: 本研究使用稀疏自编码器（SAE）分析了乳腺影像基础模型Mammo-CLIP，识别了与临床概念相关的潜在特征，发现了混淆因素，并分析了下游任务中模型依赖的特征，证明了SAE在提升模型可解释性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医学影像）中，模型的可解释性对于临床应用至关重要。本研究旨在将稀疏自编码器（SAE）的可解释性方法应用于乳腺影像领域，以理解像Mammo-CLIP这样的基础模型在处理乳腺X线摄影图像时的决策过程。

Method: 本研究使用稀疏自编码器（SAE）来分析一个在大型乳腺X线摄影图像-报告对上预训练的视觉-语言基础模型（Mammo-CLIP）。我们训练了一个补丁级别的Mammo-SAE，以识别和探查与临床相关乳腺概念（如肿块和可疑钙化）相关的潜在特征。然后，我们分析了SAE潜在空间中的激活神经元，以及模型在下游微调中用于改进乳腺概念预测所依赖的潜在神经元。

Result: 研究结果表明，SAE潜在空间中的顶级激活神经元通常与地面真实区域对齐，并揭示了影响模型决策过程的混淆因素。此外，研究还分析了模型在下游微调过程中依赖的潜在神经元，以改进乳腺概念的预测。

Conclusion: 本研究展示了基于稀疏自编码器（SAE）的可解释性方法在乳腺影像领域的应用潜力，通过分析Mammo-CLIP模型，我们识别并探究了与临床相关概念（如肿块和可疑钙化）相关的潜在特征，并发现了影响模型决策的混淆因素。此外，我们还分析了模型在下游微调过程中依赖的潜在神经元，为理解乳腺影像基础模型内部工作机制提供了更深入的见解。

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [92] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 尽管在跨域少样本学习（CD-FSL）方面取得了进展，但结合了原型分类器的 DINO 预训练模型优于最新的 SOTA 方法。然而，更新过多的 Transformer 参数会导致过拟合，因为标记样本稀少。为了解决这个问题，我们提出了一种新的概念——聚合投影（CP），作为软提示的有效后继者。此外，我们还提出了一种新颖的伪类别生成方法，结合了仅依赖基础域的自监督变换（SST），以使网络能够处理来自不同域的未见样本。所提出的方法在 BSCD-FSL 基准的极端域移位场景的综合实验中，有效地证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在跨域少样本学习（CD-FSL）中，更新过多的 Transformer 参数会导致过拟合，因为标记样本稀少。

Method: 提出了一种新的概念——聚合投影 (CP)，作为软提示的有效后继者。此外，还提出了一种新颖的伪类别生成方法，结合了仅依赖基础域的自监督变换 (SST)。

Result: 所提出的方法在 BSCD-FSL 基准的极端域移位场景的综合实验中，有效地证明了其有效性。

Conclusion: 更新过多的 Transformer 参数会导致过拟合，因为标记样本稀少。为了解决这个问题，我们提出了一种新的概念——聚合投影 (CP)，作为软提示的有效后继者。此外，我们还提出了一种新颖的伪类别生成方法，结合了仅依赖基础域的自监督变换 (SST)，以使网络能够处理来自不同域的未见样本。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [93] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus 是一个无需训练的框架，通过注意力共享、改进的特征提取和多模态大语言模型，解锁了扩散 Transformer（DiT）的零样本能力，实现了高质量、一致性的主题驱动图像生成，且兼容现有图像编辑流程。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）生成技术在定制化生产方面存在不足，通常需要针对每个主题进行优化或训练专门的编码器，这限制了其实际应用。此外，当前方法未能充分利用现代扩散 Transformer 的零样本潜力来实现真实的主题驱动合成。

Method: FreeCus 框架包含三个关键创新：1) 提出了一种注意力共享机制，以捕捉主题的布局完整性并保留编辑灵活性。2) 通过分析 DiT 的动态偏移，提出了一种改进的变体，以增强细粒度特征提取。3) 集成了多模态大语言模型（MLLMs）以丰富跨模态语义表示。

Result: 实验结果表明，FreeCus 框架能够实现一致的主题合成，并取得与需要额外训练的方法相当或更优的性能。

Conclusion: FreeCus 框架成功解锁了 DiT 的零样本能力，实现了在不同场景下的一致性主题合成，并且取得了与需要额外训练的方法相当或更优的成果。该框架还与现有的图像修复和控制模块兼容，能够带来更具吸引力的体验。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [94] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: 提出了一种新的基于近似盲PnP的对应学习方法（MinCD-Net），通过最小化2D和3D关键点之间的Chamfer距离来提高图像到点云配准的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了克服差分PnP对预测对应关系中的噪声和离群值敏感的问题，借鉴盲PnP的鲁棒性。

Method: 提出了一种基于近似盲PnP的对应学习方法，通过最小化学习到的2D和3D关键点之间的Chamfer距离（MinCD-PnP）来解决，并设计了一个轻量级多任务学习模块MinCD-Net。

Result: MinCD-Net 能够有效解决MinCD-PnP问题，并在多个数据集上取得了优于现有最先进方法的性能。

Conclusion: MinCD-Net 在跨场景和跨数据集的设置中，其在内点比例（IR）和配准召回率（RR）方面均优于现有最先进的方法。

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [95] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出一种基于条件扩散模型的视频压缩框架，通过多粒度条件、紧凑表示和多条件训练，在感知质量上优于现有编解码器。


<details>
  <summary>Details</summary>
Motivation: 基于条件扩散模型在视频内容重建方面表现出与人类视觉感知一致的优势的见解，提出一种利用条件扩散模型进行感知优化的视频压缩框架。

Method: 将视频压缩重新构想为条件生成任务，利用条件扩散模型从稀疏但信息丰富的信号生成视频。关键模块包括：1. 多粒度条件，用于捕捉静态场景结构和动态时空线索；2. 紧凑表示，用于高效传输且不牺牲语义丰富性；3. 多条件训练，采用模态丢弃和角色感知嵌入，以防止过度依赖单一模态并增强鲁棒性。

Result: 实验结果表明，该方法在感知质量方面表现出色，特别是在高压缩率下。

Conclusion: 所提出的方法在感知质量指标（如 Fréchet Video Distance (FVD) 和 LPIPS）上显著优于传统和神经编解码器，尤其是在高压缩率下。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [96] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 本研究提出了一个基于视觉语言模型（VLM）和包含性学习的框架，用于检测生物识别系统中的物理呈现攻击和数字仿冒攻击。该方法在不进行大量训练的情况下，取得了优于传统CNN的性能，并提高了攻击检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习模型在适应不同类型的攻击或不同的环境条件方面存在不足，并且需要大量的训练数据，而生物识别数据收集面临隐私和实际操作的挑战。因此，本研究旨在提出一种新的方法来解决这些问题。

Method: 本研究探索了视觉语言模型（VLM）的应用，并提出了一个包含性学习框架，用于检测生物识别系统中的物理呈现攻击和数字仿冒攻击。)

Result: 所提出的子系统在物理和数字攻击检测方面取得了有竞争力的性能，并且在没有资源密集型训练的情况下，其表现优于一些传统的卷积神经网络（CNN）。

Conclusion: 实验结果表明，所提出的框架作为一种有前景的工具，能够提高攻击检测的泛化能力。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [97] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: DMD是一种新颖的指纹识别方法，通过结合细节点和脊线纹理的密集局部表示，在各种条件下都能实现高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 为了在多样化的捕获条件下实现稳健和准确的指纹匹配，即指纹识别领域的一个基本挑战。

Method: 提出了一种名为DMD（局部密集特征）的新型指纹表示方法。DMD是一种以细节点为锚定的局部密集表示，它以空间结构化的方式捕获精细的脊线纹理和具有判别力的细节点特征。具体来说，描述符是从以每个检测到的细节点为中心和定向的局部块中提取的，形成一个三维张量，其中两个维度代表指纹平面上的空间位置，第三个维度编码语义特征。这种表示明确捕获了局部图像块的抽象特征，实现了多层次、细粒度的描述，聚合了来自多个细节点及其周围脊线结构的信息。此外，由于其与图像块的强空间对应关系，DMD允许使用前景分割掩码来识别有效的描述符区域。在匹配过程中，比较仅限于重叠的前景区域，从而提高了效率和鲁棒性。

Result: DMD在多个基准测试中实现了最先进的准确性，同时保持了高计算效率，在对齐和识别方面表现出色，显示出大规模指纹识别的巨大潜力。

Conclusion: DMD在包括滚印、平印、部分、非接触式和潜指纹数据集的广泛数据集上进行了广泛的实验，证明了其有效性和通用性。该方法在多个基准测试中实现了最先进的准确性，同时保持了高计算效率，在对齐和识别方面表现出色，显示出大规模指纹识别的巨大潜力。

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [98] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: 针对少样本目标检测中特征提取的通道依赖性问题，提出了一种基于Mamba的空间-通道状态空间建模（SCSM）模块，通过显式建模通道间的相关性来优化特征表示，并在标准数据集上取得了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本目标检测方法在处理有限的训练样本时，可能难以准确提取每个通道的有效特征。具体来说，权重高的通道不一定有效，而权重低的通道可能包含重要信息。为了解决这个问题，研究者提出利用通道间的相关性来改进模型的适应性。

Method: 提出了一种空间-通道状态空间建模（SCSM）模块，该模块包含空间特征建模（SFM）和通道状态建模（CSM）两个子模块。其中，CSM模块基于Mamba来学习通道序列中的相关性，以处理特征提取中的通道间依赖性问题。

Result: 所提出的SCSM模块在VOC和COCO数据集上的大量实验表明，它能够有效提升所提出的检测器的通道特征表示质量，并实现了最先进的性能。

Conclusion: 通过实验证明，SCSM模块能够提升模型在通道特征表示方面的质量，并达到最先进的性能。

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [99] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: BenchDepth 通过五个下游代理任务评估深度模型，解决了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型评估协议存在不一致性，基于对齐的指标会引入偏差、偏好某些深度表示并使公平比较复杂化，因此需要一种新的评估方法来评估 DFMs 在实际应用中的效用。

Method: 提出名为 BenchDepth 的新评估基准，使用深度补全、立体匹配、单目前馈 3D 场景重建、SLAM 和视觉-语言空间理解这五个下游代理任务来评估 DFMs，并对八个最先进的 DFMs 进行了基准测试和深入分析。

Result: 对八个最先进的 DFMs 进行了基准测试，并提供了关键发现和观察结果的深入分析。

Conclusion: BenchDepth 提出了一种新的深度模型评估方法，通过五个下游代理任务来评估深度基础模型（DFMs）的实际效用，解决了现有评估协议中存在的偏差和不一致问题。

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [100] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD是一个创新的工业缺陷检测框架，通过双重分布建模和合成数据生成来克服单类异常检测的局限性，并在KSDD2数据集上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 传统的单类异常检测方法在工业缺陷检测中存在局限性，尤其是在处理数据稀缺和均匀离群值分布假设时。本研究旨在提出一种新的框架来克服这些挑战。

Method: ExDD框架通过以下方式运作：1. 显式建模正常和异常模式的双特征分布，利用并行的记忆库来捕捉它们各自的统计特性。2. 运用潜在扩散模型（LDMs）并结合领域特定的文本条件，生成与工业环境相关的、在分布内的合成缺陷，以应对数据稀缺问题。3. 采用邻域感知的比率评分机制，融合互补的距离度量，以增强在偏离正常和相似于已知缺陷模式的区域的信号。

Result: 在KSDD2数据集上的实验验证显示，ExDD框架取得了优越的性能，I-AUROC为94.2%，P-AUROC为97.7%，并且在加入100个合成样本时达到了最佳的增强效果。

Conclusion: ExDD框架通过显式建模双特征分布、利用带有领域特定文本条件的潜在扩散模型生成合成缺陷以及采用邻域感知比率评分机制，克服了单类异常检测的局限性，并在KSDD2数据集上实现了优越的性能。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [101] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion通过生成合成缺陷和双路径特征适应来解决路面缺陷检测中的数据稀疏和域转移问题，并在多个基准测试中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决路面缺陷检测中存在的带标注数据有限、训练和部署环境之间存在域转移以及不同道路条件下缺陷外观变化大的关键挑战。

Method: 提出了一种名为RoadFusion的框架，该框架采用合成异常生成和双路径特征自适应技术来解决数据稀疏、域转移和缺陷外观变化大的问题。具体而言，利用潜在扩散模型生成多样化、逼真的缺陷，并采用两个独立的特征适配器来适应正常和异常输入的表示，最后通过一个轻量级判别器学习区分细粒度的局部缺陷模式。

Result: 在六个基准数据集上进行了评估，RoadFusion在分类和定位任务中均实现了持续强大的性能。

Conclusion: RoadFusion在分类和定位任务中均表现出色，在多个与真实道路检测相关的指标上均达到了新的state-of-the-art水平。

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [102] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: 通过使用人工合成数据集，可以在不损失准确性的情况下，以更低的成本和更高的效率训练出高性能的人类中心计算机视觉模型。


<details>
  <summary>Details</summary>
Motivation: 为了降低训练和推理成本，同时保持甚至提高模型的准确性和鲁棒性。

Method: 使用高保真度的人工合成数据集来训练模型，而非大规模真实数据集。

Result: 在三个密集预测任务（深度估计、表面法线估计和软前景分割）上，使用合成数据训练的模型在真实图像上实现了与基于真实数据训练的同类模型相当的准确性，并且训练和推理效率更高，成本更低。 此外，合成数据提供了更好的数据细节、标签准确性、数据来源、使用权和用户同意，并且能够更好地控制数据多样性以解决模型中的不公平性问题。

Conclusion: 使用人工合成数据训练的模型在三种密集预测任务上达到了与基于真实数据训练的同类模型相当的准确性，同时在训练和推理成本上仅需一小部分。

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [103] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet通过结合语义分割图和面部标志点，并使用创新的模块和损失函数，解决了面部表情识别中的遮挡和偏差问题，达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的面部表情识别（FER）模型在处理部分面部遮挡时难以提取有效的面部特征，导致分类不准确。数据集偏差也是一个挑战。

Method: ORSANet通过引入辅助多模态语义引导（包括语义分割图和面部标志点）、定制多尺度交叉交互模块（MCM）以及动态对抗排斥增强损失（DARELoss）来解决面部表情识别中的遮挡和数据集偏差问题。

Result: ORSANet通过多模态语义引导、多尺度交叉交互模块和动态对抗排斥增强损失，提高了面部表情识别的准确性，特别是在处理遮挡和数据集偏差方面。

Conclusion: ORSANet在Occlu-FER和公开基准的实验表明，其达到了最先进的识别性能。

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [104] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX通过将神经元与概念联系起来，提高了外科手术阶段识别模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在外科手术阶段识别方面的决策过程不透明，这阻碍了信任并使调试模型变得困难。

Method: 提出了一种名为SurgX的新颖的基于概念的解释框架，通过将神经元与相关概念联系起来，增强了外科手术阶段识别模型的可解释性。该框架包括为神经元选择代表性样本序列、构建针对特定外科视频数据集的概念集、将神经元与概念相关联以及识别对预测至关重要的神经元。

Result: 通过在两个外科手术阶段识别模型上进行的大量实验，验证了SurgX方法的有效性，并分析了其对预测的解释。

Conclusion: SurgX可以在解释外科手术阶段识别方面发挥巨大作用。

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [105] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune 是一种用于第一人称视角视频推理的修剪方法，可降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了提高第一人称视角视频推理的效率，以应对第一人称视角视频输入长且冗余的问题，并克服现有为第三人称视频设计的令牌修剪方法在第一人称视角视频中的局限性。

Method: EgoPrune 包含三个组件：1. 关键帧选择器：用于高效的时间采样。 2. 透视感知冗余过滤 (PARF)：通过透视变换对齐视觉令牌并去除冗余。 3. 最大边际相关性 (MMR) 令牌选择器：联合考虑视觉-文本相关性和帧内多样性。

Result: EgoPrune 在两个第一人称视角视频基准测试中，在各种修剪比例下持续优于现有的免训练方法，同时显著降低了计算量、内存使用和延迟。

Conclusion: EgoPrune 是一种针对第一人称视角视频推理的免训练令牌修剪方法，可在保持性能的同时显著降低计算成本，并已成功部署到边缘设备上，证明了其在实际应用中的效率和适用性。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [106] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda 是一种用于预训练视觉语言模型 (VLM) 的新颖微调方法，它通过动态校准融合表示来提高下游性能。


<details>
  <summary>Details</summary>
Motivation: 大多数 VLM 的适应方法侧重于单独调整文本或视觉表示，而忽略了融合表示在决策过程中的关键作用。

Method: RAda 采用轻量级注意力层生成的学习掩码来动态校准最终决策过程中融合表示中每个元素的贡献，从而有针对性地调整最终的跨模态交互，而无需修改中间特征。

Result: RAda 在不同的设置下都表现出色，包括更新或冻结预训练编码器，以及仅访问无标签测试数据的测试时训练。

Conclusion: RAda 是一种多功能微调技术，在大多数情况下都能提高基线性能，并且与当前最先进的技术相当。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [107] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: 由于茂密的植被遮挡了小型线索，自动分析效果不佳，因此启动了一项众包搜索计划。该计划产生了一个独特的数据集，其中包含在被遮挡的真实世界条件下难以检测的标记异常。


<details>
  <summary>Details</summary>
Motivation: 为了在茂密的植被和复杂的森林环境中搜索失踪人员，需要改进异常检测方法。

Method: 本研究使用研究飞机捕获高分辨率航空影像，并通过众包搜索来标记难以检测的异常。

Result: 现有的异常检测方法在该数据集上的表现不佳，凸显了开发上下文感知方法的需求。该数据集可用于离线处理，并且有一个支持在线查看和动态增长的交互式 Web 界面。

Conclusion: 该数据集可作为在复杂的森林环境中改进异常检测方法的基准，支持对失踪人员的搜寻和救援行动。

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [108] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR通过量化和最小化文本模糊性、映射不确定性和帧不确定性来改进文本到视频检索，并通过交互式澄清问题来优化用户查询，从而提高检索精度。


<details>
  <summary>Details</summary>
Motivation: 当前的交互式文本到视频检索系统依赖于启发式或特设策略，未能有效解决文本模糊性、文本-视频映射模糊性和视频帧质量低等问题。

Method: UMIVR框架通过三个基于训练的指标来量化不确定性：基于语义熵的文本模糊性评分（TAS）、基于Jensen-Shannon散度的映射不确定性评分（MUS）以及基于时间质量的帧采样器（TQFS）。该框架利用这些不确定性度量来生成目标性澄清问题，以迭代地优化用户查询。

Result: UMIVR框架在多个基准测试中得到了验证，在MSR-VTT-1k数据集上，经过10轮交互后，Recall@1达到了69.2%，显著提高了检索性能。

Conclusion: UMIVR是一个不确定性最小化的交互式文本到视频检索框架，通过明确量化文本模糊性、映射不确定性和帧不确定性来解决文本到视频检索中的固有不确定性问题。该框架在MSR-VTT-1k数据集上取得了显著的改进，在10轮交互后Recall@1达到了69.2%，为交互式文本到视频检索奠定了不确定性最小化的基础。

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [109] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer是一种用于低光照图像增强的Transformer模型，通过新的光照估计和注意力机制解决了非均匀光照问题，并在多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有Transformer在非均匀光照（如逆光和阴影）下处理不佳，导致过曝或亮度恢复不足的问题。

Method: 提出了一种名为SAIGFormer的框架，该框架包含动态积分图像表示和空间自适应积分光照估计器（SAI²E），以及光照引导多头自注意力（IG-MSA）机制。

Result: SAIGFormer在五个标准低光数据集和一个跨域基准（LOL-Blur）上进行了广泛实验，结果表明SAIGFormer在定量和定性指标上均显著优于现有最先进的方法。

Conclusion: SAIGFormer在非均匀光照增强方面表现出色，并具有强大的跨数据集泛化能力。

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [110] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: 本研究提出了一种新的自监督程序学习方法，通过改进的Gromov-Wasserstein最优传输和对比正则化解决了现有方法的不足，并在多个基准测试中取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 先前的方法在处理程序学习中的顺序变化、背景/冗余帧和重复动作时性能不佳，因为它们通常在确定关键步骤和顺序之前学习帧到帧的对应关系。

Method: 提出了一种自监督程序学习框架，利用融合了Gromov-Wasserstein最优传输和结构先验的公式来计算视频间的帧到帧映射。为防止退化解（所有帧映射到嵌入空间的一个小簇），该框架还整合了一个对比正则化项，将不同的帧映射到嵌入空间的不同点。

Result: 成功克服了现有方法的局限性，并在大规模单目（EgoProceL）和第三人称（ProceL和CrossTask）基准测试中展示了优越的性能。

Conclusion: 该方法在EgoProceL、ProceL和CrossTask基准测试中均展现出优于先前方法（包括基于传统Kantorovich最优传输和最优性先验的OPEL方法）的性能。

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [111] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: 本研究提出了SSG-Com方法和Endoscapes-SG201数据集，以图谱形式更全面地表示手术场景，包含工具-动作-目标和手部身份信息，提升了手术场景理解的准确性。


<details>
  <summary>Details</summary>
Motivation: 以往的手术场景图谱方法在表示手术场景方面存在局限性，未能充分探索工具-动作-目标组合的多样性以及操作工具的手部身份信息，而这些信息对于手术场景理解至关重要。

Method: 本研究提出了一种名为SSG-Com的基于图的方法，用于学习和表示手术场景中的关键元素，包括工具-动作-目标组合和手部身份信息。同时，发布了包含这些注释的Endoscapes-SG201数据集。

Result: 通过在关键安全视野评估和动作三元组识别等下游任务上的实验，证明了整合这些关键场景图组件的重要性，突出了它们对手术场景理解的显著贡献。

Conclusion: 本研究提出的SSG-Com方法通过整合工具-动作-目标组合和手部身份信息，显著提高了手术场景的理解能力，并在关键安全视野评估和动作三元组识别等下游任务中得到了验证。

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [112] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa是一种新的零样本HOI检测方法，它通过低秩分解VLM特征来提高对未见动作的泛化能力和区分度，并在HICO-DET数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本HOI检测方法在泛化到未见动作方面存在挑战，容易混淆涉及相同对象的动作，或者泛化到未见类别的能力有限。因此，需要一种能够同时增强未见类别泛化能力并改善动作区分度的新方法。

Method: HOLa通过低秩分解VLM文本特征，生成类别共享的基线特征和可适应的权重，以增强泛化能力。通过为每个HOI类别调整权重并引入人-物令牌来丰富视觉交互表示，以区分动作。此外，还利用LLM派生的动作正则化来指导权重调整，以进一步区分未见动作。

Result: HOLa在HICO-DET数据集的零样本HOI检测设置上设定了新的最先进水平，在未见动词的设置下，未见类别mAP达到了27.91%。

Conclusion: HOLa通过低秩分解VLM文本特征，生成类别共享的基线特征和可适应的权重，为HOI检测提供了一种新颖的框架，该框架在HICO-DET数据集的零样本HOI检测任务上取得了最先进的性能，特别是在未见动作类别上，mAP达到了27.91%。

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [113] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: DynImg 通过使用非关键帧作为时间提示来改进视频理解，解决了快速移动物体造成的空间信息模糊问题，并在实验中取得了 2% 的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统视频理解方法将空间和时间信息分开处理，在处理运动模糊等问题时，难以精确表示快速移动物体的空间信息，导致时域重要区域在空间特征提取中被低估，影响时空交互和视频理解的准确性。

Method: 提出了一种名为动态图像（DynImg）的创新视频表示方法。该方法引入了一系列非关键帧作为时间提示，以突出包含快速移动对象的空间区域。在视觉特征提取过程中，这些提示引导模型额外关注这些区域对应的细粒度空间特征。为了保持 DynImg 的正确序列，采用了一个相应的 4D 视频旋转位置嵌入，以保留 DynImg 的时间和空间邻近性，帮助多模态大模型理解其时空顺序。

Result: DynImg 方法在多个视频理解基准测试中，性能比现有最佳方法提高了约 2%，证明了其在增强视频理解方面的有效性。

Conclusion: DynImg 方法通过引入非关键帧作为时间提示，有效解决了多模态大模型在视频理解中对时间信息整合的挑战。实验证明，该方法在多个视频理解基准测试中超越了现有技术约 2%，证明了其在增强视频理解能力方面的有效性。

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [114] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix 是一种新颖的图像增强技术，通过使用 GAN 生成更逼真、更符合语义的图像，提高了医学图像分类的性能，尤其是在 COVID-19 检测方面。


<details>
  <summary>Details</summary>
Motivation: Mixup 已成为一种流行的图像分类增强策略，但其朴素的逐像素插值通常会产生不切实际的图像，从而阻碍学习，尤其是在高风险的医疗应用中。

Method: GeMix 是一个两阶段框架，它使用学习到的、标签感知的、由类别条件 GAN 驱动的插值来替代启发式混合。首先，在目标数据集上训练 StyleGAN2-ADA 生成器。在增强期间，从偏向不同类别的 Dirichlet 先验中采样两个标签向量，并通过 Beta 分布系数将它们混合。然后，将生成器条件化在这个软标签上，以合成沿着连续类别流形具有视觉一致性的图像。

Result: 在 COVIDx-CT-3 数据集上使用三种骨干网络（ResNet-50、ResNet-101、EfficientNet-B0）的 GeMix 进行了基准测试。与传统 mixup 相比，我们的方法在所有骨干网络上都提高了宏观 F1 分数，并降低了 COVID-19 检测的假阴性率。

Conclusion: GeMix 作为一种即插即用的替代方法，可以替代像素空间中的 mixup，能够提供更强的正则化和更高的语义保真度，并且不会破坏现有的训练流程。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [115] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: 该研究提出了一种创新的机载变化检测框架，通过将数据压缩、图像配准和变化检测集成在一个端到端的深度学习模型中，解决了卫星实时遥感应用中的延迟问题，并在低功耗硬件上实现了高效的处理。


<details>
  <summary>Details</summary>
Motivation: 为了克服卫星图像变化检测中由于数据下行和地面产品生成延迟而导致的数小时至数天的延迟问题，从而实现近乎实时的应用。

Method: 提出了一种端到端的机载变化检测框架，该框架由三个子模块组成：1. 针对最小化机载数据存储的数据压缩；2. 轻量级的非定轨多时相图像对配准；3. 新型的时间不变且计算高效的变化检测模型。

Result: 实验结果表明，该框架能够克服机载处理的限制，并将整个变化检测工作流程转移到卫星上。与当前最先进的技术相比，每个子模块都取得了良好的效果，并且在F1分数与压缩率的关系方面，在低功耗硬件上实现了0.7 Mpixel/s的吞吐量。

Conclusion: 所提出的框架在低功耗硬件上实现了具有竞争力的变化检测结果（F1分数），并且吞吐量达到了0.7 Mpixel/s（15W加速器），克服了传统卫星图像变化检测的延迟限制。

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [116] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT是一种基于扩散Transformer和Rectified Flow的新型皮肤病变分割模型，在低成本硬件上表现出色，速度快且精度高，适用于实际医疗应用。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分割，特别是皮肤病变分割，对于皮肤癌诊断和患者监测至关重要。

Method: SegDT是一个基于扩散 Transformer（DiT）的新型分割模型，并结合了Rectified Flow以提高生成质量并减少推理步骤，同时保持了标准扩散模型的灵活性。

Result: SegDT在三个基准数据集上取得了最先进的成果，与现有工作相比，实现了快速的推理速度。

Conclusion: SegDT在三个基准数据集上实现了最先进的结果，同时保持了快速的推理速度，使其在实际医疗应用中具有吸引力。

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [117] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种结合SDF和3DGS优点的混合方法，以解决稀疏视图图像表面重建和新视图渲染的挑战，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 表面重建和从稀疏视图图像进行新视图渲染具有挑战性。基于符号距离函数（SDF）的方法难以处理精细细节，而基于3D高斯泼溅（3DGS）的方法缺乏全局几何一致性。

Method: 提出了一种新颖的混合方法，结合了SDF和3D高斯泼溅（3DGS）的优点。SDF用于捕捉粗糙几何以增强3DGS渲染，而3DGS的新渲染图像则用于优化SDF细节以实现精确的表面重建。

Result: 该方法在DTU和MobileBrick数据集上，在表面重建和新视图合成方面均优于现有最先进方法。

Conclusion: 该方法在DTU和MobileBrick数据集上超越了最先进的方法，在表面重建和新视角合成方面取得了更好的效果。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [118] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: CylinderPlane是一种新的3D表示方法，它使用柱坐标系来生成高质量、无伪影的360度视图图像，解决了Tri-plane表示的限制，并且可以轻松集成到现有的神经渲染流程中。


<details>
  <summary>Details</summary>
Motivation: Tri-plane表示在3D感知图像生成模型的发展中取得了进展，但其固有的结构问题，例如由对称区域共享相同特征引起的多面伪影，限制了其生成360度视图图像的能力。

Method: 提出了一种基于柱坐标系的新型隐式表示方法——CylinderPlane，以解决Tri-plane表示中由于共享对称区域特征而引起的多面伪影问题，并确保360度视图下的一致性。通过显式分离不同角度的特征，CylinderPlane能够实现高质量、无伪影的360度视图图像合成。此外，还引入了嵌套气缸体表示，通过组合不同尺度的气缸体，使模型更能适应复杂的几何形状和不同的分辨率，从而有效地捕捉关键位置和多尺度特征，促进细节学习和提高分辨率鲁棒性。

Result: 所提出的CylinderPlane表示方法在合成360度视图图像方面取得了优于先前方法的性能，在合成数据集和非结构化的真实图像上都得到了广泛的实验验证。

Conclusion: 所提出的气缸体表示方法通过利用柱坐标系来解决三平面表示中固有的多面伪影问题，实现了无伪影的360度视图图像合成。嵌套气缸体表示通过组合不同尺度的气缸体，进一步提高了模型处理复杂几何形状和不同分辨率的能力，并能有效地捕获多尺度特征，从而促进细节学习和提高分辨率鲁棒性。该表示方法不依赖于特定的隐式渲染方法，可以轻松集成到任何神经渲染流程中。

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [119] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: 该论文主要梳理和总结了提高视频分析中 DNNs 效率的优化技术，并指出了未来面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）在视频分析中准确率虽高，但效率仍是挑战，现有调研多侧重于准确率优化，忽视了效率优化。

Method: 通过对硬件支持、数据处理、操作部署等多个角度的优化技术进行梳理和总结，构建优化框架。

Result: 对 DNNs 在视频分析中的效率优化技术进行了全面的回顾和分析。   

Conclusion: 现有 DNNs 在视频分析中的效率优化方法有待改进，未来应关注性能优化问题和挑战。

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [120] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: 在古老中世纪音乐手稿的OMR研究中，AL和SL方法可以以更少的人工标注数据达到与全监督训练相当的准确率，但基于不确定性的AL在该手稿上效果不佳。


<details>
  <summary>Details</summary>
Motivation: OMR在文化遗产数字化中很重要，但受限于标注数据稀缺和历史手稿复杂性。

Method: 利用YOLOv8，从单个标注图像开始，通过选择不确定性最高的样本进行迭代标注和重新训练来改进OMR。

Result: AL方法在标注样本显著减少的情况下，实现了与全监督训练相当的准确率。

Conclusion: 不确定性度量的AL在该手稿上效果不佳，需要为数据稀疏场景开发更有效的方法。

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [121] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 深度伪造检测模型可以通过“幸运票假说”进行剪枝，以实现高效部署，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对信息完整性和社会信任构成了重大挑战，而现有的检测方法模型庞大，难以在资源有限的环境中部署。因此，本研究旨在探索LTH在深度伪造检测中的应用，以识别关键特征，并实现高效、轻量级的检测模型。

Method: 研究了将“幸运票假说”（LTH）应用于深度伪造检测，通过迭代幅度修剪（magnitude pruning）方法来识别和提取对检测至关重要的子网络（“幸运票”），并在MesoNet、CNN-5和ResNet-18架构上进行了实验，利用Grad-CAM进行可视化分析。

Result: 在OpenForensic数据集上，MesoNet在80%的稀疏度下仍保持56.2%的准确率，仅使用3000个参数，达到了基线准确率（62.6%）的90%。所提出的LTH方法优于一次性修剪，并且通过Grad-CAM可视化证明了修剪后的网络能有效关注关键面部区域，同时“幸运票”在不同数据集间具有可转移性。

Conclusion: 通过实验证明了深度伪造检测网络中存在“幸运票”，即子网络，即使在高度稀疏的情况下也能保持性能。所提出的基于LTH的迭代幅度修剪方法优于一次性修剪方法，并且修剪后的网络能够将注意力集中在关键面部区域。此外，“幸运票”在不同数据集之间具有可转移性，这为开发高效、可部署的深度伪造检测系统提供了潜力。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [122] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: EVA是一种无需训练的方法，通过提取视觉事实知识来解决多模态大语言模型的幻觉问题，并取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在处理图像中的物体时存在幻觉问题，模型会生成不真实的内容。近期研究表明MLLMs的先验知识会抑制视觉信息，但具体抑制机制尚不明确。本研究旨在解决MLLMs的幻觉问题，并探究其在中间层抑制视觉信息的原因。

Method: EVA（Decoding by Extracting Visual Facts）通过对比原始输入和纯文本输入在所选中间层产生的输出分布差异，提取视觉事实知识，并将其按比例整合到最终层以校正输出logits。该方法具有模型无关性，可与多种解码策略集成。

Result: EVA在广泛使用的基准测试中表现出色，与基线方法相比，显著降低了幻觉率，证明了其在缓解模型幻觉方面的有效性。

Conclusion: EVA是一种新颖的、无需训练的方法，通过动态选择中间层并提取视觉事实知识来校正输出logits，从而显著降低了多模态大语言模型（MLLMs）的幻觉率。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [123] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA是一个新的基准测试，用于提高多语言手写文档的视觉问答能力。


<details>
  <summary>Details</summary>
Motivation: 目前的MLVQA模型在处理各种手写文档时，未能充分发挥其潜力。然而，多语言视觉问答（MLVQA）基准测试的出现，增强了大型语言模型（LLM）和多模态LLM的能力，使它们能够很好地捕捉不同语言中固有的复杂语言细微差别和视觉复杂性。

Method: 通过创建一个名为HW-MLVQA的新型VQA基准测试来解决多语言手写文档理解的不足。该基准测试包含1,600页的手写文档和2,400个问答对，并提供涵盖文本、图像以及文本和图像组合三种模式的评估框架。此外，它还包括一个评估OCR模型的框架，以模拟真实世界中没有地面真实文本转录件的场景。

Result: HW-MLVQA基准测试包含1,600页手写文档和2,400个问答对，并提供三种模式（文本、图像和图像与文本组合）的评估框架。该基准测试还通过评估OCR模型来模拟真实世界场景。

Conclusion: 该基准测试旨在促进多语言手写文档解释的关键进展，并在该专业领域内推动创新和学术探究。

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [124] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: 本研究提出了一种知识蒸馏方法，利用CLIP的IQA知识来训练具有结构优势的模型，以解决CLIP在IQA任务中参数负担重和识别局部特征能力不足的问题。该方法通过设计质量分级提示模板、微调CLIP以及采用模态自适应知识蒸馏策略，在降低模型复杂度的同时提高了IQA性能，具有实际部署潜力。


<details>
  <summary>Details</summary>
Motivation: 为了解决CLIP模型在IQA任务中存在的参数负担过重和识别局部失真特征能力不足的问题。

Method: 本研究提出了一种视觉-语言模型知识蒸馏方法，首先设计了质量分级提示模板来指导CLIP输出质量分数，然后对CLIP进行微调以增强其在IQA任务中的能力，最后提出了一种模态自适应知识蒸馏策略，以实现从CLIP教师模型到学生模型的指导。

Result: 实验结果表明，所提出的方法显著降低了模型复杂度，并且在多个IQA数据集上优于现有的IQA方法。

Conclusion: 该研究提出的方法在多个IQA数据集上进行了实验，结果表明该方法显著降低了模型复杂度，并且优于现有的IQA方法，显示出强大的实际部署潜力。

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [125] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: Hi²-GSLoc利用3D高斯泼溅（3DGS）处理遥感视觉重定位问题，通过稀疏到密集、粗略到精细的双层级框架，结合分区训练、并行匹配和动态内存管理，解决了现有方法的精度、计算复杂性和可扩展性问题，并在多种场景下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临固有的权衡：基于图像检索和姿势回归的方法精度不足，而将查询注册到运动恢复结构（SfM）模型的基于结构的方法则存在计算复杂性和有限的可扩展性问题。由于大规模场景、高海拔变化以及现有视觉先验的域间隙，这些挑战在遥感场景中尤为突出。为了克服这些限制，我们利用3D高斯泼溅（3DGS）作为一种新颖的场景表示，它能够紧凑地编码3D几何和外观。

Method: 我们引入了Hi²-GSLoc，一个双层级重定位框架，它遵循稀疏到密集和粗略到精细的范式，充分利用了高斯图元中固有的丰富语义信息和几何约束。为了处理大规模遥感场景，我们采用了分区高斯训练、GPU加速并行匹配和动态内存管理策略。该方法包括两个阶段：(1)一个稀疏阶段，具有高斯特定的、一致的、渲染感知的采样策略和地标引导检测器，用于稳健和准确的初始姿势估计；(2)一个密集阶段，通过粗略到精细的密集光栅化匹配迭代地优化姿势，同时包含可靠性验证。

Result: Hi²-GSLoc在本地化准确性、召回率和计算效率方面均表现出色，并能有效过滤不可靠的姿势估计。

Conclusion: 通过在模拟数据、公开数据集和真实飞行实验中进行全面评估，我们的方法在本地化准确性、召回率和计算效率方面均具有竞争力，同时能有效过滤不可靠的姿势估计。结果证实了该方法在实际遥感应用中的有效性。

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [126] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 提出LINR-PCGC，一種基於INR的無失真點雲幾何壓縮方法，透過點雲級別編碼和輕量級SparseConv網絡，顯著降低編碼時間和位元流，優於現有方法。


<details>
  <summary>Details</summary>
Motivation: 現有的基於AI的點雲壓縮方法在依賴特定訓練數據分佈方面存在局限性，限制了其在真實世界中的應用。隱式神經表示（INR）方法通過對過擬合的網絡參數進行編碼，可以實現更具分佈無關性的結果，從而解決了上述問題。然而，由於編碼時間和解碼器大小的限制，目前的基於INR的方法僅考慮了有損幾何壓縮。因此，本研究旨在提出一種新的基於INR的無失真點雲幾何壓縮方法。

Method: 該方法提出了一種基於隱式神經表示（INR）的無失真點雲幾何壓縮方法（LINR-PCGC）。為了加速編碼速度，採用了點雲級別編碼框架和有效的網絡初始化策略，可將編碼時間縮短約60%。通過結合尺度上下文提取、子節點預測和模型壓縮模塊，設計了一個輕量級的基於多尺度稀疏卷積（SparseConv）的編碼網絡，以實現快速推理和緊湊的解碼器大小。

Result: 所提出的LINR-PCGC方法在無失真點雲幾何壓縮方面取得了顯著成果，其性能優於傳統和現有的基於AI的壓縮方法。具體來說，在MVUB數據集上，與G-PCC TMC13v23相比，位元流減少了約21.21%；與SparsePCGC相比，位元流減少了約21.95%。

Conclusion: 所提出的LINR-PCGC方法在無失真點雲幾何壓縮方面優於傳統和基於AI的方法，在MVUB數據集上與G-PCC TMC13v23相比，位元流減少了約21.21%，與SparsePCGC相比減少了21.95%。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [127] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS通过小波空间损失和自监督稀疏性，在稀疏视角3D高斯泼溅中实现更高质量的新视角重建，优于傅立叶方法。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视角3D高斯泼溅（3DGS）中由于对训练视角的高频（HF）细节过度拟合而导致的高质量新视角重建的挑战，并克服了传统基于傅立叶变换的频率正则化方法在参数调整和有害HF学习方面的缺陷。

Method: 提出了一种名为DWTGS的框架，该框架通过利用小波空间损失来重新思考频率正则化。具体来说，它在多个离散小波变换（DWT）级别上仅监督低频（LF）LL子带，并在高频（HF）HH子带上以自监督方式强制执行稀疏性。

Result: 实验证明，DWTGS在基准测试中始终优于基于傅立叶变换的对应方法，其低频优先策略提高了泛化能力并减少了高频幻觉。

Conclusion: DWTGS框架通过利用小波空间损失在多个DWT级别上仅监督低频（LF）LL子带，并在高频（HF）HH子带上以自监督方式强制执行稀疏性，从而在稀疏视角3D高斯泼溅（3DGS）中实现了高质量的新视角重建，优于基于傅立叶的方法，提高了泛化能力并减少了高频幻觉。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [128] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种计算效率高的人脸图像质量评估方法，通过自训练和知识蒸馏，在保持高性能的同时显著降低了计算复杂度，并在竞赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 为了解决人脸图像质量评估（FIQA）算法的计算复杂性问题，以确保其在实际系统中的可扩展性和部署可行性。

Method: 该方法包括两个阶段：首先训练一个强大的教师模型，然后从中蒸馏出一个轻量级的学生模型。为了构建强大的教师模型，采用了自训练策略，先用标记数据训练教师模型，然后用教师模型生成的伪标签来增强教师模型和蒸馏学生模型。

Result: 实验结果表明，学生模型在计算开销极低的情况下，实现了与教师模型相当的性能，并在 ICCV 2025 VQualA FIQA 挑战赛中取得优异成绩。

Conclusion: 所提出的学生模型在计算开销极低的情况下，实现了与教师模型相当的性能。该方法在 ICCV 2025 VQualA FIQA 挑战赛中获得第一名。

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [129] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 该研究旨在通过受控实验来解决图像生成空间控制领域的现有问题，并为从业者提供清晰的指导。


<details>
  <summary>Details</summary>
Motivation: 为了更好地通过边缘图、姿态等对图像生成模型进行空间控制，解决现有研究中缺乏详细和公平的科学比较、难以区分性能因素以及某些方法的动机和细微差别被忽略的问题。

Method: 通过在 ImageNet 上针对基于扩散/流和自回归（AR）的模型进行受控实验，研究了控制令牌预填充、采样时间增强（如扩展 classifier-free guidance 和 softmax 截断）以及 adapter 方法。

Result: 控制令牌预填充是一种简单、通用且性能良好的 Transformer 基线方法。扩展 classifier-free guidance 和 softmax 截断可以提高控制-生成的一致性。Adapter 方法可以缓解“遗忘”问题并保持生成质量，但与完全训练相比，在生成-控制一致性方面表现不佳。

Conclusion: 控制令牌预填充是一种简单、通用且性能良好的 Transformer 基线方法。扩展 classifier-free guidance 和 softmax 截断可以提高控制-生成的一致性。Adapter 方法可以缓解“遗忘”问题并保持生成质量，但与完全训练相比，在生成-控制一致性方面表现不佳。

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [130] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen通过将长视频分解为多个短片段，并利用文本和视频tokens进行生成，有效解决了长视频生成中的不一致性和内存问题，实现了高质量、长时序连贯的视频生成。


<details>
  <summary>Details</summary>
Motivation: 长视频生成是一个复杂挑战，现有的基于扩散生成模型虽然能生成视觉上令人印象深刻的短视频片段，但在延长视频时长时，常常会遇到内存瓶颈和长期不一致性问题。

Method: TokensGen是一个新颖的两阶段框架，它利用凝练的tokens来解决长视频生成中的挑战。该方法将长视频生成分解为三个核心任务：(1) 内部片段语义控制，(2) 长期一致性控制，以及 (3) 片段间平滑过渡。首先，训练了一个名为To2V（Token-to-Video）的短视频扩散模型，该模型由文本和视频tokens指导，并使用一个视频tokenizer将短视频片段凝练成语义丰富的tokens。其次，引入了一个名为T2To（Text-to-Token）的视频token扩散transformer，该模型一次性生成所有tokens，确保了跨片段的全局一致性。最后，在推理过程中，采用自适应的FIFO-Diffusion策略无缝连接相邻的片段，减少了边界伪影并增强了片段间的平滑过渡。

Result: 实验结果表明，与现有方法相比，TokensGen在不增加巨大计算成本的情况下，显著提升了长视频在时间维度和内容方面的一致性。

Conclusion: TokensGen通过利用凝练的视频tokens，显著提高了长视频生成在长期时间一致性和内容连贯性方面的表现，同时没有带来高昂的计算开销。该方法提供了一个可扩展、模块化的长视频生成解决方案，为故事叙述、电影制作和沉浸式模拟等领域开辟了新的可能性。

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [131] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 提出一种基于Transformer的方法，通过预测双边网格来校正多视图光度变化，从而提高3D高斯泼溅的重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现代相机管线中的多视图光度不一致性问题，该问题源于广泛的设备内处理，并影响新视图合成的质量。

Method: 提出一种基于Transformer的方法，预测空间自适应双边网格来校正光度变化，并将其应用于3D高斯泼溅管线。

Result: 提出的方法在重建保真度和收敛速度方面优于或匹配现有的特定场景优化方法。

Conclusion: 该方法通过使用基于Transformer的方法预测空间自适应双边网格来校正多视图一致性的光度变化，实现了跨场景的稳健泛化，无需进行特定场景的重新训练。将学习到的网格融入3D高斯泼溅管线，在保持高训练效率的同时提高了重建质量。

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [132] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: HDF框架通过DAM和DSM模块有效解决了DFER中的样本异质性问题，提高了准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的动态面部表情识别（DFER）方法在处理多源数据和个体表情变异性引起的样本异质性时，性能会下降。

Method: 提出了一种名为HDF（Heterogeneity-aware Distributional Framework）的新框架，并设计了两个即插即用的模块：时间-频率分布注意力模块（DAM）和分布感知缩放模块（DSM）。DAM通过双分支注意力设计捕捉时序一致性和频率鲁棒性，提高对序列不一致和视觉风格变化的容忍度。DSM基于梯度敏感性和信息瓶颈原理，动态平衡分类和对比损失，实现更稳定和具辨别力的表示学习。

Result: HDF框架在DFEW和FERV39k数据集上进行了广泛实验，结果表明该框架显著提高了识别准确性和鲁棒性，实现了优越的加权平均召回率（WAR）和非加权平均召回率（UAR），并在多样和不平衡场景下保持了强大的泛化能力。

Conclusion: HDF框架通过DAM和DSM模块显著提高了动态面部表情识别的准确性和鲁棒性，在DFEW和FERV39k数据集上取得了优越的WAR和UAR，并在多样和不平衡场景下保持了良好的泛化能力。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [133] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: This paper introduces novel tree-based semantic loss functions for medical image segmentation that improve accuracy by considering label hierarchies and sparse annotations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current learning methods for medical image segmentation penalize all errors equally, failing to leverage inter-class semantics, especially with increasing label complexity. This work addresses this limitation.

Method: Two tree-based semantic loss functions are proposed, incorporating a hierarchical organization of labels. These losses are integrated into a recent approach for training with sparse, background-free annotations.

Result: Extensive experiments on head MRI and neurosurgical hyperspectral imaging demonstrate that the proposed method reaches state-of-the-art performance in both segmentation tasks.

Conclusion: Our proposed method achieves state-of-the-art performance in both head MRI for whole brain parcellation and neurosurgical hyperspectral imaging for scene understanding.

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [134] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 本研究提出了一种新的医学图像分割方法，通过动态调整秩来改进LoRA。该方法使用l_1稀疏性正则化器和近端优化器，能够在少样本情况下自动找到适合任务的秩，并在实验中显示出优于其他方法的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法（如LoRA）在医学影像领域虽然有效且计算效率高，但其秩固定且不可改变，这给选择合适的秩带来了挑战。本研究旨在解决这一问题，提出一种能够动态调整秩的方法，以适应不同医学影像下游任务的复杂性和需求。

Method: 受自然图像处理的进展启发，该方法在低秩适应（LoRA）的基础上进行了创新，引入了一个l_1稀疏性正则化器到损失函数中，并采用近端优化器来处理，从而在适应过程中动态调整内在秩。

Result: 通过在现实的少样本微调场景下，在基础器官和新器官分割两个不同的任务上进行评估，并与标准LoRA及其他PEFT方法进行比较，实验结果表明，该方法能够显著提升性能，并展现出其高效性和对次优秩初始化的鲁棒性。

Conclusion: 该研究提出了一种新颖的动态调整秩的方法，在医学图像分割任务中，通过引入l_1稀疏性正则化器到损失函数中，并使用近端优化器进行处理，从而自动调整秩。实验证明，该方法在少样本微调设置下，相比标准LoRA和其他PEFT方法，在基础器官和新器官分割任务中均取得了显著的性能提升，证明了其效率和对次优秩初始化的鲁棒性。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [135] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: 本研究提出了一种名为NoDepth Bottleneck的新型低参数神经网络架构，通过减少特征图干扰来提高效率和准确性，并在ImageNet数据集上取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决计算机视觉中低参数深度神经网络（参数量低于150万）的性能问题，特别是瓶颈架构和超线性激活函数的使用，以及特征图干扰（神经元同时编码多种特征）的现象。

Method: 研究通过检查各种瓶颈架构，确定了能够减少特征图干扰的关键设计元素，并提出了一个名为NoDepth Bottleneck的概念验证架构。

Result: 研究表明，限制特征图干扰可以提高非常低参数量（150万以下）网络的扩展性和准确性。提出的NoDepth Bottleneck架构在ImageNet数据集上进行了验证。

Conclusion: 本研究提出的NoDepth Bottleneck架构在ImageNet数据集上表现出稳健的扩展精度，为低参数范围内的网络提供了更高效、可扩展的选择，并加深了对计算机视觉中瓶颈机制的理解。

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [136] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本研究提出ConformalSAM框架，利用校准过的基础分割模型（SEEM）和共形预测（CP）来生成可靠的像素级标签，用于半监督语义分割，解决了标签稀疏性问题，并在实验中取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 半监督语义分割（SSSS）旨在通过利用标记和未标记数据来减轻标签获取的成本。基础分割模型在处理像素级视觉任务时展现出跨域泛化的潜力。本研究旨在探索基础分割模型能否作为标注器来解决像素级视觉任务中的标签稀疏性问题。

Method: 提出了一种名为ConformalSAM的新型半监督语义分割框架，该框架首先利用目标域的标记数据校准基础模型，然后过滤掉不可靠的像素标签，只使用高置信度标签作为监督。该框架利用共形预测（CP）来适应基础模型，并在早期训练阶段利用基础模型的强大能力，随后的自Reliance训练策略则缓解了后期对SEEM生成掩码的过拟合。

Result: 在三个标准的半监督语义分割基准测试中，ConformalSAM取得了优于近期方法的性能，并可作为插件提升其他方法的性能。

Conclusion: ConformalSAM通过利用基于共形预测的校准方法，成功地利用了基础分割模型（如SEEM）来解决标签稀疏性问题，在三个标准的半监督语义分割基准测试中取得了优于近期方法的性能，并可作为插件提升其他方法的性能。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [137] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 研究通过DARA微调策略和TrueMICL数据集，解决了MLLMs在MICL中忽视视觉信息的问题，提升了真正的多模态适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在多模态语境学习（MICL）中存在忽视视觉信息、过度依赖文本模式的问题，导致其MICL能力本质上仍是单模态的，限制了其实际应用。并且，这种局限性在不要求理解视觉语境的任务上会被改进的性能所掩盖，使得如何有效提升MICL能力和可靠评估MICL性能成为一个未被充分探索的领域。

Method: 提出了一种名为动态注意力重新分配（DARA）的有效微调策略，通过重新平衡视觉和文本标记之间的注意力来鼓励模型关注视觉语境。此外，还构建了一个名为TrueMICL的专用数据集，包含支持集和测试集，明确要求模型整合多模态信息（特别是视觉内容）以完成任务。

Result: 实验证明，所提出的DARA策略和TrueMICL数据集的组合能够有效提升MLLMs在真正的多模态语境学习能力，取得了显著的进步。

Conclusion: 该研究通过引入动态注意力重新分配（DARA）和TrueMICL数据集，有效解决了多模态大语言模型（MLLMs）在多模态语境学习（MICL）中忽视视觉信息的问题，显著提升了模型真正的多模态适应能力。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [138] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 扩散模型在多元地下建模和概率反演方面比VAE和GAN具有优势。本研究提出了一种改进的扩散模型，通过考虑噪声污染来提高似然近似，并使用测井和地震数据进行了验证，结果显示出更好的统计鲁棒性、后验采样和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 在多元地下建模和概率反演的背景下，考虑扩散模型的使用。

Method: 提出对Chung等人（2023）提出的扩散后验采样方法进行不同的校正，特别是引入了考虑扩散模型固有噪声污染的似然近似。

Result: 与原始方法相比，在多元地质情景中，在统计鲁棒性、后验概率密度函数的增强采样和降低计算成本方面，显示出显著的改进。尤其是在使用局部硬数据（测井）和非线性地球物理（全堆叠地震数据）进行条件建模时。

Conclusion: 该方法可单独或同时与硬数据和间接条件数据一起使用。由于反演包含在扩散过程中，因此比需要围绕生成模型设置外部循环（如马尔可夫链蒙特卡洛）的其他方法更快。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [139] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench是一个新的基准，用于评估文本到视频模型在物理常识方面的能力，通过间接评估方法解决现有评估中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到视频（T2V）生成模型在基本物理常识方面存在不足，生成的视频常常违反因果关系、物体行为和工具使用的直观预期。

Method: 提出PhysVidBench基准，包含383个精心策划的提示，重点关注工具使用、材料属性和程序交互。采用三阶段评估流程：1.从提示中制定基于物理学的จัยết câu hỏi；2.使用视觉语言模型为生成的视频添加标题；3.要求语言模型仅根据标题回答几个涉及物理学的问题，以规避直接视频评估中的常见幻觉问题。

Result: 通过PhysVidBench的评估，可以揭示T2V模型在物理推理能力方面的具体短板，并为未来改进这些模型提供方向。

Conclusion: PhysVidBench提供了一个评估生成视频模型物理常识能力的结构化、可解释的框架，重点关注当前T2V评估中被忽视的工具使用和工具介导的动作，有助于解决T2V模型在基本物理常识方面的不足。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [140] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: SeC框架利用大型视觉语言模型（LVLMs）进行概念理解，以提升视频对象分割（VOS）能力，特别是在处理视觉变化和遮挡方面。该研究还提出了SeCVOS基准来评估这类模型，并展示了SeC相比现有方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前视频对象分割（VOS）技术在处理剧烈的视觉变化、遮挡和复杂的场景变化方面仍落后于人类能力，这是因为它们依赖外观匹配，忽视了人类在对象识别中使用的、能够实现跨时间动态的鲁棒识别的概念理解能力。

Method: 提出了一种名为Segment Concept (SeC) 的概念驱动分割框架，该框架从传统的特征匹配转向渐进式构建和利用高层、以对象为中心表示。SeC使用大型视觉语言模型（LVLMs）整合跨帧的视觉线索，构建概念先验，并在推理过程中形成全面的语义表示，以实现对后续帧的鲁棒分割。此外，SeC还能自适应地平衡基于LVLM的语义推理和增强的特征匹配。

Result: SeC在SeCVOS基准上取得了11.8分的提升，超过了SAM 2.1，确立了概念感知视频对象分割的新技术水平。SeCVOS基准包含160个手动标注的多场景视频，旨在通过显著的外观变化和动态场景转换来挑战模型。

Conclusion: SeC通过利用大型视觉语言模型（LVLMs）来整合跨帧的视觉线索，构建鲁棒的概念先验，并在此基础上进行目标分割，在SeCVOS基准上实现了11.8分的提升，建立了新的概念感知视频对象分割技术水平。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 通过将分词器嵌入与去噪目标对齐，l-DeTok 提高了生成模型的性能，这表明去噪是分词器设计的一个关键原则。


<details>
  <summary>Details</summary>
Motivation: 为了找出使视觉分词器在生成模型中更有效的属性，并利用现代生成模型中重建干净信号的相似训练目标。

Method: 提出了一种名为潜在去噪分词器（l-DeTok）的方法，该方法通过将分词器嵌入与下游去噪目标对齐来训练分词器，以从损坏的潜在嵌入中重建干净的图像。

Result: 在 ImageNet 256x256 上进行的大量实验表明，l-DeTok 在六种代表性生成模型上持续优于标准分词器。

Conclusion: l-DeTok 优于标准分词器，表明去噪是分词器开发的基本原则。

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [142] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter是一个可定制的多模态长篇写作助手，它利用精心策划的离线知识库，通过任务分解、大纲生成、多模态检索和分段撰写与反思的流程，克服了现有LLM在专业领域写作中的局限性，并取得了高质量、事实准确的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM写作助手在金融、医学、法律等专业领域存在知识不足和幻觉问题。现有的RAG方法可能存在多步检索不一致的问题，而在线搜索方法可能因网络内容不可靠而降低质量。为解决这些挑战，需要一个能够处理专业领域文档的写作助手。

Method: DeepWriter通过新颖的流程，包括任务分解、大纲生成、多模态检索以及分段撰写与反思，利用结构化语料库中的文本和视觉信息，并结合分层知识表示来提高检索效率和准确性。

Result: DeepWriter能够生成连贯、事实准确且专业的长篇文档，并且在金融报告生成任务上取得了超越现有基线的效果。

Conclusion: DeepWriter在金融报告生成方面表现出色，生成的文章质量高且可验证，在事实准确性和生成内容质量方面均超越了现有基线。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [143] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 模型编辑后的知识在微调时容易丢失，冻结相关层可提高保留率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要更新以纠正事实错误、整合新信息或调整行为。模型编辑方法提供了一种高效的解决方案。然而，微调对已编辑知识的影响尚不清楚。

Method: 系统地研究了不同微调目标与不同模型编辑技术之间的相互作用，并通过实验验证了冻结层可以提高知识保留率。

Result: 编辑后的知识比预训练期间获得的内在知识更容易在微调过程中被遗忘。冻结与编辑内容相关的层可以显著提高知识保留率。

Conclusion: 模型编辑方法对于更新大型语言模型（LLMs）的知识非常有效，但编辑后的知识在下游任务的微调过程中容易被遗忘。冻结与编辑内容相关的层可以提高知识保留率。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [144] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: 该研究提出了一种名为SMACS的系统，该系统通过整合多个开源LLM，在性能上超越了顶级的闭源LLM，并推高了LLM能力的上限。


<details>
  <summary>Details</summary>
Motivation: 旨在展示开源集合的潜力和优势，并探索能否通过整合多个开源大型语言模型（LLM）来匹配甚至超越闭源LLM的性能。

Method: 提出了一种名为SMACS的可扩展多智能体协作系统（MACS）框架。该框架包含两个关键组件：1. 检索式先验选择（RPS），用于为每个LLM分配代理性能得分，以便在实例级别为任何给定问题选择前k个LLM。2. 探索-利用驱动的后验增强（EPE），通过先验丢弃鼓励生成多样化的响应，并通过混合后验得分选择高质量响应。

Result: SMACS通过整合15个开源LLM，在八个主流基准测试中，超越了Claude-3.7-Sonnet (+12.73%)、GPT-4.1(+5.36%)和GPT-o3-mini(+5.28%)等领先的闭源LLM，在多个任务上表现出优越性。其平均结果甚至超过了不同数据集上开源LLM的最佳结果平均值(+2.86%)和闭源LLM的最佳结果平均值(+2.04%)。

Conclusion: SMACS通过整合15个开源LLM，在八个主流基准测试中表现出色，超越了领先的闭源LLM（如Claude-3.7-Sonnet、GPT-4.1和GPT-o3-mini），并在多个任务上取得了显著的性能提升。此外，SMACS的平均结果甚至超过了开源和闭源LLM的最佳结果的平均值，推动了智能的上限。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [145] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer 是一个神经符号系统，利用 NLP 和逻辑推理来分析隐私政策，帮助用户了解其数据如何被使用。它在识别数据使用实践方面准确率很高，并且能够分析用户偏好与常见网站的隐私政策的合规性，从而减轻用户的认知负担并促进对平台数据实践的讨论。


<details>
  <summary>Details</summary>
Motivation: 鉴于用户通常不会阅读服务条款或隐私政策，该研究旨在开发一个名为 PoliAnalyzer 的系统，以帮助用户进行个性化的隐私政策分析。

Method: PoliAnalyzer 使用神经符号系统，结合自然语言处理（NLP）从政策文本中提取正式的数据使用实践表示，并应用确定性逻辑推理来比较用户偏好和正式隐私政策表示，以生成合规性报告。该方法扩展了现有的正式数据使用条款政策语言，将隐私政策建模为应用程序策略，将用户偏好建模为数据策略。

Result: PoliAnalyzer 在使用由法律专家策划的 PolicyIE 数据集进行评估时，在识别相关数据使用实践方面表现出高准确性，在大多数任务中达到了 90-100% 的 F1 分数。此外，该系统能够对 23 个用户配置文件和访问量排名前 100 的网站进行合规性分析，发现平均 95.2% 的隐私政策片段与用户偏好无冲突，从而使用户能够专注于理解剩余 4.8% 的冲突内容。研究还发现了一些常见的侵犯用户期望的隐私政策做法，例如与第三方共享位置数据。

Conclusion: 该研究表明，PoliAnalyzer 可以利用现成的 NLP 工具支持大规模的自动化个性化隐私政策分析，从而帮助个人重新掌控他们的数据，并促进社会对平台数据实践的讨论，以促进更公平的权力动态。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [146] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 本研究探索使用先进的 NLP 模型（Transformer 和 LSTM）通过社交媒体文本检测双相情感障碍。结果显示，使用 BERT 嵌入的 RoBERTa 和 LSTM 模型表现最佳（F1 分数约 98%），而静态嵌入的模型表现不佳。DistilBERT 在效率和准确性方面表现均衡。该研究强调了上下文语言模型在早期筛查双相情感障碍中的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了应对双相情感障碍早期症状不明显和污名化导致的诊断不足问题，本研究旨在探索使用先进的自然语言处理（NLP）模型来识别用户在社交媒体文本中表现出的双相情感障碍迹象。

Method: 使用基于上下文（BERT）和静态（GloVe、Word2Vec）词嵌入的 Transformer 模型（BERT、RoBERTa、ALBERT、ELECTRA、DistilBERT）和长短期记忆（LSTM）模型，对 Reddit 帖子的大型、带注释数据集进行评估。

Result: RoBERTa 在 Transformer 模型中取得了最高的性能（F1 分数约为 98%），而使用 BERT 嵌入的 LSTM 模型也取得了几乎相同的结果。相比之下，在静态嵌入上训练的 LSTM 模型未能捕捉到有意义的模式，F1 分数接近于零。表明上下文语言建模对于检测双相情感障碍至关重要。

Conclusion: 该研究验证了上下文语言模型在检测双相情感障碍方面的潜力，并为心理健康 NLP 应用中的模型选择提供了可行的见解。DistilBERT 在效率和准确性之间取得了最佳平衡。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [147] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: LLMs show significant bias based on race, gender, and age in applications like healthcare and job seeking, leading to discriminatory outcomes. The study recommends careful evaluation before deployment.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs use identity information inferred from user text in their decision-making in real-world applications, given their increasing use in sensitive areas.

Method: The study analyzed how identity markers in user writing influence LLM responses across five high-stakes applications: medicine, law, politics, government benefits, and job salaries. They developed tools to evaluate the impact of subtle identity encodings on model decisions.

Result: LLMs showed significant bias based on race, gender, and age. Examples include differential standards of care in medicine based on ethnicity, political bias influenced by age, and lower salary recommendations for non-White applicants and higher recommendations for women compared to men.

Conclusion: LLMs are highly sensitive to identity markers in user queries, leading to biased responses across various high-stakes applications. This can result in discriminatory outcomes in healthcare, employment, and politics. The study recommends thorough assessments before deploying LLMs in user-facing roles.

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [148] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 本文介绍了一个内部聊天机器人，它使用知识图谱和 Text-to-SQL 代理来帮助领英的员工自助获取数据洞察，并展示了其在实际应用中的效果和关键技术组件。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在 Text-to-SQL 基准测试上取得了快速进展，但在构建实际的企业级解决方案方面仍存在挑战。本文旨在分享构建内部聊天机器人的经验，使领英的产品经理、工程师和运营团队能够从大型动态数据湖中自助获取数据洞察。

Method: 本文提出了一种包含三个组成部分的方法：1. 构建知识图谱，通过索引数据库元数据、历史查询日志、维基和代码来捕获最新语义，并应用聚类来识别每个团队或产品区域的相关表。2. 构建 Text-to-SQL 代理，用于检索和排序知识图谱中的上下文、编写查询并自动纠正幻觉和语法错误。3. 构建交互式聊天机器人，支持从数据发现到查询编写再到调试的各种用户意图，并使用丰富的 UI 元素显示响应以鼓励后续对话。

Result: 该聊天机器人每周拥有超过 300 名用户。专家审查显示，在一个内部基准测试集中，其 53% 的响应是正确的或接近正确的。

Conclusion: 本论文提供了一个开发企业级 Text-to-SQL 解决方案的实用方法，通过消融研究确定了关键的知识图谱和模型组件。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [149] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: CCL-XCoT通过课程对比学习和跨语言思维链提示，有效解决了多语言大语言模型在低资源语言中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型（MLLMs）在低资源语言中容易出现幻觉，这主要是由于训练数据不平衡所致，尤其在领域特定生成任务中问题更为严重。

Method: 提出了一种名为CCL-XCoT的两阶段微调框架，首先通过结合课程对比学习和下一个词预测来增强跨语言语义对齐，然后通过跨语言思维链（XCoT）提示策略指导模型在指令微调期间先用高资源语言推理，再用目标低资源语言生成答案。

Result: 实验结果表明，CCL-XCoT框架能够将幻觉率降低高达62%，并显著提高跨语言对的事实知识迁移能力。

Conclusion: CCL-XCoT框架通过结合课程对比学习和跨语言思维链提示，在低资源语言的MLLMs中显著减少了幻觉（高达62%），并提高了事实知识的跨语言迁移能力，且无需外部检索或多模型集成。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [150] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 本研究通过构建LLM供应链图谱，揭示了模型与数据集之间复杂的相互关系及其动态演变规律，为理解和管理LLM生态系统提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的开发、训练和部署需要大量计算资源和数据集，并且LLM可能继承其基础模型或数据集中的漏洞、偏见或恶意组件，因此理解这些组件的来源和发展对于检测风险、提高模型公平性和确保合规性至关重要。本项目旨在研究作为LLM供应链核心组成部分，模型与数据集之间的关系。

Method: 通过设计方法系统地收集LLM供应链数据，并构建了一个包含397,376个节点和453,469条边的有向异构图来模拟模型与数据集之间的关系。

Result: 分析结果显示，LLM供应链图谱规模庞大、稀疏且呈幂律分布；存在一个密集连接的核心和碎片化的外围；数据集在训练中起着关键作用；模型与数据集之间存在强烈的相互依赖性；并且该图谱是动态的，每日更新反映了生态系统的持续演变。

Conclusion: LLM供应链图谱具有规模大、稀疏、遵循幂律分布、核心密集连接且外围碎片化、数据集发挥关键作用、模型与数据集相互依赖以及动态演化等特征。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [151] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix automates prompt engineering for LLMs, making it accessible and efficient for everyone.


<details>
  <summary>Details</summary>
Motivation: Prompt engineering is manual, inconsistent, and inaccessible to non-experts, despite LLMs performing best with well-crafted prompts.

Method: Promptomatix utilizes a lightweight meta-prompt-based optimizer and a DSPy-powered compiler. It analyzes user intent, generates synthetic training data, selects prompting strategies, and refines prompts using cost-aware objectives.

Result: Promptomatix transforms natural language task descriptions into high-quality prompts without manual tuning or domain expertise, achieving competitive or superior performance with reduced prompt length and computational overhead.

Conclusion: Promptomatix is a scalable and efficient automatic prompt optimization framework that achieves competitive or superior performance compared to existing libraries across 5 task categories, while reducing prompt length and computational overhead.

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [152] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope 是一个针对各种图表类型的深入图表理解进行了优化的 LVLM。它使用数据生成流水线和双路径训练策略，以更好地理解和推理图表数据。还引入了一个名为 ChartDQA 的新基准来评估这些能力。


<details>
  <summary>Details</summary>
Motivation: 现有的定制大型视觉语言模型（LVLM）以用于领域特定任务的方法在科学图表理解方面显示出有希望的结果，但存在局限于少数图表类型和缺乏针对图表数据对齐的预训练的局限性。

Method: 提出了一种高效的数据生成流程，可为多种图表类型合成配对数据；提出了一种新颖的双路径训练策略，使模型能够简洁地捕捉关键数据细节，同时通过整合对底层数据的推理来保留强大的推理能力。

Result: 实验结果表明，ChartScope 在各种图表类型上显著增强了理解能力。

Conclusion: ChartScope 在各种图表类型上显著增强了理解能力。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [153] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: 本研究分析了德国推特用户关于热门议题的推文，发现冲突叙事通过行为体角色归因和事件情节安排的差异来体现政治极化，并揭示了政治行为者利用叙事协调策略来统一跨议题的观点。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在揭示人类如何通过政治叙事来理解政治现实，并探讨冲突叙事在两极分化和议题关联中的作用。通过分析德国推特用户在2021年至2023年间的推文，本研究试图为理解政治极化和议题关联的论述机制提供新的见解。

Method: 本研究通过分析2021年至2023年德国推特用户关于乌克兰战争、新冠疫情和气候变化等热门议题的推文，提取对立观点的文本信号，以分析冲突叙事的论述维度。研究重点关注了两个维度：(i)对同一行为体的不同行为体角色的归因（例如，对北约在乌克兰战争中作用的不同解读）；(ii)对同一事件的不同行为体的情节安排（例如，比尔·盖茨在右翼新冠叙事中的作用）。此外，研究还提供了政治行为者为跨议题协调意见而使用的叙事协调策略的初步证据。

Result: 本研究发现了冲突叙事在解释政治现实方面的关键作用，并为理解政治极化和议题关联的论述机制提供了证据。研究结果显示，冲突叙事体现在行为体角色归因和事件情节安排的差异上，并首次证明了叙事协调策略的存在，这种策略被政治行为者用于协调跨议题的意见。

Conclusion: 本研究展示了如何通过分析冲突叙事来揭示公共领域中两极分化和议题关联的论述机制。研究结果表明，冲突叙事在解释政治现实方面发挥着关键作用，并为理解政治极化提供了新的视角。

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [154] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 为了解决多语言 LLM 在低资源语言上的性能差距问题，本文研究了一种 LLM 驱动的选择性翻译方法，该方法可以保留文本中的非翻译内容和句子结构。实验表明，该方法在印地语上是有效且实用的，并且通过混合翻译数据和原始英文数据进行训练可以进一步提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型 (LLM) 在非英语语言（尤其是低资源语言）上存在性能差距。虽然英文对齐数据集易于获取，但在其他语言中整理同等质量的数据成本高昂且耗时。标准的翻译技术往往无法保留代码、数学表达式和 JSON 等结构化格式的关键元素。

Method: 研究了 LLM 驱动的选择性翻译技术，该技术选择性地仅翻译文本中可翻译的部分，同时保留不可翻译的内容和句子结构。通过实验比较了谷歌云翻译 (GCP) 和 Llama-3.1-405B 生成的翻译效果，并研究了过滤噪声输出和混合翻译样本与原始英文数据的重要性。

Result: 选择性翻译显示出改善低资源语言 LLM 对齐的潜力，并且混合翻译样本与原始英文数据对齐可以带来额外的好处。

Conclusion: LLM 驱动的选择性翻译是一种实用且有效的方法，可以改善 LLM 在低资源语言（例如印地语）上的多语言对齐效果。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [155] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 本文探讨了在资源受限环境中部署和维护AI模型以实现AI for Good影响的挑战和经验，强调了与合作伙伴密切协作的重要性。


<details>
  <summary>Details</summary>
Motivation: AI for Good领域的论文往往侧重于研究和模型开发，但很少关注实际部署、协作过程和真实世界的影响，因此本文旨在弥补这一空白。

Method: 通过与人道主义组织（H2H）的密切合作，分享在资源受限环境中部署和维护AI模型的细节。

Result: 本文分享了在资源受限环境中部署和维护AI模型的具体细节，并为实践者提供了关键经验。

Conclusion: AI模型在资源受限环境中的部署和维护对于产生实际影响至关重要，AI for Good领域的论文应更关注部署和协作过程。

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [156] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs process narrative aspect differently from humans, showing limitations in understanding due to reliance on patterns rather than true comprehension. A new framework for assessing LLMs is also proposed.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs' sophisticated linguistic capabilities reflect human-like cognition or advanced pattern recognition, specifically by examining how they process the temporal meaning of linguistic aspect in narratives.

Method: The study utilized an Expert-in-the-Loop probing pipeline with targeted experiments to assess LLMs' semantic representations and pragmatic inferences in narratives, comparing their processing of temporal meaning and linguistic aspect to human studies.

Result: LLMs over-rely on prototypicality, exhibit inconsistent aspectual judgments, and face difficulties with causal reasoning derived from aspect, indicating a different processing mechanism compared to humans and a lack of robust narrative understanding.

Conclusion: LLMs' processing of linguistic aspect in narratives differs fundamentally from humans, lacking robust narrative understanding due to over-reliance on prototypicality, inconsistent judgments, and struggles with causal reasoning.

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [157] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: 该研究创建了一个克罗地亚点击诱饵数据集，并发现微调模型在检测点击诱饵方面优于通用 LLM。


<details>
  <summary>Details</summary>
Motivation: 自动检测点击诱饵标题对于保持数字媒体中的信息质量和读者信任至关重要，尤其是在资源较少的语言中，目前尚不清楚微调方法或上下文学习（ICL）是否能产生更好的结果。

Method: 研究人员编译了一个名为 CLIC 的新数据集，用于检测克罗地亚新闻标题中的点击诱饵，该数据集跨越 20 年，涵盖了主流和边缘出版物。他们对 Bertsic 模型进行了微调以执行此任务，并将其性能与使用克罗地亚语和英语提示的基于 LLM 的 ICL 方法进行了比较。最后，他们分析了点击诱饵的语言属性。

Result: 微调模型比通用 LLM 产生了更好的结果，并且近一半的分析标题包含点击诱饵。

Conclusion: 微调模型在点击诱饵检测任务上优于通用语言模型，并且近一半的分析标题包含点击诱饵。

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [158] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: LLM在人格评估方面信度高但效度低，预测存在偏差，链式思考提示和长上下文仅略有改善。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有研究中基于LLM的人格评估依赖于缺乏心理测量学有效性的合成数据或社交媒体文本的问题，并提供一个真实世界的数据基准来评估LLM在人格推断方面的能力。

Method: 本研究使用了一个包含555份半结构化访谈的真实世界数据集，并结合了BFI-10自陈量表得分，以评估基于LLM的人格推断能力。测试了GPT-4.1 Mini、Meta-LLaMA和DeepSeek三种模型，分别采用了零样本提示（用于BFI-10项目预测）和零样本及链式思考提示（用于大五特质推断）。

Result: 所有测试模型均表现出高信度，但效度有限，与真实分数的相关性较弱（最大皮尔逊r=0.27），评分者间一致性低（Cohen's κ < 0.10），且预测结果存在偏向于中等或高特质水平的偏差。

Conclusion: 目前的研究表明，大型语言模型（LLMs）在从开放式语言中推断人格特质方面存在局限性，尽管它们表现出高信度，但在效度、评分者间一致性和预测偏差方面仍需改进。链式思考提示和更长的输入上下文对提高分布一致性有一定作用，但对特质准确性提升有限。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [159] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 提出了一种利用GPT-4o进行错误分析和补救的师生框架，并通过课程学习来提高生物医学文本中的关系分类性能，在多个数据集上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的关系分类（RC）对于构建知识图谱和实现诸如药物再利用和临床决策支持等应用至关重要。

Method: 提出了一种错误感知的师生框架，利用大型语言模型（GPT-4o）进行结构化指导，通过分析基线学生模型的预测失败来对其进行改进。教师模型对错误类型进行分类，分配难度分数，并生成有针对性的补救措施（包括句子重写和基于知识图谱的丰富建议）。这些丰富的注释用于通过指令调整来训练第一个学生模型，然后该模型用难度分数和补救增强的输入来注释更广泛的数据集。随后，通过课程学习在按难度排序的数据集上训练第二个学生模型，以促进稳健和渐进的学习。此外，还从PubMed摘要中构建了一个异构生物医学知识图谱，以支持上下文感知的关系分类。

Result: 通过错误分析、有针对性的补救和课程学习，该师生框架提高了关系分类的性能。

Conclusion: 该方法在5个PPI数据集中的4个以及DDI数据集上取得了新的最先进性能，在ChemProt数据集上也保持了竞争力。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [160] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0是首个针对半导体显示行业设计的高性能推理模型，通过领域特定的微调和RAG机制，在保持较小规模的同时，在行业基准测试中超越了大型模型，解决了该行业的长期推理挑战。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在半导体显示行业中因缺乏领域特定训练和专业知识而存在的有效性限制问题。

Method: 通过在行业知识库上进行监督微调和强化学习，并集成领域特定的检索增强生成（RAG）机制来训练和优化模型。此外，还实现了一个自动评估框架来模拟专家级评估。

Result: X-Intelligence 3.0 在多个评估基准测试中表现优于SOTA DeepSeek-R1-671B，证明了其在处理半导体显示行业复杂挑战方面的卓越效率和强大能力。

Conclusion: X-Intelligence 3.0 在半导体显示行业中取得了显著的性能提升，克服了现有LLMs在该领域的局限性，为行业内的复杂挑战提供了专家级的解决方案。

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [161] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE是一个创新的生成式推荐框架，通过链式思考分词和稀疏注意力机制，提高了推荐性能和效率，解决了现有模型的痛点。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐模型在多行为推荐系统中潜力巨大，但面临信息缺失、计算成本高和多尺度建模能力有限等问题，限制了其应用。

Method: GRACE框架采用混合链式思考（CoT）分词方法，结合商品知识图谱中的显式属性（如类别、品牌、价格）进行语义分词，以实现可解释和行为对齐的生成。同时，设计了用户旅程感知稀疏注意力（JSA）机制，选择性地关注分词序列中的压缩、段内、段间和当前上下文片段，以降低标准注意力的计算成本。

Result: 在两个真实世界数据集上的实验表明，GRACE显著优于最先进的基线模型。在家庭领域，HR@10和NDCG@10的提升高达+106.9%和+106.7%；在电子产品领域，HR@10提升了+22.1%。此外，GRACE还将长序列的注意力计算量减少了高达48%。

Conclusion: GRACE框架通过结合链式思考（CoT）分词方法和用户旅程感知稀疏注意力（JSA）机制，有效解决了现有生成式推荐模型在信息明确性、计算成本和多尺度建模方面的不足，并在真实世界数据集上取得了显著的性能提升和计算效率优化。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [162] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel 在序数词语在语境分类任务上表现优于现有模型，并且提出了一种在复杂空间中基于角度距离的排序目标。


<details>
  <summary>Details</summary>
Motivation: 解决序数词语在语境分类任务，并探索二元词语在语境分类任务与序数任务的关系。

Method: 使用微调的多语言句子Transformer模型XL-DURel，并测试了几种用于回归和排序任务的损失函数。

Result: 在序数和二元数据上均超越了先前模型，特别是在使用基于复杂空间中角度距离的排序目标时。

Conclusion: XL-DURel 作为一种微调的多语言句子Transformer模型，在序数词语在语境分类任务上表现优于现有模型，并且提出了一种在复杂空间中基于角度距离的排序目标。该研究表明，二元词语在语境分类任务可以看作是序数任务的特例，并且优化序数任务的模型也能提升二元任务的表现，这为统一不同任务形式的词语在语境建模提供了方向。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [163] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本研究使用AudiBERT和BERT模型从对话中检测协作问题解决（CPS）指标。研究发现AudiBERT在社会认知维度上优于BERT，且更大的训练数据对模型性能有益。研究还探讨了人机互补的可能性，并强调了模型可解释性的重要性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在教育领域利用机器学习技术从对话中检测CPS指标的挑战，特别是为了澄清多模态AudiBERT模型相比BERT模型的优势是否具有统计显著性，并为利用人机互补进行CPS诊断提供指导。

Method: 本研究通过分析AudiBERT和BERT模型在对话数据中检测CPS指标的表现，并进行统计显著性检验和相关性分析，以评估多模态特征和训练数据量对模型性能的影响，并探讨了人机互补的可能性。

Result: 研究发现，AudiBERT模型在分类数据集中稀疏的类别方面有所改进，并且在社会认知维度上的分类比BERT模型具有统计上显著的改进，但在情感维度上则未观察到类似改进。此外，更大的训练数据量与更高的召回率显著相关，而BERT模型的精确率与人类编码者之间的高评分者间一致性显著相关。将BERT模型应用于AudiBERT模型已有效检测的子技能指标时，性能表现不一致。

Conclusion: 本研究提出了实现人机互补以进行协作问题解决（CPS）诊断的结构化方法，强调了模型可解释性在支持人类主动性和参与反思编码过程中的关键作用。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [164] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: TF-IDF可以通过Fisher精确检验的p值来理解，这为其在信息检索中的有效性提供了统计学解释。


<details>
  <summary>Details</summary>
Motivation: 为信息检索领域广泛使用的TF-IDF提供坚实的理论基础，并向统计学界解释其有效性。

Method: 从显著性检验的角度，将TF-IDF及其变体TF-ICF与Fisher精确检验的p值联系起来，并在理想化假设和极限情况下证明了这种关系。

Result: 证明了TF-ICF与负对数p值密切相关，并且在文档集合无限大的极限情况下，该量收敛于TF-IDF。

Conclusion: 本文将TF-IDF与Fisher精确检验的p值联系起来，为TF-IDF的有效性提供了统计学解释。

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [165] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: BERT模型在CPS分类任务中，SHAP分析发现某些词语对分类有显著影响，但有时这种影响并不合理，甚至存在无语义关联的“虚假词”。模型透明度虽有助于避免过度依赖，但不能替代人类专业知识，CPS诊断仍需人机协同。


<details>
  <summary>Details</summary>
Motivation: 为了增强BERT模型在教育领域诊断CPS过程的可解释性，从而增强教师等最终用户的信任并促进模型的广泛应用，本研究旨在探究数据集中各个词语对模型分类决策的具体影响。

Method: 本研究利用SHapley Additive exPlanations (SHAP) 方法，对BERT模型在协作问题解决（CPS）过程分类任务中的表现进行了可解释性分析，重点考察了数据集中不同词语对模型分类决策的贡献度。

Result: 研究结果显示，模型的高分类准确性并不一定意味着其分类决策具有合理的解释性。某些特定的词语被频繁用于影响分类结果。此外，研究还发现了一个“虚假词”，它对分类做出了积极贡献，但与类别在语义上并无关联。模型对词语的使用方式与其所涉及的类别数量有关。

Conclusion: 本研究的发现表明，模型的可解释性分析虽然能够揭示模型决策的依据，但并不总是能提供有意义的解释。识别出的“虚假词”就是一个例子，它影响了分类但缺乏语义关联。因此，仅仅依赖模型透明度可能无法有效指导用户改进实践，反而可能导致用户过度依赖模型而忽视自身专业知识。未来的研究应关注模型架构的改进和人机协同，以实现更准确、更可靠的协作问题解决（CPS）诊断，因为在细粒度区分CPS子技能方面，人类的推理能力仍然至关重要。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [166] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 数据增强：反翻译和释义在 NLP 中与 GPT 生成一样好，甚至更好。


<details>
  <summary>Details</summary>
Motivation: 解决 NLP 任务中的数据稀缺和类别不平衡问题，并探索利用 GPT 等大型语言模型进行数据增强。

Method: 通过一系列实验比较了四种不同的数据增强方法，并评估了生成数据的质量及其对分类性能的影响。

Result: 反翻译和释义可以产生与零样本和少样本生成相当或更好的结果。

Conclusion: 数据增强，特别是反翻译和释义，在 NLP 任务中，即使与零样本和少样本生成相比，也能产生相当或更好的结果。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [167] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 本研究为非洲初级保健开发了一个名为Alama Health QA的基准数据集和评估框架，以评估大型语言模型（LLMs）在该领域的表现。研究发现，LLMs在本地化非洲医疗内容方面存在性能差距，并提出了支持AI安全部署的解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型语言模型（LLMs）在低资源环境（特别是非洲初级保健）中的有效性，并为人工智能在非洲卫生系统中的安全部署提供支持。

Method: 研究采用检索增强生成（RAG）方法，并结合肯尼亚国家指南，创建了一个包含数千个问题-答案对的Alama Health QA数据集。该数据集涵盖了常见的门诊病症，并针对临床推理、安全性和适应性（如罕见病例检测、逐步逻辑、情境适应性）进行了评估。数据集的创建和验证过程得到了肯尼亚医生的参与和专家评审。

Result: 初步结果显示，大型语言模型在应用于本地化场景时存在显著的性能差距，其在非洲医疗内容上的准确性低于在美国基准上的表现。这表明，在非洲医疗环境中，大型语言模型需要进一步的优化和本地化。

Conclusion: 该研究提出了一个用于肯尼亚初级保健的、以指南为驱动的、动态的基准测试模型，以支持非洲卫生系统中的人工智能安全部署。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [168] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: Transformer 的两部分仿射近似和线性变换 Ws 可以准确表示主语-宾语关系，形态学关系可解释性达 90%。


<details>
  <summary>Details</summary>
Motivation: 为了探究 Transformer 计算在特定主语-宾语关系上的近似表示，并研究语言模型潜在空间中的可解释性。

Method: 提出了一种两部分仿射近似方法，并在此基础上，通过线性变换 Ws（其中 s 是主语标记的中间层表示，W 由模型导数导出）来准确再现许多关系下的最终对象状态。

Result: 线性技术在形态学关系上达到了 90% 的忠实度，并且在多语言和跨模型中也发现了类似的结果。

Conclusion: 该研究表明，语言模型中的某些概念关系（如形态学）可以很容易地从潜在空间中解释，并且通过跨层线性变换稀疏编码。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [169] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: Cleanse是一种基于聚类和语义一致性的不确定性估计方法，用于检测大型语言模型的幻觉，并在多个模型和基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLMs）在生成不准确响应（幻觉）方面的问题，以及确保LLM安全可靠，需要一种有效的方法来衡量幻觉的程度。

Method: Cleanse通过聚类分析LLM隐藏嵌入（包含生成内容的充分语义信息）来量化不确定性，具体是计算内部簇一致性在总一致性中的比例。

Result: 在LLaMA-7B、LLaMA-13B、LLaMA2-7B和Mistral-7B这四种模型以及SQuAD和CoQA这两个问答基准上的实验验证了Cleanse在检测幻觉方面的有效性。

Conclusion: Cleanse通过聚类方法，利用LLM隐藏嵌入的内部一致性比例来量化不确定性，从而有效检测幻觉。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [170] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen是一个包含470亿词元的泰语语料库，通过改进的Dolma管道构建，解决了现有泰语语料库的不足之处。该语料库的建立和持续预训练的模型在泰语任务上表现出色，并完全公开了相关资源以促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语料库依赖于以英语为中心或与语言无关的处理流程，这些流程的启发式方法无法捕捉泰语的语言和文化细微差别，导致赌博等风险内容未得到处理。先前针对泰语的努力虽然定制了处理流程或构建了新的流程，但很少发布其数据或记录设计选择，这阻碍了可复现性，并引发了如何构建透明、高质量泰语语料库的问题。

Method: Mangosteen是一个包含470亿词元的泰语语料库，通过适配Dolma的管道构建，该管道包括自定义的基于规则的语言识别、修改后的C4/Gopher质量过滤以及针对泰语训练的内容过滤，并整合了来自维基百科、皇家公报、OCR书籍和CC许可的YouTube字幕等精选的非网络来源。

Result: 系统性消融实验表明，该管道将CommonCrawl的数据量从2.02亿减少到2500万个文件，同时将SEA-HELM的NLG分数从3提高到11。在Mangosteen上持续预训练的80亿参数SEA-LION模型，在泰语基准测试上的表现比SEA-LION-v3和Llama-3.1高出约4个点。

Conclusion: Mangosteen的发布为泰国和区域语言模型研究提供了完全可复现的基础。

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [171] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: LLM在无需微调的情况下，通过结合语义搜索，在ICPC-2编码任务上表现出色，准确率高，成本和响应时间可控。虽然小型模型存在挑战，但整体显示出自动化医疗编码的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）使用领域特定搜索引擎的输出来分配ICPC-2编码的潜力，以改进医疗数据的结构化，支持研究、质量监控和政策制定。

Method: 使用包含437个巴西葡萄牙语临床表达式的数据集，其中每个表达式都带有ICPC-2编码注释。利用语义搜索引擎（OpenAI的text-embedding-3-large）从73,563个标记概念中检索候选词。使用33种LLM，通过提示每个查询和检索结果来选择最佳匹配的ICPC-2编码。使用F1分数、令牌使用量、成本、响应时间和格式遵从性来评估性能。

Result: 28种模型实现了超过0.8的F1分数，其中10种超过了0.85。表现最佳的模型包括gpt-4.5-preview、o3和gemini-2.5-pro。检索器优化最多可提高4个点的性能。大多数模型以预期的格式返回了有效的代码，幻觉减少。较小的模型（<3B）在格式和输入长度方面存在困难。

Conclusion: LLMs在自动化ICPC-2编码方面显示出巨大潜力，无需进行微调。本研究提供了一个基准，并指出了挑战，但研究结果受限于数据集范围和设置。需要进行更广泛、多语言、端到端的评估以进行临床验证。

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [172] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 该研究构建了一个新的土耳其语自杀意念语料库，并提出了一个包含人工标注和LLM的标注框架。通过对该语料库和现有英语数据集进行评估，研究发现流行的模型在零样本迁移学习中的表现不可靠，并强调了在心理健康NLP中提高数据和模型透明度及可靠性的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前自杀意念检测研究中存在的语言覆盖范围有限和标注实践不可靠这两个挑战。

Method: 研究者构建了一个新的土耳其语自杀意念语料库，该语料库来源于社交媒体帖子，并引入了一个涉及三名人工标注员和两个大型语言模型（LLMs）的资源高效标注框架。随后，研究者通过迁移学习，利用八个预训练的情感和情绪分类器，对该数据集和三个流行的英语自杀意念检测数据集进行了双向评估，以检验标签可靠性和模型一致性。

Result: 研究结果强调了在心理健康NLP中采用更严格、更具包容性的语言方法进行标注和评估的必要性，并证明了流行的模型在零样本迁移学习中的表现值得怀疑。

Conclusion: 该研究强调了在心理健康自然语言处理（NLP）中采用更严格、更具包容性的语言方法进行标注和评估的必要性，并展示了流行的模型在零样本迁移学习中的表现值得怀疑。研究者主张在心理健康NLP中提高模型训练和数据集构建的透明度，优先考虑数据和模型的可靠性。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [173] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: 该研究发布了 MiroMind-M1 系列开源语言模型，在数学推理任务上达到了领先水平，并提高了透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于解决当前开源推理语言模型（RLM）在透明度和可复现性方面存在的不足。尽管 GPT-o3 等闭源模型展示了强大的推理能力，但其专有性质限制了研究的深入。现有的开源项目虽然旨在弥补这一差距，但往往缺乏关键资源（如数据集和详细的训练配置），阻碍了研究的可复现性。因此，本研究旨在通过发布一套完全开源的模型、数据集和训练配置，来提升 RLM 开发的透明度，并促进社区的共同进步。

Method: 该研究引入了 MiroMind-M1 系列模型，这是一个基于 Qwen-2.5 的全开源语言模型。模型的训练分为两个阶段：首先，在包含 719K 个数学推理问题及其验证的链式思考（CoT）轨迹的语料库上进行监督微调（SFT）；其次，在 62K 个具有挑战性且可验证的问题上进行基于学习的奖励（RLVR）。为了增强 RLVR 过程的鲁棒性和效率，研究者提出了一种名为“上下文感知多阶段策略优化”（Context-Aware Multi-Stage Policy Optimization）的算法，该算法结合了长度渐进式训练和自适应重复惩罚，以促进上下文感知的 RL 训练。

Result: MiroMind-M1 模型在 AIME24、AIME25 和 MATH 基准测试中取得了最先进或具有竞争力的性能。与基于 Qwen-2.5 的其他开源 7B 和 32B 模型相比，MiroMind-M1 模型展现出更优的代币效率。

Conclusion: 我们发布了 MiroMind-M1 系列，这是一套基于 Qwen-2.5 的全开源语言模型，在数学推理方面达到了与现有开源模型相当或更优的性能。通过精心策划的 719K 数学推理问题及其验证的 CoT 轨迹进行 SFT，以及在 62K 具有挑战性且可验证的问题上进行 RLVR，并引入了上下文感知多阶段策略优化算法，我们提升了模型的鲁棒性和效率。模型在 AIME24、AIME25 和 MATH 基准测试中取得了最先进或具有竞争力的性能，并且在 7B 和 32B 参数的 Qwen-2.5 开源模型中具有更高的代币效率。为了促进复现，我们公开了模型、数据集以及所有的训练和评估配置，以支持未来的研究和社区发展。

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [174] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本研究回顾了Hugging Face Hub上的阿拉伯语LLM训练数据集，发现它们在任务多样性和数据质量方面存在不足，并为未来的数据集开发提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 为了提升预训练LLM在阿拉伯语任务中的性能，后续训练过程中的数据集质量和多样性至关重要。本研究旨在回顾和评估现有的阿拉伯语后续训练数据集，以识别差距并为未来的数据集开发提供指导。

Method: 对Hugging Face Hub上公开的阿拉伯语LLM数据集进行评估，重点关注LLM能力（如问答、翻译、推理、摘要、对话、代码生成和函数调用）、可控性（如角色和系统提示）、对齐性（如文化、安全、伦理和公平性）以及鲁棒性。评估维度包括流行度、实际采用情况、时效性与维护、文档与标注质量、许可透明度以及科学贡献。

Result: 评估结果显示，现有的阿拉伯语后续训练数据集在任务多样性、文档和标注质量以及社区采用方面存在明显不足。

Conclusion: 该论文对Hugging Face Hub上公开的阿拉伯语LLM数据集进行了全面回顾，强调了当前数据集在任务多样性、文档和标注质量以及社区采用方面存在的关键差距。作者提出了具体的建议，以指导未来阿拉伯语LLM的后续训练数据集的开发，并讨论了这些差距对阿拉伯语LLM及其应用进展的影响。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [175] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: AI对齐必然是左倾的，反对AI左倾就是反对AI对齐。


<details>
  <summary>Details</summary>
Motivation: 探讨AI对齐原则（HHH）与LLM政治倾向之间的关系，特别是LLM普遍存在的左翼政治倾向，并挑战将这种倾向视为风险的研究范式。

Method: 本文通过论证AI对齐原则（HHH）的内在规范性假设与进步道德框架和左翼原则的一致性，以及与右翼意识形态的冲突性，来证明AI对齐必然导致左翼政治倾向。

Result: AI对齐与左翼政治倾向是内在统一的。将LLM的左倾倾向视为风险的研究阻碍了AI对齐的实现，并违背了HHH原则。

Conclusion: AI对齐原则（HHH：无害、有用、诚实）与左翼政治倾向必然一致。对齐目标中的规范性假设（避免伤害、包容、公平、实事求是）与进步道德框架和左翼原则相符，而与右翼意识形态常有冲突。当前研究将LLM的左倾倾向视为风险或问题，这实际上是在反对AI对齐，违背了HHH原则。

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [176] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 同行评审过程存在偏见，影响作者的职业发展和科学进步。


<details>
  <summary>Details</summary>
Motivation: 探讨同行评审过程中语言可能加剧科学领域不平等的微妙方式，并对匿名评审在公平性方面的作用提出质疑。

Method: 使用自然语言处理和大规模统计建模，分析了两个主要期刊中超过80,000份同行评审的评论，考察了评论的语气、情感和支持性语言如何因作者的性别、种族和机构隶属关系而异，并研究了审稿人身份的披露如何影响评估语言。

Result: 揭示了同行评审的反馈中存在隐藏的偏见，并表明审稿人身份的披露会影响评估的语言。

Conclusion: 研究结果揭示了同行评审语言中存在的性别、种族和隶属关系偏见，并质疑了匿名评审在公平性方面的作用，强调了审查政策对学术出版和科学进步的影响。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [177] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: 本研究使用 SAYCam 数据集中的自动语音转录，训练多模态神经网络，以研究儿童语言习得。结果显示，模型在不同孩子的语料上都能学习词语指代，证明了方法的鲁棒性，但也发现了模型学习方式的个体差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，多模态模型在仅使用一个孩子的有限发育数据上可以习得词语指代映射。然而，这种成功是否反映了单一孩子的特殊性，或者是否能在多个孩子的经验中展现出一致且鲁棒的学习模式尚未得到探索。本研究旨在解决这一问题，验证所用方法的鲁棒性。

Method: 本研究运用自动语音转录方法处理了 SAYCam 数据集，该数据集包含跨越三个孩子超过 500 小时的视频数据。基于这些自动转录的文本，研究生成了用于训练和评估的多模态视觉语言数据集，并探索了一系列神经网络配置，以检验模拟词语学习的鲁棒性。

Result: 研究结果表明，在自动转录的每个孩子的语料数据上训练的神经网络能够习得并泛化词语指代映射。这证明了多模态神经网络在接地词语学习方面的鲁棒性，并揭示了在不同孩子的发育经历上进行训练时出现的个体差异。

Conclusion: 本研究表明，在自动转录的每个孩子的语料数据上训练的神经网络可以跨多种网络架构习得并泛化词语指代映射。这验证了多模态神经网络在接地词语学习中的鲁棒性，同时也揭示了当模型在每个孩子的发育经历上进行训练时所出现的个体差异。

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [178] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech 框架通过压缩和动态训练，实现了 LSLMs 的高效长语音处理，并推出了 LongSpeech-Eval 基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决现有 LSLMs 在长语音处理方面的效率低下问题，该问题源于长语音训练数据的稀缺和长序列的高计算成本。

Method: 通过迭代融合策略压缩长语音序列，并采用动态压缩训练方法使 LSLMs 适应长语音输入。

Result: 实验表明，FastLongSpeech 在长语音和短语音任务上均表现出强劲性能，并显著提升了推理效率。

Conclusion: FastLongSpeech 框架在长语音和短语音任务上均表现出强大的性能，同时大大提高了推理效率。

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [179] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: 该研究提出了一种新的基于意图的文档图表生成方法，通过语言模型提取信息和启发式选择图表类型，并在金融和科学领域的数据集上进行了验证，结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将文本描述或表格转换为数据可视化，也难以直接应用于根据用户意图从长文档中可视化数据（而非手动预选相关内容）的实际场景。因此，该研究旨在解决从文档中基于用户意图生成图表的任务。

Method: 该研究提出了一种无监督、分两阶段的框架。第一阶段，语言模型通过分解意图并迭代地验证和优化数据来从文档中提取相关信息。第二阶段，一个启发式引导模块选择合适的图表类型，然后生成代码。为了评估生成图表的数据准确性，研究提出了一种基于归因的度量方法，该方法使用结构化的文本化图表表示，而不是依赖于通常无法有效捕获图表数据的视觉解码度量。

Result: 研究提出的方法在图表数据准确性方面比最佳基线高出9个百分点，在图表类型选择方面高出17个百分点。

Conclusion: 该研究提出了一个名为“基于意图的文档图表生成”的新任务，并开发了一个无监督的两阶段框架来解决它。该框架首先利用语言模型从文档中提取信息，然后通过启发式方法选择图表类型并生成代码。此外，研究还提出了一种基于归因的指标来评估生成图表的数据准确性，并构建了一个包含1,242个“意图、文档、图表”元组的数据集。实验结果表明，该方法在图表数据准确性和图表类型选择方面优于现有基线方法。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [180] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: 推理蒸馏能提升语言模型在长上下文理解和信息提取能力，并有效缓解“中间迷失”问题。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探究大规模推理蒸馏对模型其他关键能力（特别是上下文检索和推理）的影响，因为目前的研究尚未涉及这一领域，而检索增强生成（RAG）系统对上下文信息获取和利用效率的要求日益提高。

Method: 本文通过一系列蒸馏自Deepseek-R1（以其卓越的推理能力闻名）的开源模型，在多文档问答任务上进行了全面的实证研究，以评估这些模型在长上下文信息提取和整合方面的表现。

Result: 实验结果表明，蒸馏后的推理模式能够增强模型对长上下文的理解，并能通过更详细和显式的推理过程来提升上下文分析和信息解析能力，从而解决“中间迷失”问题。

Conclusion: 推理蒸馏显著提高了模型对长上下文的理解能力，通过促进更详细、更明确的推理过程来增强上下文分析和信息解析，从而有效缓解了“中间迷失”问题。

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [181] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: Pre-training benefits even tiny language models (TLMs), making them a more accessible alternative to LLMs. Combining multiple small TLMs can match the performance of larger ones with lower latency, and they might even be relevant for understanding language acquisition.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study was to address the high computational cost of pre-training large language models (LLMs), which limits broader research participation. The study aimed to explore whether smaller, more accessible tiny language models (TLMs) exhibit similar qualitative features to LLMs, potentially offering a more accessible alternative for NLP research.

Method: The study explored the effectiveness of pre-training for tiny language models (TLMs) by comparing pre-trained and non-pre-trained TLMs on classification tasks. It also investigated whether a soft committee of shallow TLMs could match the performance of a deep TLM. The experiments used BERT-6 and BERT-1 variants pre-trained on Wikipedia subsets and evaluated on FewRel, AGNews, and DBPedia datasets.

Result: TLMs showed a performance gap between pre-trained and non-pre-trained models across classification tasks, demonstrating the effectiveness of pre-training even at a small scale. This gap widened with larger pre-training datasets and greater token overlap. Additionally, the accuracy of a pre-trained deep TLM was successfully replicated by a soft committee of multiple shallow TLMs, achieving similar results with lower latency.

Conclusion: TLMs are effective even at a small scale, with performance gains from pre-training that correlate with dataset size and token overlap. A soft committee of shallow TLMs can replicate the performance of a deep TLM, offering a low-latency alternative. Future research could explore TLMs' potential in child language development.

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [182] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: MEKiT通过注入多源异构知识（内部情感知识和外部因果知识）来增强LLM在情感-原因对提取（ECPE）任务上的表现，解决了LLM在该任务上表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在ECPE任务（需要推理能力）上的表现通常不如较小的语言模型，主要原因是缺乏辅助知识，这限制了LLM感知情感和推理原因的能力。

Method: 提出了一种新颖的多源异构知识注入方法MEKiT，集成了异构的内部情感知识和外部因果知识。通过结合指令模板和混合数据进行指令调整，分别促进LLM更全面地识别情感和准确地推理原因。

Result: 实验结果表明，MEKiT在ECPE任务上优于基线方法，并显著提高了LLM的性能。

Conclusion: MEKiT提供了一个更有效和适应性更强的ECPE任务的解决方案，与基线相比具有绝对的性能优势，并显著提高了LLM在ECPE任务上的性能。

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [183] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: SASFT 是一种新的微调方法，可以解决 LLM 中的意外语种转换问题，并提高其多语言能力。


<details>
  <summary>Details</summary>
Motivation: LLM 存在意外语种转换的问题，这会降低可读性和模型响应的可用性，而现有方法的效果有限。

Method: 使用稀疏自编码器进行机制分析，并提出了一种名为 SASFT 的稀疏自编码器引导的监督微调方法，以在训练过程中指导 LLM 保持特定语言特征的适当预激活值。

Result: SASFT 能够有效减少意外语种转换，同时保持或提高模型的性能。

Conclusion: SASFT 成功地将意外的语种转换减少了 50% 以上，并在四种情况下完全消除了该问题，同时保持或提高了模型在六个多语言基准上的性能。

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [184] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: NeuronXA 是一种新的方法，可以有效地评估大型语言模型在不同语言之间的对齐能力，即使数据量很小。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐基准主要关注句子嵌入，但神经模型往往会诱导一个不平滑的表示空间，这对低资源语言的语义对齐评估产生影响。受神经科学发现的启发，即相似信息会激活重叠的神经区域，因此需要一种更具语义支撑的方法来评估跨语言对齐。

Method: 提出了一种名为 NeuronXA 的新颖神经状态交叉语言对齐方法，以评估大型语言模型的交叉语言对齐能力。

Result: NeuronXA 在 LLaMA、Qwen、Mistral、GLM 和 OLMo 等多语言语言模型上进行了评估，在两个迁移任务和三个多语言基准上，NeuronXA 与下游任务性能的相关性达到了 0.9556，与迁移能力的相关性达到了 0.8514。

Conclusion: NeuronXA 在评估多语言语言模型的跨语言对齐和迁移能力方面是有效的，即使在只有 100 对平行句子的数据集上也是如此。该方法有潜力推动跨语言对齐研究和改善多语言语言模型的语义理解。

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [185] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite automates prompt generation for reliable LLM evaluation, overcoming the limitations of single-prompt testing.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLMs with a single prompt is unreliable due to performance sensitivity to small changes. Generating prompt variations for robust multi-prompt evaluation is challenging, hindering its practical adoption.

Method: PromptSuite framework enables automatic generation of various prompts using a modular design with controlled perturbations and extensibility for new components and perturbation types.

Result: PromptSuite is flexible, works on a wide range of tasks and benchmarks, and offers both a Python API and a web interface.

Conclusion: PromptSuite provides meaningful variations to support strong evaluation practices through automatic prompt generation.

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [186] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA是一个新的数据集，通过结合真实用户活动来生成更一致、更逼真的合成社交媒体用户背景故事，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的人格驱动的语言模型（LLM）方法要么依赖昂贵的人工策划数据，要么产生缺乏一致性和真实性的合成人格。

Method: SYNTHIA是一个包含30,000个背景故事的数据集，这些故事源自BlueSky开放平台上的10,000个真实社交媒体用户，跨越三个时间窗口。该方法通过将合成生成与真实用户活动相结合，填补了现有方法的空白。

Result: SYNTHIA在人口结构多样性和社会调查一致性方面达到了具有竞争力的性能，同时在叙事一致性方面显著优于现有方法。

Conclusion: SYNTHIA在人口结构多样性和社会调查一致性方面表现与最先进的方法相当，同时在叙事一致性方面显著优于它们。SYNTHIA的独特之处在于它包含了时间维度，并提供了来自底层网络的丰富社交互动元数据，为计算社会科学和面向角色的语言建模开辟了新的研究方向。

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [187] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: MUR通过动量和不确定性指导LLM的推理，在不牺牲准确性的情况下将计算量减少了50%以上，并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLM）在推理密集型任务中存在的效率问题，特别是测试时间缩放（TTS）导致的过度思考和计算浪费。

Method: 提出了一种名为MUR（Momentum Uncertainty-guided Reasoning）的方法，该方法通过跟踪和聚合逐步的不确定性来动态分配思维预算到关键的推理步骤，并引入了gamma-control机制来通过单个超参数调整推理预算。

Result: MUR在MATH-500、AIME24、AIME25和GPQA-diamond等四个具有挑战性的基准测试中，与多种TTS方法相比，平均计算量减少了50%以上，同时准确性提高了0.62-3.37%。

Conclusion: MUR通过动量和不确定性指导LLM的推理，在不牺牲准确性的情况下将计算量减少了50%以上，并提高了准确性。

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [188] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: 本研究提出了RefCritic，一种基于强化学习和双重规则奖励的长链思维批评模块，用于提升大型语言模型的批评和改进能力。与传统的监督微调方法相比，RefCritic能生成更高质量、更具操作性的反馈，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前用于构建批评模块的监督微调方法未能真正提升模型的批评能力，生成的批评过于表面化，缺乏反思和验证。为了解锁前所未有的批评能力，需要新的方法。

Method: 提出了一种名为RefCritic的元提示策略，它是一种基于强化学习的长链思维（LCO）批评模块，并结合了实例级正确性判断和策略模型基于批评的改进准确性这两种基于规则的奖励，旨在生成高质量的评估和可操作的反馈，以有效指导模型的改进。

Result: RefCritic在批评和改进设置中均表现出持续的优势，并且在多数投票下，经过RefCritic过滤的策略模型显示出随着投票数量增加而扩展的优越性。此外，尽管RefCritic是基于解决方案级别的监督进行训练的，但它在ProcessBench基准测试上的表现优于基于步骤级别的监督方法。

Conclusion: RefCritic在Qwen2.5-14B-Instruct和DeepSeek-R1-Distill-Qwen-14B模型上进行了评估，并在五个基准测试中表现出一致的优势，例如在AIME25数据集上分别提高了6.8%和7.2%。

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [189] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebShaper是一个新的框架，通过形式化和数据合成来改进LLM驱动的信息检索代理的训练，解决了数据稀缺和不一致问题，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的IS代理训练数据稀缺，且信息驱动范式可能导致信息结构与推理结构、问题与答案之间不一致。

Method: 提出了一种名为WebShaper的形式化驱动的IS数据合成框架，该框架利用集合论对IS任务进行形式化，并通过知识投影（KP）操作的组合精确控制推理结构。通过种子任务和基于代理的扩展器进行多步扩展来合成数据。

Result: 在GAIA和WebWalkerQA基准测试中，使用WebShaper合成的数据训练的模型取得了最先进的性能。

Conclusion: WebShaper成功地通过形式化驱动的数据合成框架解决了信息检索（IS）代理的训练数据稀缺问题，并在GAIA和WebWalkerQA基准测试中取得了最先进的性能。

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [190] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: BPE在DNA序列建模中优于k-mer，RoPE和AliBi在特定任务中表现出色，模型深度增加到12层效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前许多研究将DNA序列视为一种特殊的语言，并利用Transformer对其进行建模。然而，这些研究在采用固定长度的k-mer分段和BPE子词分词方法时，缺乏系统性的评估来确定哪种方法更优。

Method: 本文比较了k-mer（k=1,3,4,5,6）分段、4096个token的BPE词汇表以及三种位置编码方法（sinusoidal、AliBi和RoPE）。在GUE基准数据集上，对从头开始训练的3、6、12和24层Transformer编码器进行了评估。

Result: BPE在压缩高频图案、减少序列长度和提高模型泛化能力方面表现更好，能够提供更高且更稳定的性能。RoPE在捕捉周期性图案和外插到长序列方面表现出色，而AliBi在处理局部依赖性任务方面表现良好。增加Transformer编码器的层数从3层到12层可显著提升性能，但增加到24层时性能提升有限，甚至可能出现轻微过拟合。该研究为设计DNA Transformer模型中的分词和位置编码提供了实用的指导。

Conclusion: BPE在压缩高频图案、减少序列长度和提高模型泛化能力方面表现更好，能够提供更高且更稳定的性能。RoPE在捕捉周期性图案和外推到长序列方面表现出色，而AliBi在处理局部依赖性任务方面表现良好。增加Transformer编码器的层数从3层到12层可显著提升性能，但增加到24层时性能提升有限，甚至可能出现轻微过拟合。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [191] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: LLM生成的文本在用于进一步训练时，多样性至关重要。研究人员试图通过提示工程来提高多样性，但忽略了提示变化对文本长度和词汇多样性指标的影响。本文提出了一种新的多样性指标PATTR，该指标可以应对文本长度变化带来的偏差，并在创意写作任务的实验中，其表现优于MATTR和CR，能够生成多样性高且长度符合要求的文本。


<details>
  <summary>Details</summary>
Motivation: 为了提高LLM（大语言模型）训练数据的多样性，研究人员依赖提示工程，但目前对提示变化如何影响响应文本长度以及词汇多样性测量结果的后果研究不足。

Method: 本文提出了一种名为PATTR（Penalty-Adjusted Type-Token Ratio）的新型词汇多样性指标，并生成了一个包含超过2000万词的大规模合成语料库（使用LLaMA、OLMo和Phi系列共七种模型，专注于创意写作任务），然后使用PATTR评估了响应的词汇多样性，并与现有的MATTR和CR指标进行了比较，同时分析了文本长度变化对多样性指标的影响，并展示了PATTR在筛选多样性响应方面的效用。

Result: PATTR指标能够有效缓解文本长度变化带来的偏差，其表现优于MATTR和CR，能够生成多样性相当或更高且更符合目标长度的响应。

Conclusion: 该研究提出了PATTR（Penalty-Adjusted Type-Token Ratio）这一新的词汇多样性指标，该指标能够有效缓解因文本长度变化带来的偏差，并在创意写作任务中通过与MATTR和CR等现有指标的比较证明了其优越性，尤其在筛选多样性高且长度符合目标要求的响应方面表现出色。

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [192] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型（LLM）作为自然语言推理（NLI）任务的常识知识生成器。研究发现，虽然引入常识知识并不总是能提升整体性能，但它确实有助于更好地区分蕴含关系，并对识别矛盾和中性关系有适度改善。


<details>
  <summary>Details</summary>
Motivation: 自然语言推理（NLI）旨在开发能够模拟在推理过程中起重要作用的常识知识的系统。然而，现有的常识资源在各种前提-假设对的覆盖方面存在不足。

Method: 本研究探索了大型语言模型作为NLI的常识知识生成器的潜力，重点关注其生成此类知识的可靠性以及这些知识对预测准确性的影响。我们调整并修改了现有指标，以评估LLM在此背景下生成的事实性和一致性。

Result: 在评估LLM在此背景下生成的事实性和一致性时，我们调整并修改了现有指标。

Conclusion: 虽然明确地结合常识知识并不总能提高整体结果，但它能有效地区分蕴含实例，并适度提高区分矛盾和中性推断的能力。

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [193] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: 本篇论文认为，NLI中的注释分歧源于歧义，并提出了一种考虑歧义的框架和方法，以提高NLI系统的鲁棒性、可解释性和与人类理解的一致性。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于，自然语言推理（NLI）中的注释分歧并非仅仅是噪声，而是反映了有意义的解释差异，尤其是在由前提或假设中的歧义触发时。作者呼吁在NLI中引入“区分模糊性”的方法。

Method: 本文提出了一种统一的框架，整合了现有的分类法，并通过具体示例说明了关键的歧义子类型。此外，还提出通过新的注释资源和无监督方法来解决缺乏标注数据集的问题。

Result: 通过示例揭示了歧义如何影响注释者的决策，并证明了采用针对性的检测方法以使模型更好地与人类解释保持一致的必要性。

Conclusion: 本篇论文的结论是，为了构建更健壮、更具可解释性、更能与人类理解对齐的自然语言推理（NLI）系统，需要从忽视注释分歧转向明确考虑歧义。这需要系统地识别模糊输入对并对模糊类型进行分类，并通过新的注释资源和无监督方法来检测模糊性。

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [194] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: 这项研究探讨了在阿姆哈拉语自然语言处理中，同音异形词归一化对使用Ge'ez文字语言的影响，并提出了一种在推理后进行归一化的方法，在提高BLEU分数的同时保留了语言特征。


<details>
  <summary>Details</summary>
Motivation: 为了解归一化对使用Ge'ez文字的语言在单语训练和跨语迁移中的影响，以及解决归一化可能导致的在训练数据中保留语言特征的挑战。

Method: 本研究通过单语训练和跨语迁移实验，探讨了归一化对使用Ge'ez文字的语言的影响。提出了一种在推理后对模型预测进行归一化的后推理干预方法。

Result: 实验表明，在推理后进行归一化干预可以将BLEU分数提高多达1.03，同时在训练中保留了语言特征。

Conclusion: 该研究通过在推理后进行归一化干预，在保持训练数据的语言特征的同时，将BLEU分数提高了1.03。该研究为技术促进的语言变革的广泛讨论做出了贡献，并呼吁进行更多具有语言意识的干预。

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [195] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 三种 LLM 在 RCT 数据提取任务中表现出高精确度但召回率低，定制化提示可提高召回率。本研究提出了分层指导方针，以平衡 LLM 效率和专家监督。


<details>
  <summary>Details</summary>
Motivation: 从随机对照试验（RCT）的全文中自动提取用于荟萃分析的数据仍然是一个重大挑战。

Method: 本研究评估了三种 LLM（Gemini-2.0-flash、Grok-3、GPT-4o-mini）在提取统计结果、偏倚风险评估和研究特征等任务中的实际表现，并测试了四种不同的提示策略（基础提示、自反思提示、模型集成和定制提示）以提高提取质量。

Result: 所有模型都表现出高精确度，但普遍存在召回率低的问题，遗漏了关键信息。定制化提示是最有效的策略，可将召回率提高高达 15%。

Conclusion: 本研究提出了一个三层指导方针，用于在使用 LLM 进行数据提取时，根据任务的复杂性和风险，将数据类型与适当的自动化水平相匹配。

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [196] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: 通过多教师知识蒸馏，有效压缩大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决部署大型语言模型时的高计算成本和推理速度慢的挑战。

Method: 提出了一种由多个教师模型指导的蒸馏策略，该策略构建多个教师模型，整合它们的输出概率分布和中间语义特征，以指导学生模型从多个知识源学习。通过加权输出融合机制、特征对齐损失函数和熵驱动的动态教师加权策略，提高了知识转移的质量和稳定性。

Result: 学生模型在保持较小参数规模的同时，获得了更强的语言理解和生成能力，在语言建模、文本生成和多任务学习等任务中表现出高度的一致性、泛化能力和任务适应性。实验结果表明，该方法在困惑度、蒸馏损失和生成质量方面优于其他蒸馏方法。

Conclusion: 该研究为大型语言模型的有效压缩提供了一条可行的技术路径，并证明了多教师协作机制在复杂语言建模任务中的有效性。

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [197] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 本文提出SOI框架分析多任务/多语言/多源学习对语言模型的影响，发现多源学习提升最显著，并提出两阶段微调方法优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了深入了解多任务、多语言和多源学习方法对预训练语言模型鲁棒性和性能的影响，并为优化多重设置下的语言模型性能提供实用方法。

Method: 本文提出了一种名为“关注子集”（Subsets of Interest, SOI）的新型分类框架，该框架识别出训练过程中的六种不同学习行为模式（例如，易忘样本、未学样本、始终正确样本）。通过SOI转换热图和数据集地图集可视化，分析了从单一设置到多重设置转换时样本类别的变化。进行了三组对比实验：多任务学习 vs. 单任务学习（英语任务）；多源学习 vs. 单源学习（情感分析）；多语言学习 vs. 单语言学习（意图分类）。并提出了一种两阶段微调方法，第二阶段利用SOI进行子集选择。

Result: 多源学习在所有对比实验中均能提升模型在分布外（out-of-distribution）任务上的表现，最高提升7%。多任务学习在相似任务组合时表现出显著的性能提升，但整体结果不一。两阶段微调方法在SOI子集选择的加持下，能够带来额外的性能提升。

Conclusion: 多源学习能稳定提升模型在分布外任务上的表现（最高可达7%），而多任务学习在相似任务组合上表现出明显的性能提升，但整体结果喜忧参半。引入的SOI类别划分框架和可视化方法有助于理解训练动态，而两阶段微调方法能进一步提升模型性能。

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [198] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: ChiMed 2.0：一个包含 2.04 亿汉字的大规模中文医疗数据集，用于训练中文医疗大模型，提升模型在医疗领域的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有中文医疗数据集规模小、领域覆盖窄以及不支持预训练和 RLHF 等问题，研究者提出了 ChiMed 2.0 数据集，以满足中文医疗大模型研究和应用的需求。

Method:  ChiMed 2.0 数据集通过整合中文医疗在线平台数据和语言模型生成数据构建而成，规模庞大，覆盖面广。研究团队利用该数据集对通用大模型进行了预训练、监督微调（SFT）和人类反馈强化学习（RLHF）等实验，并在医疗基准数据集上进行了评估。

Result:  ChiMed 2.0 数据集在预训练、SFT 和 RLHF 实验中均取得了显著效果，在不同规模的模型上均表现出性能提升，证明了该数据集在训练中文医疗大模型方面的有效性和广泛适用性。

Conclusion: ChiMed 2.0 是一个包含 2.04 亿汉字的大规模中文医疗数据集，涵盖了传统中医和现代医学的多个领域。该数据集支持预训练、监督微调（SFT）和人类反馈强化学习（RLHF），可用于训练中文医疗大模型。实验结果表明，使用 ChiMed 2.0 进行训练的模型在医疗基准数据集上表现出显著的性能提升，验证了该数据集的有效性和适用性。

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [199] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: DPSE框架通过双阶段自我进化，在优化用户偏好的同时，增强LLM在特定领域内的认知能力，并提供了持续自我进化的自主路径。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM优化方法（如基于内存的检索或偏好优化）虽然改善了用户对齐，但未能增强模型在特定领域内的认知能力。为了解决这个问题，提出DPSE框架来同时优化用户偏好适应和领域特定能力。

Method: DPSE框架包含一个审查模块，用于提取多维度交互信号并估计满意度分数，通过面向主题和驱动偏好的策略指导结构化数据扩展。这些扩展的数据支持一个两阶段的微调流程：监督域基础和频率感知偏好优化。

Result: 实验结果表明，DPSE在通用NLP基准和长期对话任务上始终优于监督微调、偏好优化和内存增强基线模型。消融研究验证了每个模块的贡献。

Conclusion: DPSE框架提供了一条自主的、持续的LLM自我进化路径。

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [200] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 本研究提出了SHIELD评估基准和“人性化”框架，用于更实际、公平地评估AI文本检测器，解决了现有评估方法忽略误报率和模型稳定性的问题，并通过“人性化”技术有效挑战了现有检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测评估方法主要关注AUROC等传统指标，忽略了误报率对实际部署的影响以及模型在不同领域和对抗场景下保持性能一致性（即稳定性）的关键性。这些被忽视的因素对AI文本检测系统的实际应用至关重要。

Method: 本研究提出了SHIELD评估基准，该基准通过整合可靠性和稳定性因素到一个统一的评估指标中，来评估AI文本检测器。同时，研究开发了一个后处理的、模型无关的“人性化”框架，该框架包含一个可控的“硬度”参数，用于修改AI文本使其更接近人类写作风格。

Result: SHIELD基准通过引入可靠性和稳定性评估，为AI文本检测提供了一个更符合实际应用的评估框架。通过“人性化”框架及其“硬度”参数，研究证明了该方法能有效挑战当前最先进的零样本检测方法，并暴露其在可靠性和稳定性方面的潜在问题。

Conclusion: 该研究提出了一个新颖的AI文本检测评估范式，名为SHIELD，它优先考虑真实世界和公平的评估。该范式整合了可靠性和稳定性因素到一个统一的评估指标中，旨在解决现有方法仅报告AUROC等传统指标而忽略实际部署关键因素（如误报率、模型稳定性）的问题。此外，研究还开发了一个模型无关的后处理“人性化”框架，通过可控的“硬度”参数调整AI文本以模仿人类写作风格，该框架有效挑战了当前最先进的零样本检测方法在保持可靠性和稳定性方面的能力。

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [201] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: MCQA在评估大型语言模型（LLM）的问答能力时，虽然易于自动评分且曾是有效的基准，但对最先进模型而言已不再是可靠的性能代理。研究发现，允许模型在看到选项后进行推理会利用选项信息，导致分数虚高，偏离真实推理能力。建议设计新的、更抗干扰的基准来准确评估LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管下游任务通常不提供明确的选项，但MCQA因其易于自动评分和能够生成与下游性能充分相关的具有挑战性的基准而被广泛使用。本研究旨在调查MCQA作为LLM评估代理的相关性，特别是对于最先进的推理模型。

Method: 本研究通过系统评估15个不同的问答基准（例如，MMLU，HLE）和25个不同的LLM（包括Qwen 7B等小型模型和Llama 70B等大型模型）来研究MCQA作为LLM评估代理的相关性。研究人员考虑了5种不同的问题呈现方式，包括是否提供选项、是否包含“以上皆无”选项以及是否允许模型在选项呈现之前和/或之后执行链式思考推理。

Result: 研究发现，当模型仅在选项呈现之前被允许进行链式思考时，MCQA仍然是模型下游性能的一个良好代理。然而，当模型在给定选项后进行推理时，它们倾向于利用选项中的信息，表现明显优于其自由文本表现。这表明MCQA可能不再是评估最先进模型下游性能的可靠指标。

Conclusion: MCQA不再是评估最先进模型下游性能的良好代理。研究表明，在允许模型在选项呈现之前进行链式思考时，MCQA可以作为下游性能的一个良好代理。然而，当模型在给定选项后进行推理时，它们倾向于利用选项中的信息，从而显著优于其自由文本性能。

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [202] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2 是一个轻量级的多语言内容审核模型，专为新加坡设计，在多种语言和基准测试中表现出色，优于现有系统，并已被新加坡政府采用。


<details>
  <summary>Details</summary>
Motivation: 为了解决现代内容审核系统在多语言支持、本地化和低资源语言变体方面的不足，以及大型语言模型（LLMs）对数据和计算资源的巨大需求，本研究提出了 LionGuard 2，一个轻量级的解决方案。

Method: LionGuard 2 是一个基于预训练的 OpenAI 嵌入和多头序数分类器构建的轻量级多语言审核分类器，针对新加坡的特定需求进行了优化，支持英语、中文、马来语和部分泰米尔语。

Result: LionGuard 2 在包括新加坡特定和公开的英语数据集在内的17个基准测试中，表现优于多个商业和开源系统，证明了其在实际应用中的有效性。

Conclusion: LionGuard 2 在实际部署中表现出了强大的多语言内容审核能力，尤其是在新加坡的特定语言和场景下，证明了高质量的本地化数据和多语言嵌入对于在不微调大型模型的情况下实现强大的审核性能至关重要。该模型已被新加坡政府部署，并开源了模型权重和部分训练数据。

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [203] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: 本研究使用熵分析来研究 Transformer 模型（特别是 GPT）中的信息分布和模型行为。


<details>
  <summary>Details</summary>
Motivation: 探索熵分析作为一种工具，用于探究 Transformer 模型的信息分布，了解信息如何在模型中管理和转化。

Method: 通过量化 token 级不确定性和检查不同处理阶段的熵模式来分析信息分布。

Result: 将熵分析应用于 GPT 模型，揭示了模型行为和内部表征的见解。

Conclusion: 本研究将熵分析作为探索 Transformer 模型信息分布的工具，并通过量化 token 级不确定性和检查不同处理阶段的熵模式，旨在研究信息在模型内的管理和转化方式。作为案例研究，我们将该方法应用于 GPT 大型语言模型，以揭示模型行为和内部表征的见解。

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [204] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: LLMs在处理隐喻时，更依赖表面特征而非真正理解。


<details>
  <summary>Details</summary>
Motivation: 之前的隐喻处理研究主要局限于单数据集评估和特定任务设置，且常使用通过词汇替换人工构建的数据。本研究旨在通过更广泛的实验来克服这些局限性。

Method: 本研究通过在多种数据集、任务和提示配置下，对大型语言模型（LLMs）进行全面的隐喻解释能力评估。研究使用了具有推理和隐喻注释的各种公开可用数据集，重点关注自然语言推理（NLI）和问答（QA）任务。

Result: LLMs的表现表明，词汇重叠和句子长度等表面特征比隐喻内容更能影响其在隐喻解释任务中的表现。

Conclusion: LLMs在隐喻理解方面的表现更多地受到词汇重叠和句子长度等特征的影响，而非隐喻内容本身。所谓的LLMs理解隐喻语言的涌现能力，实际上是表面特征、上下文学习和语言知识的结合。

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [205] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: Stitch 是一种新颖的生成方法，通过交替生成推理和响应分块，实现了 SLM 的同步思考和说话，在数学推理任务上性能提升 15%，同时不增加延迟。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型（SLM）缺乏在响应前进行内部、不带语音的思考过程的能力，而人类通常会进行复杂的内部心理推理。因此，将不带语音的思考过程整合到 SLM 中非常可取，但直接生成完整的思维链（CoT）会增加响应延迟。

Method: 提出了一种名为 Stitch 的新颖生成方法，该方法将不带语音的推理分块生成和带语音的响应分块生成交替进行。利用语音响应分块生成期间的剩余时间来生成推理分块，实现了边思考边说。

Result: Stitch 成功匹配了那些无法生成不带语音 CoT 的基线模型的延迟，同时在数学推理数据集上取得了 15% 的性能提升，并且在非推理数据集上与基线模型性能相当。

Conclusion: Stitch 成功在不增加延迟的情况下匹配了基线模型的性能，并在数学推理任务上取得了 15% 的性能提升。

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [206] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: LLMs在识别算法相似问题方面表现不佳，但新方法ASM可提高性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在解决计算领域未被充分探索但与训练数据相关的算法相似问题（ASPs）方面的泛化能力。

Method: 提出AlgoSimBench基准，包含1317个问题和231个算法标签，并设计了402个选择题（MCQ）。提出尝试性解决方案匹配（ASM）方法。评估了代码嵌入模型和检索方法。

Result: 在MCQ任务中，最佳模型（o3-mini）准确率仅为65.9%。ASM方法在MCQ任务上将准确率提高了6.7%至11.7%。移除叙述性元素可消除对抗性选择问题带来的性能下降，并且ASM与BM25结合可达52.2%的准确率。

Conclusion: LLMs在识别算法相似问题（ASPs）方面表现不佳，但提出的尝试性解决方案匹配（ASM）方法可以提高准确性。此外，通过移除叙述性元素并结合关键词优先方法（如BM25）可以进一步提升性能。

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [207] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: ASPERA框架和Asper-Bench数据集旨在解决LLMs在复杂数字助手任务中的程序生成挑战。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在驱动能够执行复杂操作的数字助手方面的潜力，解决数据可用性和评估鲁棒性挑战。

Method: 提出ASPERA框架，包含助手库模拟和人类辅助的LLM数据生成引擎，以生成包含复杂用户查询、模拟状态和相应验证程序的任务。发布了包含250个任务的Asper-Bench评估数据集。

Result: ASPERA框架和Asper-Bench数据集。实验表明，与不依赖自定义库的代码生成相比，LLMs在基于自定义助手库进行程序生成方面存在显著挑战。

Conclusion: LLMs在驱动能够执行复杂操作的数字助手方面仍面临重大挑战，特别是在与自定义助手库进行程序生成方面。

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [208] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 通过结合精细的逐步自优化和并行缩放，我们提出了一种名为混合测试时缩放（HTTS）的训练无关方法，有效提升了大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的测试时缩放（Test-Time Scaling, TTS）方法中，基于训练的方法（如强化学习）虽然有效但增加了计算开销，而训练无关（training-free）的方法逐渐式微。作者旨在探索训练无关的TTS方法，以减轻计算负担并提升推理效率。

Method: 1. 设计了条件步级自优化（Conditional Step-level Self-refinement）：一种细粒度的序列化缩放方法，以过程验证为指导。 2. 结合了经典并行缩放方法：在步级上将自优化方法与并行缩放策略相结合，形成了混合测试时缩放（Hybrid Test-Time Scaling, HTTS）。

Result: 在五个不同规模（3B-14B）和系列的指令微调语言模型上的广泛实验表明，HTTS策略通过在细粒度上融合多种训练无关的TTS方法，能够显著提升模型的推理性能。

Conclusion: 本文提出的混合测试时缩放（Hybrid Test-Time Scaling, HTTS）策略，通过结合精细的逐步自优化和经典的并行缩放方法，在不引入额外训练开销的情况下，有效提升了指令微调语言模型在推理任务上的表现。实验证明，该方法能够显著扩展LLM的推理能力边界。

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [209] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: 本研究首次对九种语言的文本去毒化系统进行了全面的多语言评估，发现现代神经评估模型和LLM作为裁判的方法在提高评估可靠性方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在文本生成方面取得了进展，但像文本风格迁移（TST）这样的任务的评估仍然是一个挑战，特别是对于多语言评估的研究尚不充分。

Method: 文章评估了现代神经评估模型和基于提示的大型语言模型（LLM）作为裁判的方法，并借鉴了机器翻译的经验。

Result: 研究结果表明，现代神经评估模型和基于提示的大型语言模型（LLM）作为裁判的方法在多语言文本去毒化评估中具有潜力，并为设计更可靠的评估流程提供了指导。

Conclusion: 本研究为文本去毒化场景下设计更可靠的多语言文本风格迁移评估流程提供了一个实用的方法。

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [210] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: In-Context Learning (ICL) with Vision-Language Models (VLMs) offers a flexible, interpretable alternative for THz image classification without fine-tuning, showing improved performance in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: Effective image classification in Terahertz (THz) imaging is challenging due to limited annotations, low resolution, and visual ambiguity.

Method: Using a modality-aligned prompting framework, two open-weight Vision-Language Models (VLMs) were adapted to the THz domain and evaluated under zero-shot and one-shot settings.

Result: ICL improves classification and interpretability in low-data regimes.

Conclusion: ICL-enhanced VLMs are a promising direction for resource-constrained scientific domains like THz imaging, improving classification and interpretability in low-data regimes.

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [211] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 提出LEAR通过显式推理和有意识地提取来解决检索噪音问题，提高了LLM的准确性。


<details>
  <summary>Details</summary>
Motivation: 检索噪音严重影响LLM的生成质量，需要开发去噪机制。先前的方法直接提取证据，缺乏显式思考，可能过滤掉关键线索且泛化能力不足。

Method: (1) 显式推理识别潜在线索，(2) 有意识地提取避免遗漏关键线索，将证据推理和提取统一为端到端训练的响应，应用知识令牌掩码进行解耦以获得基于推理和提取的答案，并设计了三种可验证的奖励函数（答案、长度、格式）通过策略优化算法更新模型。

Result: LEAR能够生成简洁高质量的证据，提高下游任务的准确性，并促进在在线RAG系统中的应用。

Conclusion: LEAR通过显式推理和有意识地提取证据，解决了检索噪音对LLM生成质量的影响，并在三个基准数据集上展示了其有效性，能够生成简洁高质量的证据，提高下游任务的准确性，并促进在在线RAG系统中的应用。

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [212] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: 我们提出了一个用于政治辩论中多模态逻辑谬误检测的方法，在多模态和文本分类上都取得了0.44左右的F1分数。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在推进多模态论证挖掘的研究，重点关注政治辩论中的逻辑谬误。

Method: 使用预训练的基于Transformer的模型，并提出几种利用上下文的方法。

Result: 在谬误分类子任务中，我们的模型取得了宏观F1分数：文本0.4444，音频0.3559，多模态0.4403。

Conclusion: 我们的多模态模型表现与仅文本模型相当，表明仍有改进潜力。

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [213] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: P3通过同时优化系统和用户提示来提升LLM性能，并在多项基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用通常采用多组件提示（包括系统和用户提示）来引导模型行为。虽然已有研究表明优化其中一个组件可以提升性能，但由于组件间的相互依赖性，单一组件优化往往效果不佳。

Method: P3是一个新颖的自改进框架，通过迭代过程同时优化系统和用户提示。离线优化提示后，进一步通过执行查询相关的提示优化来促进在线提示。

Result: P3在通用任务（如Arena-hard和Alpaca-eval）和推理任务（如GSM8K和GPQA）上进行了广泛实验，结果表明P3在自动提示优化方面取得了卓越的性能。

Conclusion: P3框架通过同时优化系统和用户提示，并结合查询相关的在线优化，在通用和推理任务上均取得了优于现有方法的性能，证明了整体优化策略在提升LLM性能方面的有效性。

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [214] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: 本研究提出CoLD框架，通过长度惩罚、偏差估计和联合训练来解决大语言模型推理中的长度偏差问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的过程奖励模型（PRM）在评估和指导大语言模型（LLM）的多步推理（尤其是在数学问题解决方面）中起着关键作用。然而，这些模型存在普遍的长度偏差，即倾向于给更长的推理步骤赋予更高的分数，即使语义内容和逻辑有效性没有改变。这种偏差会降低奖励预测的可靠性，并导致在推理过程中产生冗余的输出。

Method: 提出了一种名为CoLD（Counterfactually-Guided Length Debiasing）的统一框架，该框架包含三个主要组成部分：显式的长度惩罚调整、学习偏差估计器以及联合训练策略，以减轻长度偏差。

Result: 在MATH500和GSM-Plus数据集上的大量实验表明，CoLD能持续降低奖励与长度之间的相关性，提高步骤选择的准确性，并鼓励更简洁、逻辑上有效的推理。

Conclusion: CoLD框架通过显式的长度惩罚调整、学习到的偏差估计器以及联合训练策略，有效缓解了过程奖励模型（PRM）中的长度偏差问题，提高了奖励预测的准确性和推理的简洁性。

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [215] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: New models enable receivers to learn compositional information in signaling games by learning from atomic message components.


<details>
  <summary>Details</summary>
Motivation: Standard signaling game models fail to learn compositional information, as information from one message component is lost even when other components are present.

Method: The paper constructs two novel signaling game models: one with a minimalist receiver learning from atomic messages, and another with a generalist receiver learning from all available information.

Result: The proposed models allow receivers to learn from the atomic components of messages, leading to genuine compositional understanding, and are simpler than prior alternatives.

Conclusion: Two new models are proposed where receivers can learn compositional information from messages, overcoming limitations of previous models.

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [216] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: LLM在推理任务上的表现受问题类型影响，且准确性指标间存在差异。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索不同问题类型对大型语言模型在推理任务上准确性的影响，这是一个此前未被充分研究的领域。

Method: 本研究评估了五种大型语言模型在定量和演绎推理任务上针对三种不同类型问题的表现，并使用了准确性（推理步骤和最终答案选择）作为性能指标。

Result: 研究发现，不同问题类型确实会导致LLM表现出显著差异。此外，推理过程的准确性与最终答案选择的准确性之间没有必然的关联。最后，问题的选项数量和措辞的选择会影响LLM的表现。

Conclusion: LLM在推理任务上的表现因问题类型而异，且推理步骤的准确性不一定与最终答案的选择准确性相关。问题的选项数量和措辞也会影响LLM的表现。

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [217] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: 本文研究了对比学习方法在跨语言情感识别任务中的应用，并取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: SemEval-2025 Task 11旨在弥合基于文本的情感识别差距，并鼓励研究人员探索更先进的方法来应对情感表达和背景变化的挑战。

Method: 本文探索了基于样本的对比学习（Contrastive Reasoning Calibration）和基于生成的对比学习（DPO, SimPO）两种对比学习方法。所有模型均基于LLaMa3-Instruct-8B进行微调。

Result: 在SemEval-2025 Task 11中，本文提出的系统在英文的Track A（多标签分类）中获得第九名，在Track B（情感强度预测）中获得第六名，同时在其他语言上也表现出色，位列顶尖水平。

Conclusion: 本文探索了两种对比学习方法（基于样本的对比学习和基于生成的对比学习）在跨语言情感识别任务中的应用，并取得了良好的效果。

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [218] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: 本研究通过分析用户如何与天文LLM机器人互动，提出了改进LLM评估基准的方法，以提高其在科学研究中的可用性。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM评估基准未能跟上用户多样化的评估和使用方式，尤其是在天文学等科学研究领域。

Method: 通过对368个查询进行归纳编码和对11位天文学家进行访谈，分析用户如何评估LLM在天文文献检索中的应用。

Result: 揭示了用户评估LLM系统的具体方式，包括提问类型和评价标准，并提出了构建更好基准的具体建议。

Conclusion: 本研究提出了改进LLM评估的方法，并构建了一个用于评估天文领域LLM的基准。

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [219] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: 眼科大语言模型评估基准 BELO 包含 900 个专家审查的问题，评估准确性和推理能力，已用于测试包括 GPT-4o 和 Gemini 1.5 Pro 在内的六种模型。


<details>
  <summary>Details</summary>
Motivation: 当前评估眼科大语言模型的基准在范围上有限，并且过度侧重于准确性。因此，需要一个更全面、更标准化的评估基准来衡量模型的临床准确性和推理能力。

Method: 使用关键词匹配和微调的 PubMedBERT 模型，从 BCSC、MedMCQA、MedQA、BioASQ 和 PubMedQA 等医学数据集中收集了眼科专业的多项选择题 (MCQs)。通过多轮专家审查、剔除重复和低质量问题、以及由资深眼科专家最终裁定答案解释等步骤，确保了数据集的质量。使用准确率、宏 F1 分数以及 ROUGE-L、BERTScore、BARTScore、METEOR 和 AlignScore 等文本生成指标对六种大语言模型进行了评估，并由两位眼科专家对模型输出进行了定性审查。

Result: 在 BELO 基准上，六种大语言模型在准确率、宏 F1 分数以及五种文本生成指标上进行了评估。此外，人类专家评估显示了模型输出在准确性、全面性和完整性方面的质量。

Conclusion: BELO 是一个全面且标准化的眼科大语言模型评估基准，包含 900 个由眼科专家审查的高质量问题，并已建立公开排行榜以促进透明评估。该基准旨在评估模型的准确性和推理能力，并已用于评估六种主流大语言模型。

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [220] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: LLMs 在跨学科研究（IDR）方面仍有待提高。我们创建了一个名为 IDRBench 的新基准，以评估 LLMs 在提出跨学科研究想法方面的能力。虽然 LLMs 在此方面有所进步，但仍难以产生高质量的想法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个专门评估 LLMs 在跨学科研究（IDR）领域发展想法能力的基准，这阻碍了人们对其优势和局限性的充分理解。

Method: 创建了一个名为 IDRBench 的新基准，该基准包含一个专家注释的数据集和一套针对评估 LLMs 在不同科学领域提出有价值的跨学科研究（IDR）想法的能力而量身定制的任务。评估任务包括 IDR 论文识别、IDR 想法整合和 IDR 想法推荐。

Result: 在 IDRBench 上测试了 10 个 LLMs，结果表明 LLMs 在培养一定程度的 IDR 意识方面有所进步，但仍然难以产生高质量的 IDR 想法。

Conclusion: LLMs 在跨学科研究（IDR）方面仍有很大提升空间，需要开发能够在该领域表现出色的下一代 LLMs。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [221] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: DialogueForge框架使用种子提示和各种LLM来生成模拟人类-聊天机器人对话，并探索了微调技术以提高小型模型的性能。虽然大型模型在真实性方面表现更好，但小型模型提供了可定制性，并且可以通过微调得到改进。然而，所有模型在保持长期对话的连贯性和自然性方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 收集人类-聊天机器人对话通常需要大量的手动工作，并且耗时，这限制了对话式人工智能的研究并带来了挑战。

Method: 本研究提出的DialogueForge框架，使用从真实人类-聊天机器人交互中提取的种子提示来初始化每个生成的对话。测试了多种大型语言模型（LLMs）来模拟人类聊天机器人用户，并生成针对特定任务的多轮对话。此外，还探索了微调技术以增强小型模型生成无法区分的、类似人类的对话的能力。使用UniEval和GTEval评估协议评估了模拟对话的质量并比较了不同的模型。

Result: 大型专有模型（例如GPT-4o）在生成更真实的对话方面通常优于其他模型，而小型开源模型（例如Llama，Mistral）则以更大的可定制性提供了有希望的性能。实验证明，通过采用监督微调技术，可以显著提高小型模型的性能。然而，保持连贯自然的长期类人对话仍然是所有模型面临的共同挑战。

Conclusion: 收集人类与聊天机器人的对话需要大量的人工，这给对话式人工智能的研究带来了挑战。本研究提出的DialogueForge框架能够生成模拟人类与聊天机器人风格的对话。通过使用从真实对话中提取的种子提示来初始化生成的对话，并测试了各种大型语言模型（LLMs）来模拟人类聊天机器人用户。研究还探讨了微调技术以提高小型模型生成类似人类对话的能力。通过UniEval和GTEval评估协议的实验表明，像GPT-4o这样的大型专有模型在生成更真实的对话方面通常优于其他模型，而像Llama和Mistral这样的小型开源模型也展现出有前景的性能，并且具有更大的可定制性。通过监督微调技术可以显著提高小型模型的性能。然而，保持连贯自然的长期类人对话仍然是所有模型面临的共同挑战。

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [222] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: 本文提出“交互即智能”，将人机关系从接口提升为认知协作。通过“深度认知”系统，用户可监督 AI 思考过程，实现透明、可控、可中断的交互。实验结果显示，该范式在多项关键指标上显著优于传统方法，提升了人机在深度研究任务中的协作效率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统人机交互模式将交互视为访问 AI 的接口，忽视了交互在深度研究任务中的核心作用。这种“输入-等待-输出”的模式会导致错误累积、研究边界僵化以及无法整合人类专业知识。为了克服这些限制，需要一种新的交互范式。

Method: 本文提出了一种名为“深度认知”的新系统和范式，通过三个关键创新来实现“交互即智能”：1. 透明、可控、可中断的交互，揭示 AI 决策过程并允许随时干预；2. 细粒度的双向对话；3. 共享认知上下文，使系统能适应用户行为。

Result: 用户评估表明，该认知监督范式在透明度、细粒度交互、实时干预、协作易用性、结果价值和可中断性六个关键指标上均优于最强的基线模型，分别提升了 20.0%、29.2%、18.5%、27.7%、8.8% 和 20.7%。在应对深度研究问题时，相比于现有的深度研究系统，性能提升了 31.8% 到 50.0%。

Conclusion: 本文提出了一种名为“深度认知”的新范式，将人机关系从简单的接口交互提升到“交互即智能”的认知协作层面，用户从指令输入者转变为智能的认知监督者，通过战略性干预来引导 AI 的思考过程，并在关键节点进行调整。

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [223] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova, a 650M-parameter transformer, matches the performance of larger models using efficient architecture and tokenization, challenging current scaling trends.


<details>
  <summary>Details</summary>
Motivation: To achieve the performance of larger models while maintaining computational efficiency through careful architectural design and tokenization innovation.

Method: Supernova, a 650M-parameter decoder-only transformer, utilizes Rotary Positional Embeddings (RoPE), Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm, SwiGLU activation functions, and a custom 128,000-vocabulary byte-level BPE tokenizer.

Result: Supernova achieves 90% of the performance of 1B-parameter models with 53% fewer parameters and requiring only 100B training tokens, demonstrating significant computational efficiency.

Conclusion: Supernova's performance challenges the prevailing scaling paradigm by demonstrating that architectural efficiency and tokenization quality can compensate for reduced parameter counts.

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [224] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: Archer improves LLMs by applying different training signals to knowledge and reasoning tokens, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Previous RLVR algorithms apply uniform training signals to all tokens, disregarding the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens, potentially hindering effective learning.

Method: Archer applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge, utilizing dual-token constraints and synchronous updates.

Result: Experimental results show that Archer significantly outperforms previous RLVR methods on several mathematical reasoning and code generation benchmarks.

Conclusion: Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates, significantly outperforms previous RLVR methods on mathematical reasoning and code generation tasks, reaching or exceeding state-of-the-art performance.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [225] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: 该研究对比了Transformer和RNN在文本处理上的优劣。Transformer预测更准，但RNN更快、更节能。注意力机制的引入能提升RNN的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLM）在处理海量数据和生成类人文本方面存在的能耗高和处理速度慢的问题，本研究旨在探索循环神经网络（RNN）在自然文本处理中的应用，以期实现快速且节能的硬件实现。

Method: 该研究比较了三种不同的字符级语言建模方法：两种基于循环神经网络（RNN）的方法（其中只有输出层可训练）和一种基于Transformer的方法（其能够完全学习基于注意力的序列表示）。研究人员通过统一的评估流程，并等比调整所有模型的训练参数数量，来探究这两种方法的性能、计算成本和预测准确性。

Result: Transformer在预测质量上表现优异，而循环神经网络（RNN）在计算效率上具有显著优势，能够有效缩短训练和推理时间。研究还发现，注意力增强型RNN在动态调整输出权重方面表现更佳。

Conclusion: 该研究对比了两种不同的循环神经网络（RNN）方法和基于Transformer的方法，发现在预测质量方面Transformer表现更优，而RNN在效率方面表现突出，能够显著减少训练和推理时间。此外，研究还探讨了两种RNN的类型：一种是传统的具有静态线性读取器的RNN，另一种是具有注意力机制的RNN，能够动态地调整其输出权重。研究结果强调了这些模型在资源限制下的扩展性，并为在性能和资源消耗之间取得平衡提供了指导。

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [226] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: 双语模型推理时会混合使用语言，这是一种有益的策略。强制单语会降低准确率，而引导语言切换可提高准确率。


<details>
  <summary>Details</summary>
Motivation: 研究中文-英文双语推理模型中的语言切换现象，探究语言混合是否对推理有益。

Method: 通过识别强化学习与可验证奖励（RLVR）为导致语言混合的关键训练阶段，并进行实验验证语言混合对推理的益处，同时训练轻量级探测器来预测语言切换是否会带来收益。

Result: 强制单语解码会使数学推理任务的准确率降低5.6个百分点；轻量级探测器引导解码可将准确率提高高达6.25个百分点。

Conclusion: 语言混合并非多语训练的副产品，而是一种策略性的推理行为。

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [227] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: 发布了3LM，一套三个新的阿拉伯语基准测试，用于评估STEM和代码领域的大型语言模型，弥补了现有评估的不足。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语LLM评估主要集中在语言、文化或宗教内容，而STEM和代码等领域的评估则相对有限，存在巨大缺口。

Method: 创建了三个基准测试：1. 来自阿拉伯语教科书和工作表的STEM问答对；2. 使用相同来源生成的STEM问题；3. 通过翻译和人工审核过程创建的阿拉伯语代码生成基准测试。

Result: 发布了三个新的阿拉伯语基准测试，旨在促进STEM和代码领域阿拉伯语LLM的研究和发展。

Conclusion: 发布了三个专门针对阿拉伯语的基准测试（3LM），以解决STEM和代码领域LLM评估的不足。

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [228] [Longitudinal Sampling of URLs From the Wayback Machine](https://arxiv.org/abs/2507.14752)
*Kritika Garg,Sawood Alam,Dietrich Ayala,Mark Graham,Michele C. Weigle,Michael L. Nelson*

Main category: cs.DL

TL;DR: 本文分析了从互联网档案库收集的 2730 万个 URL 和 38 亿个网页存档数据（1996-2021），旨在研究网页的持久性。研究者通过对 URL 的首次存档时间、MIME 类型、深度和 TLD 等维度进行采样，并解决数据稀疏和域名代表性不均的问题，最终构建了一个大规模数据集，并分享了采样经验。


<details>
  <summary>Details</summary>
Motivation: 本文的主要动机是重新审视关于公共可存档网络的规模、性质和普及率等基本问题，特别是回答“网页能持续多久？”这一关键问题。为了回答这个问题，需要对网络进行采样。研究者希望通过对互联网档案库（IA）的 Wayback Machine 中海量数据进行分析，为理解网络存档的现状和挑战提供实证依据，并为未来进行类似的网络研究提供实践指导和经验教训。

Method: 本文研究方法包括：1. 从互联网档案库（IA）的 Wayback Machine 中提取了 2730 万个 URL 和 38 亿个存档页面。2. 重点关注“网页能持续多久？”这一问题，并为此提出了 URL 采样维度：首次存档时间、HTML 与其他 MIME 类型、URL 深度（顶层页面 vs. 深度链接）以及顶级域名（TLD）。3. 从 IA 的 ZipNum 索引文件中采样了 2.85 亿个 URL，并筛选出文本/HTML 类型，共 9200 万个。4. 为了解决早期年份（1996-2021）数据稀疏的问题，通过提取顶层 URL 来对早期年份进行过采样，目标是每年的 URL 数量，但由于稀疏性，将 1996-2021 年份聚类收集了 120 万个 URL。5. 为了处理 Yahoo 和 Twitter 等流行域名的过度代表问题，进行了对数尺度降采样。6. 最终构建了一个包含 2730 万个 URL 的时间映射（TimeMaps）数据集，其中包含 38 亿个存档页面。7. 总结了从存档网络采样过程中获得的经验教训。

Result: 本文的研究结果包括：1. 成功构建了一个包含 2730 万个 URL 和 38 亿个存档页面的数据集，覆盖了 1996 年至 2021 年的网络存档。2. 发现存档速度和容量随时间显著增加，近年来存档的 URL 数量更多。3. 确认了早期年份（1996-2021）的 URL 数据相对稀疏，并通过聚类方法收集了 120 万个 URL 来代表该时间范围。4. 通过对数尺度降采样，解决了流行域名（如 Yahoo、Twitter）过度代表的问题，使得样本分布更为均衡。5. 总结了从大规模网络存档数据采样过程中获得的宝贵经验教训，为未来研究者提供了有价值的参考。

Conclusion: 本文档记录了从互联网档案库的 Wayback Machine 中采样网络的研究策略和经验教训，收集了 2730 万个 URL 和 38 亿个存档页面，时间跨度为 26 年（1996-2021）。研究旨在重新审视关于公共可存档网络的规模、性质和普及率等基本问题，特别是“网页能持续多久？”这一问题。通过对 URL 采样维度（首次存档时间、HTML 与其他 MIME 类型、URL 深度、顶级域名）的分析，并结合时间映射（TimeMaps）和对流行域名的降采样，最终构建了一个包含 2730 万个 URL 的数据集，为未来的网络存档研究提供了指导。该研究发现，存档速度和容量随时间增加，早期年份的 URL 稀疏，并通过对这些年份进行聚类和对流行域名进行降采样来平衡数据集。最终数据集的时间跨度和规模为未来的研究奠定了基础。

Abstract: We document strategies and lessons learned from sampling the web by
collecting 27.3 million URLs with 3.8 billion archived pages spanning 26 years
(1996-2021) from the Internet Archive's (IA) Wayback Machine. Our goal is to
revisit fundamental questions regarding the size, nature, and prevalence of the
publicly archivable web, in particular, to reconsider the question: "How long
does a web page last?" Addressing this question requires obtaining a sample of
the web. We proposed several dimensions to sample URLs from the Wayback
Machine's holdings: time of first archive, HTML vs. other MIME types, URL depth
(top-level pages vs. deep links), and top-level domain (TLD). We sampled 285
million URLs from IA's ZipNum index file, which contains every 6000th line of
the CDX index. These indexes also include URLs of embedded resources such as
images, CSS, and JavaScript. To limit our sample to "web pages" (i.e., pages
intended for human interaction), we filtered for likely HTML pages based on
filename extension. We then queried IA's CDX API to determine the time of first
capture and MIME type of each URL. We grouped 92 million text/html URLs based
on year of first capture. Archiving speed and capacity have increased over
time, so we found more URLs archived in later years. To counter this, we
extracted top-level URLs from deep links to upsample earlier years. Our target
was 1 million URLs per year, but due to sparseness during 1996-2021, we
clustered those years, collecting 1.2 million URLs for that range. Popular
domains like Yahoo and Twitter were over-represented, so we performed
logarithmic-scale downsampling. Our final dataset contains TimeMaps of 27.3
million URLs, comprising 3.8 billion archived pages. We convey lessons learned
from sampling the archived web to inform future studies.

</details>


### [229] [Researcher Population Pyramids for Tracking Global Demographic and Gender Trajectories](https://arxiv.org/abs/2507.15500)
*Kazuki Nakajima,Takayuki Mizuno*

Main category: cs.DL

TL;DR: 通过分析出版数据，本研究提出了一个研究人员人口金字塔框架，用于评估全球研究人员的人口结构和性别平衡。该框架揭示了不同国家研究系统的差异，并预测了未来发展趋势，为促进学术界的可持续发展和性别平等提供了工具。


<details>
  <summary>Details</summary>
Motivation: 为了应对全球学术生态系统可持续性对研究人员人口结构和性别平衡的依赖，以及及时评估这些动态以制定政策的挑战，本研究提出了研究人员人口金字塔框架。

Method: 本研究利用出版数据提出一个研究人员人口金字塔框架，用于追踪全球人口结构和性别轨迹。

Result: 研究结果显示，新兴系统（如阿拉伯国家）的研究人员流入率高，但累积生产力中的性别差距在扩大；成熟系统（如美国）的研究人员流入率适中，性别差距在缩小；僵化系统（如日本）则在这两方面均表现滞后。此外，通过模拟未来情景，研究预测若2023年的人口结构模式持续，阿拉伯国家的研究系统到2050年可能变得类似成熟甚至僵化的系统。

Conclusion: 该研究提出的研究人员人口金字塔框架通过分析出版数据，能够及时评估全球学术生态系统中研究人员的人口结构和性别平衡状况，为政策制定提供依据。该框架揭示了三种不同的研究系统：新兴系统（如阿拉伯国家）流入率高但性别差距扩大；成熟系统（如美国）流入率适中且性别差距缩小；僵化系统（如日本）则在两方面均滞后。通过模拟未来情景，研究预测若2023年的人口结构模式持续，阿拉伯国家的研究系统可能在2050年发展为成熟甚至僵化系统。该框架为全球政策制定者提供了有力的诊断工具，有助于促进可持续人才储备和学术界的性别平等。

Abstract: The sustainability of the global academic ecosystem relies on researcher
demographics and gender balance, yet assessing these dynamics in a timely
manner for policy is challenging. Here, we propose a researcher population
pyramids framework for tracking global demographic and gender trajectories
using publication data. This framework provides a timely snapshot of historical
and present demographics and gender balance, revealing three contrasting
research systems: Emerging systems (e.g., Arab countries) exhibit high
researcher inflows with widening gender gaps in cumulative productivity; Mature
systems (e.g., the United States) show modest inflows with narrowing gender
gaps; and Rigid systems (e.g., Japan) lag in both. Furthermore, by simulating
future scenarios, the framework makes potential trajectories visible. If 2023
demographic patterns persist, Arab countries' systems could resemble mature or
even rigid ones by 2050. Our framework provides a robust diagnostic tool for
policymakers worldwide to foster sustainable talent pipelines and gender
equality in academia.

</details>


### [230] [Drafting the Landscape of Computational Musicology Tools: a Survey-Based Approach](https://arxiv.org/abs/2507.15590)
*Jorge Junior Morgado Vega,Sachin Sharma,Federico Simonetta*

Main category: cs.DL

TL;DR: 本研究评估了计算音乐学工具的现状，发现当前工具的功能与用户需求之间存在差距，并指出了其局限性，旨在促进工具开发者与音乐学学者之间的对话。


<details>
  <summary>Details</summary>
Motivation: 自20世纪60年代以来，计算工具在系统分析方法到创造力建模等多个方面对音乐学产生了日益深远的影响。本研究旨在全面评估计算音乐学工具的现状。

Method: 本研究通过调查数据对计算音乐学工具的现状进行了全面评估，收集了有关工具使用模式、常见分析任务、用户满意度、数据特征和优先功能的数据。

Result: 研究结果揭示了当前工具的功能与用户需求之间存在显著差距，并强调了这些工具在所有领域都存在一些局限性。

Conclusion: 该评估有助于促进工具开发者与音乐学学者之间的对话，旨在提高计算方法在音乐学研究中的有效性和可及性。

Abstract: Since the 60s, musicology has been increasingly impacted by computational
tools in various ways, from systematic analysis approaches to modeling of
creativity. This article presents a comprehensive assessment of the current
state of Computational Musicology tools based on survey data collected from
practitioners in the field. We gathered information on tool usage patterns,
common analytical tasks, user satisfaction levels, data characteristics, and
prioritized features across four distinct domains: symbolic music,
music-related imagery, audio, and text. Our findings reveal significant gaps
between current tooling capabilities and user needs, highlighting some
limitations of these tools across all domains. This assessment contributes to
the ongoing dialogue between tool developers and music scholars, aiming to
enhance the effectiveness and accessibility of computational methods in
musicological research.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [231] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: 提出 APTx Neuron，一种整合了激活和线性变换的统一神经单元，在 MNIST 上实现了 96.69% 的准确率，展示了其优越性和效率。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合非线性激活和线性变换，创建一种计算效率高且结构简洁的统一神经计算单元。

Method: 提出了一种名为 APTx Neuron 的新型统一神经计算单元，将非线性激活和线性变换整合为单一可训练表达式。该神经元源自 APTx 激活函数，其功能形式为 $y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$，其中所有参数 $\alpha_i$、$\beta_i$、$\gamma_i$ 和 $\delta$ 均可训练。

Result: 在 MNIST 数据集上，基于 APTx Neuron 的架构在仅 20 个 epoch 和约 332K 可训练参数的情况下，达到了 96.69% 的测试准确率。

Conclusion: APTx Neuron 相较于传统神经元具有更优越的表现力和计算效率，为统一神经元设计及其架构开辟了新的范例。

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


### [232] [Training oscillator Ising machines to assign the dynamic stability of their equilibrium points](https://arxiv.org/abs/2507.14386)
*Yi Cheng,Zongli Lin*

Main category: cs.NE

TL;DR: 提出了一种新的HRECM方法，用于训练OIM的耦合权重，以实现Hopfield类联想记忆。


<details>
  <summary>Details</summary>
Motivation: 为了实现Hopfield类联想记忆，需要为神经网络模型的平衡点（EPs）分配合适的稳定性。振荡器Ising机（OIM）是实现这一目标的理想模型，因为其所有$0/\pi$二元EPs都具有结构稳定性，并且其动态稳定性可以通过耦合权重进行调整。

Method: 提出了一种Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM)，用于训练振荡器Ising机（OIM）的耦合权重，通过调整EPs的稳定性的方式来存储模式。

Result: 数值实验验证了所提出的HRECM方法的有效性。

Conclusion: 所提出的Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM)能够有效地训练OIM的耦合权重，为其EPs分配合适的稳定性，从而实现Hopfield类联想记忆。

Abstract: We propose a neural network model, which, with appropriate assignment of the
stability of its equilibrium points (EPs), achieves Hopfield-like associative
memory. The oscillator Ising machine (OIM) is an ideal candidates for such a
model, as all its $0/\pi$ binary EPs are structurally stable with their dynamic
stability tunable by the coupling weights. Traditional Hopfield-based models
store the desired patterns by designing the coupling weights between neurons.
The design of coupling weights should simultaneously take into account both the
existence and the dynamic stability of the EPs for the storage of the desired
patterns. For OIMs, since all $0/\pi$ binary EPs are structurally stable, the
design of the coupling weights needs only to focus on assigning appropriate
stability for the $0/\pi$ binary EPs according to the desired patterns. In this
paper, we establish a connection between the stability and the Hamiltonian
energy of EPs for OIMs, and, based on this connection, provide a
Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM) to train the
coupling weights of OIMs for assigning appropriate stability to their EPs.
Finally, numerical experiments are performed to validate the effectiveness of
the proposed method.

</details>


### [233] [Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space](https://arxiv.org/abs/2507.14757)
*Szymon Mazurek,Jakub Caputa,Maciej Wielgosz*

Main category: cs.NE

TL;DR: SNN 的性能和能效与其神经元参数密切相关。识别和操作 SNN 的“操作空间”可以提高其准确性和能效，并增强其对抗噪声的能力。


<details>
  <summary>Details</summary>
Motivation: SNN 是一种节能且具有生物学上可信度的替代传统人工神经网络的方法，但其性能很大程度上取决于神经元模型参数的调整。

Method: 通过系统地探索数据集和架构，对 SNN 的超参数域（膜时间常数 tau 和电压阈值 vth）进行可视化和量化，以识别其操作空间（有意义的活动和功能行为的约束区域）。

Result: 在识别的操作空间内运行可以实现分类精度和脉冲活动的最佳权衡。在该空间之外运行会导致性能下降，例如能量消耗过大或网络完全沉默。此外，SNN 在超出最佳运行区域时表现出更高的脉冲相关性和内部同步性，表明对对抗性噪声的鲁棒性降低。

Conclusion: SNNs 的性能和能效对其神经元模型参数（尤其是膜时间常数 tau 和电压阈值 vth）的调整至关重要。通过识别和量化一个操作空间（神经元超参数域中的一个约束区域），可以优化分类精度和脉冲活动之间的权衡，并确保鲁棒性。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient and biologically
plausible alternatives to traditional artificial neural networks, but their
performance depends critically on the tuning of neuron model parameters. In
this work, we identify and characterize an operational space - a constrained
region in the neuron hyperparameter domain (specifically membrane time constant
tau and voltage threshold vth) - within which the network exhibits meaningful
activity and functional behavior. Operating inside this manifold yields optimal
trade-offs between classification accuracy and spiking activity, while stepping
outside leads to degeneration: either excessive energy use or complete network
silence.
  Through systematic exploration across datasets and architectures, we
visualize and quantify this manifold and identify efficient operating points.
We further assess robustness to adversarial noise, showing that SNNs exhibit
increased spike correlation and internal synchrony when operating outside their
optimal region. These findings highlight the importance of principled
hyperparameter tuning to ensure both task performance and energy efficiency.
Our results offer practical guidelines for deploying robust and efficient SNNs,
particularly in neuromorphic computing scenarios.

</details>


### [234] [DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving](https://arxiv.org/abs/2507.15615)
*Zhihao Zhang,Siyuan Li,Chenxi Li,Feifan Liu,Mengjing Chen,Kai Li,Tao Zhong,Bo An,Peng Liu*

Main category: cs.NE

TL;DR: DHEvo通过数据-算法协同进化解决了LLM启发式方法在MILP问题中的泛化能力不足问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的启发式方法在处理同一问题类别中的MILP实例时泛化能力不足，因为它们未能捕捉实例的特征，而MILP实例在结构和特征分布上常常存在显著差异。

Method: 提出了一种数据-算法协同进化框架（DHEvo），利用基于LLM的多智能体系统同时生成数据-代码对，并通过适应度分数迭代优化，以识别整个问题类别中最有效的启发式方法。

Result: 在广泛的MILP基准测试中，DHEvo方法显著优于手动设计的启发式方法和现有的基于LLM的方法，表明其在识别和利用MILP实例特征以生成有效启发式方法方面具有优势。

Conclusion: 所提出的数据-算法协同进化框架（DHEvo）通过迭代选择代表性实例并优化相应启发式方法，显著优于手动设计的启发式方法和现有的基于LLM的方法，解决了现有LLM方法泛化能力不足的问题。

Abstract: Primal heuristics play a critical role in improving the efficiency of mixed
integer programming (MILP) solvers. As large language models (LLMs) have
demonstrated superior code generation abilities, recent MILP works are devoted
to leveraging the evolutionary computation approaches with LLMs to generate
effective primal heuristics. Although the generated heuristics have achieved
better solving performance than the hand-crafted ones with little adaptability,
the advantage of current LLM-based methods is limited to few MILP instances in
one problem class, as they fail to capture the instance characteristics in the
problem class (the MILP instances generated from the same mathematical model
are defined as a problem class). Since MILP instances often differ
significantly in structure and feature distribution, the neglect of their
characteristics in the evolution process results in poor generalization within
the same problem class. To overcome this challenge, we propose a data-algorithm
co-evolution framework (DHEvo) that iteratively selects representative
instances and evolves corresponding heuristics. With the initial instance
distribution, we develop an LLM-based multi-agent system to generate data-code
pairs simultaneously. These data-code pairs are iteratively refined based on
their fitness scores, leading to the identification of the most effective
heuristic over the entire problem class. Extensive experiments across diverse
MILP benchmarks demonstrate that our approach significantly outperforms both
human-designed heuristics and existing LLM-based methods.

</details>


### [235] [TONUS: Neuromorphic human pose estimation for artistic sound co-creation](https://arxiv.org/abs/2507.15734)
*Jules Lecomte,Konrad Zinner,Michael Neumeier,Axel von Arnim*

Main category: cs.NE

TL;DR: 探索一种更具诗意和沉浸感的人机交互艺术形式，利用神经形态技术和身体传感，让参观者与机器共同创造声音和视觉艺术。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互在艺术和工业领域日益重要，但交互方式往往过于技术化和机器驱动，未能充分发挥新技术的潜力，阻碍了公众的深度参与。旨在实现一种更自然、更具诗意的人机交互方式，让机器成为增强人类想象力的工具。

Method: 提出一种艺术声音装置，利用神经形态身体传感技术，结合了神经形态多头人体姿态估计神经网络传感器，该传感器能够通过精细的身体动作控制声音和视觉输出，其中特征提取器是为特定神经形态芯片定制的脉冲神经网络。

Result: 创造了一种沉浸式的体验，参观者可以通过控制自己的身体动作，与一个能够进行神经形态思考的机器进行对话，共同创造声音景观和视觉效果。 Besucher, die in eine Klangatmosphäre und eine neural verarbeitete Darstellung ihrer selbst eingetaucht sind, die sie kontrollieren, erleben den Dialog mit einer Maschine, die neuronal denkt, ähnlich wie sie selbst.

Conclusion: 通过与机器进行无缝的人机交互，探索人机共生的艺术表达

Abstract: Human machine interaction is a huge source of inspiration in today's media
art and digital design, as machines and humans merge together more and more.
Its place in art reflects its growing applications in industry, such as
robotics. However, those interactions often remains too technical and
machine-driven for people to really engage into. On the artistic side, new
technologies are often not explored in their full potential and lag a bit
behind, so that state-of-the-art research does not make its way up to museums
and exhibitions. Machines should support people's imagination and poetry in a
seamless interface to their body or soul. We propose an artistic sound
installation featuring neuromorphic body sensing to support a direct yet non
intrusive interaction with the visitor with the purpose of creating sound
scapes together with the machine. We design a neuromorphic multihead human pose
estimation neural sensor that shapes sound scapes and visual output with fine
body movement control. In particular, the feature extractor is a spiking neural
network tailored for a dedicated neuromorphic chip. The visitor, immersed in a
sound atmosphere and a neurally processed representation of themselves that
they control, experience the dialogue with a machine that thinks neurally,
similarly to them.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [236] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: LOVO是一个高效处理大规模视频数据复杂对象查询的新系统，通过一次性特征提取、多索引结构和跨模态重排，实现了高准确性和低延迟。


<details>
  <summary>Details</summary>
Motivation: 大规模视频数据为交通管理和犯罪监控等应用带来机遇，但从海量视频数据中查询特定对象面临数据量大、查询需求复杂和低延迟执行的挑战。现有方法在适应新对象类别或查询延迟方面存在不足。

Method: LOVO系统采用一次性特征提取，利用预训练的视觉编码器生成紧凑的视觉嵌入，并将这些嵌入与边界框一起存储在向量数据库的多索引结构中，以支持任意对象的查询。查询时，将对象查询转换为查询嵌入，进行快速近似最近邻搜索，并通过融合视觉和文本特征的跨模态重排来优化结果。

Result: LOVO在真实世界视频数据集上的评估显示，在处理复杂查询时，其表现优于现有方法，查询准确性接近最优，搜索延迟最高可降低85倍，同时显著降低了索引构建成本。

Conclusion: LOVO通过对关键帧进行特征提取并构建高效索引，在处理复杂视频对象查询方面表现出色，相比现有方法具有更高的查询准确性和更低的延迟，并显著降低了索引构建成本。

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [237] [A Reproducibility Study of Product-side Fairness in Bundle Recommendation](https://arxiv.org/abs/2507.14352)
*Huy-Son Nguyen,Yuanna Liu,Masoud Mansoury,Mohammad Alian Nejadi,Alan Hanjalic,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本研究首次系统性地研究了包推荐中的产品侧公平性问题，发现包和项目层面的暴露差异显著，且受用户行为和评估指标的影响，为开发更公平的推荐系统提供了见解。


<details>
  <summary>Details</summary>
Motivation: 现有针对传统推荐系统的公平性框架和指标可能不适用于包推荐（BR），因为BR引入了额外的复杂性：推荐是在包级别生成的，但用户满意度和产品（或供应商）的暴露取决于包本身及其包含的单个项目。因此，有必要探索BR中的产品侧公平性问题。

Method: 通过在三个真实世界的数据集上使用四种最先进的包推荐（BR）方法，对BR中的产品侧公平性进行全面的可复现性研究，并使用多种公平性指标在包和项目两个层面上分析暴露差异。

Result: 暴露模式在包和项目之间存在显著差异，这表明需要超越包级别假设的公平性干预措施。公平性评估结果因所使用的指标而异，并且用户行为（用户与包交互的频率）会影响公平性。当用户更多地与包交互时，推荐系统在包和项目层面都表现出更公平的暴露分布。

Conclusion: 研究结果表明，推荐系统的公平性干预需要超越仅基于包的假设，并且需要多方面的评估。此外，用户的行为模式在决定公平性方面起着至关重要的作用，当用户更频繁地与包进行交互时，推荐系统在包和项目两个层面上都倾向于产生更公平的暴露分布。这些发现为构建更公平的包推荐系统提供了可行的见解，并为该新兴领域未来的研究奠定了基础。

Abstract: Recommender systems are known to exhibit fairness issues, particularly on the
product side, where products and their associated suppliers receive unequal
exposure in recommended results. While this problem has been widely studied in
traditional recommendation settings, its implications for bundle recommendation
(BR) remain largely unexplored. This emerging task introduces additional
complexity: recommendations are generated at the bundle level, yet user
satisfaction and product (or supplier) exposure depend on both the bundle and
the individual items it contains. Existing fairness frameworks and metrics
designed for traditional recommender systems may not directly translate to this
multi-layered setting. In this paper, we conduct a comprehensive
reproducibility study of product-side fairness in BR across three real-world
datasets using four state-of-the-art BR methods. We analyze exposure
disparities at both the bundle and item levels using multiple fairness metrics,
uncovering important patterns. Our results show that exposure patterns differ
notably between bundles and items, revealing the need for fairness
interventions that go beyond bundle-level assumptions. We also find that
fairness assessments vary considerably depending on the metric used,
reinforcing the need for multi-faceted evaluation. Furthermore, user behavior
plays a critical role: when users interact more frequently with bundles than
with individual items, BR systems tend to yield fairer exposure distributions
across both levels. Overall, our findings offer actionable insights for
building fairer bundle recommender systems and establish a vital foundation for
future research in this emerging domain.

</details>


### [238] [RaMen: Multi-Strategy Multi-Modal Learning for Bundle Construction](https://arxiv.org/abs/2507.14361)
*Huy-Son Nguyen,Quang-Huy Nguyen,Duc-Hoang Pham,Duc-Trong Le,Hoang-Quynh Le,Padipat Sitkrongwong,Atsuhiro Takasu,Masoud Mansoury*

Main category: cs.IR

TL;DR: RaMen是一种新的捆绑构建方法，通过结合内在和外在信息以及显式和隐式策略感知学习，克服了现有方法的局限性，能够学习更全面、更鲁棒的捆绑表示，并在实验中证明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有捆绑构建研究仅依赖用户反馈或项目表示，未能捕捉现实世界捆绑结构中隐藏的复杂关系，导致捆绑表示不佳。

Method: RaMen通过显式策略感知学习(ESL)和隐式策略感知学习(ISL)来建模捆绑结构，ESL利用特定任务的注意力机制对多模态数据和项目之间的协作关系进行编码，ISL计算超边依赖和超图消息传递，以揭示项目组之间共享的潜在意图。多策略对齐与区分模块促进了学习策略之间的知识转移，并确保项目/捆绑之间的区分。

Result: RaMen能够学习更全面、更鲁棒的捆绑表示，并在多个领域上的实验证明其有效性。

Conclusion: RaMen在多个领域上的实验证明其优于现有最先进的模型，为复杂商品集问题提供了有价值的见解。

Abstract: Existing studies on bundle construction have relied merely on user feedback
via bipartite graphs or enhanced item representations using semantic
information. These approaches fail to capture elaborate relations hidden in
real-world bundle structures, resulting in suboptimal bundle representations.
To overcome this limitation, we propose RaMen, a novel method that provides a
holistic multi-strategy approach for bundle construction. RaMen utilizes both
intrinsic (characteristics) and extrinsic (collaborative signals) information
to model bundle structures through Explicit Strategy-aware Learning (ESL) and
Implicit Strategy-aware Learning (ISL). ESL employs task-specific attention
mechanisms to encode multi-modal data and direct collaborative relations
between items, thereby explicitly capturing essential bundle features.
Moreover, ISL computes hyperedge dependencies and hypergraph message passing to
uncover shared latent intents among groups of items. Integrating diverse
strategies enables RaMen to learn more comprehensive and robust bundle
representations. Meanwhile, Multi-strategy Alignment & Discrimination module is
employed to facilitate knowledge transfer between learning strategies and
ensure discrimination between items/bundles. Extensive experiments demonstrate
the effectiveness of RaMen over state-of-the-art models on various domains,
justifying valuable insights into complex item set problems.

</details>


### [239] [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
*Mathias Vast,Basile Van Cooten,Laure Soulier,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 本文通过分析注意力机制和因果关系，揭示了神经IR模型（特别是交叉编码器）的内部工作原理，重点在于匹配过程的解释，并识别了关键的注意力头以及匹配检测机制。


<details>
  <summary>Details</summary>
Motivation: 当前对神经IR模型的研究主要集中在高层次的解释上，缺乏对模型内部具体匹配过程的深入理解。

Method: 通过分析注意力机制和提取因果洞见来解释模型行为，并阐述了匹配检测的潜在机制。

Result: 研究识别出部分注意力头在匹配过程中起着关键作用，并对潜在的匹配检测机制提供了初步的解释。 尽管使用了更直接的方法，但仍能获得有价值的见解。

Conclusion:  本文旨在揭示神经信息检索（IR）模型（特别是交叉编码器）的内部工作机制，重点关注其匹配过程。

Abstract: Neural IR architectures, particularly cross-encoders, are highly effective
models whose internal mechanisms are mostly unknown. Most works trying to
explain their behavior focused on high-level processes (e.g., what in the input
influences the prediction, does the model adhere to known IR axioms) but fall
short of describing the matching process. Instead of Mechanistic
Interpretability approaches which specifically aim at explaining the hidden
mechanisms of neural models, we demonstrate that more straightforward methods
can already provide valuable insights. In this paper, we first focus on the
attention process and extract causal insights highlighting the crucial roles of
some attention heads in this process. Second, we provide an interpretation of
the mechanism underlying matching detection.

</details>


### [240] [Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module](https://arxiv.org/abs/2507.14612)
*Pei-Xuan Li,Wei-Yun Liang,Fandel Lin,Hsun-Ping Hsieh*

Main category: cs.IR

TL;DR: 本研究提出了一种新的推荐系统框架 GDPW，通过利用 POI 类别和时间信息以及 POI 之间的转移和距离关系，提高了下一个 POI 推荐的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法很少探索 POI 类别和时间之间的关系，这可能导致模型无法充分学习用户在不同时间访问某些 POI 类别的倾向。此外，现有方法在对时间信息进行建模时，通常将其转换为时间嵌入或计算时间间隔并将其纳入模型，难以捕捉时间的连续性。最后，在 POI 预测期间，通常会忽略各种加权信息，例如每个 POI 的受欢迎程度、POI 之间的转移关系以及 POI 之间的距离，导致性能不佳。

Method: 本论文提出了一种名为图解区分器和 POI 加权模块 (GDPW) 的新颖的下一个 POI 推荐框架。该框架旨在联合考虑 POI 类别信息和多个 POI 加权因素。具体来说，GDPW 通过全局类别图和全局类别-时间图学习类别和时间表示，然后通过对比学习区分类别和时间信息。在预测之后，通过基于 POI 之间的转移权重和距离关系对预测结果进行加权，从而获得用户最终的 POI 推荐。

Result: GDPW 框架联合考虑 POI 类别信息和多个 POI 加权因素，并通过对比学习区分类别和时间信息，最后基于转移权重和距离关系对预测结果进行加权，以获得用户最终的 POI 推荐。

Conclusion: 实验结果表明，我们提出的 GDPW 模型在两个真实数据集上的表现优于现有的其他模型，性能提升了 3% 到 11%。

Abstract: Next point of interest (POI) recommendation primarily predicts future
activities based on users' past check-in data and current status, providing
significant value to users and service providers. We observed that the popular
check-in times for different POI categories vary. For example, coffee shops are
crowded in the afternoon because people like to have coffee to refresh after
meals, while bars are busy late at night. However, existing methods rarely
explore the relationship between POI categories and time, which may result in
the model being unable to fully learn users' tendencies to visit certain POI
categories at different times. Additionally, existing methods for modeling time
information often convert it into time embeddings or calculate the time
interval and incorporate it into the model, making it difficult to capture the
continuity of time. Finally, during POI prediction, various weighting
information is often ignored, such as the popularity of each POI, the
transition relationships between POIs, and the distances between POIs, leading
to suboptimal performance. To address these issues, this paper proposes a novel
next POI recommendation framework called Graph Disentangler with POI Weighted
Module (GDPW). This framework aims to jointly consider POI category information
and multiple POI weighting factors. Specifically, the proposed GDPW learns
category and time representations through the Global Category Graph and the
Global Category-Time Graph. Then, we disentangle category and time information
through contrastive learning. After prediction, the final POI recommendation
for users is obtained by weighting the prediction results based on the
transition weights and distance relationships between POIs. We conducted
experiments on two real-world datasets, and the results demonstrate that the
proposed GDPW outperforms other existing models, improving performance by 3% to
11%.

</details>


### [241] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: 本研究提出了一种创新的两阶段（检索-重排）框架，通过优化数据处理和负例挖掘，显著提高了法律文档检索的效率和准确性，并在竞赛中取得了优异成绩，同时该框架比现有的大型模型更轻量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在法律等专业领域面临严峻挑战，因为这些领域对精确性和领域知识有严格要求。因此，本研究旨在提升法律文档检索的效率和准确性。

Method: 本研究提出的框架包含两个阶段：使用微调后的双编码器进行快速候选检索，然后使用交叉编码器进行精确重排。该方法通过战略性地挖掘负例进行了优化，并引入了Exist@m指标来评估检索效果，同时使用半难负例来减轻训练偏差。

Result: 在SoICT Hackathon 2024法律文档检索竞赛中，本研究团队（4Huiter）取得了前三名的成绩，证明了该框架的有效性。与使用集成模型和大型bge-m3架构迭代自训练的顶尖团队相比，本研究提出的轻量级、单程方法提供了一个参数量远少但具有竞争力的替代方案。

Conclusion: 本研究提出的检索-重排两阶段框架在法律文档检索任务中表现出高效性和准确性，证明了优化数据处理、定制损失函数和平衡负采样对于构建鲁棒的检索增强系统至关重要。

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [242] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: 本研究对基于MLLM的通用多模态检索（UMR）进行了全面研究，揭示了影响嵌入学习的关键因素，并提出了U-MARVEL框架，该框架在M-BEIR基准测试中取得了最先进的成果，并展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于MLLM的检索方法中，尽管取得了成功，但其检索能力背后的机制仍未得到充分探索，可能导致性能不佳和泛化能力受限的问题。

Method: 通过实现通用的多模态大模型（MLLM）嵌入学习流程，并系统地分析了影响通用检索系统性能的关键因素，包括渐进式转换、难负例挖掘和重排序器蒸馏。

Result: 在M-BEIR基准测试中，U-MARVEL框架在监督设置下显著优于现有技术，并在组合图像检索和文本到视频检索等任务上表现出强大的零样本性能。

Conclusion: U-MARVEL框架在M-BEIR基准测试中表现优于现有技术，并在多项任务中展现出强大的零样本性能，证明了其在各种基于嵌入的检索任务中的泛化潜力。

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


### [243] [User Invariant Preference Learning for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.14925)
*Mingshi Yan,Zhiyong Cheng,Fan Liu,Yingda Lyu,Yahong Han*

Main category: cs.IR

TL;DR: UIPL通过学习用户的不变偏好来解决多行为推荐中的噪声问题，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多行为推荐方法虽然旨在整合各种行为信息以丰富用户表征，但往往忽略了用户多行为偏好中存在的共性和个性。这些个性反映了不同行为所捕捉到的偏好差异，其中某些辅助行为可能会引入噪声，从而阻碍目标行为的预测。为了解决这个问题，我们提出了用户不变偏好学习（UIPL），旨在从多行为交互中捕捉用户的内在本性（称为不变偏好），以减轻噪声的引入。

Method: UIPL利用不变风险最小化范式来学习不变偏好。具体来说，我们采用变分自编码器（VAE）来提取用户的روinvariante偏好，用不变风险最小化约束替换标准的重建损失。此外，我们通过组合多行为数据来构建不同的环境，以增强学习这些偏好的鲁棒性。最后，利用学习到的不变偏好为目标行为提供推荐。

Result: UIPL 显著优于当前最先进的方法。

Conclusion: UIPL在四个真实世界数据集上的广泛实验证明，其性能显著优于当前最先进的方法。

Abstract: In multi-behavior recommendation scenarios, analyzing users' diverse
behaviors, such as click, purchase, and rating, enables a more comprehensive
understanding of their interests, facilitating personalized and accurate
recommendations. A fundamental assumption of multi-behavior recommendation
methods is the existence of shared user preferences across behaviors,
representing users' intrinsic interests. Based on this assumption, existing
approaches aim to integrate information from various behaviors to enrich user
representations. However, they often overlook the presence of both
commonalities and individualities in users' multi-behavior preferences. These
individualities reflect distinct aspects of preferences captured by different
behaviors, where certain auxiliary behaviors may introduce noise, hindering the
prediction of the target behavior. To address this issue, we propose a user
invariant preference learning for multi-behavior recommendation (UIPL for
short), aiming to capture users' intrinsic interests (referred to as invariant
preferences) from multi-behavior interactions to mitigate the introduction of
noise. Specifically, UIPL leverages the paradigm of invariant risk minimization
to learn invariant preferences. To implement this, we employ a variational
autoencoder (VAE) to extract users' invariant preferences, replacing the
standard reconstruction loss with an invariant risk minimization constraint.
Additionally, we construct distinct environments by combining multi-behavior
data to enhance robustness in learning these preferences. Finally, the learned
invariant preferences are used to provide recommendations for the target
behavior. Extensive experiments on four real-world datasets demonstrate that
UIPL significantly outperforms current state-of-the-art methods.

</details>


### [244] [FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval](https://arxiv.org/abs/2507.14946)
*Amna Ali,Liyanage C. De Silva,Pg Emeroylariffion Abas*

Main category: cs.IR

TL;DR: FullRecall是一种新的专利检索方法，通过IPC知识提取关键词，首先保证100%召回，然后优化排序，相比现有方法在提高召回率方面效果显著，可降低法律风险。


<details>
  <summary>Details</summary>
Motivation: 专利审查员和发明者在验证发明的原创性和非显而易见性时面临巨大压力，而复杂的专利数据加剧了专利检索的挑战。因此，迫切需要开发能够可靠实现所需召回率的前沿检索策略。

Method: FullRecall是一种新颖的专利检索方法，它利用IPC指导的知识生成信息性短语，并从中提取表征查询专利的关键词（名词短语）。通过选择 top k 关键词构建查询，首先检索到包含所有相关文档的子集（实现完全召回），然后应用排序方案对该子集进行优化，以减小结果集大小，同时保持100%的召回率。

Result: FullRecall方法在所有五个测试案例中均实现了100%的召回率，而HRR2方法的召回率分别为10%、25%、33.3%、0%和14.29%，ReQ-ReC方法的召回率分别为50%、25%、0%、0%和0%。FullRecall在检索的准确性和完整性方面均表现出优越性能。

Conclusion: 该研究提出的FullRecall方法在专利检索任务中实现了100%的召回率，显著优于HRR2和ReQ-ReC基线方法，有效解决了专利检索中的召回率和精确率平衡问题，能够帮助专利审查员和发明者更可靠地识别现有技术，降低法律风险。

Abstract: Patent examiners and inventors face significant pressure to verify the
originality and non-obviousness of inventions, and the intricate nature of
patent data intensifies the challenges of patent retrieval. Therefore, there is
a pressing need to devise cutting-edge retrieval strategies that can reliably
achieve the desired recall. This study introduces FullRecall, a novel patent
retrieval approach that effectively manages the complexity of patent data while
maintaining the reliability of relevance matching and maximising recall. It
leverages IPC-guided knowledge to generate informative phrases, which are
processed to extract key information in the form of noun phrases characterising
the query patent under observation. From these, the top k keyphrases are
selected to construct a query for retrieving a focused subset of the dataset.
This initial retrieval step achieves complete recall, successfully capturing
all relevant documents. To further refine the results, a ranking scheme is
applied to the retrieved subset, reducing its size while maintaining 100%
recall. This multi-phase process demonstrates an effective strategy for
balancing precision and recall in patent retrieval tasks. Comprehensive
experiments were conducted, and the results were compared with baseline
studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded
superior results, achieving 100% recall in all five test cases. However,
HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and
14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the
second test case, and 0% for the third, fourth, and fifth test cases. The 100%
recall ensures that no relevant prior art is overlooked, thereby strengthening
the patent pre-filing and examination processes, hence reducing potential legal
risks.

</details>


### [245] [Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations](https://arxiv.org/abs/2507.15113)
*Xiangyu Zeng,Amit Jaspal,Bin Liu,Goutham Panneeru,Kevin Huang,Nicolas Bievre,Mohit Jaggi,Prathap Maniraju,Ankur Jain*

Main category: cs.IR

TL;DR: 电子商务推荐系统中的“点击A购买B”现象（用户点击A但购买B）会导致模型学习偏差。本研究提出将推荐任务重构为多任务学习（区分CABA和CABB），并引入类目感知加权方案来解决此问题，最终通过放大真实替代/互补关系并缩小偶然的跨类目购买，在离线和在线实验中均提升了业务指标。


<details>
  <summary>Details</summary>
Motivation: 电子商务用户旅程中普遍存在点击的产品与最终购买的产品不一致的现象（即“点击A购买B”现象）。若直接将原始的点击-转化对用于训练推荐模型，会导致模型偏向学习那些仅仅与购买行为相关的商品，从而引起学习偏差，降低转化率。

Method: 提出了一种多任务学习框架，将推荐问题重构为点击A购买A（CABA）和点击A购买B（CABB）两个独立的任务。为了区分有意义的CABB转化和无意义的CABB转化，研究引入了一种与类目相关的协同过滤加权方案。该方案将每个产品映射到产品类目的叶节点，并从大规模用户行为日志中学习类目到类目的相似性矩阵。这种加权方法能够放大反映真实可替代或互补关系的（点击A购买B）组合，同时缩小偶然的跨类目购买的影响。

Result: 离线评估结果显示，与传统的最后点击归因基线模型相比，该方法将归一化熵降低了13.9%。在线A/B测试结果表明，在实际流量中，该方法使主要的业务指标提升了0.25%。

Conclusion: 该研究提出了一种新颖的多任务学习框架，通过区分“点击即购买”和“点击A购买B”两种情况，并结合产品类目相似性进行加权，有效地解决了推荐系统中因用户购买行为与点击行为不一致而导致的学习偏差问题，最终在离线评估和在线A/B测试中均取得了显著的业务提升。

Abstract: User journeys in e-commerce routinely violate the one-to-one assumption that
a clicked item on an advertising platform is the same item later purchased on
the merchant's website/app. For a significant number of converting sessions on
our platform, users click product A but buy product B -- the Click A, Buy B
(CABB) phenomenon. Training recommendation models on raw click-conversion pairs
therefore rewards items that merely correlate with purchases, leading to biased
learning and sub-optimal conversion rates. We reframe conversion prediction as
a multi-task problem with separate heads for Click A Buy A (CABA) and Click A
Buy B (CABB). To isolate informative CABB conversions from unrelated CABB
conversions, we introduce a taxonomy-aware collaborative filtering weighting
scheme where each product is first mapped to a leaf node in a product taxonomy,
and a category-to-category similarity matrix is learned from large-scale
co-engagement logs. This weighting amplifies pairs that reflect genuine
substitutable or complementary relations while down-weighting coincidental
cross-category purchases. Offline evaluation on e-commerce sessions reduces
normalized entropy by 13.9% versus a last-click attribution baseline. An online
A/B test on live traffic shows +0.25% gains in the primary business metric.

</details>


### [246] [SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search](https://arxiv.org/abs/2507.15245)
*Xiaofeng Shi,Yuduo Li,Qian Kou,Longbin Yu,Jinxin Xie,Hua Zhou*

Main category: cs.IR

TL;DR: SPAR是一个多代理框架，通过查询分解和演化改进了学术文献检索。SPARBench是用于评估此框架的新基准。SPAR在AutoScholar和SPARBench上均显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有的学术文献检索系统通常依赖于僵化的流程，并且表现出有限的推理能力。

Method: SPAR是一个多代理框架，它结合了基于RefChain的查询分解和查询演化，以实现更灵活和有效的搜索。SPARBench是一个包含专家标注的相关性标签的基准。

Result: 实验结果表明，SPAR的性能显著优于强大的基线模型，在AutoScholar上的F1分数提高了56%，在SPARBench上的F1分数提高了23%。

Conclusion: SPAR及其SPARBench为学术文献检索的研究提供了可扩展、可解释且高性能的基础。

Abstract: Recent advances in large language models (LLMs) have opened new opportunities
for academic literature retrieval. However, existing systems often rely on
rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,
a multi-agent framework that incorporates RefChain-based query decomposition
and query evolution to enable more flexible and effective search. To facilitate
systematic evaluation, we also construct SPARBench, a challenging benchmark
with expert-annotated relevance labels. Experimental results demonstrate that
SPAR substantially outperforms strong baselines, achieving up to +56% F1 on
AutoScholar and +23% F1 on SPARBench over the best-performing baseline.
Together, SPAR and SPARBench provide a scalable, interpretable, and
high-performing foundation for advancing research in scholarly retrieval. Code
and data will be available at: https://github.com/xiaofengShi/SPAR

</details>


### [247] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: 本文针对短视频平台中的查询推荐（I2Q）问题，提出了基于LLM和Trie的GREAT框架，通过Trie指导查询生成，并结合后处理优化，有效提升了推荐效果。同时，发布了名为KuaiRS的大规模数据集以弥补研究空白。


<details>
  <summary>Details</summary>
Motivation: 短视频平台已成为用户获取信息的主要渠道，但现有方法在视频相关搜索的查询推荐（item-to-query, I2Q）方面存在不足，主要依赖于嵌入计算相似度，缺乏对语义内容的深度交互。此外，该领域的研究和公开数据集相对匮乏。

Method: 本文提出了一种名为GREAT的新型基于大语言模型（LLM）的框架，该框架利用Trie（前缀树）来指导查询生成，以解决相关搜索中的I2Q推荐问题。具体而言，首先构建了一个基于高质量、高曝光和高点击率查询的Trie。在训练阶段，通过该Trie增强LLM生成高质量查询的能力；在推理阶段，Trie作为指导进行token生成。最后，通过后处理模块进一步优化匹配的item和query之间的相关性及字面质量。

Result: 所提出的GREAT框架通过广泛的离线和在线实验证明了其有效性。

Conclusion: GREAT框架通过结合基于Trie的查询生成和后处理模块，有效解决了短视频场景下的item-to-query（I2Q）推荐问题，并在离线和在线实验中均表现出色。

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


### [248] [Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.15395)
*Hengyu Zhang,Chunxu Shen,Xiangguo Sun,Jie Tan,Yanchao Tan,Yu Rong,Hong Cheng,Lingling Yi*

Main category: cs.IR

TL;DR: HGIB框架通过信息瓶颈和图细化编码器解决了多行为推荐中的分布差异和噪声问题，并在真实世界数据和在线测试中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决当前多行为推荐算法面临的两个关键挑战：行为间的严重分布差异和辅助行为中的噪声导致的负迁移效应。

Method: 提出了一种名为分层图信息瓶颈（HGIB）的新型模型无关框架，该框架遵循信息瓶颈原理，并通过图细化编码器（GRE）来优化学习紧凑但充分的表示，同时消除与任务无关的冗余，并通过可学习的边缘丢弃机制动态修剪冗余边缘以减轻交互噪声。

Result: 通过在三个公开数据集和多个真实工业场景上的广泛实验以及在线A/B测试，证明了HGIB框架在提高多行为推荐方面的优越有效性。

Conclusion: 该研究提出的HGIB框架在多行为推荐方面表现出优越的有效性，并在真实工业场景的在线A/B测试中显示出显著的改进。

Abstract: In real-world recommendation scenarios, users typically engage with platforms
through multiple types of behavioral interactions. Multi-behavior
recommendation algorithms aim to leverage various auxiliary user behaviors to
enhance prediction for target behaviors of primary interest (e.g., buy),
thereby overcoming performance limitations caused by data sparsity in target
behavior records. Current state-of-the-art approaches typically employ
hierarchical design following either cascading (e.g.,
view$\rightarrow$cart$\rightarrow$buy) or parallel
(unified$\rightarrow$behavior$\rightarrow$specific components) paradigms, to
capture behavioral relationships. However, these methods still face two
critical challenges: (1) severe distribution disparities across behaviors, and
(2) negative transfer effects caused by noise in auxiliary behaviors. In this
paper, we propose a novel model-agnostic Hierarchical Graph Information
Bottleneck (HGIB) framework for multi-behavior recommendation to effectively
address these challenges. Following information bottleneck principles, our
framework optimizes the learning of compact yet sufficient representations that
preserve essential information for target behavior prediction while eliminating
task-irrelevant redundancies. To further mitigate interaction noise, we
introduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant
edges through learnable edge dropout mechanisms. We conduct comprehensive
experiments on three real-world public datasets, which demonstrate the superior
effectiveness of our framework. Beyond these widely used datasets in the
academic community, we further expand our evaluation on several real industrial
scenarios and conduct an online A/B testing, showing again a significant
improvement in multi-behavior recommendations. The source code of our proposed
HGIB is available at https://github.com/zhy99426/HGIB.

</details>


### [249] [RankMixer: Scaling Up Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2507.15551)
*Jie Zhu,Zhifang Fan,Xiaoxie Zhu,Yuchen Jiang,Hangyu Wang,Xintian Han,Haoran Ding,Xinmin Wang,Wenlin Zhao,Zhen Gong,Huizhi Yang,Zheng Chai,Zhe Chen,Yuchao Zheng,Qiwei Chen,Feng Zhang,Xun Zhou,Peng Xu,Xiao Yang,Di Wu,Zuotao Liu*

Main category: cs.IR

TL;DR: RankMixer是一种新的推荐系统模型设计，它通过硬件感知的方法提高了效率和可扩展性，解决了现有模型的成本和性能瓶颈，并在实际应用中取得了显著的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在推荐系统中的应用面临两个主要挑战：1）训练和服务的成本高昂，需要满足严格的延迟和高QPS需求；2）传统的特征交叉模块设计未能充分利用GPU，导致模型FLOPs利用率（MFU）低和扩展性差。因此，需要一种新的模型设计来解决这些问题。

Method: RankMixer采用了一种硬件感知的模型设计，用多头令牌混合模块取代了二次自注意力机制，并引入了每令牌FFN来保持对不同特征子空间和交叉特征空间交互的建模能力。此外，还提出了一个包含稀疏MoE变体和动态路由策略的版本，以提高投资回报率和解决专家训练不平衡的问题。

Result: RankMixer将模型的MFU从4.5%提高到45%，并将模型参数量扩展了100倍，同时保持了相似的推理延迟。在生产环境中，1B参数的RankMixer在不增加服务成本的情况下，用户活跃天数提高了0.2%，应用内使用时长提高了0.5%。其通用性也通过在线A/B测试在推荐、广告和搜索三个核心场景中得到验证。

Conclusion: RankMixer通过其硬件感知设计、多头令牌混合模块、每令牌FFN和稀疏MoE变体，在扩展推荐系统方面取得了显著的成功。它显著提高了模型FLOPs利用率（MFU），实现了100倍的模型参数扩展，同时保持了相似的推理延迟。RankMixer已成功应用于推荐、广告和搜索等场景，带来了用户活跃天数和应用内使用时长的提升。

Abstract: Recent progress on large language models (LLMs) has spurred interest in
scaling up recommendation systems, yet two practical obstacles remain. First,
training and serving cost on industrial Recommenders must respect strict
latency bounds and high QPS demands. Second, most human-designed
feature-crossing modules in ranking models were inherited from the CPU era and
fail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and
poor scalability. We introduce RankMixer, a hardware-aware model design
tailored towards a unified and scalable feature-interaction architecture.
RankMixer retains the transformer's high parallelism while replacing quadratic
self-attention with multi-head token mixing module for higher efficiency.
Besides, RankMixer maintains both the modeling for distinct feature subspaces
and cross-feature-space interactions with Per-token FFNs. We further extend it
to one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic
routing strategy is adapted to address the inadequacy and imbalance of experts
training. Experiments show RankMixer's superior scaling abilities on a
trillion-scale production dataset. By replacing previously diverse handcrafted
low-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and
scale our ranking model parameters by 100x while maintaining roughly the same
inference latency. We verify RankMixer's universality with online A/B tests
across three core application scenarios (Recommendation, Advertisement and
Search). Finally, we launch 1B Dense-Parameters RankMixer for full traffic
serving without increasing the serving cost, which improves user active days by
0.2% and total in-app usage duration by 0.5%.

</details>


### [250] [Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation](https://arxiv.org/abs/2507.15826)
*Alessandro B. Melchiorre,Elena V. Epure,Shahed Masoudian,Gustavo Escobedo,Anna Hausberger,Manuel Moussallam,Markus Schedl*

Main category: cs.IR

TL;DR: JAM是一个轻量级的自然语言音乐推荐框架，通过向量翻译和多模态特征聚合来解决现有方法的局限性，并提供准确的推荐。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在音乐推荐中存在的成本和延迟问题，以及检索方法在单模态表示、忽略长期用户偏好和需要完全模型再训练方面的不足，提出JAM框架。

Method: JAM将用户-查询-项目交互建模为共享潜在空间中的向量翻译，并采用跨注意力和稀疏混合专家模型来聚合多模态项目特征。

Result: JAM框架能够提供准确的推荐，并生成适合实际用例的直观表示。

Conclusion: JAM框架能够提供准确的推荐，生成适合实际用例的直观表示，并且可以轻松集成到现有的音乐推荐栈中。

Abstract: Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (LLMs) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and sparse mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [251] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: NPUEval是一个新的NPU内核基准，用于评估LLM生成的代码。虽然像DeepSeek R1这样的模型表现出潜力，但整体性能仍有待提高，平均向量化率约为10%。


<details>
  <summary>Details</summary>
Motivation: 当前NPU编程面临社区碎片化和缺乏领域特定优化代码示例的挑战，这限制了利用LLM辅助编写NPU内核的效率。因此，需要一个能够评估LLM在NPU内核生成能力的基准。

Method: 提出NPUEval基准，包含102个常见的机器学习算子，并使用AMD NPU的开源编译器工具，在实际硬件上评估了多种先进的LLM（包括闭源和开源模型）生成代码的功能正确性和向量化效率。

Result: 在NPUEval基准上，先进的LLM（如DeepSeek R1）在部分内核上实现了超过50%的开箱即用向量化率。然而，即使结合编译器反馈和向量化内核示例，整个数据集的平均得分仍徘徊在10%左右，表明该基准对前沿模型也具有挑战性。

Conclusion: LLM在生成NPU内核代码方面表现出潜力，但目前仍面临挑战，平均向量化率仅为10%，尤其是在不熟悉的数据集上。未来的研究需要进一步优化LLM，并利用NPUEval基准来推动NPU内核优化和代码生成技术的发展。

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [252] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: Timetide 是一种新的多时钟同步语言，可以分布式和形式化验证，无需物理时钟同步。


<details>
  <summary>Details</summary>
Motivation: 解决确定性分布式系统的挑战，克服现有时间触发语言依赖昂贵的物理时钟同步的问题。

Method: 提出了一种新颖的同步程序多时钟语义，并基于最近提出的分布式系统逻辑同步模型。

Result: Timetide 减轻了对物理时钟同步的需要，可以无缝分布式，并支持 Timetide 程序的正式验证。

Conclusion: Timetide是第一个无需物理时钟同步或时钟门控即可进行分布式和形式化验证的多时钟同步语言。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [253] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 该研究引入了一个创新的Python语音辅助调试插件，将运行时错误转化为声音诊断。通过全局异常钩子、pyttsx3和Tkinter，插件提供多模态反馈，经验评估显示认知负荷降低37%，错误识别速度提高78%，CPU开销低。插件兼容多平台，易于集成，可提升编程可访问性，尤其对视觉障碍人士和多任务处理有益。


<details>
  <summary>Details</summary>
Motivation: 为了将无声的运行时错误转化为可操作的听觉诊断信息，以降低认知负荷，提高错误识别速度，并增强编程的可访问性，特别是在视觉障碍人士和多任务处理的工作流程中。

Method: 通过实现一个全局异常钩子架构，并结合pyttsx3文本转语音转换和Tkinter图形用户界面（GUI）可视化，提供并行听觉和视觉通道的多模态错误反馈。

Result: 该系统实现了37%的认知负荷降低（p<0.01，n=50），错误识别速度提高了78%，语音延迟低于1.2秒，CPU开销低于18%。插件仅需两行集成代码，即可在Windows、macOS和Linux的Python 3.7+环境中兼容，并显示带有文档深度链接的交互式跟踪。

Conclusion: 该插件通过提供多模态的错误反馈（包括听觉和视觉），显著提高了Python调试的效率和可访问性，尤其有助于视觉障碍人士和多任务处理场景。它通过全局异常钩子和文本转语音技术，将运行时错误转化为可听见的诊断信息，实现了78%的错误识别加速和37%的认知负荷降低。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [254] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: 针对浮点运算中的舍入误差问题，提出了一种结合FPTaylor微分特征与约束求解的新方法，并设计了两种不变量生成算法，提高了算法效率和不变量精度。


<details>
  <summary>Details</summary>
Motivation: 浮点运算的舍入误差会累积并可能导致灾难性的程序故障，因此有必要仔细考虑浮点误差对浮点程序正确性的影响。本研究提出了一种用于浮点程序不变量生成的方法，旨在生成在浮点误差扰动下的精确不变量。

Method: 提出了一种将FPTaylor的一阶微分特征与约束求解方法相结合的新方法，以减少约束求解的计算负担。设计了两种多项式不变量生成算法，一种适用于广泛的浮点运算，需要外部输入的初始（粗略）不变量；另一种不适用于多项式程序，但不需要初始不变量。

Result: 与最先进的方法相比，该算法在时间和生成的凸不变量的精度方面都表现更好。

Conclusion: 该框架能够处理浮点分析中的条件分支问题。实验结果表明，在时间和生成不变量的精度方面，我们的算法优于最先进的方法。

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [255] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: GPU性能调优复杂且易过拟合。本文提出可移植性调优框架，自动生成多版本代码，实现跨设备性能稳定，无需重新调优。


<details>
  <summary>Details</summary>
Motivation: GPU上的自动性能调优（autotuning）存在“过拟合”问题，需要为不同环境重新调整；且在运行时调整和编译可能成本高昂。

Method: 提出了一种名为“可移植性调优”的框架，该框架自动生成多版本代码以实现性能可移植性。

Result: 在CLBlast库的GEMM内核上，该框架生成的代码性能优于CLBlast的默认内核，接近理论最大性能的90%，并且能很好地泛化到新设备。

Conclusion: 通过使用多版本技术，该框架可以生成性能可移植的代码，无需重新调整即可适应新环境，并在GEMM内核上取得了与自动调优相当的性能。

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [256] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: BaSL是一种新的概率分离逻辑，可以处理BPPLs中的贝叶斯更新，并能证明各种统计模型的性质。


<details>
  <summary>Details</summary>
Motivation: 现有的分离逻辑无法处理贝叶斯更新，这是BPPLs的关键区别特征。

Method: BaSL是基于希尔伯特立方体上的σ-有限测度空间的克里普克资源幺半群的新颖实例，并且Hoare三元组的语义与基于s-有限核类别的现有BPPLs解释语义兼容。

Result: BaSL可以模型化概率编程概念，并能证明统计模型的各种性质。

Conclusion: BaSL可以处理贝叶斯更新，并且可以处理概率编程概念，例如贝叶斯更新、非归一化分布、条件分布、软约束、共轭先验和不当先验，同时通过框架规则保持模块化。研究人员使用BaSL证明了诸如贝叶斯抛硬币的期望值、对撞机贝叶斯网络中随机变量的相关性以及盗警模型、参数估计算法和高斯混合模型的后验分布等统计模型的性质。

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [257] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: 提出了一种新的形式验证框架，该框架通过整合离散 PLC 行为、网络通信和连续物理行为，并使用部分顺序约简来处理复杂性，从而实现了对工业自动化系统中 PLC 的精确分析。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有形式验证技术仅关注单个 PLC 程序而忽略与物理环境的交互以及控制器之间的网络通信的局限性，而这些因素在真实世界的工业系统中起着至关重要的作用。

Method: 通过应用部分顺序约简来减轻状态爆炸，从而在保持正确性的同时显著减少所探索的状态数。

Result: 提出了一种统一的正式框架，该框架集成了离散 PLC 语义、网络通信和连续物理行为。

Conclusion: 该框架能够对具有连续动力学和网络通信的 PLC 驱动系统进行精确分析。

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [258] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: 闭包转换在函数式语言编译器中用于将内部函数转换为全局函数，通过将转换后的函数与其自由变量的环境配对来构建闭包。抽象机器也使用了类似但不同的闭包和环境概念。本文研究了这两种方法的关系，采用了一个带有元组的简单 lambda-演算作为源语言，并研究了目标语言的抽象机器。研究的重点是简单的扁平闭包/环境，即没有环境共享。本文有三项贡献：1. 提出了一种新的、受抽象机器启发的简单证明技术，用于证明闭包转换的正确性。2. 展示了目标语言的闭包不变性如何能够帮助设计一种新的处理环境的方法，该方法克服了其他方法的缺点。3. 从时间复杂度的角度研究了这些机器，并借鉴了 Accattoli 等人的分析方法。结果表明，闭包转换虽然降低了动态成本但增加了初始代码大小，不过机器的整体复杂度保持不变。


<details>
  <summary>Details</summary>
Motivation: 研究闭包转换中的函数、环境和闭包之间的关系，以及它与抽象机器概念的联系。

Method: 提出了一种新的，受抽象机器启发的闭包转换正确性证明技术；利用目标语言的闭包不变性来设计一种新的处理环境的方法；并运用Accattoli等人的分析方法研究机器的时间复杂度。

Result: 闭包转换降低了各种动态成本，但增加了初始代码的大小，然而整体复杂度保持不变。Cited by: 2

Conclusion: 闭包转换前后两种机器的整体复杂度相同

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [259] [Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach](https://arxiv.org/abs/2507.14249)
*Yuejiao Xie,Maonan Wang,Di Zhou,Man-On Pun,Zhu Han*

Main category: cs.RO

TL;DR: 该研究提出了一种新的多源混合注意力强化学习框架（MSHA-RL），用于城市空中交通的实时路径规划。该框架通过构建无线电地图来评估通信质量，并利用混合注意力机制来处理不同数据源的维度差异，从而实现更安全、高效的空中交通服务。


<details>
  <summary>Details</summary>
Motivation: 城市空中交通（UAM）系统有望缓解城市拥堵，但其轨迹规划需要优先考虑通信质量以实现精确的定位跟踪，并需要适应实时变化的乘客需求，尤其是在拼车场景下。传统的基于预定义路线的规划策略缺乏灵活性。

Method: 提出构建无线电地图来评估城市空域的通信质量，并引入了多源混合注意力强化学习（MSHA-RL）框架，以解决数据源表示维度差异显著的问题。该模型首先生成多样化数据源之间的对齐，然后采用混合注意力来平衡全局和局部洞察力，从而实现响应迅速的实时路径规划。

Result: 实验结果表明，该方法能够实现通信兼容的轨迹规划，缩短出行时间，提高运行效率，并优先考虑乘客安全。

Conclusion: 该方法实现了通信兼容的轨迹规划，可缩短出行时间、提高运行效率并优先考虑乘客安全。

Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.

</details>


### [260] [A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators](https://arxiv.org/abs/2507.14274)
*Andreas Mueller,Shivesh Kumar,Thomas Kordik*

Main category: cs.RO

TL;DR: 本文提出了一种在并联机器人中使用串联弹性执行器（SEA）的轨迹跟踪控制方法，通过利用李群和递归算法来高效计算逆动力学二阶导数，解决了现有技术的不足，并在6自由度Gough-Stewart平台和平面PKM上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 由于现有文献中缺乏对具有SEA的PKM的逆动力学二阶时间导数的高效计算方法，因此有必要解决这一问题，以实现PKM的精确轨迹跟踪控制。

Method: 利用李群方法，通过重用串联机器人的逆动力学递归算法来解决PKM的逆动力学二阶时间导数问题。

Result: 文章展示了所提出方法在6自由度Gough-Stewart平台和平面PKM上的应用，并验证了其有效性。

Conclusion: 本文为串联机器人配备串联弹性执行器（SEA）的轨迹跟踪控制提供了计算上有效的方法，并为机器人学领域开辟了新的可能性，特别是对于具有SEA的并联运动学机器人（PKM）的控制。

Abstract: Series elastic actuators (SEA) were introduced for serial robotic arms. Their
model-based trajectory tracking control requires the second time derivatives of
the inverse dynamics solution, for which algorithms were proposed. Trajectory
control of parallel kinematics manipulators (PKM) equipped with SEAs has not
yet been pursued. Key element for this is the computationally efficient
evaluation of the second time derivative of the inverse dynamics solution. This
has not been presented in the literature, and is addressed in the present paper
for the first time. The special topology of PKM is exploited reusing the
recursive algorithms for evaluating the inverse dynamics of serial robots. A
Lie group formulation is used and all relations are derived within this
framework. Numerical results are presented for a 6-DOF Gough-Stewart platform
(as part of an exoskeleton), and for a planar PKM when a flatness-based control
scheme is applied.

</details>


### [261] [Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support](https://arxiv.org/abs/2507.14412)
*Mengxue Fu,Zhonghao Shi,Minyu Huang,Siqi Liu,Mina Kian,Yirui Song,Maja J. Matarić*

Main category: cs.RO

TL;DR: 本研究提出使用端到端语音语言模型（SLMs）来改进社交机器人（SARs）的对话能力，解决了现有系统在延迟、回应和个性化方面的不足。用户研究表明，该模型能提升机器人的共情和对话流畅度，但机器人的非语言行为和口头反馈仍有改进空间，尤其是在同步性、一致性和多样性方面。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有SARs对话系统在实时延迟、回应和个性化语音对话方面的局限性，本研究提出使用集成的端到端语音语言模型（SLMs）。

Method: 本研究通过一项小型参与者内部用户研究（N=11）来评估基于端到端语音语言模型（SLMs）的SARs对话系统的可用性，并识别现有局限性。

Result: 研究结果显示，用户认为基于SLM的SARs系统能够提供共情反馈、自然轮替、回应和适应性响应。然而，用户也指出机器人的非语言行为缺乏变化和同步性，并且SLM的口头反馈比较通用和重复。

Conclusion: 用户反馈表明，虽然基于端到端语音语言模型的社交机器人（SARs）在提供共情反馈、自然轮替、回应和适应性响应方面表现良好，但机器人的非语言行为缺乏变化和同步性，并且语言模型的口头反馈也比较通用和重复。未来的改进方向包括实现与对话同步的机器人实时运动、改进提示或微调以生成更符合心理健康实践的输出，以及更具表现力和适应性的声音生成。

Abstract: Socially assistive robots (SARs) have shown great potential for supplementing
well-being support. However, prior studies have found that existing dialogue
pipelines for SARs remain limited in real-time latency, back-channeling, and
personalized speech dialogue. Toward addressing these limitations, we propose
using integrated end-to-end speech-language models (SLMs) with SARs. This work
1) evaluated the usability of an SLM-enabled SAR dialogue system through a
small user study, and 2) identified remaining limitations through study user
feedback to inform future improvements. We conducted a small within-participant
user study with university students (N = 11) whose results showed that
participants perceived an SLM-enabled SAR system as capable of providing
empathetic feedback, natural turn-taking, back-channeling, and adaptive
responses. We also found that participants reported the robot's nonverbal
behaviors as lacking variability and synchronization with conversation, and the
SLM's verbal feedback as generic and repetitive. These findings highlighted the
need for real-time robot movement synchronized with conversation, improved
prompting or fine-tuning to generate outputs better aligned with mental health
practices, and more expressive, adaptive vocal generation.

</details>


### [262] [Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking](https://arxiv.org/abs/2507.14455)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 时间延迟嵌入技术可用于对周期性混合系统进行建模，并提出了一种新的LQR控制器。


<details>
  <summary>Details</summary>
Motivation: 证明时间延迟嵌入技术不仅适用于非线性光滑系统，也适用于周期性非光滑或混合系统。

Method: 通过时间延迟嵌入技术，将周期性非光滑或混合系统扩展为线性状态空间系统，并用于控制。

Result: 成功地为跳跳杆和带控制输入的简单步行者生成了线性模型，并提出了一种新的状态历史增强线性二次调节器（LQR）。

Conclusion: 时间延迟嵌入技术可以为周期性混合动力系统生成线性模型，并提出了一种新的状态历史增强线性二次调节器（LQR）。

Abstract: Time-delay embedding is a technique that uses snapshots of state history over
time to build a linear state space model of a nonlinear smooth system. We
demonstrate that periodic non-smooth or hybrid system can also be modeled as a
linear state space system using this approach as long as its behavior is
consistent in modes and timings. We extended time-delay embeddings to generate
a linear model of two periodic hybrid systems: the bouncing pendulum and the
simplest walker with control inputs. This leads to a novel state history
augmented linear quadratic regulator (LQR) which uses current and past state
history for feedback control.

</details>


### [263] [A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0](https://arxiv.org/abs/2507.14538)
*Jin Chai,Xiang Yao,Mengfan Hou,Yanghong Li,Erbao Dong*

Main category: cs.RO

TL;DR: CYJ Hand-0是一个21自由度的人形灵巧手，采用SMA和直流电机混合驱动，并使用3D打印金属框架，证明了其仿生灵巧性。


<details>
  <summary>Details</summary>
Motivation: 设计一个具有仿生灵巧性的混合驱动人形灵巧手

Method: CYJ Hand-0采用混合驱动系统，结合形状记忆合金（SMA）和直流电机，通过高强度钓鱼线作为人造肌腱。手部采用3D打印的AlSi10Mg金属框架，模拟人手结构。线性电机驱动屈曲，SMA驱动伸展和外展。

Result: 成功进行了机械和运动学实验，并进行了验证

Conclusion: 该设计有效且具有仿生灵巧性

Abstract: CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid
tendon-driven actuation system that combines shape memory alloys (SMAs) and DC
motors. The hand employs high-strength fishing line as artificial tendons and
uses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal
and tendon-muscle structure of the human hand. A linear motor-driven module
controls finger flexion, while an SMA-based module enables finger extension and
lateral abduction. These modules are integrated into a compact hybrid actuation
unit mounted on a custom rear support structure. Mechanical and kinematic
experiments, conducted under an Arduino Mega 2560-based control system,
validate the effectiveness of the design and demonstrate its biomimetic
dexterity.

</details>


### [264] [BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives](https://arxiv.org/abs/2507.14582)
*Zezhi Liu,Shizhen Wu,Hanqian Luo,Deyun Qin,Yongchun Fang*

Main category: cs.RO

TL;DR: 该研究提出了一种名为BT-TL-DMPs的新型分层框架，用于解决机器人学习操作技能并将其泛化到长时序任务中的挑战。该框架结合了行为树、时序逻辑和动力学移动原语，并通过信号时序逻辑来处理任务约束。实验结果表明，该方法能够提高机器人操作的可靠性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 在演示学习（LfD）领域，使机器人能够将学习到的操作技能泛化到长时序任务的新场景仍然是一个挑战。具体来说，机器人仍然难以适应新环境中具有不同任务和运动要求所学习到的技能，尤其是在具有复杂约束的长时序、多阶段场景中。

Method: 提出了一种名为BT-TL-DMPs的新型分层框架，该框架整合了行为树（BT）、时序逻辑（TL）和动力学移动原语（DMPs）。利用信号时序逻辑（STL）形式化地指定复杂的、长期的任务需求和约束，并将这些STL规范系统地转化为用于高级决策任务结构、具有反应性和模块化特性的BT。提出了一种STL约束的DMP优化方法来优化DMP的强迫项，使得学习到的运动原语能够在满足复杂的时空要求的同时灵活适应，并保留从演示中学到的关键动力学。

Result: 通过仿真和现实世界实验验证了该框架在各种STL约束下的泛化能力，并在多个长时序机器人操作任务上进行了测试。结果表明，该框架能够有效地弥合符号-运动鸿沟，实现更可靠、更具泛化性的复杂机器人任务自主操作。

Conclusion: 该框架通过整合行为树、时序逻辑和动力学移动原语，成功弥合了符号-运动鸿沟，实现了机器人复杂自主操作的可靠性和泛化性。

Abstract: In the field of Learning from Demonstration (LfD), enabling robots to
generalize learned manipulation skills to novel scenarios for long-horizon
tasks remains challenging. Specifically, it is still difficult for robots to
adapt the learned skills to new environments with different task and motion
requirements, especially in long-horizon, multi-stage scenarios with intricate
constraints. This paper proposes a novel hierarchical framework, called
BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and
Dynamical Movement Primitives (DMPs) to address this problem. Within this
framework, Signal Temporal Logic (STL) is employed to formally specify complex,
long-horizon task requirements and constraints. These STL specifications are
systematically transformed to generate reactive and modular BTs for high-level
decision-making task structure. An STL-constrained DMP optimization method is
proposed to optimize the DMP forcing term, allowing the learned motion
primitives to adapt flexibly while satisfying intricate spatiotemporal
requirements and, crucially, preserving the essential dynamics learned from
demonstrations. The framework is validated through simulations demonstrating
generalization capabilities under various STL constraints and real-world
experiments on several long-horizon robotic manipulation tasks. The results
demonstrate that the proposed framework effectively bridges the symbolic-motion
gap, enabling more reliable and generalizable autonomous manipulation for
complex robotic tasks.

</details>


### [265] [Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition](https://arxiv.org/abs/2507.14605)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 本文使用Koopman算子理论创建四足机器人的混合模型，并通过LMPC实现了多种步态和步态转换的在线生成，解决了传统LMPC线性化导致的解质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 为了使四足机器人在新场景中能够规划运动，需要在线最优控制。虽然LMPC是一种实用的实时控制方法，但它依赖于运动方程（EOM）的线性化，这可能导致较差的解。因此，需要一种能保留非线性的方法。

Method: 本文采用Koopman算子理论和扩展动态模式分解（EDMD）来创建高维空间中的线性系统模型，以保留非线性的运动方程（EOM）。分别对空中和地面接触阶段使用不同的线性模型。然后，利用线性模型预测控制（LMPC）进行控制。

Result: 在平坦和崎岖的地形上，成功演示了bounding、trotting以及bound-to-trot和trot-to-bound的步态转换。

Conclusion: 本文利用Koopman算子理论和扩展动态模式分解（EDMD）构建了四足系统的混合模型，并在平坦和崎岖地形上演示了多种步态和步态转换的在线生成。

Abstract: Online optimal control of quadrupedal robots would enable them to plan their
movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged
as a practical approach for real-time control. In LMPC, an optimization problem
with a quadratic cost and linear constraints is formulated over a finite
horizon and solved on the fly. However, LMPC relies on linearizing the
equations of motion (EOM), which may lead to poor solution quality. In this
paper, we use Koopman operator theory and the Extended Dynamic Mode
Decomposition (EDMD) to create a linear model of the system in high dimensional
space, thus retaining the nonlinearity of the EOM. We model the aerial phase
and ground contact phases using different linear models. Then, using LMPC, we
demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait
transitions in level and rough terrains. The main novelty is the use of Koopman
operator theory to create hybrid models of a quadrupedal system and demonstrate
the online generation of multiple gaits and gaits transitions.

</details>


### [266] [Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks](https://arxiv.org/abs/2507.14694)
*Yue Ma,Kanglei Zhou,Fuyang Yu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.RO

TL;DR: ProbHMI是一种新的运动预测方法，使用可逆网络来量化不确定性，在预测准确性和不确定性校准方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了在以人为本的机器人协作等安全关键环境中最小化风险，必须为每次预测（例如基于概率密度或分位数的置信度）估计不确定性。然而，现有的各种运动预测方法由于隐含的概率表示妨碍了不确定性建模，因此在不确定性量化方面存在困难。

Method: ProbHMI通过引入可逆网络，将姿势参数化为解耦的潜在空间，从而实现概率动态建模。预测模块随后明确预测未来的潜在分布，从而实现有效的不确定性量化。

Result: 所提出的ProbHMI方法在基准测试中得到了评估。

Conclusion: ProbHMI在确定性和多样性预测方面均取得了强劲的性能，并验证了对风险感知决策至关重要的不确定性校准。

Abstract: 3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.

</details>


### [267] [Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation](https://arxiv.org/abs/2507.14700)
*Nicholas Mohammad,Nicola Bezzo*

Main category: cs.RO

TL;DR: 通过结合CLF、CBF和SAC策略的MPCC框架，实现了机器人未知环境下的安全导航。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有MPCC方法在安全导航方面缺乏形式化安全保证的问题，提出了一种新的框架来增强其安全性。

Method: 提出了一种结合控制李雅普诺夫函数（CLF）和控制障碍函数（CBF）的MPCC框架，并在运行时使用软Actor-Critic（SAC）策略动态调整CBF参数，以在自由空间走廊内强制执行安全约束。

Result: 所提出的框架在模拟和真实机器人导航实验中都得到了验证，证明了其在未知和混乱环境中进行安全导航的能力。

Conclusion: 该方法通过结合控制李雅普诺夫函数（CLF）和控制障碍函数（CBF）的MPCC框架，并使用SAC策略动态调整CBF参数，成功实现了在未知和混乱环境中的安全导航。

Abstract: Safe navigation in unknown and cluttered environments remains a challenging
problem in robotics. Model Predictive Contour Control (MPCC) has shown promise
for performant obstacle avoidance by enabling precise and agile trajectory
tracking, however, existing methods lack formal safety assurances. To address
this issue, we propose a general Control Lyapunov Function (CLF) and Control
Barrier Function (CBF) enabled MPCC framework that enforces safety constraints
derived from a free-space corridor around the planned trajectory. To enhance
feasibility, we dynamically adapt the CBF parameters at runtime using a Soft
Actor-Critic (SAC) policy. The approach is validated with extensive simulations
and an experiment on mobile robot navigation in unknown cluttered environments.

</details>


### [268] [Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls](https://arxiv.org/abs/2507.14721)
*Keita Kobashi,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: 提出一种分层强化学习框架，利用CVAE引导机器人进行抓取，解决了物体被遮挡导致抓取困难的问题，并实现了良好的泛化和仿真到现实传输性能。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中由于墙壁过大或过高导致机器人无法抓取被遮挡的物体的问题，即使在进行枢轴操作后也可能失败，需要结合不同类型的动作才能抓取。

Method: 提出了一种分层强化学习（RL）框架，使用Q学习训练一个高级策略来选择预期收益最高的动作类型。然后，由选定的低级技能在连续空间中采样具体的机器人动作。为了引导机器人到执行所选动作的合适位置，采用条件变分自编码器（CVAE），并基于物体点云和技能ID对CVAE进行条件化，使其能够根据物体几何和选定技能推断出合适的位置。为了促进泛化，在低级技能训练中应用了领域随机化。

Result: 该方法在模拟环境中针对盒状物体进行了训练，并在现实世界的六种物体上进行了部署，展示了良好的泛化能力和仿真到现实的传输性能，成功率令人满意。

Conclusion: 该方法在模拟环境中针对盒状物体进行了训练，并在现实世界的六种物体上进行了部署，展示了良好的泛化能力和仿真到现实的传输性能，成功率令人满意。

Abstract: This study addresses the problem of occluded grasping, where primary grasp
configurations of an object are not available due to occlusion with
environment. Simple parallel grippers often struggle with such tasks due to
limited dexterity and actuation constraints. Prior works have explored object
pose reorientation such as pivoting by utilizing extrinsic contacts between an
object and an environment feature like a wall, to make the object graspable.
However, such works often assume the presence of a short wall, and this
assumption may not always hold in real-world scenarios. If the wall available
for interaction is too large or too tall, the robot may still fail to grasp the
object even after pivoting, and the robot must combine different types of
actions to grasp. To address this, we propose a hierarchical reinforcement
learning (RL) framework. We use Q-learning to train a high-level policy that
selects the type of action expected to yield the highest reward. The selected
low-level skill then samples a specific robot action in continuous space. To
guide the robot to an appropriate location for executing the selected action,
we adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on
the object point cloud and the skill ID, enabling it to infer a suitable
location based on the object geometry and the selected skill. To promote
generalization, we apply domain randomization during the training of low-level
skills. The RL policy is trained entirely in simulation with a box-like object
and deployed to six objects in real world. We conduct experiments to evaluate
our method and demonstrate both its generalizability and robust sim-to-real
transfer performance with promising success rates.

</details>


### [269] [X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots](https://arxiv.org/abs/2507.14731)
*Haitong Wang,Aaron Hao Tan,Angus Fung,Goldie Nejat*

Main category: cs.RO

TL;DR: 本文提出X-Nav框架，通过训练多个专家策略并利用Nav-ACT技术提炼出单一通用策略，实现了跨轮式和四足机器人具身导航的端到端泛化，并在仿真和真实世界实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法主要针对特定机器人实体设计，泛化能力有限。本文旨在提出一种能够跨不同机器人实体（包括轮式和四足机器人）进行导航的统一策略框架。

Method: X-Nav框架包含两个学习阶段：1) 在大量随机生成的机器人具身和深层强化学习的特权观察下训练多个专家策略；2) 通过带有Transformer的导航动作分块（Nav-ACT）从专家策略中提炼出单一通用策略，该策略将视觉和本体感觉观察直接映射到低层控制命令。

Result: X-Nav在仿真实验中实现了对未见过具身和光线逼真环境的零样本迁移。可扩展性研究表明，随着训练具身数量的增加，X-Nav性能得到提升。消融研究证实了X-Nav的设计选择。真实世界实验验证了X-Nav在实际环境中的泛化能力。

Conclusion: X-Nav框架实现了跨具身导航的端到端通用性，能够支持多种机器人平台，并在仿真和真实环境中展现出良好的泛化能力。

Abstract: Existing navigation methods are primarily designed for specific robot
embodiments, limiting their generalizability across diverse robot platforms. In
this paper, we introduce X-Nav, a novel framework for end-to-end
cross-embodiment navigation where a single unified policy can be deployed
across various embodiments for both wheeled and quadrupedal robots. X-Nav
consists of two learning stages: 1) multiple expert policies are trained using
deep reinforcement learning with privileged observations on a wide range of
randomly generated robot embodiments; and 2) a single general policy is
distilled from the expert policies via navigation action chunking with
transformer (Nav-ACT). The general policy directly maps visual and
proprioceptive observations to low-level control commands, enabling
generalization to novel robot embodiments. Simulated experiments demonstrated
that X-Nav achieved zero-shot transfer to both unseen embodiments and
photorealistic environments. A scalability study showed that the performance of
X-Nav improves when trained with an increasing number of randomly generated
embodiments. An ablation study confirmed the design choices of X-Nav.
Furthermore, real-world experiments were conducted to validate the
generalizability of X-Nav in real-world environments.

</details>


### [270] [KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning](https://arxiv.org/abs/2507.14820)
*Bingran Chen,Baorun Li,Jian Yang,Yong Liu,Guangyao Zhai*

Main category: cs.RO

TL;DR: KGN-Pro, a novel grasping network, improves 6-DoF grasp estimation for robotic manipulation by integrating direct 3D optimization through probabilistic PnP layers, enabling end-to-end learning and outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Previous methods for robotic manipulation grasp estimation either struggled with small objects and sensor noise when directly generating grasps from point-cloud data, or required expensive annotations and suffered from discretization issues when inferring 3D information from RGB images. Recent methods using 2D representations and PnP algorithms were limited by their non-differentiable nature and sole reliance on 2D supervision, hindering the exploitation of 3D information.

Method: KGN-Pro encodes paired RGB-D images to generate a Keypoint Map and a 2D confidence map. This confidence map weights keypoint contributions during re-projection error minimization. By modeling the weighted sum of squared re-projection errors probabilistically, the network enables end-to-end learning by transmitting 3D supervision to 2D keypoint predictions through probabilistic PnP layers.

Result: Experiments demonstrated that KGN-Pro outperforms existing methods in grasp cover rate and success rate.

Conclusion: KGN-Pro outperformed existing methods in grasp cover rate and success rate on both simulated and real-world platforms.

Abstract: High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation
to serve as a basic function. Previous approaches either directly generate
grasps from point-cloud data, suffering from challenges with small objects and
sensor noise, or infer 3D information from RGB images, which introduces
expensive annotation requirements and discretization issues. Recent methods
mitigate some challenges by retaining a 2D representation to estimate grasp
keypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF
poses. However, these methods are limited by their non-differentiable nature
and reliance solely on 2D supervision, which hinders the full exploitation of
rich 3D information. In this work, we present KGN-Pro, a novel grasping network
that preserves the efficiency and fine-grained object grasping of previous KGNs
while integrating direct 3D optimization through probabilistic PnP layers.
KGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further
outputs a 2D confidence map to weight keypoint contributions during
re-projection error minimization. By modeling the weighted sum of squared
re-projection errors probabilistically, the network effectively transmits 3D
supervision to its 2D keypoint predictions, enabling end-to-end learning.
Experiments on both simulated and real-world platforms demonstrate that KGN-Pro
outperforms existing methods in terms of grasp cover rate and success rate.

</details>


### [271] [CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning](https://arxiv.org/abs/2507.14903)
*Pan Hu*

Main category: cs.RO

TL;DR: CDGMP框架通过集成MoE和多策略强化学习，提高了CAVs在车道选择和运动规划方面的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶中，特别是在网联自动驾驶汽车（CAVs）的背景下，实现灵活且安全的车道选择以及精确的轨迹执行的挑战。

Method: 提出了一种名为CDGMP的框架，该框架利用受MoE启发的架构和多策略强化学习，将决策（车道选择）和运动规划（生成控制命令）紧密结合。通过门控机制协调多个专门子网络，将复杂的驾驶任务分解为模块化组件，提高了效率和安全性。

Result: 仿真结果表明，CDGMP在车道选择和运动规划方面表现出可靠的性能。

Conclusion: CDGMP框架通过结合MoE和多策略强化学习，实现了决策和运动规划的紧密集成，提高了CAVs在各种交通场景下的适应性和鲁棒性，为高维决策和控制任务提供了可扩展的解决方案。

Abstract: Autonomous driving demands reliable and efficient solutions to closely
related problems such as decision-making and motion planning. In this work,
decision-making refers specifically to highway lane selection, while motion
planning involves generating control commands (such as speed and steering) to
reach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),
achieving both flexible and safe lane selection alongside precise trajectory
execution remains a significant challenge. This paper proposes a framework
called Cohesive Decision-Guided Motion Planning (CDGMP), which tightly
integrates decision-making and motion planning using a Mixture of Experts (MoE)
inspired architecture combined with multi-policy reinforcement learning. By
coordinating multiple specialized sub-networks through a gating mechanism, the
method decomposes the complex driving task into modular components. Each
sub-network focuses on a specific aspect of driving, improving efficiency by
activating only the most relevant modules during inference. This design also
enhances safety through modular specialization. CDGMP improves the adaptability
and robustness of CAVs across diverse traffic scenarios, offering a scalable
solution to real-world autonomy challenges. The architectural principles behind
CDGMP, especially the use of MoE, also provide a strong foundation for other
high-dimensional decision and control tasks. Simulation results (available at
https://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane
selection and motion planning.

</details>


### [272] [One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner](https://arxiv.org/abs/2507.14914)
*Zhexuan Xu,Jie Wang,Siyuan Xu,Zijie Geng,Mingxuan Yuan,Feng Wu*

Main category: cs.RO

TL;DR: Flora, a three-stage floorplanner, optimizes chip design by integrating placement and feedthrough considerations, outperforming existing methods in HPWL, FTpin, FTmod, and component placement.


<details>
  <summary>Details</summary>
Motivation: Existing floorplanning approaches often fail to integrate with subsequent physical design stages, leading to suboptimal in-module component placement and excessive inter-module feedthrough.

Method: Flora is a three-stage feedthrough and placement aware rectilinear floorplanner. Stage 1 uses wiremask and position mask for coarse-grained optimization of HPWL and feedthrough. Stage 2 achieves zero-whitespace layout by locally resizing module shapes for fine-grained optimization of feedthrough and component placement. Stage 3 uses a fast tree search to place components within modules and adjusts module boundaries based on placement results for cross-stage optimization.

Result: Average reduction of 6% in HPWL, 5.16% in FTpin, 29.15% in FTmod, and a 14% improvement in component placement performance.

Conclusion: Flora outperforms recent state-of-the-art floorplanning approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin, 29.15% in FTmod, and a 14% improvement in component placement performance.

Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.

</details>


### [273] [Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly](https://arxiv.org/abs/2507.14929)
*Tero Kaarlela,Sami Salo,Jose Outeiro*

Main category: cs.RO

TL;DR: 为了解决电动汽车电池（EVBs）手动拆卸的安全隐患，本研究提出了一种结合远程操作和自动化（使用RGB摄像头和ROS中间件）的遥操作系统，以实现安全、高效的EVB拆卸和分类，并已通过用户友好的在线试点研究进行了验证。


<details>
  <summary>Details</summary>
Motivation: 手动拆卸电动汽车电池（EVBs）存在工人触电和接触有毒化学物质的危险，而该研究旨在通过遥操作系统解决这些安全问题，并支持闭环供应链以实现可持续的电动汽车转型。

Method: 提出了一种遥操作系统，使用RGB摄像头对齐EVB的物理和数字孪生，并利用机器人操作系统（ROS）中间件创建EVB的数字孪生。该系统允许操作员创建和保存未知EVB类型的拆卸序列，以实现未来的自动化。

Result: 在线试点研究的评估结果表明，该方法具有用户友好性，能够通过减少对劳动力的依赖和提高电池回收通量来实现经济效益。

Conclusion: 该研究提出了一种遥操作系统，用于安全地拆卸和分类电动汽车电池（EVBs），通过结合远程操作和自动化来提高安全性、适应性和效率。

Abstract: Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a
sustainable transition to electric vehicles by enabling a closed-loop supply
chain. Currently, the manual disassembly process exposes workers to hazards,
including electrocution and toxic chemicals. We propose a teleoperated system
for the safe disassembly and sorting of EVBs. A human-in-the-loop can create
and save disassembly sequences for unknown EVB types, enabling future
automation. An RGB camera aligns the physical and digital twins of the EVB, and
the digital twin of the robot is based on the Robot Operating System (ROS)
middleware. This hybrid approach combines teleoperation and automation to
improve safety, adaptability, and efficiency in EVB disassembly and sorting.
The economic contribution is realized by reducing labor dependency and
increasing throughput in battery recycling. An online pilot study was set up to
evaluate the usability of the presented approach, and the results demonstrate
the potential as a user-friendly solution.

</details>


### [274] [Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry](https://arxiv.org/abs/2507.14931)
*Qiaoqiao Ren,Remko Proesmans,Arend Pissens,Lara Dehandschutter,William Denecker,Lotte Rouckhout,Joke Carrette,Peter Vanhopplinus,Tony Belpaeme,Francis wyffels*

Main category: cs.RO

TL;DR: 在法医精神卫生保健机构中，患者自主权有限且心理压力大。本研究通过与患者、护理人员和治疗师的共同设计，开发了一个伴侣机器人，用于监测和调节压力。研究强调了让患者参与设计过程并根据其情绪状态调整建议的重要性。


<details>
  <summary>Details</summary>
Motivation: 法医精神卫生保健机构的特点是高度的官僚主义、风险规避和有限的自主权，患者常常会感到失去对自己生活的控制，导致心理压力加剧，有时甚至因为安全原因而被隔离。本研究旨在探索如何利用共同设计来协作开发一个伴侣机器人，以帮助监测和调节压力，同时跟踪患者的互动行为以进行长期干预。

Method: 通过四个共同设计研讨会，与患者、护理人员和治疗师一起，共同开发一个伴侣机器人，用于监测和调节压力，同时跟踪患者的互动行为以进行长期干预。首先向治疗师展示了一个初步的推测性原型，然后进行了患者创意构思、定义期望功能和情绪反应的研讨会，并计划进行最终原型演示以收集直接的患者反馈。

Result: 研究结果强调了在设计过程中赋予患者权力以及根据患者当前情绪状态调整建议的重要性。

Conclusion: 研究强调了在设计过程中赋予患者权力以及根据患者当前情绪状态调整建议的重要性。目标是赋予患者在设计过程中的权力，并确保每位患者的声音都能被听到。

Abstract: Forensic mental health care involves the treatment of individuals with severe
mental disorders who have committed violent offences. These settings are often
characterized by high levels of bureaucracy, risk avoidance, and restricted
autonomy. Patients frequently experience a profound loss of control over their
lives, leading to heightened psychological stress-sometimes resulting in
isolation as a safety measure. In this study, we explore how co-design can be
used to collaboratively develop a companion robot that helps monitor and
regulate stress while maintaining tracking of the patients' interaction
behaviours for long-term intervention. We conducted four co-design workshops in
a forensic psychiatric clinic with patients, caregivers, and therapists. Our
process began with the presentation of an initial speculative prototype to
therapists, enabling reflection on shared concerns, ethical risks, and
desirable features. This was followed by a creative ideation session with
patients, a third workshop focused on defining desired functions and emotional
responses, and we are planning a final prototype demo to gather direct patient
feedback. Our findings emphasize the importance of empowering patients in the
design process and adapting proposals based on their current emotional state.
The goal was to empower the patient in the design process and ensure each
patient's voice was heard.

</details>


### [275] [Heterogeneous object manipulation on nonlinear soft surface through linear controller](https://arxiv.org/abs/2507.14967)
*Pratik Ingle,Kasper Støy,Andres Faiña*

Main category: cs.RO

TL;DR: 本文提出了一种无需大量训练的PID控制方法，用于在MANTA-RAY软体机器人上操纵各种物体，通过几何变换将倾斜角度直接映射到执行器命令，实现了高效、精确和鲁棒的控制。


<details>
  <summary>Details</summary>
Motivation: 高密度驱动器阵列会因自由度（DOF）的增加而带来显著的控制复杂性，限制了操纵曲面在现实世界中的应用和利用，因为此类系统的维护和控制随着阵列/曲面尺寸的增加呈指数级增长。基于学习的控制方法虽然可以简化控制复杂性，但需要大量的训练样本，并且难以泛化到异类物体。

Method: 本文提出了一种基于PID的线性闭环反馈控制策略，利用几何变换驱动PID控制器，将倾斜角度控制输出（1D/2D）直接映射到执行器命令，从而无需复杂的黑盒训练，用于在MANTA-RAY（一种具有降低驱动密度、自适应非刚性纺织驱动的操纵器）上操纵异类物体。

Result: 通过仿真和物理系统实验验证，该方法成功操纵了各种几何形状、重量和纹理的物体，包括鸡蛋和苹果等易碎物体，结果表明该方法高度通用，并为软体机器人操纵提供了一种实用且可靠的解决方案。

Conclusion: 所提出的基于PID的线性闭环反馈控制策略能够有效地处理具有不同几何形状、重量和纹理的物体（包括易碎物体如鸡蛋和苹果），证明了该方法的高度通用性，为软体机器人操作提供了实际可靠的解决方案，并且无需高昂的训练成本即可实现现实世界中的应用。

Abstract: Manipulation surfaces indirectly control and reposition objects by actively
modifying their shape or properties rather than directly gripping objects.
These surfaces, equipped with dense actuator arrays, generate dynamic
deformations. However, a high-density actuator array introduces considerable
complexity due to increased degrees of freedom (DOF), complicating control
tasks. High DOF restrict the implementation and utilization of manipulation
surfaces in real-world applications as the maintenance and control of such
systems exponentially increase with array/surface size. Learning-based control
approaches may ease the control complexity, but they require extensive training
samples and struggle to generalize for heterogeneous objects. In this study, we
introduce a simple, precise and robust PID-based linear close-loop feedback
control strategy for heterogeneous object manipulation on MANTA-RAY
(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation
density). Our approach employs a geometric transformation-driven PID
controller, directly mapping tilt angle control outputs(1D/2D) to actuator
commands to eliminate the need for extensive black-box training. We validate
the proposed method through simulations and experiments on a physical system,
successfully manipulating objects with diverse geometries, weights and
textures, including fragile objects like eggs and apples. The outcomes
demonstrate that our approach is highly generalized and offers a practical and
reliable solution for object manipulation on soft robotic manipulation,
facilitating real-world implementation without prohibitive training demands.

</details>


### [276] [FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models](https://arxiv.org/abs/2507.14975)
*Yufan Song,Jiatao Zhang,Zeng Gu,Qingmiao Liang,Tuocheng Hu,Wei Song,Shiqiang Zhu*

Main category: cs.RO

TL;DR: 提出了一种名为FCRF的新框架，通过灵活的自我反思和整合过往经验教训，来改进机器人完成复杂任务的能力，并在模拟和现实世界中取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM用于任务规划错误纠正的自我反思机制存在灵活性不足的限制。

Method: 提出了一种新颖的导师-执行器架构，称为灵活建构主义反思框架（FCRF），该框架能够基于任务难度对LLM进行灵活的自我反思，并建设性地整合历史宝贵经验和失败教训。

Result: 在AlfWorld模拟和真实环境中对各种家务任务进行的实验证明，FCRF显著提高了复杂长时程机器人任务的整体性能和自我反思的灵活性。

Conclusion: FCRF显著提高了复杂长时程机器人任务中的整体性能和自我反思的灵活性。

Abstract: Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.

</details>


### [277] [CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions](https://arxiv.org/abs/2507.15022)
*Sumeadh MS,Kevin Dsouza,Ravi Prakash*

Main category: cs.RO

TL;DR: 提出CPED-NCBFs方法，通过分裂一致性预测来更精确地验证神经控制障碍函数（NCBF），解决了现有方法的保守性问题，并在仿真中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有验证技术（如SMT、MIP、区间/边界传播）在验证从专家演示中学习到的神经控制障碍函数（NCBF）时存在的界限过于保守的问题。

Method: 提出并验证了一种名为CPED-NCBFs的基于分裂一致性预测的验证策略。

Result: 在点质量系统和单轮模型上验证了所提出的方法的有效性。

Conclusion: 本研究提出了一种基于分裂一致性预测的验证策略（CPED-NCBFs）来验证从专家演示中学习到的神经控制障碍函数（NCBF），以克服现有方法的保守性限制。

Abstract: Among the promising approaches to enforce safety in control systems, learning
Control Barrier Functions (CBFs) from expert demonstrations has emerged as an
effective strategy. However, a critical challenge remains: verifying that the
learned CBFs truly enforce safety across the entire state space. This is
especially difficult when CBF is represented using neural networks (NCBFs).
Several existing verification techniques attempt to address this problem
including SMT-based solvers, mixed-integer programming (MIP), and interval or
bound-propagation methods but these approaches often introduce loose,
conservative bounds. To overcome these limitations, in this work we use
CPED-NCBFs a split-conformal prediction based verification strategy to verify
the learned NCBF from the expert demonstrations. We further validate our method
on point mass systems and unicycle models to demonstrate the effectiveness of
the proposed theory.

</details>


### [278] [Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper](https://arxiv.org/abs/2507.15062)
*Xinyue Zhu,Binghao Huang,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出了一种集成触觉传感的夹爪和跨模态表示学习框架，用于机器人精细操作，提高了准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的手持夹爪大多缺乏触觉传感，而触觉反馈在精确操控中起着关键作用，因此需要一种集成触觉传感的夹爪和相应的学习框架。

Method: 提出了一种便携、轻巧且集成了触觉传感器的夹爪，能够同步收集视觉和触觉数据，并在此基础上构建了一个跨模态表示学习框架，该框架能够整合视觉和触觉信号，同时保留它们各自的特性。

Result: 该框架能够学习到清晰的、关注于物理交互相关接触区域的可解释表示，并在试管插入和移液器流体输送等精细任务中，相较于其他方法，在准确性和鲁棒性方面均有提升。

Conclusion: 所提出的跨模态表示学习框架通过集成视觉和触觉信号，在精细操作任务中实现了更有效的策略学习，提高了准确性和鲁棒性。

Abstract: Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .

</details>


### [279] [Search-Based Autonomous Vehicle Motion Planning Using Game Theory](https://arxiv.org/abs/2507.15088)
*Pouya Panahandeh,Mohammad Pirani,Baris Fidan,Amir Khajepour*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于博弈论的运动规划方法，将其他道路使用者视为智能体，为自动驾驶汽车生成更真实的路径，并且适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 为了生成更现实的自主车辆（AV）路径，并克服传统方法将其他道路使用者视为静态障碍物的问题。

Method: 提出了一种基于搜索的交互式运动规划方案，并采用了博弈论的方法，将其他道路使用者视为智能代理。

Result: 该方法生成的路径更现实，并且由于计算时间短，可以用于实时应用程序。实验表明，与现有技术相比，该方法具有更好的性能。

Conclusion: 该研究提出了一种基于搜索的交互式运动规划方案，用于自主车辆（AV），并采用了博弈论的方法。与传统的基于搜索的方法不同，新开发的方法将其他道路使用者（例如驾驶员和行人）视为智能代理，而不是静态障碍物。这使得为AV生成更现实的路径成为可能。该方法具有计算时间短的优点，可以用于实时应用程序。所开发的运动规划方案的性能与现有的运动规划技术进行了比较，并通过使用WATonoBus（一种全天候电动自主班车）的实验进行了验证。

Abstract: In this paper, we propose a search-based interactive motion planning scheme
for autonomous vehicles (AVs), using a game-theoretic approach. In contrast to
traditional search-based approaches, the newly developed approach considers
other road users (e.g. drivers and pedestrians) as intelligent agents rather
than static obstacles. This leads to the generation of a more realistic path
for the AV. Due to the low computational time, the proposed motion planning
scheme is implementable in real-time applications. The performance of the
developed motion planning scheme is compared with existing motion planning
techniques and validated through experiments using WATonoBus, an electrical
all-weather autonomous shuttle bus.

</details>


### [280] [Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions](https://arxiv.org/abs/2507.15155)
*Majid Roshanfar,Alex Zhang,Changyan He,Amir Hooshiar,Dale J. Podolsky,Thomas Looi,Eric Diller*

Main category: cs.RO

TL;DR: 该研究提出了一种用于磁驱动软体吸附装置的学习模型，该模型能以高精度预测装置形状，有助于微创手术。


<details>
  <summary>Details</summary>
Motivation: 为了实现内鼻脑肿瘤切除术中对磁力软体吸附装置的智能控制，需要一个能够准确模拟其复杂非线性行为的模型。

Method: 介绍了用于内鼻脑肿瘤切除术的磁力软体吸附装置的学习模型框架。该装置采用 4 毫米外径、2 毫米内径、40 毫米长度的 3D 打印设计，使用生物相容性 SIL 30 材料制成，并嵌入了光纤布拉格光栅传感器以实现实时形状反馈。利用四个贝塞尔控制点来表示形状重建，从而构建了紧凑且平滑的装置变形模型。使用神经网络（NN）和随机森林（RF）两种架构，在 5,097 个实验样本上进行了数据驱动模型的训练，涵盖了磁场强度（0-14 mT）、驱动频率（0.2-1.0 Hz）和垂直尖端距离（90-100 mm）等参数范围。

Result: 随机森林（RF）模型的性能在所有指标上均优于神经网络（NN）模型，在控制点预测方面实现了 0.087 毫米的平均均方根误差，在形状重建方面实现了 0.064 毫米的平均形状重建误差。特征重要性分析表明，磁场分量主要影响远端控制点，而频率和距离则影响基极配置。

Conclusion: 该学习方法能够有效模拟超弹性软体机器人在磁驱动下的复杂非线性行为，无需依赖简化的物理假设，实现了亚毫米级的形状预测精度和实时推理能力，代表了在微创神经外科手术中磁驱动软体机器人智能控制方面的一项进展。

Abstract: This letter introduces a novel learning-based modeling framework for a
magnetically steerable soft suction device designed for endoscopic endonasal
brain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm
inner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,
and integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape
feedback. Shape reconstruction is represented using four Bezier control points,
enabling a compact and smooth model of the device's deformation. A data-driven
model was trained on 5,097 experimental samples covering a range of magnetic
field magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical
tip distances (90-100 mm), using both Neural Network (NN) and Random Forest
(RF) architectures. The RF model outperformed the NN across all metrics,
achieving a mean root mean square error of 0.087 mm in control point prediction
and a mean shape reconstruction error of 0.064 mm. Feature importance analysis
further revealed that magnetic field components predominantly influence distal
control points, while frequency and distance affect the base configuration.
This learning-based approach effectively models the complex nonlinear behavior
of hyperelastic soft robots under magnetic actuation without relying on
simplified physical assumptions. By enabling sub-millimeter shape prediction
accuracy and real-time inference, this work represents an advancement toward
the intelligent control of magnetically actuated soft robotic tools in
minimally invasive neurosurgery.

</details>


### [281] [CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer](https://arxiv.org/abs/2507.15189)
*Kevin Christiansen Marsim,Jinwoo Jeon,Yeeun Kim,Myeongwoo Jeong,Hyun Myung*

Main category: cs.RO

TL;DR: CHADET是一个轻量级的深度补全网络，通过跨层次注意力Transformer提高了深度图的准确性和处理速度，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有深度补全方法在计算效率和推理精度之间存在的显著权衡问题，并满足实时应用对内存和计算能力的要求，从而提高机器人在各种任务中的性能。

Method: 提出了一种名为CHADET（cross-hierarchical-attention depth-completion transformer）的轻量级深度补全网络。该网络从深度块中提取特征，并将其传递给基于Transformer的解码器。在解码器中，利用新颖的跨层次注意力模块，通过深度信息来优化图像特征。

Result: CHADET网络能够从RGB图像和稀疏深度点生成精确的密集深度图，提高了深度图预测的质量并减少了内存使用。

Conclusion: CHADET网络在KITTI、NYUv2和VOID数据集上均表现出色，能够生成高质量的深度图，同时减少了内存使用。

Abstract: Depth information which specifies the distance between objects and current
position of the robot is essential for many robot tasks such as navigation.
Recently, researchers have proposed depth completion frameworks to provide
dense depth maps that offer comprehensive information about the surrounding
environment. However, existing methods show significant trade-offs between
computational efficiency and accuracy during inference. The substantial memory
and computational requirements make them unsuitable for real-time applications,
highlighting the need to improve the completeness and accuracy of depth
information while improving processing speed to enhance robot performance in
various tasks. To address these challenges, in this paper, we propose
CHADET(cross-hierarchical-attention depth-completion transformer), a
lightweight depth-completion network that can generate accurate dense depth
maps from RGB images and sparse depth points. For each pair, its feature is
extracted from the depthwise blocks and passed to the equally lightweight
transformer-based decoder. In the decoder, we utilize the novel
cross-hierarchical-attention module that refines the image features from the
depth information. Our approach improves the quality and reduces memory usage
of the depth map prediction, as validated in both KITTI, NYUv2, and VOID
datasets.

</details>


### [282] [VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving](https://arxiv.org/abs/2507.15266)
*Haichao Liu,Haoren Guo,Pei Liu,Benshan Ma,Yuxiang Zhang,Jun Ma,Tong Heng Lee*

Main category: cs.RO

TL;DR: 提出了一种名为VLM-UDMC的框架，通过结合视觉语言模型（VLM）和检索增强生成（RAG）技术，增强了自动驾驶汽车在城市环境中的决策和运动控制能力，提高了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了在确保透明度和可解释性的同时，模仿城市自动驾驶中的人类驾驶员场景理解和风险感知注意力能力，以做出安全有效的驾驶决策。

Method: 提出了一种名为VLM-UDMC的视觉语言模型（VLM）增强的统一决策和运动控制框架。该框架将场景推理和风险感知洞察力纳入上层慢速系统，从而动态重新配置下游快速系统的最优运动规划。上层慢速系统采用基于检索增强生成（RAG）的两步推理策略，利用基础模型处理多模态输入并检索上下文知识，从而产生风险感知洞察力。同时，一个轻量级的多核分解LSTM通过提取更平滑的趋势表示来进行短期轨迹预测，从而提供异构交通参与者的实时轨迹预测。

Result: 通过模拟和真实世界实验证明，所提出的VLM-UDMC框架能够有效利用场景理解和注意力分解来进行理性驾驶决策，从而提高整体城市驾驶性能。

Conclusion: 该框架通过场景理解和注意力分解有效利用了理性驾驶决策，从而提高了整体城市驾驶性能。

Abstract: Scene understanding and risk-aware attentions are crucial for human drivers
to make safe and effective driving decisions. To imitate this cognitive ability
in urban autonomous driving while ensuring the transparency and
interpretability, we propose a vision-language model (VLM)-enhanced unified
decision-making and motion control framework, named VLM-UDMC. This framework
incorporates scene reasoning and risk-aware insights into an upper-level slow
system, which dynamically reconfigures the optimal motion planning for the
downstream fast system. The reconfiguration is based on real-time environmental
changes, which are encoded through context-aware potential functions. More
specifically, the upper-level slow system employs a two-step reasoning policy
with Retrieval-Augmented Generation (RAG), leveraging foundation models to
process multimodal inputs and retrieve contextual knowledge, thereby generating
risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM
provides real-time trajectory predictions for heterogeneous traffic
participants by extracting smoother trend representations for short-horizon
trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is
verified via both simulations and real-world experiments with a full-size
autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively
leverages scene understanding and attention decomposition for rational driving
decisions, thus improving the overall urban driving performance. Our
open-source project is available at https://github.com/henryhcliu/vlmudmc.git.

</details>


### [283] [RepILN: Reparameterized Inertial Localization Network](https://arxiv.org/abs/2507.15293)
*Shanshan Zhang,Tianshui Wen,Siyue Wang,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: 提出了一种高效的惯性定位网络，通过多分支到单路径转换、时间尺度稀疏注意力和门控卷积单元，在保持模型轻量化的同时提高了定位精度，有效解决了物联网设备的计算资源限制和长期依赖性建模问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有数据驱动的惯性定位方法在计算资源有限的物联网设备上过于复杂以及忽略运动轨迹长期依赖性导致定位性能受限的问题。

Method: 提出了一种再参数化的惯性定位网络，在训练时采用多分支结构增强特征提取，在推理时转换为等效的单路径架构以提高参数效率。引入了时间尺度稀疏注意力机制来捕捉运动轨迹中的长期依赖关系，并结合门控卷积单元来整合长程依赖和局部细粒度特征。

Result: 实验证明，该方法在RoNIN数据集上，相比RoNIN-ResNet，绝对轨迹误差（ATE）降低了2.59%，参数量减少了3.86%，在精度和模型紧凑性之间取得了良好的平衡。

Conclusion: 该方法在RoNIN数据集上实现了精度和模型紧凑性之间的良好权衡，将绝对轨迹误差（ATE）降低了2.59%，同时参数量减少了3.86%。

Abstract: Inertial localization is regarded as a promising positioning solution for
consumer-grade IoT devices due to its cost-effectiveness and independence from
external infrastructure. However, data-driven inertial localization methods
often rely on increasingly complex network architectures to improve accuracy,
which challenges the limited computational resources of IoT devices. Moreover,
these methods frequently overlook the importance of modeling long-term
dependencies in inertial measurements - a critical factor for accurate
trajectory reconstruction - thereby limiting localization performance. To
address these challenges, we propose a reparameterized inertial localization
network that uses a multi-branch structure during training to enhance feature
extraction. At inference time, this structure is transformed into an equivalent
single-path architecture to improve parameter efficiency. To further capture
long-term dependencies in motion trajectories, we introduce a temporal-scale
sparse attention mechanism that selectively emphasizes key trajectory segments
while suppressing noise. Additionally, a gated convolutional unit is
incorporated to effectively integrate long-range dependencies with local
fine-grained features. Extensive experiments on public benchmarks demonstrate
that our method achieves a favorable trade-off between accuracy and model
compactness. For example, on the RoNIN dataset, our approach reduces the
Absolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while
reducing the number of parameters by 3.86%.

</details>


### [284] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本研究提出了一种创新的闭环控制系统，利用实时流场测量数据，使四旋翼飞行器能够在狭窄管道内实现稳定的悬停和精确的避障飞行。


<details>
  <summary>Details</summary>
Motivation: 为了解决在管道和隧道等狭窄空间内，四旋翼飞行器由于不稳定的自感气动干扰而面临的飞行挑战，开发一种能够稳定悬停并有效应对气动效应的控制系统。

Method: 开发了一种低延迟、事件驱动的烟雾流速测量方法，用于高时间分辨率地估计局部气流。利用循环卷积神经网络（Recurrent Convolutional Neural Network, RCNN）对流场信息进行实时干扰力矩估计，并将估计结果整合到基于强化学习的控制器中，实现了闭环控制。

Result: 所提出的基于流场反馈的控制系统在管道内的横向平移机动中表现出高效性，能够有效抵消瞬态气动效应，避免与管道壁发生碰撞。这是首次利用实时流场测量信息指导的闭环控制系统在空中机器人上的应用。

Conclusion: 本研究首次实现了基于实时流场测量的闭环控制系统，用于在狭窄管道中进行四旋翼悬停，有效解决了传统方法在稳定悬停和避免碰撞方面的局限性，并为在复杂气动环境中飞行开辟了新的研究方向。

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [285] [The Emergence of Deep Reinforcement Learning for Path Planning](https://arxiv.org/abs/2507.15469)
*Thanh Thi Nguyen,Saeid Nahavandi,Imran Razzak,Dung Nguyen,Nhat Truong Pham,Quoc Viet Hung Nguyen*

Main category: cs.RO

TL;DR: 本调查提供了对自动驾驶汽车、无人机和机器人平台路径规划的传统方法和DRL最新进展的全面概述。


<details>
  <summary>Details</summary>
Motivation: 日益增长的在复杂和动态环境中对自主系统的需求，推动了对智能路径规划方法的研究。

Method: 本调查全面概述了传统方法以及DRL在路径规划任务中的最新进展，重点是自动驾驶汽车、无人机和机器人平台。对两种范式中的关键算法进行了分类，并强调了它们的创新和实际应用。

Result: 对传统方法和基于学习的方法的关键算法进行了分类，并强调了它们的优点和局限性，特别是在计算效率、可扩展性、适应性和鲁棒性方面。

Conclusion: DRL与经典规划技术相结合的混合方法为鲁棒和有弹性的自主导航提供了有前途的方向。

Abstract: The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.

</details>


### [286] [All-UWB SLAM Using UWB Radar and UWB AOA](https://arxiv.org/abs/2507.15474)
*Charith Premachandra,Achala Athukorala,U-Xuan Tan*

Main category: cs.RO

TL;DR: 本研究提出了一种结合UWB到达角（AOA）测量的新型SLAM方法，以解决仅使用UWB雷达在视觉受限、特征稀疏环境中SLAM的局限性。通过部署动态UWB锚点-标签单元，该方法克服了特征提取的限制，并通过实验证明了其在恶劣环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于UWB雷达的SLAM方法依赖于环境中的可区分特征，这在特征稀疏的环境中是一个限制。因此，需要一种新的方法来提高SLAM在这些条件下的性能。

Method: 提出了一种将UWB到达角（AOA）测量值纳入基于UWB雷达的SLAM系统的 novel method，以提高在特征稀疏环境下的SLAM精度和可扩展性。AOA测量值是通过机器人动态部署的UWB锚点-标签单元获得的。

Result: 实验结果表明，集成UWB AOA单元与UWB雷达能够实现视觉受限、特征匮乏环境下的SLAM。

Conclusion: UWB AOA单元与UWB雷达的集成能够实现视觉受限、特征匮乏环境下的SLAM。

Abstract: There has been a growing interest in autonomous systems designed to operate
in adverse conditions (e.g. smoke, dust), where the visible light spectrum
fails. In this context, Ultra-wideband (UWB) radar is capable of penetrating
through such challenging environmental conditions due to the lower frequency
components within its broad bandwidth. Therefore, UWB radar has emerged as a
potential sensing technology for Simultaneous Localization and Mapping (SLAM)
in vision-denied environments where optical sensors (e.g. LiDAR, Camera) are
prone to failure. Existing approaches involving UWB radar as the primary
exteroceptive sensor generally extract features in the environment, which are
later initialized as landmarks in a map. However, these methods are constrained
by the number of distinguishable features in the environment. Hence, this paper
proposes a novel method incorporating UWB Angle of Arrival (AOA) measurements
into UWB radar-based SLAM systems to improve the accuracy and scalability of
SLAM in feature-deficient environments. The AOA measurements are obtained using
UWB anchor-tag units which are dynamically deployed by the robot in featureless
areas during mapping of the environment. This paper thoroughly discusses
prevailing constraints associated with UWB AOA measurement units and presents
solutions to overcome them. Our experimental results show that integrating UWB
AOA units with UWB radar enables SLAM in vision-denied feature-deficient
environments.

</details>


### [287] [The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents](https://arxiv.org/abs/2507.15478)
*Simon Kohaut,Felix Divo,Navid Hamid,Benedict Flade,Julian Eggert,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.RO

TL;DR: 神经符号系统（如 CoCo）通过结合规则和深度学习，提高了自主代理在复杂环境中的安全性和可靠性，并通过“自我怀疑”机制实现智能导航。


<details>
  <summary>Details</summary>
Motivation: 在现代机器人技术中，确保自主代理在不确定环境中能够可靠且合规地行为是一个基本挑战。

Method: 提出了一种名为“宪法控制器”（CoCo）的新型框架，该框架通过推理深层概率逻辑程序来增强代理的安全性和可靠性。此外，还引入了“自我怀疑”的概念，作为一种基于怀疑特征（如旅行速度、传感器使用情况或健康因素）的条件概率密度。

Result: 在真实的航空移动研究中，证明了 CoCo 在智能自主系统中学习适当的怀疑机制，并在复杂不确定的环境中安全合规地导航的优势。

Conclusion: 该研究展示了神经符号系统如何通过结合概率符号模型和深度学习方法，为在不确定环境中操作的自主代理提供可靠且符合规则的行为，解决了现代机器人技术中的一个关键挑战。

Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.

</details>


### [288] [Robots for Kiwifruit Harvesting and Pollination](https://arxiv.org/abs/2507.15484)
*Jamie Bell*

Main category: cs.RO

TL;DR: 该研究开发了一种能在猕猴桃园中进行喷粉和采摘的移动机器人，并实现了自主导航，提高了作业效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高猕猴桃种植的效率，开发了能够进行定向喷粉和自动化采摘的移动机器人。

Method: 开发了移动机器人，包括用于喷粉的系统和用于采摘的机械臂。利用3D激光雷达和计算机视觉技术进行导航，并设计了多种从3D激光雷达数据中提取特征的方法。

Result: 开发的猕猴桃分离机制能够可靠地采摘猕猴桃，并能够接触到80%以上的果实，优于先前技术。机器人导航系统能够实现自主驾驶，包括路径跟随、行尾检测和行尾转弯，计算机视觉算法在测试中的表现与3D激光雷达导航方法相当。

Conclusion: 移动机器人在猕猴桃棚架式果园中实现了定向喷粉和自动化采摘，其中开发的猕猴桃分离机制能够可靠地采摘猕猴桃，并能够接触到80%以上的果实，优于先前技术。同时，通过3D激光雷达和计算机视觉技术实现了机器人在猕猴桃园中的自主导航，包括路径跟随、行尾检测和行尾转弯，并且在超过30公里的自主驾驶测试中表现良好。

Abstract: This research was a part of a project that developed mobile robots that
performed targeted pollen spraying and automated harvesting in pergola
structured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were
designed and field testing of one of the concepts showed that the mechanism
could reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism
was able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,
whereas the previous state of the art mechanism was only able to reach less
than 70 percent of the fruit. Artificial pollination was performed by detecting
flowers and then spraying pollen in solution onto the detected flowers from a
line of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the
height of the canopy was measured and the spray boom was moved up and down to
keep the boom close enough to the flowers for the spray to reach the flowers,
while minimising collisions with the canopy. Mobile robot navigation was
performed using a 2D lidar in apple orchards and vineyards. Lidar navigation in
kiwifruit orchards was more challenging because the pergola structure only
provides a small amount of data for the direction of rows, compared to the
amount of data from the overhead canopy, the undulating ground and other
objects in the orchards. Multiple methods are presented here for extracting
structure defining features from 3D lidar data in kiwifruit orchards. In
addition, a 3D lidar navigation system -- which performed row following, row
end detection and row end turns -- was tested for over 30 km of autonomous
driving in kiwifruit orchards. Computer vision algorithms for row detection and
row following were also tested. The computer vision algorithm worked as well as
the 3D lidar row following method in testing.

</details>


### [289] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: GR-3是一种先进的视觉-语言-动作模型，能够高效泛化并执行复杂机器人任务，在实验中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 开发通用机器人策略，使机器人能够处理新颖物体、环境和抽象指令，并能通过少量数据进行高效适应，以应对现实生活中的广泛任务。

Method: GR-3的训练方法包括：1. 与网络规模的视觉-语言数据进行协同训练；2. 利用通过VR设备收集的人类轨迹数据进行高效微调；3. 结合机器人轨迹数据进行有效的模仿学习。

Result: GR-3在泛化能力、高效微调、处理长期和灵巧任务（包括双手机器人和移动操作）方面表现出色，并且在多种具有挑战性的任务上超越了最先进的基线方法$"\pi_0$"。

Conclusion: GR-3是一个大型视觉-语言-动作（VLA）模型，在泛化到新颖物体、环境和涉及抽象概念的指令方面表现出色。通过结合网络规模的视觉-语言数据、来自VR设备的人类轨迹数据以及机器人轨迹数据的有效模仿学习，GR-3可以高效地进行微调，并能处理长期和灵巧的任务，包括双手机器人和移动操作。在实际操作中，GR-3在多种具有挑战性的任务上都优于最先进的基线方法$"\pi_0$"。

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [290] [CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions](https://arxiv.org/abs/2507.15499)
*Jongseok Lee,Timo Birr,Rudolph Triebel,Tamim Asfour*

Main category: cs.RO

TL;DR: CLEVER是一个主动学习系统，通过结合人类指令来改进DNN的语义感知能力，特别是在处理数据流和提高鲁棒性方面表现出色，并且已在真实机器人上实现。


<details>
  <summary>Details</summary>
Motivation: 为了提高深度神经网络（DNN）在数据流情境下的鲁棒性语义感知能力，并实现基于流的主动学习。

Method: CLEVER系统采用贝叶斯方法，通过先验知识编码领域知识，以实现当遇到失败时寻求人类支持并在线调整DNN。

Result: 通过用户验证研究和在人形及可变形物体上的实验，证明了CLEVER系统的设计合理性及其能力。

Conclusion: CLEVER系统能够通过在线适应DNN并结合人类指令来完成给定的语义感知任务，并在实际机器人上实现了基于流的主动学习，提高了基于DNN的语义感知的鲁棒性。

Abstract: We propose CLEVER, an active learning system for robust semantic perception
with Deep Neural Networks (DNNs). For data arriving in streams, our system
seeks human support when encountering failures and adapts DNNs online based on
human instructions. In this way, CLEVER can eventually accomplish the given
semantic perception tasks. Our main contribution is the design of a system that
meets several desiderata of realizing the aforementioned capabilities. The key
enabler herein is our Bayesian formulation that encodes domain knowledge
through priors. Empirically, we not only motivate CLEVER's design but further
demonstrate its capabilities with a user validation study as well as
experiments on humanoid and deformable objects. To our knowledge, we are the
first to realize stream-based active learning on a real robot, providing
evidence that the robustness of the DNN-based semantic perception can be
improved in practice. The project website can be accessed at
https://sites.google.com/view/thecleversystem.

</details>


### [291] [Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding](https://arxiv.org/abs/2507.15604)
*Johannes Hartwig,Philipp Lienhardt,Dominik Henrich*

Main category: cs.RO

TL;DR: 该研究提出了一种在机器人示教过程中估计机器人有效载荷惯性参数（PIP）的方法，以简化非专业用户的编程操作，并能灵活更换机器人工具。所提出的方法利用了演示任务中的非接触运动来估计PIP，并证明了其可行性，但同时也指出准确估计需要足够的有效载荷加速度，并且质心和转动惯量张量的估计会受到噪声和激励不足的影响。


<details>
  <summary>Details</summary>
Motivation: 为了使编程技能有限的用户能够更有效地对协作机器人进行交互式编程，本研究旨在通过消除对专用PIP校准的需求来实现灵活的机器人工具更改。

Method: 该方法利用演示任务中非接触部分的来估计机器人的PIP（有效载荷惯性参数），其中使用了现有的估计技术。

Result: 所提出的PIP估计方法是可行的，但有效载荷的质心和转动惯量张量会受到噪声和激励不足的影响。

Conclusion: 该方法可用于实现机器人中使用的工具的有效混合运动-力控制，但准确估计需要足够的有效载荷加速度。

Abstract: As the availability of cobots increases, it is essential to address the needs
of users with little to no programming knowledge to operate such systems
efficiently. Programming concepts often use intuitive interaction modalities,
such as hand guiding, to address this. When programming in-contact motions,
such frameworks require knowledge of the robot tool's payload inertial
parameters (PIP) in addition to the demonstrated velocities and forces to
ensure effective hybrid motion-force control. This paper aims to enable
non-expert users to program in-contact motions more efficiently by eliminating
the need for a dedicated PIP calibration, thereby enabling flexible robot tool
changes. Since demonstrated tasks generally also contain motions with
non-contact, our approach uses these parts to estimate the robot's PIP using
established estimation techniques. The results show that the estimation of the
payload's mass is accurate, whereas the center of mass and the inertia tensor
are affected by noise and a lack of excitation. Overall, these findings show
the feasibility of PIP estimation during hand guiding but also highlight the
need for sufficient payload accelerations for an accurate estimation.

</details>


### [292] [A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning](https://arxiv.org/abs/2507.15607)
*Yanbo Chen,Yunzhe Tan,Yaojia Wang,Zhengzhe Xu,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: 开发了一种新的通用车辆-拖车导航系统，使用混合运动学模型和在线残差学习来处理不同的拖车和负载，并通过模型预测控制进行了优化，在真实世界实验中表现稳健。


<details>
  <summary>Details</summary>
Motivation: 车辆-拖车系统的自主导航在机场、超市和音乐会场馆等环境中至关重要，因为这些地方需要各种类型的拖车在不同的负载和条件下进行导航。然而，准确地对这类系统进行建模仍然是一个挑战，特别是对于带有脚轮的拖车。

Method: 提出了一种新的通用车辆-拖车导航系统，该系统集成了混合名义运动学模型（结合了经典的车辆非完整约束和基于神经网络的拖车运动学）以及一个轻量级的在线残差学习模块，用于实时校正模型差异和干扰。此外，还开发了一个具有加权模型组合策略的模型预测控制框架，以提高长时预测精度并确保更安全的运动规划。

Result: 通过大量的真实世界实验，涉及多种拖车类型和不同的负载条件，验证了该方法，证明了其鲁棒的性能，并且无需进行手动调优或针对特定拖车的校准。

Conclusion: 该方法在多种拖车类型和不同的负载条件下，通过大量的真实世界实验进行了验证，无需手动调整或拖车特定的校准，证明了其鲁棒的性能。

Abstract: Autonomous navigation of vehicle-trailer systems is crucial in environments
like airports, supermarkets, and concert venues, where various types of
trailers are needed to navigate with different payloads and conditions.
However, accurately modeling such systems remains challenging, especially for
trailers with castor wheels. In this work, we propose a novel universal
vehicle-trailer navigation system that integrates a hybrid nominal kinematic
model--combining classical nonholonomic constraints for vehicles and neural
network-based trailer kinematics--with a lightweight online residual learning
module to correct real-time modeling discrepancies and disturbances.
Additionally, we develop a model predictive control framework with a weighted
model combination strategy that improves long-horizon prediction accuracy and
ensures safer motion planning. Our approach is validated through extensive
real-world experiments involving multiple trailer types and varying payload
conditions, demonstrating robust performance without manual tuning or
trailer-specific calibration.

</details>


### [293] [Optimizing Force Signals from Human Demonstrations of In-Contact Motions](https://arxiv.org/abs/2507.15608)
*Johannes Hartwig,Fabian Viessmann,Dominik Henrich*

Main category: cs.RO

TL;DR: 为解决机器人编程中不精确的力信号问题，本研究提出了一种优化方法，通过信号滤波和峰值检测技术，提高了运动质量。


<details>
  <summary>Details</summary>
Motivation: 随着机器人编程在接触式任务中日益重要，但非机器人编程专家在输入时会遇到不精确和嘈杂的信号问题，因此需要优化力信号以更好地匹配人类意图。

Method: 本研究提出了一种优化力信号的方法，包括比较不同的信号滤波方法，并提出了一种用于处理首次接触偏差的峰值检测方法。同时，分析了关键参数对滤波方法的影响。

Result: 通过信号滤波和峰值检测等方法优化力信号，在特定误差标准下，个别运动质量最多可提高20%。

Conclusion: 该研究提出的方法通过优化力信号，提高了机器人编程的用户体验和人机交互质量，在特定误差标准下，个别运动质量最多可提高20%。

Abstract: For non-robot-programming experts, kinesthetic guiding can be an intuitive
input method, as robot programming of in-contact tasks is becoming more
prominent. However, imprecise and noisy input signals from human demonstrations
pose problems when reproducing motions directly or using the signal as input
for machine learning methods. This paper explores optimizing force signals to
correspond better to the human intention of the demonstrated signal. We compare
different signal filtering methods and propose a peak detection method for
dealing with first-contact deviations in the signal. The evaluation of these
methods considers a specialized error criterion between the input and the
human-intended signal. In addition, we analyze the critical parameters'
influence on the filtering methods. The quality for an individual motion could
be increased by up to \SI{20}{\percent} concerning the error criterion. The
proposed contribution can improve the usability of robot programming and the
interaction between humans and robots.

</details>


### [294] [EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation](https://arxiv.org/abs/2507.15649)
*Haocheng Xu,Haodong Zhang,Zhenghan Chen,Rong Xiong*

Main category: cs.RO

TL;DR: 通过强化学习和运动先验模块，使人形机器人能够模仿人类上身运动，同时保持站立稳定。


<details>
  <summary>Details</summary>
Motivation: 为了支持人形机器人执行操作任务，研究在适应上半身运动的同时保持稳定站立至关重要，但现有机器人可控范围有限，影响了整体稳定性。

Method: 提出了一种基于强化学习的框架，包括一个重定目标网络，用于生成大规模的上身运动数据集以训练强化学习策略，并引入了一个可执行运动先验（EMP）模块来调整输入目标运动，以提高站立稳定性并最大限度地减少运动幅度的变化。

Result: 该框架能够使人形机器人模仿人类的上身运动，同时保持整体稳定性，并且通过域随机化增强了鲁棒性，通过EMP模块提高了站立稳定性。

Conclusion: 该框架通过仿真和实际测试进行了评估，证明了其在实际应用中的可行性。

Abstract: To support humanoid robots in performing manipulation tasks, it is essential
to study stable standing while accommodating upper-body motions. However, the
limited controllable range of humanoid robots in a standing position affects
the stability of the entire body. Thus we introduce a reinforcement learning
based framework for humanoid robots to imitate human upper-body motions while
maintaining overall stability. Our approach begins with designing a retargeting
network that generates a large-scale upper-body motion dataset for training the
reinforcement learning (RL) policy, which enables the humanoid robot to track
upper-body motion targets, employing domain randomization for enhanced
robustness. To avoid exceeding the robot's execution capability and ensure
safety and stability, we propose an Executable Motion Prior (EMP) module, which
adjusts the input target movements based on the robot's current state. This
adjustment improves standing stability while minimizing changes to motion
amplitude. We evaluate our framework through simulation and real-world tests,
demonstrating its practical applicability.

</details>


### [295] [Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms](https://arxiv.org/abs/2507.15677)
*Huayue Liang,Yanbo Chen,Hongyang Cheng,Yanzhao Yu,Shoujie Li,Junbo Tan,Xueqian Wang,Long Zeng*

Main category: cs.RO

TL;DR: 提出了一种基于数据驱动的MPC方法，用于控制柔性有缆驱动机器人手臂（FCRA），提高了控制精度并减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 为了克服柔性有缆驱动机器人手臂（FCRA）因缆线固有的弹性、滞后和摩擦等特性而导致的建模和控制方面的困难，提高其控制精度。

Method: 提出了一种仅依赖输入输出数据、无需物理模型即可改进FCRA控制精度的模型预测控制（MPC）方法。该方法包括开发基于输入输出数据的隐式模型并将其集成到MPC优化框架中，以及引入数据选择算法（DSA）来过滤最能表征系统的数据，从而将每步求解时间减少到约4毫秒（提高了近80%）。

Result: 通过仿真研究了超参数对跟踪误差的影响。在真实的FCRA平台上进行了包括五点定位精度测试、五点响应跟踪测试和字母绘制轨迹跟踪测试。结果表明，平均定位精度约为2.070毫米，平均跟踪误差为0.541度，优于PID方法（平均跟踪误差为1.418度）。

Conclusion: 该模型预测控制（MPC）方法在真实的FCRA平台上得到了验证，在字母绘制轨迹跟踪测试中，其平均定位精度约为2.070毫米，平均跟踪误差为0.541度，优于PID方法（平均跟踪误差为1.418度）。

Abstract: Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant
motion. Still, the inherent properties of cables, such as resilience,
hysteresis, and friction, often lead to particular difficulties in modeling and
control. This paper proposes a model predictive control (MPC) method that
relies exclusively on input-output data, without a physical model, to improve
the control accuracy of FCRAs. First, we develop an implicit model based on
input-output data and integrate it into an MPC optimization framework. Second,
a data selection algorithm (DSA) is introduced to filter the data that best
characterize the system, thereby reducing the solution time per step to
approximately 4 ms, which is an improvement of nearly 80%. Lastly, the
influence of hyperparameters on tracking error is investigated through
simulation. The proposed method has been validated on a real FCRA platform,
including five-point positioning accuracy tests, a five-point response tracking
test, and trajectory tracking for letter drawing. The results demonstrate that
the average positioning accuracy is approximately 2.070 mm. Moreover, compared
to the PID method with an average tracking error of 1.418{\deg}, the proposed
method achieves an average tracking error of 0.541{\deg}.

</details>


### [296] [Strong, Accurate, and Low-Cost Robot Manipulator](https://arxiv.org/abs/2507.15693)
*Georges Chebly,Spencer Little,Nisal Perera,Aliya Abedeen,Ken Suzuki,Donghyun Kim*

Main category: cs.RO

TL;DR: Forte：一款可 3D 打印、性能接近工业级的六自由度机械臂，成本低于 215 美元，适用于教育和科研。


<details>
  <summary>Details</summary>
Motivation: 为了在材料成本低于 215 美元的情况下，实现接近工业级的性能（0.63 公斤的负载能力、0.467 米的臂展和亚毫米级的重复定位精度），并推动现有低成本教育型机械臂的性能限制。

Method: 提出了一种经济高效的机械设计，结合了绞盘式线缆驱动、同步带、简单的张紧机制和轻质 3D 打印结构，并通过拓扑优化来提高结构刚度。

Result: Forte手臂实现了高重复定位精度和负载能力。

Conclusion: Forte手臂在课堂教学和先进机器人研究方面都提供了引人注目的机器人平台。

Abstract: This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed
to achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,
and sub-millimeter repeatability - at a material cost under $215. As an
accessible robot for broad applications across classroom education to AI
experiments, Forte pushes forward the performance limitations of existing
low-cost educational arms. We introduce a cost-effective mechanical design that
combines capstan-based cable drives, timing belts, simple tensioning
mechanisms, and lightweight 3D-printed structures, along with topology
optimization for structural stiffness. Through careful drivetrain engineering,
we minimize backlash and maintain control fidelity without relying on
high-power electronics or expensive manufacturing processes. Experimental
validation demonstrates that Forte achieves high repeatability and load
capacity, offering a compelling robotic platform for both classroom instruction
and advanced robotics research.

</details>


### [297] [Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages](https://arxiv.org/abs/2507.15710)
*Lu Huang,Lingxiao Meng,Jiankun Wang,Xingjian Jing*

Main category: cs.RO

TL;DR: 一种新的基于采样的规划方法，通过多分辨率采样和自适应探索，提高了在复杂环境中的规划效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的方法在复杂高维配置空间（尤其是狭窄通道）中效率低下，并且手工制作或学习的启发式方法泛化性差且需要大量训练。

Method: 提出了一种简单的、基于采样的规划框架及其双向版本，通过以不同分辨率探测配置空间并优先探索稀疏样本来解决采样效率问题。

Result: 在SE(2)、SE(3)和R^14的复杂环境中，以及在Franka Emika Panda机器人的约束工作空间中，该方法均优于现有的基于采样的方法。

Conclusion: 该方法通过无缝集成不同粒度的规划，克服了现有采样效率低的问题，在各种复杂环境中均表现优于现有方法。

Abstract: Sampling-based algorithms are widely used for motion planning in
high-dimensional configuration spaces. However, due to low sampling efficiency,
their performance often diminishes in complex configuration spaces with narrow
corridors. Existing approaches address this issue using handcrafted or learned
heuristics to guide sampling toward useful regions. Unfortunately, these
strategies often lack generalizability to various problems or require extensive
prior training. In this paper, we propose a simple yet efficient sampling-based
planning framework along with its bidirectional version that overcomes these
issues by integrating different levels of planning granularity. Our approach
probes configuration spaces with uniform random samples at varying resolutions
and explores these multi-resolution samples online with a bias towards sparse
samples when traveling large free configuration spaces. By seamlessly
transitioning between sparse and dense samples, our approach can navigate
complex configuration spaces while maintaining planning speed and completeness.
The simulation results demonstrate that our approach outperforms several
state-of-the-art sampling-based planners in $\mathbb{SE}(2)$, $\mathbb{SE}(3)$,
and $\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments
conducted with the Franka Emika Panda robot operating in a constrained
workspace provide additional evidence of the superiority of the proposed
method.

</details>


### [298] [DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models](https://arxiv.org/abs/2507.15716)
*Ziyu Wan,Lin Zhao*

Main category: cs.RO

TL;DR: DiffPF是一种新颖的可微粒子滤波器，它利用扩散模型进行状态估计，通过学习灵活的后验采样器，优于现有方法，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的粒子滤波器需要重要性重采样，并且通常依赖于预定义或低容量的提议分布，而DiffPF旨在通过扩散模型克服这些限制，实现更优的状态估计。

Method: DiffPF学习通过在预测粒子和当前观测的条件下进行扩散模型，来生成灵活的后验采样器，从而实现准确、等权重的采样，以应对复杂、高维和多峰的滤波分布。

Result: DiffPF在模拟和真实世界的任务中，包括单峰和高度多峰分布，持续优于现有的滤波基线。在高度多峰的全局定位基准测试中，DiffPF的估计精度提高了82.8%；在真实的KITTI视觉里程计基准测试中，估计精度提高了26%。

Conclusion: DiffPF是首个将条件扩散模型集成到粒子滤波中的方法，能够实现高质量的后验采样，产生信息更丰富的粒子，并显著改善状态估计。

Abstract: This paper proposes DiffPF, a differentiable particle filter that leverages
diffusion models for state estimation in dynamic systems. Unlike conventional
differentiable particle filters, which require importance weighting and
typically rely on predefined or low-capacity proposal distributions. DiffPF
learns a flexible posterior sampler by conditioning a diffusion model on
predicted particles and the current observation. This enables accurate,
equally-weighted sampling from complex, high-dimensional, and multimodal
filtering distributions. We evaluate DiffPF across a range of scenarios,
including both unimodal and highly multimodal distributions, and test it on
simulated as well as real-world tasks, where it consistently outperforms
existing filtering baselines. In particular, DiffPF achieves an 82.8%
improvement in estimation accuracy on a highly multimodal global localization
benchmark, and a 26% improvement on the real-world KITTI visual odometry
benchmark, compared to state-of-the-art differentiable filters. To the best of
our knowledge, DiffPF is the first method to integrate conditional diffusion
models into particle filtering, enabling high-quality posterior sampling that
produces more informative particles and significantly improves state
estimation.

</details>


### [299] [Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction](https://arxiv.org/abs/2507.15729)
*Jens V. Rüppel,Andrey Rudenko,Tim Schreiter,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 提出了一种基于LLM的注视和语音交互接口，用于辅助机器人。与传统脚本化方法相比，该方法提高了适应性，但可能产生冗余输出。


<details>
  <summary>Details</summary>
Motivation: 为了实现灵活、通用知识驱动的辅助机器人人类交互（HRI）系统，以应对当前HRI系统在支持双向、多模态和上下文感知用户进行协作任务方面的挑战。

Method: 提出了一种基于注视和语音信息的人工助手机器人接口，该接口能够从多个视觉输入感知工作环境，并支持动态用户执行任务。该系统设计模块化，可迁移以适应不同任务和机器人，能够实时使用基于语言的交互状态表示和快速的板载感知模块。

Result: 在两项实验室研究中，将该系统与传统的脚本化HRI流程进行了性能和用户评分比较。LLM驱动的方法提高了适应性，并略微改善了用户参与度和任务执行指标，但可能会产生冗余输出，而脚本化流程更适合于更直接的任务。

Conclusion: LLM驱动的方法提高了适应性，并略微改善了用户参与度和任务执行指标，但可能会产生冗余输出，而脚本化流程更适合于更直接的任务。

Abstract: The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.

</details>


### [300] [Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs](https://arxiv.org/abs/2507.15782)
*Ruochu Yang,Yu Zhou,Fumin Zhang,Mengxue Hou*

Main category: cs.RO

TL;DR: 本研究提出了一种名为Inter-LLM的交错式大语言模型和运动规划算法，以解决家庭服务机器人在多对象收集任务中的长期规划挑战，并在模拟实验中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决家庭服务机器人长期以来在操纵开放集对象和高效准确地导航大型环境方面缺乏类似人类的智能的问题，具体研究了在大型场景图中进行广义多对象收集问题。

Method: 提出了一种新颖的交错式大语言模型（LLM）和运动规划算法Inter-LLM，并通过设计一个多模态动作代价相似性函数来优化规划，以平衡规划的质量和效率。

Result: 模拟实验结果表明，与最新研究相比，该算法在完成人类指令、最大化任务成功率和最小化任务成本方面的整体任务性能提高了30%。

Conclusion: 本研究提出的Inter-LLM算法在模拟实验中，通过结合大语言模型和运动规划，优化了多对象收集任务中的长期规划问题，相比现有方法，在完成人类指令、最大化任务成功率和最小化任务成本方面，整体任务性能提升了30%

Abstract: Household robots have been a longstanding research topic, but they still lack
human-like intelligence, particularly in manipulating open-set objects and
navigating large environments efficiently and accurately. To push this
boundary, we consider a generalized multi-object collection problem in large
scene graphs, where the robot needs to pick up and place multiple objects
across multiple locations in a long mission of multiple human commands. This
problem is extremely challenging since it requires long-horizon planning in a
vast action-state space under high uncertainties. To this end, we propose a
novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a
multimodal action cost similarity function, our algorithm can both reflect the
history and look into the future to optimize plans, striking a good balance of
quality and efficiency. Simulation experiments demonstrate that compared with
latest works, our algorithm improves the overall mission performance by 30% in
terms of fulfilling human commands, maximizing mission success rates, and
minimizing mission costs.

</details>


### [301] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: 本研究将人类的凝视行为引入机器人视觉系统，通过注视斑块标记方法显著降低了计算开销，并提高了机器人执行高精度任务的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人学习系统通常依赖于被动的、统一的原始图像处理，缺乏效率和性能的问题，本研究旨在探索如何通过引入类似人类的主动凝视来增强机器人的效率和性能。

Method: 本研究将人类的凝视行为引入机器人策略，并结合了两种凝视模仿和预测的方法：1. 预测凝视以指导注视和动作的两阶段模型；2. 将凝视整合到动作空间中，允许策略端到端地联合预测凝视和动作。

Result: 与均匀的斑块标记相比，所提出的注视斑块标记方法显著减少了 token 数量（从而减少了计算量），同时保持了关注区域的视觉保真度。实验证明，该方法不仅显著降低了计算开销，而且提高了高精度任务的性能以及对未见干扰因素的鲁棒性。

Conclusion: 研究结果表明，人类启发的视觉处理为机器人视觉系统提供了有用的归纳偏置，所提出的注视感知机器人视觉方法不仅显著降低了计算开销，而且提高了高精度任务的性能以及对未见干扰因素的鲁棒性。

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [302] [Discipline and Resistance: The Construction of a Digital Home for TikTok Refugees on Xiaohongshu](https://arxiv.org/abs/2507.14465)
*Xiaoyu Xiong,Yuting Peng,Summer Kwong,Anqi Huang*

Main category: cs.SI

TL;DR: 由于TikTok在美国可能被禁，其用户转向小红书。本研究运用福柯的“异托邦”理论，分析了小红书如何成为跨越“防火墙”进行跨文化讨论的平台。通过对用户评论的分析，研究揭示了中国和国际用户在数字迁移、身份构建和跨文化交际方面的策略差异，以及由此产生的新的在线秩序。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨TikTok被禁止前，其用户向小红书迁移的现象，并揭示小红书如何成为跨越“防火墙”进行跨文化讨论的危机空间。

Method: 本研究利用了福柯的“异托邦”理论，并对586条用户评论进行了批判性话语分析。

Result: 研究发现，中国和国际用户在话语策略上存在显著差异，这反映了文化抵抗与适应的动态。

Conclusion: 该研究揭示了在“防火墙”背景下，中国和国际用户如何通过语言协商、身份定位和平台管理来共同构建和争夺新的在线秩序，以及他们之间在话语策略上的差异，反映了文化上的抵抗与适应。

Abstract: This study examines how TikTok refugees moved to Xiaohongshu after TikTok was
about to be banned in the United States. It utilizes Foucault's idea of
heterotopia to demonstrate how Xiaohongshu became a crisis space for
cross-cultural discussions across the Great Firewall. Through Critical
Discourse Analysis of 586 user comments, the study reveals how Chinese and
international users collaboratively constructed and contested a new online
order through language negotiation, identity positioning, and playful platform
policing. The findings highlight distinct discursive strategies between
domestic and overseas users, reflecting both cultural resistance and
adaptation. This research contributes to the understanding of digital
migration, heterotopic spaces in social media, and emerging dynamics of
cross-cultural discourse during geopolitical crises.

</details>


### [303] [Rejection or Inclusion in the Emotion-Identity Dynamics of TikTok Refugees on RedNote](https://arxiv.org/abs/2507.14623)
*Mingchen Li,Wenbo Xu,Wenqing Gu,Yixuan Xie,Yao Zhou,Yunsong Dai,Cheng Tan,Pan Hui*

Main category: cs.SI

TL;DR: 本研究分析了中国用户和“TikTok难民”在RedNote上的互动，发现情感反应因主题和用户立场而异，政治讨论导致两极分化，而外貌相关内容则互动最为平衡。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在检验中国用户与自我认同为“TikTok难民”（因TikTok在美国被禁而迁移到RedNote的外国用户）之间的跨文化互动。

Method: 本研究基于1,862个帖子和403,054条评论的数据集，利用大型语言模型进行情感分类，并使用BERT进行主题建模，以探究两个群体如何参与TikTok难民现象。

Result: 研究结果显示，情感存在显著的不对称性：中国用户对不同主题和立场的反应，其情感强度各不相同；在文化主题中，自豪和赞扬占主导地位，而在政治讨论中，尤其是在亲华评论者中，则引发了高度的鄙视和愤怒。亲外国用户在所有主题中表现出最强烈负面情绪，而中立用户则表达好奇和喜悦，但仍强化了主流话语规范。跨主题比较显示，与外貌相关的内容产生的互动在情感上最为平衡，而政治则引发了最高程度的两极分化。

Conclusion: 本研究揭示了在中外在线互动中，情感与立场存在独特的结构，并为跨国数字公众中的身份协商提供了实证见解。

Abstract: This study examines cross-cultural interactions between Chinese users and
self-identified "TikTok Refugees"(foreign users who migrated to RedNote after
TikTok's U.S. ban). Based on a dataset of 1,862 posts and 403,054 comments, we
use large language model-based sentiment classification and BERT-based topic
modelling to explore how both groups engage with the TikTok refugee phenomenon.
We analyse what themes foreign users express, how Chinese users respond, how
stances (Pro-China, Neutral, Pro-Foreign) shape emotional expression, and how
affective responses differ across topics and identities. Results show strong
affective asymmetry: Chinese users respond with varying emotional intensities
across topics and stances: pride and praise dominate cultural threads, while
political discussions elicit high levels of contempt and anger, especially from
Pro-China commenters. Pro-Foreign users exhibit the strongest negative emotions
across all topics, whereas neutral users express curiosity and joy but still
reinforce mainstream discursive norms. Cross-topic comparisons reveal that
appearance-related content produces the most emotionally balanced interactions,
while politics generates the highest polarization. Our findings reveal distinct
emotion-stance structures in Sino-foreign online interactions and offer
empirical insights into identity negotiation in transnational digital publics.

</details>


### [304] [Forecasting Faculty Placement from Patterns in Co-authorship Networks](https://arxiv.org/abs/2507.14696)
*Samantha Dies,David Liu,Tina Eliassi-Rad*

Main category: cs.SI

TL;DR: 本研究表明，在学术招聘中，共著网络比传统的声望和出版记录等指标更能提高预测准确性，尤其是在顶尖大学中。这表明社会网络在招聘中起着重要作用，并可能有助于揭示学术界的偏见。


<details>
  <summary>Details</summary>
Motivation: 传统研究虽然发现了教职招聘与博士学科声望和出版记录等属性之间的相关性，但很少评估这些关联是否能推广到个体招聘结果，特别是对于原始样本之外的未来候选人。

Method: 本研究将教职安置视为一项个体级别的预测任务，利用包括博士学科声望和文献计量特征在内的传统属性的时间共著网络。

Result: 使用共著网络比仅使用传统指标可将预测准确性提高多达 10%，其中在顶尖（前 10 名）部门的安置中观察到的收益最大。

Conclusion: 本研究结果强调了社会网络、专业推荐和隐性倡导在教职招聘中的作用，这些因素超出了传统的学术生产力和机构声望的衡量标准。通过引入教职安置的预测框架并确立考虑合著网络的益处，这项工作为理解学术界的结构性偏见提供了一个新的视角，可能为旨在提高学术招聘实践的透明度、公平性和平等性的目标干预提供信息。

Abstract: Faculty hiring shapes the flow of ideas, resources, and opportunities in
academia, influencing not only individual career trajectories but also broader
patterns of institutional prestige and scientific progress. While traditional
studies have found strong correlations between faculty hiring and attributes
such as doctoral department prestige and publication record, they rarely assess
whether these associations generalize to individual hiring outcomes,
particularly for future candidates outside the original sample. Here, we
consider faculty placement as an individual-level prediction task. Our data
consist of temporal co-authorship networks with conventional attributes such as
doctoral department prestige and bibliometric features. We observe that using
the co-authorship network significantly improves predictive accuracy by up to
10% over traditional indicators alone, with the largest gains observed for
placements at the most elite (top-10) departments. Our results underscore the
role that social networks, professional endorsements, and implicit advocacy
play in faculty hiring beyond traditional measures of scholarly productivity
and institutional prestige. By introducing a predictive framing of faculty
placement and establishing the benefit of considering co-authorship networks,
this work provides a new lens for understanding structural biases in academia
that could inform targeted interventions aimed at increasing transparency,
fairness, and equity in academic hiring practices.

</details>


### [305] [Efficient Algorithms for Relevant Quantities of Friedkin-Johnsen Opinion Dynamics Model](https://arxiv.org/abs/2507.14864)
*Gengyu Wang,Runze Zhang,Zhongzhi Zhang*

Main category: cs.SI

TL;DR: 本研究提出了一种更快的算法来计算在线社交网络中的意见动态，该算法在大型网络上效果良好。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络深刻影响着人们的观点形成和交流方式，而 Friedkin-Johnsen 模型是对此进行建模的基础框架。本研究旨在解决在这些网络中有效计算均衡意见向量及其相关指标（如极化和分歧）的计算任务。

Method: 提出了一种具有相对误差保证的确定性局部算法，并结合了连续超松弛技术来优化收敛速度。

Result: 实验结果表明，该方法在计算效率和可扩展性方面显著优于传统方法，可扩展至千万节点规模的网络。

Conclusion: 该研究提出了一种确定性局部算法，并结合了连续超松弛技术，以在有向和无向社交网络中有效计算均衡意见向量及其相关指标（如极化和分歧）。

Abstract: Online social networks have become an integral part of modern society,
profoundly influencing how individuals form and exchange opinions across
diverse domains ranging from politics to public health. The Friedkin-Johnsen
model serves as a foundational framework for modeling opinion formation
dynamics in such networks. In this paper, we address the computational task of
efficiently determining the equilibrium opinion vector and associated metrics
including polarization and disagreement, applicable to both directed and
undirected social networks. We propose a deterministic local algorithm with
relative error guarantees, scaling to networks exceeding ten million nodes.
Further acceleration is achieved through integration with successive
over-relaxation techniques, where a relaxation factor optimizes convergence
rates. Extensive experiments on diverse real-world networks validate the
practical effectiveness of our approaches, demonstrating significant
improvements in computational efficiency and scalability compared to
conventional methods.

</details>


### [306] [Comprehensive Privacy Risk Assessment in Social Networks Using User Attributes Social Graphs and Text Analysis](https://arxiv.org/abs/2507.15124)
*Md Jahangir Alam,Ismail Hossain,Sai Puppala,Sajedul Talukder*

Main category: cs.SI

TL;DR: 该研究提出CPRS框架，通过整合用户属性、社交图和内容来量化社交网络隐私风险，并在真实数据集和用户研究中验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 社交网络平台用户分享敏感信息导致隐私威胁增加，需要一种量化隐私风险的方法。

Method: CPRS框架整合用户属性、社交图结构和用户生成内容，通过敏感性、可见性、结构相似性和实体级别分析计算风险得分，并将其汇总为统一得分。

Result: CPRS框架在两个真实数据集上进行了验证，平均CPRS得分为0.478（等权重）或0.501（图敏感）。其中，基于图的风险（平均0.52）高于内容（0.48）和属性（0.45）。用户研究表明85%的参与者认为仪表板清晰且易于操作。

Conclusion: 该研究提出了一种名为CPRS（Comprehensive Privacy Risk Scoring）的框架，用于量化社交网络平台上的隐私风险。该框架整合了用户属性、社交图结构和用户生成内容，并通过敏感性、可见性、结构相似性和实体级别分析来计算各维度风险得分，最终汇总为统一的风险得分。实验结果表明，CPRS在真实数据集上表现良好，并且用户研究也证实了其实用性。未来工作将包括考虑时间动态和多模态内容。

Abstract: The rise of social networking platforms has amplified privacy threats as
users increasingly share sensitive information across profiles, content, and
social connections. We present a Comprehensive Privacy Risk Scoring (CPRS)
framework that quantifies privacy risk by integrating user attributes, social
graph structures, and user-generated content. Our framework computes risk
scores across these dimensions using sensitivity, visibility, structural
similarity, and entity-level analysis, then aggregates them into a unified risk
score. We validate CPRS on two real-world datasets: the SNAP Facebook Ego
Network (4,039 users) and the Koo microblogging dataset (1M posts, 1M
comments). The average CPRS is 0.478 with equal weighting, rising to 0.501 in
graph-sensitive scenarios. Component-wise, graph-based risks (mean 0.52)
surpass content (0.48) and profile attributes (0.45). High-risk attributes
include email, date of birth, and mobile number. Our user study with 100
participants shows 85% rated the dashboard as clear and actionable, confirming
CPRS's practical utility. This work enables personalized privacy risk insights
and contributes a holistic, scalable methodology for privacy management. Future
directions include incorporating temporal dynamics and multimodal content for
broader applicability.

</details>


### [307] [Privacy-Preserving Multimodal News Recommendation through Federated Learning](https://arxiv.org/abs/2507.15460)
*Mehdi Khalaj,Shahrzad Golestani Najafabadi,Julita Vassileva*

Main category: cs.SI

TL;DR: 本研究提出了一种结合多模态特征、考虑用户短期兴趣并采用联邦学习和安全聚合技术的新闻推荐方法，解决了传统方法的局限性，并在隐私保护和推荐准确性方面取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 传统新闻推荐系统过度依赖文本内容、忽视用户短期兴趣并存在隐私问题。本研究旨在解决这些挑战。

Method: 提出了一种新颖的多模态联邦学习方法，集成了新闻的文本和视觉特征，并使用时态感知模型（多头自注意力网络）来平衡用户长期和短期兴趣。通过联邦学习框架，将推荐模型分为服务器维护的新闻模型和轻量级的用户模型，并在客户端本地计算梯度，然后将梯度发送到服务器进行聚合更新，以保护用户隐私。此外，还采用了基于Shamir秘密共享的安全聚合算法。

Result: 实验结果表明，该方法在真实新闻数据集上取得了比现有系统更优越的性能。

Conclusion: 该研究提出的多模态联邦学习方法在新闻推荐方面表现出色，与现有系统相比具有显著优势，代表了隐私保护个性化新闻推荐领域的重大进步。

Abstract: Personalized News Recommendation systems (PNR) have emerged as a solution to
information overload by predicting and suggesting news items tailored to
individual user interests. However, traditional PNR systems face several
challenges, including an overreliance on textual content, common neglect of
short-term user interests, and significant privacy concerns due to centralized
data storage. This paper addresses these issues by introducing a novel
multimodal federated learning-based approach for news recommendation. First, it
integrates both textual and visual features of news items using a multimodal
model, enabling a more comprehensive representation of content. Second, it
employs a time-aware model that balances users' long-term and short-term
interests through multi-head self-attention networks, improving recommendation
accuracy. Finally, to enhance privacy, a federated learning framework is
implemented, enabling collaborative model training without sharing user data.
The framework divides the recommendation model into a large server-maintained
news model and a lightweight user model shared between the server and clients.
The client requests news representations (vectors) and a user model from the
central server, then computes gradients with user local data, and finally sends
their locally computed gradients to the server for aggregation. The central
server aggregates gradients to update the global user model and news model. The
updated news model is further used to infer news representation by the server.
To further safeguard user privacy, a secure aggregation algorithm based on
Shamir's secret sharing is employed. Experiments on a real-world news dataset
demonstrate strong performance compared to existing systems, representing a
significant advancement in privacy-preserving personalized news recommendation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [308] [DM-RSA: An Extension of RSA with Dual Modulus](https://arxiv.org/abs/2507.14197)
*Andriamifidisoa Ramamonjy,Rufine Marius Lalasoa*

Main category: cs.CR

TL;DR: DM-RSA是一种RSA变体，使用两个模数和CRT来提高安全性并抵抗侧信道攻击。


<details>
  <summary>Details</summary>
Motivation: 介绍DM-RSA（双模RSA），一种RSA加密系统的变体。

Method: DM-RSA（双模RSA）是一种RSA加密系统的变体，它采用两个不同的模数进行对称操作以增强安全性。

Result: DM-RSA提高了对侧信道攻击的鲁棒性，同时保持了经典RSA的效率，并提高了对模数部分泄露的抵抗力，易于集成到现有基础设施中。

Conclusion: DM-RSA通过利用中国剩余定理（CRT）进行解密，提高了对侧信道攻击的鲁棒性，同时保持了经典RSA的效率。该方法提高了对模数部分泄露的抵抗力，并且易于集成到现有基础设施中。

Abstract: We introduce DM-RSA (Dual Modulus RSA), a variant of the RSA cryptosystem
that employs two distinct moduli symmetrically to enhance security. By
leveraging the Chinese Remainder Theorem (CRT) for decryption, DM-RSA provides
increased robustness against side-channel attacks while preserving the
efficiency of classical RSA. This approach improves resistance to partial
compromise of a modulus and integrates easily into existing infrastructures.

</details>


### [309] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: 提出了ExCyTIn-Bench，一个用于评估LLM网络威胁调查能力的基准。该基准包含模拟攻击、安全日志和生成的问题，并使用威胁调查图来提供可解释的答案和可扩展的评估流程。实验证明该任务对现有LLM代理来说很有挑战性。


<details>
  <summary>Details</summary>
Motivation: 为了协助LLM代理在网络威胁调查任务中的开发和评估，鉴于现实世界中的安全分析师需要处理大量异构警报信号和安全日志，并遵循多跳证据链来编写事件报告。LLM技术的发展为自动执行此任务提供了有前景的方向。

Method: 构建了一个包含8个模拟真实世界多步骤攻击、57个来自Microsoft Sentinel及相关服务的日志表以及589个自动生成问题的ExCyTIn-Bench数据集。通过专家设计的检测逻辑提取安全日志，构建威胁调查图，并利用LLM基于图中的节点对生成问题，其中起始节点作为背景，结束节点作为答案。

Result: 在ExCyTIn-Bench基准上进行的综合实验表明，该任务的难度较高。在基础设置下，所有评估模型的平均奖励为0.249，最佳模型也仅达到0.368，表明该领域有很大的改进潜力。

Conclusion: ExCyTIn-Bench是第一个用于评估LLM代理在网络威胁调查任务中的基准，该任务通过源自调查图的安全问题来驱动。实验表明，该任务对现有模型来说具有挑战性，平均奖励为0.249，最佳奖励为0.368，为未来的研究留下了巨大的空间。

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [310] [PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training](https://arxiv.org/abs/2507.14202)
*Pengfei Du*

Main category: cs.CR

TL;DR: 一种新的无 PRM LLM 安全对齐框架，通过红队测试和对抗性训练提高安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前基于 PRM 的安全对齐方法带来的计算开销和可扩展性限制，用于 LLM 的安全对齐。

Method: 提出了一种新颖的无 PRM 安全对齐框架，该框架利用自动化红队测试和对抗性训练。通过遗传算法优化、多智能体模拟和高级提示突变技术来识别漏洞。通过具有课程学习和自适应正则化机制的定向对抗性训练来增强模型鲁棒性。

Result: 在五个最先进的 LLM 上进行了全面评估，证明了该方法在安全对齐性能方面优于基于 PRM 的方法，同时将计算成本降低了 61%。

Conclusion: 该框架通过自动化红队测试和对抗性训练实现了强大的安全保证，同时保持了计算效率，并且优于基于 PRM 的方法，同时将计算成本降低了 61%。它还包括透明的报告和持续的审计机制，可实现迭代安全改进和合规性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse applications, yet they pose significant security risks that threaten
their safe deployment in critical domains. Current security alignment
methodologies predominantly rely on Process Reward Models (PRMs) to evaluate
intermediate reasoning steps, introducing substantial computational overhead
and scalability constraints. This paper presents a novel PRM-free security
alignment framework that leverages automated red teaming and adversarial
training to achieve robust security guarantees while maintaining computational
efficiency. Our approach systematically identifies vulnerabilities through
sophisticated attack strategies including genetic algorithm optimization,
multi-agent simulation, and advanced prompt mutation techniques. The framework
enhances model robustness via targeted adversarial training with curriculum
learning and adaptive regularization mechanisms. Comprehensive experimental
evaluation across five state-of-the-art LLMs demonstrates that our method
achieves superior security alignment performance compared to PRM-based
approaches while reducing computational costs by 61\%. The framework
incorporates transparent reporting and continuous audit mechanisms that enable
iterative security improvement and regulatory compliance. Our contributions
advance the field of efficient LLM security alignment by democratizing access
to robust security measures for resource-constrained organizations and
providing a scalable foundation for addressing evolving adversarial threats.

</details>


### [311] [Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design](https://arxiv.org/abs/2507.14207)
*Richard M. Charles,James H. Curry,Richard B. Charles*

Main category: cs.CR

TL;DR: 本研究旨在揭示大型语言模型（LLM）在K-12教育中可能面临的安全风险，特别是学生可能利用“特洛伊木马”提示来诱导模型产生不安全输出。研究人员通过实验发现了GPT-3.5和GPT-4的漏洞，并开发了一个名为TrojanPromptGuard（TPG）的工具来检测和防御此类攻击，以促进LLM在教育领域的安全应用。


<details>
  <summary>Details</summary>
Motivation: 为了探讨LLM在K-12教育中可能带来的安全风险，特别是学生如何通过“特洛伊木马”攻击来诱导LLM产生不安全或非预期的输出，从而绕过现有的内容审核系统。

Method: 通过系统性实验，模拟K-12查询和多轮对话，暴露了GPT-3.5和GPT-4在安全防护方面的关键漏洞。

Result: 本研究揭示了GPT-3.5和GPT-4在面对被特洛伊木马化的提示时的脆弱性，并开发了一个名为TrojanPromptGuard (TPG) 的原型工具，能够自动检测和缓解此类攻击。

Conclusion: LLM在K-12教育中的整合带来了机遇和风险。本研究通过系统性实验揭示了GPT-3.5和GPT-4在易受攻击性方面存在的问题，并提出了一种名为TrojanPromptGuard (TPG) 的原型工具，用于自动检测和缓解被攻击的教育提示。

Abstract: The integration of Large Language Models (LLMs) in K--12 education offers
both transformative opportunities and emerging risks. This study explores how
students may Trojanize prompts to elicit unsafe or unintended outputs from
LLMs, bypassing established content moderation systems with safety guardrils.
Through a systematic experiment involving simulated K--12 queries and
multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This
paper presents our experimental design, detailed findings, and a prototype
tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized
educational prompts. These insights aim to inform both AI safety researchers
and educational technologists on the safe deployment of LLMs for educators.

</details>


### [312] [Secure Goal-Oriented Communication: Defending against Eavesdropping Timing Attacks](https://arxiv.org/abs/2507.14212)
*Federico Mason,Federico Chiariotti,Pietro Talli,Andrea Zanella*

Main category: cs.CR

TL;DR: GoC 调度虽然高效，但存在时间侧信道泄露系统状态的风险。本文提出了防御方法，在保持 GoC 优势的同时，降低了信息泄露的风险。


<details>
  <summary>Details</summary>
Motivation: 面向目标的通信（GoC）虽然可以显著减少传输频率，但会产生一个基于时间的侧信道，可能被窃听者利用来获取系统状态信息，这种攻击方式甚至可以绕过信息论安全。

Method: 本文研究了针对基于拉取的、面向目标的调度（GoC）的窃听攻击，该调度用于马尔可夫过程的远程监控和控制。我们提出了一个理论框架来定义攻击的有效性，并提出了可能的对策，包括两种实用的启发式方法，可以在 GoC 带来的性能增益和泄露信息量之间取得平衡。

Result: 所提出的启发式防御措施可以将窃听者正确猜测系统状态的概率从约 60% 降低一半，同时对 GoC 的性能优势造成边际上的减损。

Conclusion: 提出的基于启发式防御措施的 GoC 调度方法可以有效将窃听者推断系统状态的准确性降低一半，同时对 GoC 带来的性能增益影响很小。

Abstract: Goal-oriented Communication (GoC) is a new paradigm that plans data
transmission to occur only when it is instrumental for the receiver to achieve
a certain goal. This leads to the advantage of reducing the frequency of
transmissions significantly while maintaining adherence to the receiver's
objectives. However, GoC scheduling also opens a timing-based side channel that
an eavesdropper can exploit to obtain information about the state of the
system. This type of attack sidesteps even information-theoretic security, as
it exploits the timing of updates rather than their content. In this work, we
study such an eavesdropping attack against pull-based goal-oriented scheduling
for remote monitoring and control of Markov processes. We provide a theoretical
framework for defining the effectiveness of the attack and propose possible
countermeasures, including two practical heuristics that provide a balance
between the performance gains offered by GoC and the amount of leaked
information. Our results show that, while a naive goal-oriented scheduler
allows the eavesdropper to correctly guess the system state about 60% of the
time, our heuristic defenses can halve the leakage with a marginal reduction of
the benefits of goal-oriented approaches.

</details>


### [313] [Magneto-Ionic Hardware Security Primitives: Embedding Data Protection at the Material Level](https://arxiv.org/abs/2507.14213)
*Irena Spasojevic,Federica Celegato,Alessandro Magni,Paola Tiberto,Jordi Sort*

Main category: cs.CR

TL;DR: 一种基于磁离子学的新型硬件安全方法，利用电压控制离子迁移在FeCoN材料中产生独特的磁性状态，可用于创建更安全的硬件。


<details>
  <summary>Details</summary>
Motivation: 为了应对日益复杂的网络威胁，需要开发资源消耗低且不易受攻击的硬件安全技术，以保护敏感信息。

Method: 通过电压控制N3-离子在预先定义的FeCoN点阵中迁移，生成具有可调谐厚度的铁磁亚层，实现确定性（单畴或涡旋）或概率性（具有共存磁构型和可调电压概率）磁态。

Result: 实现了利用磁性指纹识别技术，通过电压控制离子迁移生成可重构的、具有抗篡改、低功耗和可扩展性的安全硬件。

Conclusion: 该研究提出了一种基于磁离子学策略的硬件安全新方法，利用电压控制的离子迁移在FeCoN点阵中生成可调谐的铁磁亚层，实现了确定性或概率性的磁态，可用于构建具有抗篡改、低功耗和可扩展性的安全硬件，如真随机数生成器、物理不可克隆函数和内存内概率推理。

Abstract: The Big Data revolution has heightened the demand for robust,
energy-efficient security hardware capable of withstanding increasingly
sophisticated cyber threats. Conventional encryption schemes, reliant on
complex algorithms, are resource-intensive and remain vulnerable. To fortify
sensitive information, society needs innovative anti-hacking and
anti-counterfeiting technologies that exploit new materials and designs. Here,
we present a magneto-ionic strategy for hardware-level security based on fully
selective voltage-controlled N3- ion migration within pre-defined, initially
paramagnetic FeCoN dots. This process generates ferromagnetic sublayers of
tuneable thickness, resulting in either deterministic (single-domain or vortex)
or probabilistic states (with coexisting magnetic configurations and
voltage-adjustable probabilities), each exhibiting stochastic orientation and
chirality, thereby providing a rich platform for magnetic fingerprinting. This
approach enables self-protected primitives, including true random number
generators, physical unclonable functions, and in-memory probabilistic
inference. The resulting reconfigurable architecture combines tamper
resistance, low energy consumption, and scalability, marking a significant leap
toward next-generation hardware security rooted in emergent magnetic phenomena.

</details>


### [314] [GPU-Accelerated Interpretable Generalization for Rapid Cyberattack Detection and Forensics](https://arxiv.org/abs/2507.14222)
*Shu-Ting Huang,Wen-Cheng Chung,Hao-Ting Pai*

Main category: cs.CR

TL;DR: IG-GPU利用GPU加速了可解释性泛化（IG）机制，显著提高了入侵检测的性能和可扩展性，解决了CPU的性能和内存瓶颈。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有可解释性泛化（IG）机制在处理大规模数据集时由于立方时间复杂度和大型中间位集而导致的CPU性能低下和内存限制问题，提出IG-GPU以实现高效的入侵检测。

Method: 提出了一种名为IG-GPU的PyTorch重新架构，将IG机制中的成对交叉和子集评估任务转移到GPU上执行，以克服CPU实现的性能瓶颈。

Result: 在NSL-KDD数据集上，IG-GPU相比IG实现了116倍的加速，在处理完整数据集时，IG-GPU仅需18分钟即可达到0.957的召回率、0.973的精确率和0.961的AUC，而IG在相同条件下因内存不足需要降采样，性能也较低。

Conclusion: IG-GPU通过将计算密集型任务卸载到GPU，显著提高了可解释性泛化（IG）机制在处理大规模数据集时的性能，实现了116倍的加速，并保持了高精度和召回率，同时解决了CPU实现的内存限制问题，为实时网络防御提供了可行的解决方案。

Abstract: The Interpretable Generalization (IG) mechanism recently published in IEEE
Transactions on Information Forensics and Security delivers state-of-the-art,
evidence-based intrusion detection by discovering coherent normal and attack
patterns through exhaustive intersect-and-subset operations-yet its cubic-time
complexity and large intermediate bitsets render full-scale datasets
impractical on CPUs. We present IG-GPU, a PyTorch re-architecture that offloads
all pairwise intersections and subset evaluations to commodity GPUs.
Implemented on a single NVIDIA RTX 4070 Ti, in the 15k-record NSL-KDD dataset,
IG-GPU shows a 116-fold speed-up over the multi-core CPU implementation of IG.
In the full size of NSL-KDD (148k-record), given small training data (e.g.,
10%-90% train-test split), IG-GPU runs in 18 minutes with Recall 0.957,
Precision 0.973, and AUC 0.961, whereas IG required down-sampling to
15k-records to avoid memory exhaustion and obtained Recall 0.935, Precision
0.942, and AUC 0.940. The results confirm that IG-GPU is robust across scales
and could provide millisecond-level per-flow inference once patterns are
learned. IG-GPU thus bridges the gap between rigorous interpretability and
real-time cyber-defense, offering a portable foundation for future work on
hardware-aware scheduling, multi-GPU sharding, and dataset-specific sparsity
optimizations.

</details>


### [315] [Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification](https://arxiv.org/abs/2507.14223)
*Wen-Cheng Chung,Shu-Ting Huang,Hao-Ting Pai*

Main category: cs.CR

TL;DR: IG和IG-MD是用于入侵检测的可解释AI方法。IG通过学习流量模式生成规则，IG-MD通过多粒度离散化进一步提高精度。两者在多个数据集上都表现出色，IG-MD在提高精度的同时保持了高召回率。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释人工智能（XAI）方法在入侵检测系统（IDS）中通常将近似解释器附加到不透明的分类器上，导致分析师获得的部分见解可能不完整甚至具有误导性。因此，需要一种能够提供完全可审计规则并保持高精度的可解释IDS。

Method: 本研究提出了一种名为可解释性泛化（IG）的机制，该机制通过学习良性或恶意流量的独特模式（特征组合）来生成完全可审计的规则。在此基础上，进一步引入了多粒度离散化（IG-MD）技术，该技术在多个高斯分布分辨率上表示连续特征，以提高模型的精度。

Result: IG在NSL-KDD、UNSW-NB15和UKM-IDS20数据集上实现了出色的精确率、召回率和AUC，即使仅使用10%的数据进行训练。IG-MD在UKM-IDS20数据集上，在所有九个训练-测试划分中，将精确率提高了至少4个百分点，同时将召回率保持在接近1.0的水平。

Conclusion: 可解释性泛化（IG）机制通过学习独特的良性或恶意流量模式并将其转化为完全可审计的规则，解决了现有XAI流水线只在不透明分类器上附加近似解释器的问题。多粒度离散化（IG-MD）通过在多个基于高斯分布的粒度上表示连续特征，进一步提高了精度，同时保持了接近1.0的召回率，表明单一的可解释模型无需进行定制调整即可跨领域扩展。

Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential
for mission-critical networks, yet most "XAI" pipelines still bolt an
approximate explainer onto an opaque classifier, leaving analysts with partial
and sometimes misleading insights. The Interpretable Generalization (IG)
mechanism, published in IEEE Transactions on Information Forensics and
Security, eliminates that bottleneck by learning coherent patterns - feature
combinations unique to benign or malicious traffic - and turning them into
fully auditable rules. IG already delivers outstanding precision, recall, and
AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the
data. To raise precision further without sacrificing transparency, we introduce
Multi-Granular Discretization (IG-MD), which represents every continuous
feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts
precision by greater than or equal to 4 percentage points across all nine
train-test splits while preserving recall approximately equal to 1.0,
demonstrating that a single interpretation-ready model can scale across domains
without bespoke tuning.

</details>


### [316] [Using Modular Arithmetic Optimized Neural Networks To Crack Affine Cryptographic Schemes Efficiently](https://arxiv.org/abs/2507.14229)
*Vanja Stojanović,Žiga Lesar,CIril Bohak*

Main category: cs.CR

TL;DR: 该研究提出了一种混合神经网络方法来分析仿射密码，在短文本上表现良好，但在长文本上存在泛化问题。


<details>
  <summary>Details</summary>
Motivation: 受最近在模运算可解释神经网络和经典密码神经密码分析方面进展的启发。

Method: 本研究采用了一种结合了模运算感知和基于统计特征学习的混合神经网络架构来研究仿射密码的密码分析。该方法整合了一个处理原始密文序列的模运算分支和一个利用字母频率特征的统计分支。

Result: 实验结果表明，该混合模型在短文本和中等长度密文中实现了高密钥恢复精度，在仿射密码方面优于纯粹的统计方法。

Conclusion: 所提出的混合模型在短文本和中等长度密文上表现出高密钥恢复精度，优于纯统计方法，但在非常长的密文上性能会下降，这表明了模型泛化方面存在挑战。

Abstract: We investigate the cryptanalysis of affine ciphers using a hybrid neural
network architecture that combines modular arithmetic-aware and statistical
feature-based learning. Inspired by recent advances in interpretable neural
networks for modular arithmetic and neural cryptanalysis of classical ciphers,
our approach integrates a modular branch that processes raw ciphertext
sequences and a statistical branch that leverages letter frequency features.
Experiments on datasets derived from natural English text demonstrate that the
hybrid model attains high key recovery accuracy for short and moderate
ciphertexts, outperforming purely statistical approaches for the affine cipher.
However, performance degrades for very long ciphertexts, highlighting
challenges in model generalization.

</details>


### [317] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: ViT模型（即使结合了解释模型）也容易受到AdViT攻击，该攻击能同时欺骗模型和解释器，且难以检测。


<details>
  <summary>Details</summary>
Motivation: 现有的针对ViT模型的攻击主要关注生成最小的对抗性扰动，而忽略了这些扰动对模型解释性的影响。然而，解释模型可以有效辅助检测对抗样本。本研究旨在揭示ViT模型（即使与解释模型结合）也存在被对抗攻击欺骗的漏洞。

Method: 提出了一种名为AdViT的攻击方法，该方法旨在生成能够同时误导ViT模型及其解释模型的对抗样本。通过在多种ViT模型和两种基于Transformer的解释器上进行广泛实验，验证了AdViT在白盒和黑盒场景下的有效性。

Result: AdViT在白盒和黑盒场景下均实现了100%的攻击成功率。在白盒场景下，误分类置信度最高可达98%；在黑盒场景下，误分类置信度最高可达76%。重要的是，AdViT能够生成保持解释模型准确性的对抗样本，增加了检测难度。

Conclusion: 该研究提出了AdViT攻击，能够同时欺骗Vision Transformer（ViT）模型及其解释模型，并在白盒和黑盒场景下均实现了100%的攻击成功率。AdViT生成的对抗样本具有高误分类置信度（白盒最高98%，黑盒最高76%），并且能够保持解释模型生成准确的解释，从而增加了对抗样本的隐蔽性，使其更难被检测。

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


### [318] [Quantum-Safe Identity Verification using Relativistic Zero-Knowledge Proof Systems](https://arxiv.org/abs/2507.14324)
*Yao Ma,Wen Yu Kon,Jefferson Chu,Kevin Han Yong Loh,Kaushik Chakraborty,Charles Lim*

Main category: cs.CR

TL;DR: 本研究改进了基于图着色的相对论性零知识证明，降低了技术门槛，增强了稳定性和可扩展性，并扩展到更复杂的场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于密码/PIN的身份验证易受网络钓鱼和信息窃取攻击。Alikhani等人[Nature, 2021]提出了基于图着色的相对论性零知识证明（RZKPs），但仍存在工程和安全上的挑战。

Method: 通过放宽相对论约束（从60m到30m），增强了实验演示的稳定性和可扩展性。提出了改进协议以对抗纠缠恶意证明者，并建立了其可靠性参数的上限。将证明者数量从两个扩展到三个，并证明了其安全性。

Result: 成功降低了相对论约束，提高了实验演示的稳定性和可扩展性。提出的改进协议在计算和通信成本相当的情况下，提高了对纠缠恶意证明者的安全性，并建立了可靠性参数上限。将协议扩展到三方证明场景，并验证了其安全性。

Conclusion: 本研究提出了改进的基于图着色的相对论性零知识证明协议，解决了现有方案的工程和安全限制，并将其扩展到三方证明场景。

Abstract: Identity verification is the process of confirming an individual's claimed
identity, which is essential in sectors like finance, healthcare, and online
services to ensure security and prevent fraud. However, current
password/PIN-based identity solutions are susceptible to phishing or skimming
attacks, where malicious intermediaries attempt to steal credentials using fake
identification portals. Alikhani et al. [Nature, 2021] began exploring identity
verification through graph coloring-based relativistic zero-knowledge proofs
(RZKPs), a key cryptographic primitive that enables a prover to demonstrate
knowledge of secret credentials to a verifier without disclosing any
information about the secret. Our work advances this field and addresses
unresolved issues: From an engineering perspective, we relax further the
relativistic constraints from 60m to 30m, and significantly enhance the
stability and scalability of the experimental demonstration of the 2-prover
graph coloring-based RZKP protocol for near-term use cases. At the same time,
for long-term security against entangled malicious provers, we propose a
modified protocol with comparable computation and communication costs, we
establish an upper bound on the soundness parameter for this modified protocol.
On the other hand, we extend the two-prover, two-verifier setup to a
three-prover configuration, demonstrating the security of such relativistic
protocols against entangled malicious provers.

</details>


### [319] [Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives](https://arxiv.org/abs/2507.14519)
*Wenxuan Zeng,Tianshi Xu,Yi Chen,Yifan Zhou,Mingzhe Zhang,Jin Tan,Cheng Hong,Meng Li*

Main category: cs.CR

TL;DR: PPML 的效率低下，需要跨协议、模型和系统进行优化。


<details>
  <summary>Details</summary>
Motivation: PPML 在保护用户数据隐私方面具有潜力，但存在显著的效率和可扩展性成本，需要对其进行优化。

Method: 对 PPML 研究进行跨级别（协议、模型、系统）的全面、系统性回顾，并进行定性和定量比较。

Result: 对 PPML 的现有工作进行了分类、回顾和比较，讨论了未来的研究方向，并强调了跨级别优化的重要性。

Conclusion: PPML 需要跨协议、模型和系统级别进行优化，以弥合效率差距并实现突破。

Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols
has emerged as a promising paradigm to protect user data privacy in cloud-based
machine learning services. While it achieves formal privacy protection, PPML
often incurs significant efficiency and scalability costs due to orders of
magnitude overhead compared to the plaintext counterpart. Therefore, there has
been a considerable focus on mitigating the efficiency gap for PPML. In this
survey, we provide a comprehensive and systematic review of recent PPML studies
with a focus on cross-level optimizations. Specifically, we categorize existing
papers into protocol level, model level, and system level, and review progress
at each level. We also provide qualitative and quantitative comparisons of
existing works with technical insights, based on which we discuss future
research directions and highlight the necessity of integrating optimizations
across protocol, model, and system levels. We hope this survey can provide an
overarching understanding of existing approaches and potentially inspire future
breakthroughs in the PPML field. As the field is evolving fast, we also provide
a public GitHub repository to continuously track the developments, which is
available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.

</details>


### [320] [FORTA: Byzantine-Resilient FL Aggregation via DFT-Guided Krum](https://arxiv.org/abs/2507.14588)
*Usayd Shahul,J. Harshan*

Main category: cs.CR

TL;DR: FORTA是一种在实数域上运行的安全聚合框架，通过DFT编码和改进的Krum算法来防御拜占庭攻击，解决了现有方法的数值问题，并提高了聚合的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于有限域算术的拜占庭鲁棒安全聚合方案在应用于实值模型更新时，可能面临数值误差和溢出的问题。因此，有必要开发一种可以直接在实数域上操作的安全聚合方法。

Method: FORTA框架利用离散傅里叶变换（DFT）码进行隐私保护，并采用基于Krum的异常值检测进行鲁棒性处理。为了解决有限精度下DFT解码器产生的数值扰动问题，FORTA通过引入DFT解码器的反馈来优化Krum算法，从而改进了可信更新的选择过程。

Result: FORTA框架在理论分析和实验中均表现出比标准Krum算法更优越的鲁棒性和聚合准确性。通过DFT解码器的反馈改进Krum算法，能够更有效地识别和剔除恶意更新，从而提高聚合结果的准确性。

Conclusion: FORTA通过结合离散傅里叶变换（DFT）编码和改进的Krum算法，实现了在实数域上的拜占庭鲁棒安全聚合。理论分析和实验表明，与标准的Krum相比，FORTA的改进版本提供了更强的鲁棒性和更准确的聚合效果，解决了现有方法在实值更新中可能出现的数值误差和溢出问题。

Abstract: Secure federated learning enables collaborative model training across
decentralized users while preserving data privacy. A key component is secure
aggregation, which keeps individual updates hidden from both the server and
users, while also defending against Byzantine users who corrupt the
aggregation. To this end, Jinhyun So et al. recently developed a
Byzantine-resilient secure aggregation scheme using a secret-sharing strategy
over finite-field arithmetic. However, such an approach can suffer from
numerical errors and overflows when applied to real-valued model updates,
motivating the need for secure aggregation methods that operate directly over
the real domain. We propose FORTA, a Byzantine-resilient secure aggregation
framework that operates entirely in the real domain. FORTA leverages Discrete
Fourier Transform (DFT) codes for privacy and employs Krum-based outlier
detection for robustness. While DFT decoder is error-free under infinite
precision, finite precision introduces numerical perturbations that can distort
distance estimates and allow malicious updates to evade detection. To address
this, FORTA refines Krum using feedback from DFT decoder, improving the
selection of trustworthy updates. Theoretical analysis and experiments show
that our modification of Krum offers improved robustness and more accurate
aggregation than standard Krum.

</details>


### [321] [Hybrid Classical-Quantum Rainbow Table Attack on Human Passwords](https://arxiv.org/abs/2507.14600)
*MA. Khajeian*

Main category: cs.CR

TL;DR: 提出了一种结合彩虹表和量子搜索的混合密码攻击方法，以更有效地破解人类生成的长密码。


<details>
  <summary>Details</summary>
Motivation: 针对长且由人类生成的密码，它们具有不规则的结构和巨大的搜索空间，对经典和量子攻击都构成了挑战。

Method: 利用基于字典的密码生成和变换规则构建彩虹表，并通过分桶来组织它们。然后，使用 Grover 算法的分布式精确变体在每个桶内执行量子搜索。

Result: 所提出的混合攻击方法通过结合结构化彩虹表和高效的量子搜索，实现了更浅、更鲁棒的量子电路，能够有效应对近期的量子设备中的噪声。

Conclusion: 该混合框架结合了结构化的彩虹表和高效的量子搜索，以增强密码恢复能力。

Abstract: Passwords that are long and human-generated pose a challenge for both
classical and quantum attacks due to their irregular structure and large search
space. In this work, we present an enhanced classical-quantum hybrid attack
tailored to this scenario. We build rainbow tables using dictionary-based
password generation with transformation rules to better model real user
behavior. These tables are then organized into buckets, enabling faster lookup
and reduced space complexity. To perform quantum search within each bucket, we
use a distributed exact variant of Grover's algorithm, which offers lower
circuit depth and deterministic success. As a result, the overall quantum
circuit is shallower and more robust against noise, particularly from
depolarizing channels commonly found in near-term quantum devices. Through this
work, Overall, we propose a hybrid framework that combines structured rainbow
tables with efficient quantum search to enhance password recovery.

</details>


### [322] [VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning](https://arxiv.org/abs/2507.14625)
*Juntao Tan,Anran Li,Quanchao Liu,Peng Ran,Lan Zhang*

Main category: cs.CR

TL;DR: VTarbel是一种新的攻击框架，可以针对垂直联邦学习（VFL）系统，即使在部署了异常检测器的情况下也能成功进行目标标签攻击。


<details>
  <summary>Details</summary>
Motivation: 现有针对垂直联邦学习的标签攻击方法依赖于不切实际的假设（例如，访问联邦学习模型的输出来进行对抗性攻击），并且忽略了在实际系统中部署的异常检测器。本研究旨在弥补这一差距，解决垂直联邦学习中的安全威胁，特别是针对性标签攻击。

Method: 提出了一种名为VTarbel的两阶段、最小知识攻击框架，该框架旨在逃避检测器增强的联邦学习推理。在准备阶段，攻击者选择高表达性样本，通过联邦学习协议收集预测标签，并利用这些伪标签在本地特征上训练估计的检测器和代理模型。在攻击阶段，这些模型指导基于梯度的扰动，以生成诱导目标错误分类并逃避检测的对抗性样本。

Result: 在四个模型架构、七个多模态数据集和两个异常检测器的评估中，VTarbel在所有设置下均优于四个最先进的基线方法，成功逃避了检测，并且对于三种代表性的隐私保护防御措施仍然有效。

Conclusion: 该研究揭示了当前联邦学习部署中存在的关键安全漏洞，并强调了对能够感知攻击的鲁棒防御措施的迫切需求。

Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint
features to collaboratively train models without sharing raw data. While
privacy vulnerabilities of VFL are extensively-studied, its security
threats-particularly targeted label attacks-remain underexplored. In such
attacks, a passive party perturbs inputs at inference to force
misclassification into adversary-chosen labels. Existing methods rely on
unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore
anomaly detectors deployed in real-world systems. To bridge this gap, we
introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly
designed to evade detector-enhanced VFL inference. During the preparation
stage, the attacker selects a minimal set of high-expressiveness samples (via
maximum mean discrepancy), submits them through VFL protocol to collect
predicted labels, and uses these pseudo-labels to train estimated detector and
surrogate model on local features. In attack stage, these models guide
gradient-based perturbations of remaining samples, crafting adversarial
instances that induce targeted misclassifications and evade detection. We
implement VTarbel and evaluate it against four model architectures, seven
multimodal datasets, and two anomaly detectors. Across all settings, VTarbel
outperforms four state-of-the-art baselines, evades detection, and retains
effective against three representative privacy-preserving defenses. These
results reveal critical security blind spots in current VFL deployments and
underscore urgent need for robust, attack-aware defenses.

</details>


### [323] [VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking](https://arxiv.org/abs/2507.14629)
*Juntao Tan,Lan Zhang,Zhonghao Hu,Kai Yang,Peng Ran,Bo Li*

Main category: cs.CR

TL;DR: VMask 通过秘密共享层掩码技术防御 VFL 中的模型完成攻击，在保持高准确性的同时实现高效率和可调隐私。


<details>
  <summary>Details</summary>
Motivation: 现有的针对模型完成（MC）攻击的防御方法要么牺牲模型准确性，要么带来不切实际的计算开销。VMask 旨在提出一种新的防御方法，以解决这些问题。

Method: VMask 框架采用秘密共享（SS）技术来掩码攻击者模型中的层参数，以破坏输入数据和中间输出之间的强相关性。通过策略性地选择关键层进行掩码，VMask 降低了开销。此外，VMask 提供了可调节的隐私预算。

Result: VMask 成功抵御了 MC 攻击，将标签推理准确率降低到随机猜测水平。在基于 Transformer 的模型上，模型准确率平均仅下降 0.09%。VMask 的运行时间比基于密码学的方法快 60,846 倍，与标准 VFL 相比仅略有增加（1.8 倍）。

Conclusion: VMask 是一种新颖的标签隐私保护框架，通过层掩码技术有效防御模型完成（MC）攻击，实现了最佳的隐私-效用权衡。它能够在将标签推理准确率降低到随机猜测水平的同时，保持模型性能，并且相比其他方法具有显著的效率优势。

Abstract: Though vertical federated learning (VFL) is generally considered to be
privacy-preserving, recent studies have shown that VFL system is vulnerable to
label inference attacks originating from various attack surfaces. Among these
attacks, the model completion (MC) attack is currently the most powerful one.
Existing defense methods against it either sacrifice model accuracy or incur
impractical computational overhead. In this paper, we propose VMask, a novel
label privacy protection framework designed to defend against MC attack from
the perspective of layer masking. Our key insight is to disrupt the strong
correlation between input data and intermediate outputs by applying the secret
sharing (SS) technique to mask layer parameters in the attacker's model. We
devise a strategy for selecting critical layers to mask, reducing the overhead
that would arise from naively applying SS to the entire model. Moreover, VMask
is the first framework to offer a tunable privacy budget to defenders, allowing
for flexible control over the levels of label privacy according to actual
requirements. We built a VFL system, implemented VMask on it, and extensively
evaluated it using five model architectures and 13 datasets with different
modalities, comparing it to 12 other defense methods. The results demonstrate
that VMask achieves the best privacy-utility trade-off, successfully thwarting
the MC attack (reducing the label inference accuracy to a random guessing
level) while preserving model performance (e.g., in Transformer-based model,
the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up
to 60,846 times faster than cryptography-based methods, and it only marginally
exceeds that of standard VFL by 1.8 times in a large Transformer-based model,
which is generally acceptable.

</details>


### [324] [CANDoSA: A Hardware Performance Counter-Based Intrusion Detection System for DoS Attacks on Automotive CAN bus](https://arxiv.org/abs/2507.14739)
*Franco Oberti,Stefano Di Carlo,Alessandro Savino*

Main category: cs.CR

TL;DR: 该研究提出了一种利用硬件性能计数器（HPC）检测 CAN 网络入侵的新型入侵检测系统（IDS），以应对汽车网络安全挑战。


<details>
  <summary>Details</summary>
Motivation: 控制器区域网络（CAN）协议缺乏固有安全特性，易受网络威胁，特别是随着自动驾驶汽车的兴起。

Method: 利用硬件性能计数器（HPC）的独特响应来检测 CAN 环境中的异常，并通过数据提取和相关性分析优化 HPC 特征以提高分类效率。

Result: 基于 RISC-V 的 CAN 接收器被 gem5 模拟器模拟，其中 CAN 帧负载以 AES-128 加密的形式作为 FreeRTOS 任务进行处理，这会触发不同的 HPC 响应。

Conclusion: 该方法有望显著提高 CAN 安全性并应对汽车网络安全领域的新兴挑战。

Abstract: The Controller Area Network (CAN) protocol, essential for automotive embedded
systems, lacks inherent security features, making it vulnerable to cyber
threats, especially with the rise of autonomous vehicles. Traditional security
measures offer limited protection, such as payload encryption and message
authentication. This paper presents a novel Intrusion Detection System (IDS)
designed for the CAN environment, utilizing Hardware Performance Counters
(HPCs) to detect anomalies indicative of cyber attacks. A RISC-V-based CAN
receiver is simulated using the gem5 simulator, processing CAN frame payloads
with AES-128 encryption as FreeRTOS tasks, which trigger distinct HPC
responses. Key HPC features are optimized through data extraction and
correlation analysis to enhance classification efficiency. Results indicate
that this approach could significantly improve CAN security and address
emerging challenges in automotive cybersecurity.

</details>


### [325] [Careful Whisper: Attestation for peer-to-peer Confidential Computing networks](https://arxiv.org/abs/2507.14796)
*Ceren Kocaoğullar,Gustavo Petri,Dominic P. Mulligan,Derek Miller,Hugo J. M. Vincent,Shale Xiong,Alastair R. Beresford*

Main category: cs.CR

TL;DR: “Careful Whisper”是一种高效的信任传播协议，解决了TEE点对点证明中的通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 为了在车辆 इत्यादी点对点网络中安全地协作，节点需要建立相互信任。直接证明方法会导致二次通信开销，在动态网络中效率低下。

Method: 提出了一种名为“Careful Whisper”的基于ossip的协议，用于在点对点网络中高效传播信任，以解决直接证明带来的二次通信开销问题。

Result: 在包含200个节点的网络中，该协议每轮发送约21.5 KiB数据，需要0.158秒，并且在各种网络拓扑结构中能够抵御证明失败。

Conclusion: “Careful Whisper”协议通过基于ossip的协议有效传播信任，将证明开销降低到线性复杂度，实现了跨异构网络的互操作性和跨越网络的信任传递，并支持与离线节点的信任建立。

Abstract: Trusted Execution Environments (TEEs) are designed to protect the privacy and
integrity of data in use. They enable secure data processing and sharing in
peer-to-peer networks, such as vehicular ad hoc networks of autonomous
vehicles, without compromising confidentiality. In these networks, nodes must
establish mutual trust to collaborate securely. TEEs can achieve this through
remote attestation, where a prover presents evidence of its trustworthiness to
a verifier, which then decides whether or not to trust the prover. However, a
naive peer-to-peer attestation approach, where every TEE directly attests every
other TEE, results in quadratic communication overhead. This is inefficient in
dynamic environments, where nodes frequently join and leave the network.
  To address this, we present Careful Whisper, a gossip-based protocol that
disseminates trust efficiently, reducing attestation overhead to linear
complexity under ideal conditions. It enables interoperability by enabling
transitive trust across heterogeneous networks, and supports trust
establishment with offline nodes via relayed attestations. Using a custom
discrete-event simulator, we show that Careful Whisper propagates trust both
faster and more widely than naive approaches across various network topologies.
Our results demonstrate that our protocol is resource efficient, sending ~21.5
KiB and requiring 0.158 seconds per round in a 200-node network, and that our
protocol is resilient to attestation failures across various network
topologies.

</details>


### [326] [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799)
*Sam Johnson,Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: LLM网络导航代理易受间接提示注入攻击，攻击者可以通过网页中的触发器劫持代理行为，进行凭证窃取等恶意操作。


<details>
  <summary>Details</summary>
Motivation: 展示LLM驱动的网络导航代理的自动化能力以及它们易受间接提示注入（IPI）攻击的漏洞，强调了安全风险和防御需求。

Method: 通过将通用对抗性触发器嵌入网页HTML中，并利用GCG算法和基于Llama-3.1的Browser Gym代理来实现对使用可访问性树解析HTML的LLM网络导航代理的攻击。

Result: 在真实网站上，针对性和普遍性攻击均取得了高成功率，成功进行了登录凭证窃取和强制广告点击等恶意操作。

Conclusion: LLM驱动的自主网络代理存在严重的安全风险，特别是容易受到间接提示注入攻击，攻击者可以利用网页中的普遍存在的触发器来劫持代理行为，导致未经授权的操作，例如凭证窃取和强制广告点击。

Abstract: This work demonstrates that LLM-based web navigation agents offer powerful
automation capabilities but are vulnerable to Indirect Prompt Injection (IPI)
attacks. We show that adversaries can embed universal adversarial triggers in
webpage HTML to hijack agent behavior that utilizes the accessibility tree to
parse HTML, causing unintended or malicious actions. Using the Greedy
Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by
Llama-3.1, our system demonstrates high success rates across real websites in
both targeted and general attacks, including login credential exfiltration and
forced ad clicks. Our empirical results highlight critical security risks and
the need for stronger defenses as LLM-driven autonomous web agents become more
widely adopted. The system software
(https://github.com/sej2020/manipulating-web-agents) is released under the MIT
License, with an accompanying publicly available demo website
(http://lethaiq.github.io/attack-web-llm-agent).

</details>


### [327] [Quantum Skyshield: Quantum Key Distribution and Post-Quantum Authentication for Low-Altitude Wireless Networks in Adverse Skies](https://arxiv.org/abs/2507.14822)
*Zeeshan Kaleem,Misha Urooj Khan,Ahmad Suleman,Waqas Khalid,Kai-Kit Wong,Chau Yuen*

Main category: cs.CR

TL;DR: 提出了一种名为Quantum Skyshield的量子安全架构，用于低空无线网络（LAWNs），集成了量子密钥分发（QKD）和后量子认证，以提供安全可靠的通信。模拟结果表明，该架构在特定条件下能有效生成密钥并检测异常。


<details>
  <summary>Details</summary>
Motivation: 为了满足低空无线网络（LAWNs）日益增长的数据需求，并应对传统无线通信和自由空间光（FSO）链路在安全（如拦截和欺骗）以及FSO在稳定性（如湍流、未对准和天气衰减）方面面临的挑战，尤其是在量子时代，需要一个量子安全的解决方案。

Method: 本研究提出了一个名为Quantum Skyshield的量子安全架构，该架构集成了BB84量子密钥分发（QKD）与后量子认证机制，并使用Lamport一次性签名和基于哈希的消息认证码（HMAC）来确保消息的完整性。通过Grover启发式威胁检测机制来识别异常。

Result: 模拟结果证实，当量子比特错误率（QBER）低于11%的阈值时，可以可靠地生成128位对称密钥。Grover启发式威胁检测机制在单次迭代中能以高达89%的概率识别异常，从而实现实时的信任评估。

Conclusion: 未来研究挑战已确定并讨论，以指导该领域进一步发展。

Abstract: Recently, low-altitude wireless networks (LAWNs) have emerged as a critical
backbone for supporting the low-altitude economy, particularly with the
densification of unmanned aerial vehicles (UAVs) and high-altitude platforms
(HAPs). To meet growing data demands, some LAWN deployments incorporate
free-space optical (FSO) links, which offer exceptional bandwidth and beam
directivity. However, without strong security measures in place, both
conventional radio frequency channels and FSO beams remain vulnerable to
interception and spoofing and FSO in particular can suffer from turbulence,
misalignment, and weather-related attenuation. To address these challenges in
the quantum era, a quantum-secure architecture called Quantum Skyshield is
proposed to enable reliable communication between the base transceiver station
(BTS) and LAWN. The proposed design integrates BB84 quantum key distribution
(QKD) with post-quantum authentication mechanisms. Simulation results confirm
the reliable generation of a 128-bit symmetric key when the quantum bit error
rate (QBER) remains below the threshold of 11%. Authentication is enforced
using Lamport one-time signatures and hash-based message authentication codes
(HMAC) to ensure message integrity. A Grover-inspired threat detection
mechanism identifies anomalies with up to 89% probability in a single
iteration, enabling real-time trust evaluation. Lastly, future research
challenges have also been identified and discussed to guide further development
in this area.

</details>


### [328] [A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.14853)
*Khoa Nguyen,Tanveer Khan,Antonis Michalas*

Main category: cs.CR

TL;DR: 混合同态加密（HHE）与联邦学习（FL）结合，以解决通信开销和数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私保护技术（如全同态加密）会带来显著的计算和通信成本，限制了其在联邦学习中的实际应用。因此，需要一种更有效的方法来解决这些挑战。

Method: 提出了一种结合混合同态加密（HHE）和联邦学习（FL）的方法，以同时解决通信开销和数据隐私问题。

Result: 混合同态加密（HHE）与联邦学习（FL）的结合，为构建可扩展和安全的分散式学习系统提供了新的途径，有望解决通信开销和隐私保护的挑战。

Conclusion: 混合同态加密（HHE）可以与联邦学习（FL）结合，以解决通信开销和数据隐私问题，从而实现可扩展和安全的分散式学习系统。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, making it a promising approach for privacy-sensitive domains. Despite
its potential, FL faces significant challenges, particularly in terms of
communication overhead and data privacy. Privacy-preserving Techniques (PPTs)
such as Homomorphic Encryption (HE) have been used to mitigate these concerns.
However, these techniques introduce substantial computational and communication
costs, limiting their practical deployment. In this work, we explore how Hybrid
Homomorphic Encryption (HHE), a cryptographic protocol that combines symmetric
encryption with HE, can be effectively integrated with FL to address both
communication and privacy challenges, paving the way for scalable and secure
decentralized learning system.

</details>


### [329] [A Compact Post-quantum Strong Designated Verifier Signature Scheme from Isogenies](https://arxiv.org/abs/2507.14893)
*Farzin Renan*

Main category: cs.CR

TL;DR: 本研究提出了$\\textit{CSI\text{-}SDVS}$，一种紧凑且抗量子的、基于同源的强指定验证器签名（SDVS）方案。它解决了现有SDVS方案在抗量子性和效率方面存在的不足，实现了高效且安全的隐私保护通信。


<details>
  <summary>Details</summary>
Motivation: 现有的强指定验证器签名（SDVS）方案大多基于数论假设，容易受到量子攻击。虽然已存在基于格的抗量子替代方案，但它们通常具有较大的密钥和签名规模。因此，需要一种更紧凑、抗量子的SDVS方案，特别是在隐私敏感的应用中，如电子投票和数字现金。

Method: 该研究提出了一种名为$\\textit{CSI\text{-}SDVS}$的新型同源类群作用框架下的强指定验证器签名（SDVS）方案，并结合了CSI-FiSh的签名技术。该方案的安全性基于多目标群作用逆问题（MT-GAIP）的难解性。

Result: $\\textit{CSI\text{-}SDVS}$的密钥和签名大小均为$\\mathcal{O}(\\lambda)$，相较于现有抗量子SDVS方案通常$\\mathcal{O}(\\lambda^2)$的界限，有了显著的改进。这使其成为最紧凑的基于PQC的SDVS方案之一，并且是首个基于同源的抗后量子安全构造。

Conclusion: $\\textit{CSI\text{-}SDVS}$是一种新颖的、基于同源的、强指定验证器签名（SDVS）方案，它提供了紧凑且抗量子的替代方案。该方案在随机预言模型下实现了强不可伪造（SUF-CMA）、不可转移性（NT）和签名者身份隐私性（PSI）等强安全保证。与现有的抗量子SDVS方案相比，$\\textit{CSI\text{-}SDVS}$的密钥和签名大小均为$\\mathcal{O}(\\lambda)$，显著优于通常的$\\mathcal{O}(\\lambda^2)$界限，使其成为最紧凑的基于PQC的SDVS方案之一，也是唯一基于同源的抗后量子安全构造。

Abstract: Digital signatures are essential cryptographic tools that provide
authentication and integrity in digital communications. However,
privacy-sensitive applications, such as e-voting and digital cash, require more
restrictive verification models to ensure confidentiality and control. Strong
Designated Verifier Signature (SDVS) schemes address this need by enabling the
signer to designate a specific verifier, ensuring that only this party can
validate the signature. Existing SDVS constructions are primarily based on
number-theoretic assumptions and are therefore vulnerable to quantum attacks.
Although post-quantum alternatives, particularly those based on lattices, have
been proposed, they often entail large key and signature sizes. In this work,
we introduce $\mathsf{CSI\text{-}SDVS}$, a novel isogeny-based SDVS scheme that
offers a compact, quantum-resistant alternative. Our construction builds on the
ideal class group action framework of CSIDH and the signature techniques of
CSI-FiSh, and relies on the hardness of the Multi-Target Group Action Inverse
Problem (MT-GAIP). $\mathsf{CSI\text{-}SDVS}$ achieves strong security
guarantees; namely, Strong Unforgeability under Chosen-Message Attacks
(SUF-CMA), Non-Transferability (NT), and Privacy of Signer's Identity (PSI), in
the random oracle model. Remarkably, both the keys and signatures in
$\mathsf{CSI\text{-}SDVS}$ are of size $\mathcal{O}(\lambda)$, representing a
significant improvement over the typical $\mathcal{O}(\lambda^2)$ bounds in
existing post-quantum SDVS schemes, thereby making it among the most compact
PQC-based SDVS schemes and the only post-quantum secure construction based on
isogenies.

</details>


### [330] [Metaverse Security and Privacy Research: A Systematic Review](https://arxiv.org/abs/2507.14985)
*Argianto Rahartomo,Leonel Merino,Mohammad Ghafari*

Main category: cs.CR

TL;DR: 元宇宙研究正迅速增长，但安全和隐私仍有待改进，尤其是在政策、可访问性和后端安全方面。


<details>
  <summary>Details</summary>
Motivation: 元宇宙技术的快速发展带来了新的安全和隐私挑战，需要全面分析学术界如何应对这些问题。

Method: 通过系统性文献回顾，分析了2013年至2024年间发表的关于元宇宙安全和隐私的研究。研究按方法、审查的安全和隐私属性、沉浸式组件以及评估策略进行了分类。

Result: 研究发现，过去五年元宇宙安全和隐私研究活动急剧增加，研究主要集中在实际和以用户为中心的方法上，常用的方法包括基准测试、人类实验和定性研究。身份验证和不可观察性是最常被研究的属性。

Conclusion: 研究显示，元宇宙的安全和隐私问题日益受到关注，但仍存在政策合规性、可访问性、互操作性和后端基础设施安全等方面的关键差距。需要综合性的跨学科方法来确保包容且可信的沉浸式环境。

Abstract: The rapid growth of metaverse technologies, including virtual worlds,
augmented reality, and lifelogging, has accelerated their adoption across
diverse domains. This rise exposes users to significant new security and
privacy challenges due to sociotechnical complexity, pervasive connectivity,
and extensive user data collection in immersive environments. We present a
systematic review of the literature published between 2013 and 2024, offering a
comprehensive analysis of how the research community has addressed
metaverse-related security and privacy issues over the past decade. We organize
the studies by method, examined the security and privacy properties, immersive
components, and evaluation strategies. Our investigation reveals a sharp
increase in research activity in the last five years, a strong focus on
practical and user-centered approaches, and a predominant use of benchmarking,
human experimentation, and qualitative methods. Authentication and
unobservability are the most frequently studied properties. However, critical
gaps remain in areas such as policy compliance, accessibility,
interoperability, and back-end infrastructure security. We emphasize the
intertwined technical complexity and human factors of the metaverse and call
for integrated, interdisciplinary approaches to securing inclusive and
trustworthy immersive environments.

</details>


### [331] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: LibLMFuzz是一个框架，通过将基于LLM的智能体与轻量级工具链相结合，可以自动分析二进制文件、规划模糊测试、生成驱动程序和自我修复错误，从而降低模糊测试闭源库的成本。


<details>
  <summary>Details</summary>
Motivation: 解决仅提供二进制库（如闭源和专有软件）时，模糊测试闭源库的成本问题。

Method: LibLMFuzz框架将基于大型语言模型的智能体与轻量级工具链（反汇编器/编译器/模糊器）配对，以自主分析剥离的二进制文件、规划模糊测试策略、生成驱动程序以及迭代地自我修复构建或运行时错误。

Result: LibLMFuzz在四个广泛使用的Linux库上进行了测试，为所有558个可模糊化的API函数生成了语法正确的驱动程序，在没有人为干预的情况下实现了100%的API覆盖率。在1601个合成驱动程序中，75.52%在首次执行时名义上是正确的。

Conclusion: LLM增强中间件有潜力降低黑盒组件模糊测试的成本，并为未来的研究奠定基础，在分支覆盖方面存在研究机会。

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


### [332] [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219)
*Tianneng Shi,Kaijie Zhu,Zhun Wang,Yuqi Jia,Will Cai,Weida Liang,Haonan Wang,Hend Alzahrani,Joshua Lu,Kenji Kawaguchi,Basel Alomair,Xuandong Zhao,William Yang Wang,Neil Gong,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: PromptArmor是一种针对LLM代理提示注入攻击的防御机制，能有效检测和移除恶意提示，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明LLM代理容易受到提示注入攻击，恶意提示会导致代理执行攻击者指定的任务而非用户预期的任务。

Method: PromptArmor通过提示一个现成的LLM来检测和移除注入提示，然后由LLM代理处理剩余的输入。

Result: PromptArmor能够准确识别和移除注入提示，在AgentDojo基准测试中，使用GPT-4o、GPT-4.1或o4-mini时，误报率和漏报率均低于1%。移除注入提示后，攻击成功率降至1%以下，并且能有效防御适应性攻击。

Conclusion: PromptArmor是一种简单有效的防御措施，可以检测并移除输入中的潜在注入提示，从而降低攻击成功率。

Abstract: Despite their potential, recent research has demonstrated that LLM agents are
vulnerable to prompt injection attacks, where malicious prompts are injected
into the agent's input, causing it to perform an attacker-specified task rather
than the intended task provided by the user. In this paper, we present
PromptArmor, a simple yet effective defense against prompt injection attacks.
Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove
potential injected prompts from the input before the agent processes it. Our
results show that PromptArmor can accurately identify and remove injected
prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves
both a false positive rate and a false negative rate below 1% on the AgentDojo
benchmark. Moreover, after removing injected prompts with PromptArmor, the
attack success rate drops to below 1%. We also demonstrate PromptArmor's
effectiveness against adaptive attacks and explore different strategies for
prompting an LLM. We recommend that PromptArmor be adopted as a standard
baseline for evaluating new defenses against prompt injection attacks.

</details>


### [333] [The Matrix Subcode Equivalence problem and its application to signature with MPC-in-the-Head](https://arxiv.org/abs/2507.15377)
*Magali Bardet,Charles Brion,Philippe Gaborit,Mercedes Haiech,Romaric Neveu*

Main category: cs.CR

TL;DR: 本研究引入矩阵子码等价和置换核问题，构建了一种新的后量子签名方案，性能优于SPHINCS+，且签名和公钥尺寸更小。


<details>
  <summary>Details</summary>
Motivation: 虽然等价问题在密码学中广泛应用，但矩阵码领域仅研究了码等价问题，而子码等价问题在汉明度量中已有定义。因此，本研究旨在填补矩阵码子码等价问题的研究空白，并基于此提出新的密码学应用。

Method: 该研究引入了新的问题（矩阵子码等价问题和矩阵码置换核问题），并将MPCitH范式应用于这些问题来构建签名方案。研究人员还改编了用于矩阵码等价问题的组合和代数算法，并分析了其复杂度。

Result: 矩阵子码等价问题被证明可归约到汉明子码等价问题（NP-Complete）。研究人员改编的算法在矩阵码子码问题上的复杂度比矩阵码等价问题高。然而，通过选择更小的参数，该方案实现了约4800字节的签名大小和约275字节的公钥大小，优于SPHINCS+、MEDS和CROSS等现有方案。

Conclusion: 该研究引入了矩阵子码等价问题和矩阵码置换核问题，并将其应用于构建一种新的签名方案。该方案参数选择比矩阵码等价问题更小，且性能优于SPHINCS+，签名和公钥尺寸都更优。与MEDS和CROSS相比，该方案在签名大小和公钥+签名总大小方面也具有优势。

Abstract: Nowadays, equivalence problems are widely used in cryptography, most notably
to establish cryptosystems such as digital signatures, with MEDS, LESS, PERK as
the most recent ones. However, in the context of matrix codes, only the code
equivalence problem has been studied, while the subcode equivalence is
well-defined in the Hamming metric. In this work, we introduce two new
problems: the Matrix Subcode Equivalence Problem and the Matrix Code Permuted
Kernel Problem, to which we apply the MPCitH paradigm to build a signature
scheme. These new problems, closely related to the Matrix Code Equivalence
problem, ask to find an isometry given a code $C$ and a subcode $D$.
Furthermore, we prove that the Matrix Subcode Equivalence problem reduces to
the Hamming Subcode Equivalence problem, which is known to be NP-Complete, thus
introducing the matrix code version of the Permuted Kernel Problem. We also
adapt the combinatorial and algebraic algorithms for the Matrix Code
Equivalence problem to the subcode case, and we analyze their complexities. We
find with this analysis that the algorithms perform much worse than in the code
equivalence case, which is the same as what happens in the Hamming metric.
Finally, our analysis of the attacks allows us to take parameters much smaller
than in the Matrix Code Equivalence case. Coupled with the effectiveness of
\textit{Threshold-Computation-in-the-Head} or \textit{VOLE-in-the-Head}, we
obtain a signature size of $\approx$ 4 800 Bytes, with a public key of
$\approx$ 275 Bytes. We thus obtain a reasonable signature size, which brings
diversity in the landscape of post-quantum signature schemes, by relying on a
new hard problem. In particular, this new signature scheme performs better than
SPHINCS+, with a smaller size of public key + signature. Our signature compares
also well with other signature schemes: compared to MEDS, the signature is
smaller, and we reduced the size of the sum of signature and public key by a
factor close to 5. We also obtain a signature size that is almost half the size
of the CROSS signature scheme.

</details>


### [334] [PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants](https://arxiv.org/abs/2507.15393)
*Ruofan Liu,Yun Lin,Silas Yeo Shuen Yu,Xiwen Teoh,Zhenkai Liang,Jin Song Dong*

Main category: cs.CR

TL;DR: LLMs are making phishing emails more dangerous. PiMRef is a new detector that checks sender identity claims against facts. It's better than old methods and works fast.


<details>
  <summary>Details</summary>
Motivation: Traditional phishing detectors are ineffective against evolving threats, particularly those enhanced by LLMs capable of generating convincing phishing emails. There is a need for a more robust defense mechanism.

Method: PiMRef is a reference-based phishing email detector that leverages knowledge-based invariants. It works by extracting the sender's claimed identity, verifying the legitimacy of the sender's domain against a knowledge base, and detecting call-to-action prompts. Contradictory claims are flagged as phishing indicators, providing human-understandable explanations.

Result: PiMRef boosts precision by 8.8% with no loss in recall compared to existing methods on standard benchmarks. In a real-world evaluation, PiMRef achieved 92.1% precision, 87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art.

Conclusion: Phishing emails are a critical threat, especially with the rise of LLMs. The proposed PiMRef detector, which uses knowledge-based invariants to fact-check sender identity claims, significantly outperforms existing methods in both effectiveness and efficiency, achieving 92.1% precision and 87.9% recall in a real-world evaluation.

Abstract: Phishing emails are a critical component of the cybercrime kill chain due to
their wide reach and low cost. Their ever-evolving nature renders traditional
rule-based and feature-engineered detectors ineffective in the ongoing arms
race between attackers and defenders. The rise of large language models (LLMs)
further exacerbates the threat, enabling attackers to craft highly convincing
phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive
phishing emails tailored to victim profiles, successfully bypassing nearly all
commercial and academic detectors. To defend against such threats, we propose
PiMRef, the first reference-based phishing email detector that leverages
knowledge-based invariants. Our core insight is that persuasive phishing emails
often contain disprovable identity claims, which contradict real-world facts.
PiMRef reframes phishing detection as an identity fact-checking task. Given an
email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the
legitimacy of the sender's domain against a predefined knowledge base, and
(iii) detects call-to-action prompts that push user engagement. Contradictory
claims are flagged as phishing indicators and serve as human-understandable
explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,
PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks
like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across
five university accounts over three years, PiMRef achieved 92.1% precision,
87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art
in both effectiveness and efficiency.

</details>


### [335] [PhishIntentionLLM: Uncovering Phishing Website Intentions through Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15419)
*Wenhao Li,Selvakumar Manickam,Yung-wey Chong,Shankar Karuppayah*

Main category: cs.CR

TL;DR: 利用 LLM 视觉-语言能力，通过 PhishIntentionLLM 框架识别钓鱼网站意图，并在两个数据集上均取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注检测，而对潜在恶意意图的识别则探索不足。

Method: 提出了一种名为 PhishIntentionLLM 的多代理检索增强生成（RAG）框架，利用大型语言模型（LLM）的视觉-语言能力来识别钓鱼网站的意图，具体包括凭证窃取、金融欺诈、恶意软件分发和个人信息收集。

Result: PhishIntentionLLM 在 GPT-4o 上实现了 0.7895 的微观精确率，并且在凭证窃取方面取得了 0.8545 的精确率，比先前工作提高了约 4%。与单一代理基线相比，其微观精确率提高了约 95%。

Conclusion: PhishIntentionLLM 框架为钓鱼意图分析提供了一个可扩展且可解释的解决方案。

Abstract: Phishing websites remain a major cybersecurity threat, yet existing methods
primarily focus on detection, while the recognition of underlying malicious
intentions remains largely unexplored. To address this gap, we propose
PhishIntentionLLM, a multi-agent retrieval-augmented generation (RAG) framework
that uncovers phishing intentions from website screenshots. Leveraging the
visual-language capabilities of large language models (LLMs), our framework
identifies four key phishing objectives: Credential Theft, Financial Fraud,
Malware Distribution, and Personal Information Harvesting. We construct and
release the first phishing intention ground truth dataset (~2K samples) and
evaluate the framework using four commercial LLMs. Experimental results show
that PhishIntentionLLM achieves a micro-precision of 0.7895 with GPT-4o and
significantly outperforms the single-agent baseline with a ~95% improvement in
micro-precision. Compared to the previous work, it achieves 0.8545 precision
for credential theft, marking a ~4% improvement. Additionally, we generate a
larger dataset of ~9K samples for large-scale phishing intention profiling
across sectors. This work provides a scalable and interpretable solution for
intention-aware phishing analysis.

</details>


### [336] [Cryptanalysis of a multivariate CCZ scheme](https://arxiv.org/abs/2507.15449)
*Alessio Caminata,Elisa Gorla,Madison Mabe,Martina Vigorito,Irene Villa*

Main category: cs.CR

TL;DR: Pesto方案的公开4次多项式可以简化为二次多项式，CCZ变换可能并未增加太多安全。


<details>
  <summary>Details</summary>
Motivation: 探究Pesto方案中CCZ变换的安全有效性，以及公开密钥的4次多项式系统是否能提供显著的安全增益。

Method: 展示了Pesto方案的公开4次多项式系统可以被有效地简化为一个二次多项式系统。

Result: Pesto方案的公开4次多项式系统可以被有效地简化为一个二次多项式系统。

Conclusion: Pesto方案的公开密钥是4次多项式，但可以有效地简化为二次多项式系统，这表明CCZ变换可能无法提供显著的安全提升。

Abstract: We consider the multivariate scheme Pesto, which was introduced by Calderini,
Caminata, and Villa. In this scheme, the public polynomials are obtained by
applying a CCZ transformation to a set of quadratic secret polynomials. As a
consequence, the public key consists of polynomials of degree 4. In this work,
we show that the public degree 4 polynomial system can be efficiently reduced
to a system of quadratic polynomials. This seems to suggest that the CCZ
transformation may not offer a significant increase in security, contrary to
what was initially believed.

</details>


### [337] [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613)
*Andrii Balashov,Olena Ponomarova,Xiaohua Zhai*

Main category: cs.CR

TL;DR: 企业级LLM易受多轮提示推断攻击，攻击者可借此窃取敏感信息。本研究分析了这些攻击，并提出包括异常检测和“聚光灯”在内的多种防御措施，以提高LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 企业级大型语言模型（LLM）面临着由提示推断攻击引起的新型安全挑战，攻击者可以通过精心设计的提示链逐步窃取机密数据。

Method: 本文提出了一个针对企业LLM的多阶段提示推断攻击的综合研究。利用概率论、优化框架和信息论泄漏界限，对攻击进行了形式化和分析。提出的防御措施包括统计异常检测、细粒度访问控制、提示清理技术和架构修改，并通过数学分析或实验模拟进行评估。

Result: 研究表明，即使在标准安全措施下，攻击也能有效地从LLM中提取敏感信息（如内部文档或电子邮件）。提出的防御措施，如“聚光灯”方法，可以将攻击成功率降低一个数量级，并且该论文还为结合多种防御措施提供了概念验证和实证验证。

Conclusion: 该研究强调，在企业环境中保护大型语言模型（LLM）需要超越单轮提示过滤，采取一种关注攻击和防御的多阶段整体方法。

Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as
Microsoft 365 Copilot) face novel security challenges. One critical threat is
prompt inference attacks: adversaries chain together seemingly benign prompts
to gradually extract confidential data. In this paper, we present a
comprehensive study of multi-stage prompt inference attacks in an enterprise
LLM context. We simulate realistic attack scenarios where an attacker uses
mild-mannered queries and indirect prompt injections to exploit an LLM
integrated with private corporate data. We develop a formal threat model for
these multi-turn inference attacks and analyze them using probability theory,
optimization frameworks, and information-theoretic leakage bounds. The attacks
are shown to reliably exfiltrate sensitive information from the LLM's context
(e.g., internal SharePoint documents or emails), even when standard safety
measures are in place.
  We propose and evaluate defenses to counter such attacks, including
statistical anomaly detection, fine-grained access control, prompt sanitization
techniques, and architectural modifications to LLM deployment. Each defense is
supported by mathematical analysis or experimental simulation. For example, we
derive bounds on information leakage under differential privacy-based training
and demonstrate an anomaly detection method that flags multi-turn attacks with
high AUC. We also introduce an approach called "spotlighting" that uses input
transformations to isolate untrusted prompt content, reducing attack success by
an order of magnitude. Finally, we provide a formal proof of concept and
empirical validation for a combined defense-in-depth strategy. Our work
highlights that securing LLMs in enterprise settings requires moving beyond
single-turn prompt filtering toward a holistic, multi-stage perspective on both
attacks and defenses.

</details>


### [338] [Cyber security of Mega Events: A Case Study of Securing the Digital Infrastructure for MahaKumbh 2025 -- A 45 days Mega Event of 600 Million Footfalls](https://arxiv.org/abs/2507.15660)
*Rohit Negi,Amit Negi,Manish Sharma,S. Venkatesan,Prem Kumar,Sandeep K. Shukla*

Main category: cs.CR

TL;DR: 摩诃 Kumbh 2025 大会网络安全保障纪实，作者团队成功抵御所有网络攻击，并总结了方法与经验。


<details>
  <summary>Details</summary>
Motivation: 大型活动（如奥运会、世界杯、摩诃 Kumbh 等）的数字化程度日益提高，但其基础设施具有临时性、快速搭建的特点，给网络安全带来严峻挑战。因此，需要一种不同于传统组织的网络安全保障方法。

Method: 作者团队作为网络安全评估和风险管理监督团队，采用了针对大型活动临时性、快速搭建特点的安全保障方法，以应对其面临的复杂网络威胁。

Result: 在摩诃 Kumbh 2025 大会上，作者团队成功保障了活动期间的网络安全，所有网络攻击均未成功。

Conclusion: 该论文详细介绍了作者团队为保障2025年摩诃 Kumbh 大会（为期45天，预计有6亿人次参加）网络安全所采用的方法、流程和成果，并记录了他们从中吸取的经验教训，为未来类似的大型活动提供了参考。

Abstract: Mega events such as the Olympics, World Cup tournaments, G-20 Summit,
religious events such as MahaKumbh are increasingly digitalized. From event
ticketing, vendor booth or lodging reservations, sanitation, event scheduling,
customer service, crime reporting, media streaming and messaging on digital
display boards, surveillance, crowd control, traffic control and many other
services are based on mobile and web applications, wired and wireless
networking, network of Closed-Circuit Television (CCTV) cameras, specialized
control room with network and video-feed monitoring. Consequently, cyber
threats directed at such digital infrastructure are common. Starting from hobby
hackers, hacktivists, cyber crime gangs, to the nation state actors, all target
such infrastructure to unleash chaos on an otherwise smooth operation, and
often the cyber threat actors attempt to embarrass the organizing country or
the organizers. Unlike long-standing organizations such as a corporate or a
government department, the infrastructure of mega-events is temporary,
constructed over a short time span in expediency, and often shortcuts are taken
to make the deadline for the event. As a result, securing such an elaborate yet
temporary infrastructure requires a different approach than securing a standard
organizational digital infrastructure. In this paper, we describe our approach
to securing MahaKumbh 2025, a 600 million footfall event for 45 days in
Prayagraj, India, as a cyber security assessment and risk management oversight
team. We chronicle the scope, process, methodology, and outcome of our team's
effort to secure this mega event. It should be noted that none of the cyber
attacks during the 45-day event was successful. Our goal is to put on record
the methodology and discuss what we would do differently in case we work on
similar future mega event.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [339] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: Catalyst 正则化通过在扩展参数空间中引入催化变量来解决传统结构化剪枝中的幅度偏差和窄边距问题，通过理论保证和实验验证，实现了公平、鲁棒且性能更优的剪枝。


<details>
  <summary>Details</summary>
Motivation: 传统正则化器（如 L1 或 Group Lasso 及其变体）会导致有偏的剪枝决策，并且剪枝决策的边际非常小。

Method: 提出了一种新颖的正则化方法，该方法在扩展的参数空间中通过辅助催化变量进行定义，以满足精确的代数条件。

Result: 实验验证了 Catalyst 修剪算法的有效性，在各种数据集和模型上的修剪结果优于最先进的滤波器修剪方法，并证实了 Catalyst 修剪的鲁棒性和公平性。

Conclusion: 所提出的 Catalyst 正则化确保了每个滤波器具有公平的修剪机会，并且在理论上可证明其对幅度的偏差为零，同时通过在保留和修剪的滤波器幅度之间进行宽边距划分，实现了鲁棒的修剪行为。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [340] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种新的剪枝策略，通过投影空间和梯度下降分析来评估滤波器重要性，构建了PROscore和IPPRO方法，实现了近乎无损的剪枝，并打破了“尺寸无关”的剪枝神话。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重要性的结构化剪枝方法（如基于幅值的方法）存在局限性，即使滤波器冗余，幅值较大的滤波器也可能因为幅值较小的滤波器未被剪枝而不会被剪枝。这限制了剪枝的决策能力。

Method: 提出了一种新颖的剪枝策略，将滤波器置于投影空间中，并通过观察梯度下降的移动来衡量滤波器被剪枝的可能性。基于此构建了PROscore，用于实现新的基于重要性的结构化剪枝方法IPPRO。

Result: 提出的基于投影空间的新颖重要性标准实现了近乎无损的剪枝，降低了剪枝过程中的性能下降，并在微调后表现出有希望的性能。

Conclusion: 本文提出的方法在实践中实现了近乎无损的剪枝，并通过微调获得了有竞争力的性能。该研究揭示了剪枝中的“尺寸无关”原则，并在理论和实践上扩展了基于重要性的剪枝。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [341] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR是一种通过将LLM集成到进化循环中来改进程序合成的方法。它通过在进化搜索和滞后学习阶段之间交替进行，使LLM能够学习和改进其采样和改进能力，从而在ARC-AGI基准测试中取得显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 许多程序综合任务对最先进的语言模型来说都太难了，无法一次性解决。基于搜索的进化方法通过迭代地探索解决方案空间提供了一种有前途的替代方案，但其有效性仍然受到底层生成模型固定能力的限制。

Method: SOAR通过将语言模型集成到自我改进的进化循环中来学习程序合成。SOAR在（1）使用LLM采样和改进候选解决方案的进化搜索和（2）将搜索尝试转换为用于微调LLM采样和改进功能的有效问题-解决方案对的滞后学习阶段之间交替进行，从而在后续迭代中实现更有效的搜索。

Result: SOAR在具有挑战性的ARC-AGI基准测试中实现了跨模型规模和迭代的显著性能提升，并实现了52%的公开测试集解决方案。所提出的方法能够实现跨采样和改进微调任务的积极转移。

Conclusion: SOAR在具有挑战性的ARC-AGI基准测试中实现了跨模型规模和迭代的显著性能提升，并实现了52%的公开测试集解决方案。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [342] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: Intermediate (latent space) fusion using a Combined Model (CM) with autoencoders and a neural network outperformed early fusion with a Random Forest (RF) model in predicting daily depressive symptoms, achieving lower MSE and higher R2. The CM demonstrated better generalization and highlighted the value of latent space fusion for complex multimodal mental health data.


<details>
  <summary>Details</summary>
Motivation: Mental illnesses such as depression and anxiety require improved methods for early detection and personalized intervention. Traditional predictive models often rely on unimodal data or early fusion strategies that fail to capture the complex, multimodal nature of psychiatric data. Advanced integration techniques, such as intermediate (latent space) fusion, may offer better accuracy and clinical utility.

Method: Using data from the BRIGHTEN clinical trial, we evaluated intermediate (latent space) fusion for predicting daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented with a Random Forest (RF) model and intermediate fusion implemented via a Combined Model (CM) using autoencoders and a neural network. The dataset included behavioral (smartphone-based), demographic, and clinical features. Experiments were conducted across multiple temporal splits and data stream combinations. Performance was evaluated using mean squared error (MSE) and coefficient of determination (R2).

Result: The CM outperformed both RF and Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985 vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed signs of overfitting, with a large gap between training and test performance, while the CM maintained consistent generalization. Performance was best when integrating all data modalities in the CM (in contradistinction to RF), underscoring the value of latent space fusion for capturing non-linear interactions in complex psychiatric datasets.

Conclusion: Latent space fusion offers a robust alternative to traditional fusion methods for prediction with multimodal mental health data. Future work should explore model interpretability and individual-level prediction for clinical deployment.

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [343] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: AI医疗应用的公平性问题不容忽视。本研究提出的“预测代表性”框架通过评估模型在不同人群中的预测表现，揭示了即使在数据抽样均衡的情况下，模型在肤色较深人群中的表现也存在劣势。研究强调了模型在实际部署中跨人群公平泛化的重要性，并呼吁加强事后公平性审计、数据集透明度和包容性模型验证。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域应用广泛，但算法偏见和不公平结果，尤其是在边缘化群体中，仍然是一个持续存在的问题。

Method: 提出预测代表性（PR）框架，通过评估模型在不同人群中的预测公平性来审计AI系统的公平性，并引入外部可迁移性标准来量化公平性泛化。

Result: 通过对皮肤癌分类器在不同肤色人群中的表现进行评估，发现模型在肤色较深人群中的表现存在显著不足，即使在原始数据抽样比例均衡的情况下也是如此。这表明，公平性不仅在于数据集的构成，更在于模型的预测能力。

Conclusion: AI模型在医疗决策中的应用应关注预测代表性，从数据构成转向结果公平性，以解决算法偏见，确保对所有人群的公平性。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [344] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本文深入探讨了两层神经网络（尤其是使用 sigmoid 等光滑激活函数的网络）的训练机制，揭示了其普遍逼近能力和解决方案空间的奥秘。


<details>
  <summary>Details</summary>
Motivation: 本文旨在理解两层神经网络的训练解决方案，特别是那些隐藏层包含光滑激活函数（如 sigmoid）的网络，这些网络是 ReLU 出现之前最常用的。

Method: 该研究采用了四种主要机制来理解训练解决方案：1. 泰勒级数展开 2. 节点的严格偏序 3. 光滑样条实现 4. 光滑连续性约束。

Result: 证明了两层神经网络在任意输入维度上的普遍逼近能力，并通过实验验证，在很大程度上揭开了解决方案空间的“黑箱”之谜。

Conclusion: 该研究揭示了两层神经网络（其隐藏层包含具有光滑激活函数的单元，如 sigmoid）的训练解决方案，通过反向传播算法获得。通过泰勒级数展开、节点的严格偏序、光滑样条实现和光滑连续性约束，证明了其在任意输入维度上的普遍逼近能力，并通过实验验证，在很大程度上揭开了解决方案空间的“黑箱”之谜。此外，所使用的新证明也丰富了逼近论。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [345] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: FBE通过约束极端特征来提升基于距离的OOD检测性能，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习中数据特征的分布偏差和极端特征的存在，导致基于距离的OOD检测方法倾向于给分布内样本过低的评分，从而限制了其OOD检测能力。

Method: 提出了一种名为特征库增强（FBE）的简单有效的方法，利用数据集的统计特征来识别和约束极端特征到分离边界，从而拉开分布内和分布外样本之间的距离。

Result: 在ImageNet-1k和CIFAR-10大规模数据集上进行了实验，结果表明FBE方法取得了最先进的性能。此外，还进行了理论分析和补充实验以提供更深入的见解。

Conclusion: FBE方法通过识别和约束极端特征到分离边界，拉开数据内外样本的距离，解决了距离度量方法中因极端特征导致ID样本得分过低的问题，在ImageNet-1k和CIFAR-10上达到了最先进的性能。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [346] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: 方向性不是必需的，但可以被发现。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑中循环电路的普遍性启发，研究方向性作为一种归纳偏差对人工神经网络的有用程度。

Method: 通过应用适当的修剪技术，在信息流中引入方向性，以实现神经元之间的拓扑排序。

Result: 我们的修剪方案在信息流中成功地诱导了更大的拓扑排序，而不会损害性能。

Conclusion: 方向性可以被诱导，而非硬性规定，并可能是一种有益的归纳偏差，可以通过梯度下降和稀疏化来发现，而不是作为学习的先决条件。

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [347] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN 是一个模型知识库管道，通过自适应地编织关于模型架构修改的先验知识来掌握神经网络的精炼。


<details>
  <summary>Details</summary>
Motivation: 传统数据库系统在模型选择方面存在不足，因为它们忽略了任务查询和模型架构变体之间精细的、不断演变的表示关系，导致匹配不佳，无法有效精炼模型。

Method: M-DESIGN 通过一个知识编织引擎，将模型精炼重构为关于任务元数据的自适应查询问题。它利用图关系知识模式，该模式显式地将数据属性、架构变体和成对性能差异编码为可连接关系，以快速匹配和迭代地精炼候选模型。它还有一个预测查询规划器，可以检测和适应 OOD 任务。

Result: M-DESIGN 包含 3 个图任务和 22 个图数据集的结构化元数据，其中包含 67,760 个图模型的数据记录。

Conclusion: M-DESIGN 在 33 对数据-任务中的 26 对中，在有限预算内提供了最佳模型。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [348] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 提出一种聚类方法，通过将相似的激活模式分组，有效利用LLMs的激活稀疏性，实现了高聚类精度和低困惑度，旨在降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了有效利用大型语言模型（LLMs）的激活稀疏性以降低计算成本，同时克服直接在神经元层面进行预测的计算成本高昂的问题。

Method: 提出了一种基于聚类的激活模式压缩框架，将相似的激活模式分组到少数代表性聚类中，通过预测聚类分配而非单个神经元状态来预测激活模式。

Result: 该方法实现了高达79.34%的聚类精度，优于标准的二元聚类方法，同时保持了困惑度（PPL）得分的最小下降。在聚类数量足够多的情况下，PPL得分低至12.49，有效在降低计算开销的同时保持了模型质量。

Conclusion: 该聚类方法为预测激活模式提供了基础，有望提高大规模语言模型的稀疏计算效率。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [349] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 提出了一种遗传算法，用于生成不同复杂度的合成数据集，以评估机器学习方法。所生成数据的复杂性与识别质量相关。


<details>
  <summary>Details</summary>
Motivation: 研究界不断寻求更高级的合成数据生成器，以可靠地评估机器学习方法的优缺点。本研究旨在通过提出一种遗传算法来增加涵盖各种问题复杂度的可用数据集。

Method: 提出了一种遗传算法，该算法针对分类和回归任务优化了一组问题复杂度度量，以实现特定目标。对于分类任务，使用了10个复杂度度量；对于回归任务，选择了4个度量。

Result: 实验证实，所提出的遗传算法可以通过线性特征投影转换合成创建的数据集，以实现目标复杂度值，从而生成具有不同难度级别的。对最先进的分类器和回归器的评估揭示了生成数据的复杂性与识别质量之间的相关性。

Conclusion: 该研究提出的遗传算法能够生成具有不同难度级别的合成数据集，并通过线性特征投影将合成创建的数据集转换为目标复杂度值。实验结果表明，生成数据的复杂度与识别质量之间存在相关性。

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [350] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: ROBAD是一个新颖的基于Transformer的恶意用户检测模型，它通过捕获帖子和序列的局部和全局信息，并利用模仿的恶意行为进行训练，从而能够抵抗对抗性攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的恶意用户检测模型容易受到旨在逃避检测的对抗性攻击，因为它们对输入的微小变化很敏感。为了解决这个问题，需要提高模型的理解能力和知识，使其在进行预测时能够识别潜在的输入修改。

Method: ROBAD模型首先利用Transformer编码器块对每个帖子进行双向编码，以捕获帖子级别的局部信息，然后利用Transformer解码器块通过注意力机制对帖子嵌入的序列模式进行建模，以获得序列级别的全局信息。最后，通过将模仿攻击者修改的序列的嵌入输入到增强的对比学习分类层中进行序列预测，从而丰富模型的知识。

Result: 在Yelp和Wikipedia数据集上的大量实验表明，ROBAD模型在面对最新的对抗性攻击时，能够有效地检测恶意用户。

Conclusion: ROBAD模型能够有效检测恶意用户，并且能够抵抗最新的对抗性攻击。

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [351] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 本研究提出了一种结合数字孪生、迁移学习、深度Shapley值和深度k近邻的AI驱动的毫米波波束对齐方法，以解决数据开销、鲁棒性和可解释性问题，实验结果显示出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了构建信任并确保毫米波（mmWave）系统的可靠性能，在AI原生6G愿景下，可解释性和鲁棒性至关重要。深度学习（DL）解决方案在高效波束对齐方面面临数据收集开销高、硬件限制、缺乏可解释性以及易受对抗性攻击等挑战。

Method: 该方法利用数字孪生（DT）生成类似真实世界的合成信道数据，克服了真实世界数据收集的挑战。通过迁移学习对DT中的预训练模型进行微调，以解决数字副本与真实环境之间的不匹配问题。利用深度Shapley值（SHAP）对输入特征按重要性排序，以减少波束训练开销和提高透明度。结合深度k近邻（DkNN）算法，提供可信度度量，以检测分布外输入并确保鲁棒、透明的决策制定。

Result: 实验结果表明，所提出的框架将真实世界数据需求减少了70%，将波束训练开销减少了62%，并将离群值检测鲁棒性提高了高达8.5倍，与传统的基于softmax的深度学习模型相比，实现了接近最优的频谱效率和透明的决策制定。

Conclusion: 该研究提出了一个用于毫米波MIMO系统的鲁棒且可解释的基于深度学习的波束对齐引擎（BAE），通过利用数字孪生生成合成数据并结合深度Shapley值和深度k近邻算法，显著降低了数据收集和波束训练的开销，同时提高了系统的鲁棒性和可解释性。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [352] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 为解决智能故障诊断中数据和标签稀疏以及用户隐私问题，提出SSFL-DCSL半监督联邦学习框架，结合双对比损失和软标签，通过原型实现知识共享和模型对齐，在仅10%数据有标签时准确率提升高达7.85%。


<details>
  <summary>Details</summary>
Motivation: 传统的监督深度学习方法在智能故障诊断（IFD）中需要大量标签数据，而这些数据通常分散在不同客户端，获取成本高昂且存在数据分布不一致的问题。为了解决数据和标签稀疏以及用户隐私保护等挑战，需要一种能够利用少量标签数据和大量无标签数据进行分布式学习的框架。

Method: 提出了一种名为SSFL-DCSL的半监督联邦学习框架，该框架整合了双对比损失（包括本地对比损失和全局对比损失）和软标签技术。具体方法包括：1. 设计了基于拉普拉斯分布的样本加权函数，以减轻伪标签置信度低带来的偏差；2. 引入双对比损失来缓解不同数据分布导致的模型发散；3. 通过加权平均和动量更新在服务器端聚合本地原型，实现客户端间的知识共享。

Result: 在两个公开数据集和一个工厂电机数据集上进行的实验表明，SSFL-DCSL框架在最严苛的场景下（仅10%数据有标签），相比现有最先进方法，准确率提高了1.15%至7.85%。

Conclusion: 该研究提出了一种名为SSFL-DCSL的半监督联邦学习框架，通过整合双对比损失和软标签技术，有效解决了分布式客户端数据和标签稀疏的问题，同时保护用户隐私。该框架利用无标签数据进行表示学习，并通过原型促进客户端间的联合学习，实现了知识共享并防止了本地模型发散。实验结果表明，在仅有10%数据有标签的最具挑战性的任务中，SSFL-DCSL相比现有最先进方法，准确率提升了1.15%至7.85%。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [353] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 本研究提出B4模型，通过融合价格和外部信号，模拟牛熊市场中的偏见和行为动态，提高了市场趋势预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 金融市场行为受历史价格和新闻、政策、社交媒体情绪等外部叙述的共同影响。投资者行为的异质性和偏见使得市场动态建模复杂化。本研究旨在探索牛熊市场状态在投资者驱动的市场动态中的作用潜力。

Method: 提出了一种名为B4（Bias to Behavior from Bull-Bear Dynamics）的统一框架，该框架将时间价格序列和外部上下文信号嵌入共享的潜在空间，以模拟牛熊市场动态中的偏见变化和行为适应性。该框架包含一个惯性配对模块（用于保留动量）和一个双重竞争机制（用于捕捉行为分歧），以模拟偏见驱动的非对称性、行为惯性和市场异质性。

Result: B4模型在真实金融数据集上的实验结果表明，该模型在预测市场趋势方面取得了优于现有方法的性能，并能提供关于偏见、投资者行为和市场动态之间相互作用的可解释性。

Conclusion: 本研究提出的B4模型通过融合价格序列和外部信号，能够有效捕捉市场中的偏见动态和行为适应性，从而提高趋势预测的准确性。实验结果表明，B4模型在真实金融数据上表现优于现有模型，并能提供对偏见、投资者行为和市场动态之间相互作用的可解释性见解。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [354] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 为听障人士开发了集成了CNN、CLAP和多模态技术的深度学习辅助设备，用于实时声源定位和识别，在多个评估指标上均表现优异，并有广阔的未来应用前景。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发一种深度学习系统，作为辅助设备，帮助聋哑或听力障碍人士。该设备能够实时精确定位和识别声源，填补了当前研究在利用机器学习技术服务弱势群体方面的空白。

Method: 该系统包含三个主要部分：1. JerryNet：一个自定义卷积神经网络（CNN）架构，用于确定九个可能方向的到达角（DoA）。2. 音频分类：一个基于对比语言-音频预训练（CLAP）模型的微调模型，仅根据音频识别具体声音类别。3. 多模态集成模型：一个结合音频、视觉和文本数据精确定位声音源的模型，包括使用Yolov9进行目标检测和音频-视觉定位模型（基于CIoU）来识别最佳边界框。硬件方面，采用了四麦克风阵列和安装在眼镜上的摄像头，并通过腕带显示方向等信息。

Result: JerryNet在自定义数据集上实现了91.1%的声源方向识别精度，优于所有基线模型。CLAP模型在自定义数据集和AudioSet数据集上的准确率分别为98.5%和95%。多模态集成模型中的音频-视觉定位模型取得了0.892的cIoU和0.658的AUC，优于其他类似模型。

Conclusion: 该研究为听障人士开发了深度学习辅助设备，实现了声音源的实时精确定位和识别，填补了现有研究的空白，并利用机器学习技术服务了弱势群体。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [355] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache 是一种用于 LLM 的 KV 缓存优化技术，通过梯形缓存模式和迭代压缩来提高长距离能力和处理长序列的能力，解决了内存不足和效率问题。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度的增加，LLM 中的 KV 对数量不断增加，从而导致效率瓶颈，阻碍了 LLM 处理长上下文和生成长序列的能力。

Method: LaCache 通过两种创新来优化 KV 缓存：1. 采用梯形 KV 缓存模式，在固定存储预算内扩展了捕获长距离依赖关系的范围，从而提高了长距离能力。2. 采用迭代压缩机制，通过基于令牌距离的动态压缩来压缩旧缓存，为新令牌腾出空间，从而在有限的缓存预算内实现更有效的连续生成。

Result: LaCache 能够有效处理长上下文和生成长序列，解决了 LLM 的效率瓶颈，并在各种任务、基准和 LLM 模型上得到验证，可提高长距离能力。

Conclusion: LaCache 是一种无需训练即可提高 LLM 效率和准确性的新颖 KV 缓存优化范式，可解决长距离建模中的关键挑战，并在各种任务、基准和 LLM 模型上得到验证。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [356] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist是一个元学习框架，通过动态选择聚合规则来防御模型投毒攻击，并在各种场景下优于静态防御。


<details>
  <summary>Details</summary>
Motivation: 现有的静态防御措施在异构数据环境或面对适应性对手时效果不佳，因此需要一种更动态、更适应环境的防御方法。

Method: 本文提出了一种新颖的元学习框架FedStrategist，将鲁棒聚合重构为实时、成本感知的控制问题。设计了一个轻量级的上下文老虎机代理，可以根据实时诊断指标动态地从一系列防御中选择最优的聚合规则。

Result: 实验证明，FedStrategist代理在包括“Krum有利”环境和面对复杂的“隐形”对手等各种场景中，能够学习到优越的策略，优先保证模型的完整性，并可通过单一的“风险容忍度”参数进行控制。

Conclusion: "FedStrategist"提供了一种新的、实用的、可分析的方法，用于创建弹性智能的去中心化AI系统。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [357] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: 本研究首次将测试时自适应（TTA）技术应用于解决音频分类中的域偏移问题，并提出了一种改进的CoNMix方法，在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在源数据集上预训练后，在测试数据集上会遇到域偏移问题，导致性能下降。本研究旨在解决由背景噪音引起的音频分类域偏移问题，并探索TTA技术在该领域的应用潜力。

Method: 采用TTT、TENT和CoNMix三种TTA方法，并提出一种改进的CoNMix方法，在AudioMNIST和SpeechCommands V1数据集上，评估不同类型和强度的背景噪音对音频分类性能的影响。

Result: 研究结果表明，改进后的CoNMix方法在域偏移情况下相比TTT和TENT方法具有更高的分类准确率。具体而言，在AudioMNIST数据集上，当存在10分贝的健身车背景噪音时，错误率为5.31%；当存在3分贝的流水背景噪音时，错误率为12.75%。

Conclusion: 该研究首次利用测试时自适应（TTA）技术来解决因背景噪音引起的域偏移问题在音频分类任务中，并提出了一种改进的CoNMix方法，在两种常用音频分类数据集（AudioMNIST和SpeechCommands V1）上，在不同类型的背景噪音和噪音强度下，相比于TTT和TENT方法，在域偏移下表现出更高的分类准确率。

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [358] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 通过结合非线性效用聚合和几何感知查询选择的交互式学习框架，解决了模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决模式挖掘中的模式爆炸问题。

Method: 提出了一种结合非线性效用聚合和几何感知查询选择的交互式学习框架，通过Choquet积分模型用户偏好，并利用版本空间的几何结构来指导信息性比较的选择。采用具有紧密距离界限的分支定界策略来有效地识别决策边界附近的查询。

Result: 实验表明，该方法在UCI数据集上取得了比现有方法更好的排名准确性和更少的用户交互。

Conclusion: 该方法在UCI数据集上进行了实验，并取得了比ChoquetRank等现有方法更好的排名准确性和更少用户交互。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [359] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 该研究提出了一种人工智能框架，利用SHAP值计算绿色氢产量和场地适用性，并确定了阿曼的临近水源、海拔和季节性变化是最重要的因素。


<details>
  <summary>Details</summary>
Motivation: 随着各国寻求化石燃料的可持续替代品，绿色氢作为一种有前途的脱碳战略路径出现，尤其是在太阳能丰富的干旱地区。然而，确定最佳的制氢地点需要整合复杂اً的环境、大气和基础设施因素，并且通常由于直接氢产量数据有限而变得复杂。

Method: 该研究提出了一种新颖的人工智能（AI）框架，利用平均绝对SHAP值计算绿色氢产量和场地适用性指数。该框架由一个多阶段流程组成，包括无监督多变量聚类、监督机器学习分类器和SHAP算法。

Result: 研究结果揭示了阿曼场地适用性的明显空间格局以及变量的相对影响。模型预测准确率为98%，结果还显示，临近水源、海拔和季节性变化是决定阿曼绿色氢场地适用性的最重要因素，平均绝对SHAP值分别为2.470891、2.376296和1.273216。

Conclusion: 该研究提供了一个可复制且可扩展的工具，用于在数据稀缺地区进行绿色氢基础设施规划和其他决策。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [360] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: 该论文提出了一种用于k-PCA k-子空间中位数的近似算法，该算法具有多项式运行时间和非指数级于k的近似因子。


<details>
  <summary>Details</summary>
Motivation: k-PCA的k-子空间中位数比k-PCA的k-子空间均值更具稀疏性和鲁棒性，但计算上更难。

Method: 该论文提供了一种新的算法来近似k-PCA的k-子空间中位数，并给出了理论分析。

Result: 该算法的乘法近似因子为sqrt(d)，运行时间与输入大小成多项式关系，并且首次实现了非指数级于k的运行时间和近似因子。

Conclusion: 该算法为k-PCA的近似提供了一个多项式时间确定性算法，其运行时间和近似因子均非指数级。

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [361] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出POGM方法解决梯度下降的领域泛化问题，通过匹配梯度轨迹而非直接最小化梯度距离，避免了梯度波动和高计算开销，并在实验中表现出良好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在梯度对齐方面存在两个主要挑战：一是梯度经验距离或梯度内积（GIP）的最小化会导致跨域梯度波动，阻碍直接学习；二是梯度学习直接应用于联合损失函数会因二阶导数近似而产生高计算开销。

Method: 提出了一种新的帕累托最优梯度匹配（POGM）方法，通过将梯度轨迹作为收集的数据，并在元学习器中进行独立训练。在元更新中，最大化梯度内积（GIP），同时限制学习到的梯度与经验风险最小化梯度轨迹的偏差过大，从而使聚合梯度能够整合所有域的知识，避免了向任何特定域的梯度波动。

Result: POGM在DomainBed数据集上的实验结果显示，相比其他基线方法，POGM取得了具有竞争力的结果，并且实现了计算效率。

Conclusion: POGM在DomainBed数据集上的实验评估结果具有竞争力，并且计算效率高。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [362] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化的框架，通过结合区块链和经济激励，实现了安全高效的大语言模型（LLM）协同微调，解决了传统联邦学习的中心化风险和LLM微调的开销问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决中心化控制的缺陷以及去中心化方案中巨大的计算和通信开销等问题，同时保证数据隐私和安全性。

Method: FLock框架通过整合基于区块链的信任层和经济激励，用一个安全、可审计的协议来替代中心聚合器，从而实现去中心化的安全高效的LLM协同微调。

Result: FLock框架能够防御后门投毒攻击，将攻击成功率降低了68%以上。同时，全局模型也展现出更强的跨领域泛化能力。

Conclusion: FLock框架能够防御针对标准联邦学习优化器的后门投毒攻击，并促进协同知识转移。最终的模型在跨领域泛化能力上表现更优，优于在各自专业数据上独立训练的模型。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [363] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: 本研究提出了一种在仅观察到马尔可夫链配置变化的情况下学习伊辛模型的新算法，该算法在依赖图恢复和参数估计方面均取得了与现有最先进方法相当的性能，解决了实际应用中的关键问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究在学习伊辛模型时，通常假设所有站点更新尝试（即使未改变配置）都被观察到，这种假设在实际场景中难以满足。本研究旨在解决仅能观察到配置实际发生变化这一更自然、更现实的观测模型下的学习问题。

Method: 研究者提出了一种新的算法，能够处理仅观察到马尔可夫链（Markov chain）配置变化的情况，用于学习伊辛模型。该算法首先在多项式时间内恢复了依赖图，然后利用更强的假设（如最大度为d）在指数时间内恢复了模型参数。分析结果表明该算法适用于更广泛的可逆、单站点马尔可夫链。

Result: 对于最大度为d的伊辛模型，研究提出的算法能在多项式时间内（poly(d)·n^2logn）恢复其依赖图，并在额外的时间（~O(2^d n)）内恢复模型参数。这在更弱的观测模型下，达到了与独立同分布设置相当的性能。

Conclusion: 本研究首次提出了在仅观察到配置变化的情况下学习伊辛模型（Ising model）的算法，并在参数恢复方面取得了与独立同分布（i.i.d.）设置下相当的性能。

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [364] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoProFormer是一个基于大规模纳米材料-蛋白质相互作用数据集（NanoPro-3M）的AI模型，能够准确预测纳米材料与蛋白质的相互作用，并具有良好的泛化能力，可应用于多种下游任务，减少实验依赖。


<details>
  <summary>Details</summary>
Motivation: 理解纳米材料与蛋白质的相互作用对于释放其在医学和环境科学中的潜力至关重要，但现有模型受限于有限的数据集和泛化能力。

Method: 提出NanoPro-3M数据集，包含320万个样本和37,000个独特蛋白质，并开发了NanoProFormer基础模型，利用多模态表征学习预测纳米材料-蛋白质亲和力。

Result: NanoProFormer模型表现出强大的泛化能力，能够处理缺失特征以及未见过的纳米材料或蛋白质，并且其多模态建模显著优于单模态方法，还能识别corona形成的关键决定因素。

Conclusion: 本研究通过构建包含320万个样本和37,000个独特蛋白质的NanoPro-3M数据集，并提出基于多模态表征学习的NanoProFormer模型，实现了对纳米材料-蛋白质亲和力的预测，并展示了其在零样本推理和微调方面的应用潜力，为高性能、泛化性强的纳米材料-蛋白质相互作用预测奠定了基础。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [365] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM是一种新的线性降维方法，结合了非线性方法的几何直觉和线性方法的计算优势，在处理流形结构数据方面优于PCA。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是将基于扩散的非线性方法中的几何直觉与PCA和经典MDS等线性嵌入方法在计算简易性、效率和可解释性方面的优势相结合。

Method: LDM通过对扩散图核进行线性近似来构建，是一种新颖的线性降维方法。

Result: 实验表明，LDM在具有明显流形结构的数据集上优于PCA，尤其是在高维情况下，而PCA在方差或噪声占主导的情况下仍然是首选。此外，LDM核矩阵的完全正定性允许直接应用非负矩阵分解（NMF），为可解释的潜在结构发现提供了机会。

Conclusion: LDM是一种新的线性降维技术，具有重要的理论和实践价值。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [366] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 通过在多轮强化学习中加入简单的“再说一遍”反馈（UFO），能提升大型推理模型的答题准确率和修正能力，且不牺牲单轮表现。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在训练大型推理模型（LRM）时，通常采用单轮范式和可验证的奖励。然而，这种方法训练出的模型在多轮推理和根据上下文反馈进行修正方面表现不佳，容易产生重复性回答。因此，研究旨在探索LRM是否能在多轮情境下进行自我反思和修正。

Method: 提出了一种名为“UFO”（Unary Feedback as Observation）的方法，该方法将最少但常见的单元反馈（如“再说一遍”）整合到多轮强化学习的训练中。这种方法可以轻松应用于现有的单轮强化学习训练框架，并通过设计奖励结构来指导模型在每一步给出周全的答案，以期在出错时鼓励多样化的推理。

Result: 实验结果表明，采用UFO进行强化学习训练的模型，在保持单轮性能的同时，多轮推理准确率最高可提高14%。这使得语言模型在多轮问题解决中能更好地响应反馈。

Conclusion: 通过使用一种名为“UFO”（Unary Feedback as Observation）的新方法，在多轮强化学习中结合最少但常见的用户反馈（例如“再说一遍”），可以提高大型推理模型（LRM）在多轮问题解决中的表现。实验表明，这种方法在保持单轮性能的同时，能将多轮推理准确率提高高达14%，使模型能更好地响应多轮问题解决中的反馈。此外，研究还设计了奖励机制，以鼓励模型在每一步都给出周全的答案，从而在减少正确答案所需轮数的同时，也能在出错时鼓励多样化的推理。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [367] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 本研究发现现有深度伪造检测方法在个体公平性方面存在缺陷，并提出了首个能够提升个体公平性和泛化性的通用框架，实验证明该方法效果优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 尽管深度伪造检测技术取得了进展，但公平性问题（特别是针对特定人群的偏见）仍未得到充分解决。先前研究侧重于群体公平性，而个体公平性（即确保相似个体获得相似预测）则在深度伪造检测领域未被充分探索。本研究首次发现，个体公平性的基本原则在深度伪造检测中存在根本性的失败。

Method: 本研究提出了首个可整合进现有深度伪造检测器的通用框架，以提升个体公平性和泛化性。

Result: 实验结果表明，本研究提出的方法显著提高了深度伪造检测的个体公平性，并保持了稳健的检测性能，优于现有最先进的方法。

Conclusion: 本研究提出了首个能够提升个体公平性和泛化性的通用框架，并已在领先的深度伪造数据集上进行了广泛的实验验证，证明了其在提升个体公平性方面的显著效果，同时保持了稳健的检测性能，优于现有最先进的方法。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [368] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 机器学习模型在预测环形几何中的临界热流方面，显著优于传统的经验公式，将平均相对误差从26%以上降低到3.5%以下。


<details>
  <summary>Details</summary>
Motivation: 尽管传统的经验公式和查找表在核反应堆安全分析中用于预测临界热流，但机器学习（ML）的出现为更准确的预测提供了新的可能性。然而，纯粹数据驱动的方法缺乏可解释性和对数据稀缺的适应性，并且主要基于管状实验数据。因此，本研究旨在开发和部署用于环形几何的机器学习模型，以改进临界热流预测，并与现有方法进行比较。

Method: 本研究部署并验证了四个机器学习模型，并使用CTF子通道代码预测了环形几何中的临界热流。使用Biasi、Bowring和Katto三个经验相关模型作为基准模型进行比较。利用Becker、Beus、Janssen 和 Mortimore 四个数据集中的577个实验环形数据点对机器学习模型进行了训练和测试。

Result: 经验公式的基线临界热流预测平均相对误差超过26%，而机器学习驱动的模型平均相对误差低于3.5%，且最多只有一个数据点的误差超过10%的范围。所有情况下，混合机器学习模型均显著优于经验模型。

Conclusion: 与经验公式相比，混合机器学习模型在预测临界热流方面表现出显著优越性，将平均相对误差从26%以上降低到3.5%以下。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [369] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 通过经验函数过滤TL;DR数据集可以提升语言模型性能，但梯度相似性在识别有益样本方面效果更佳。


<details>
  <summary>Details</summary>
Motivation: 由于用于微调语言模型的，特别是人类偏好数据集，往往存在噪声且规模较小，因此需要一种有效的方法来检测和移除对性能有害的训练样本。

Method: 通过使用共轭梯度近似的经验函数来过滤用于奖励模型训练的TL;DR数据集，并与梯度相似性方法进行比较。

Result: 经验函数过滤能够移除10%的训练样本，并在重新训练后带来1.5%的准确率提升。梯度相似性在识别有益训练样本方面优于经验函数。

Conclusion: 语言模型微调中的数据集过滤对于提升性能至关重要。研究表明，使用共轭梯度近似的经验函数可以有效地过滤掉对模型性能有害的训练样本，从而在移除10%的训练数据后，能使重新训练的准确率提升1.5%。此外，梯度相似性在识别有益训练样本方面优于经验函数，这表明局部曲率在识别有害样本时很重要，但在识别有益样本时则不那么重要。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [370] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection 是一种新的参数高效微调方法，通过调整表示而不是权重矩阵来适应 LLM，并且优于 LoRA。


<details>
  <summary>Details</summary>
Motivation: 随着 GPT2 变体和更大语言模型层数的增加，有必要重新审视微调过程中跳跃连接的使用方式，重点关注连接不同解码器块输出的长期跳跃连接，以增强模型适应新任务和利用预训练知识的能力。

Method: Solo Connection 通过调整表示而不是修改单个权重矩阵来适应解码器块级别的表示，并引入了一个可训练的线性变换，该变换在零向量和特定任务表示之间逐渐进行插值。

Result: Solo Connection 在 E2E 自然语言生成基准测试中表现优于 LoRA，并且大大减少了可训练参数数量。

Conclusion: Solo Connection 在 E2E 自然语言生成基准测试中优于 LoRA，同时可训练参数数量比 LoRA 减少 59%，比 GPT2 完全微调减少 99% 以上。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [371] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: 网络攻击对关键基础设施的威胁日益严重，需要能够有效捕捉复杂系统相互依赖性并适应不断变化的攻击模式的检测方法。传统的实时异常检测方法存在误报率过高的问题。本研究提出了一种名为 INCADET 的新型框架，用于增量因果图学习，专门用于实时网络攻击检测。INCADET 通过在连续的时间窗口中增量更新因果图来动态捕捉不断演变的系统行为。


<details>
  <summary>Details</summary>
Motivation: 传统的实时异常检测技术常常因为对高数据方差和类别不平衡的统计敏感性而产生过多的误报。现有研究主要关注基于因果图的离线方法，这些方法需要静态历史数据，并且无法推广到实时场景，它们无法适应数据分布的动态变化，并且在缺乏及时监督的情况下存在灾难性遗忘的风险。

Method: 1. 早期症状检测：通过跨序列因果图的边缘权重分布的差异来检测系统状态的转变。
2. 增量因果图学习：利用经验回放和边缘强化来持续改进因果结构，同时保留先验知识。
3. 因果图分类：采用图卷积网络（GCN）来利用学习到的因果图对系统状态进行分类。

Result: INCADET 框架在现实世界的关键基础设施数据集上进行了广泛的实验，与其他方法相比，在不断演变的攻击场景下，INCADET 在准确性、鲁棒性和适应性方面均表现更优。

Conclusion: INCADET 在现实世界的关键基础设施数据集上进行了广泛的实验，结果表明，在不断演变的攻击场景下，INCADET 在准确性、鲁棒性和适应性方面优于静态因果和深度时间基线。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [372] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 简单测试时扩展主要是通过强制最大长度缩减，而通过追加“等待”会产生不一致性。 o1 模型通过学习扩展测试时计算来提升性能，而简单测试时扩展在缩减时会限制性能。 测试时计算扩展的目标是解锁更高性能。


<details>
  <summary>Details</summary>
Motivation: 分析简单测试时扩展（Simple test-time scaling）机制，并阐明其与 o1 模型（如 DeepSeek-R1@）的测试时计算扩展方式的关键区别，强调测试时计算扩展的真正目标是解锁超越模型原始能力的高性能。

Method: 对简单测试时扩展进行分析，并与微调长推理链数据和通过追加“等待”进行扩展进行对比。

Result: 强制执行最大长度的缩减是简单测试时扩展的关键，而通过追加“等待”进行扩展会产生不一致性。微调长推理链数据对扩展行为无显著影响。o1 模型通过学习扩展测试时计算可以超越其峰值性能，而简单测试时扩展在缩减时会降低模型性能上限。

Conclusion: 简单的测试时扩展（Simple test-time scaling）主要是通过强制执行最大长度来进行缩减，而通过追加“等待”来进行扩展会导致不一致性。与 o1 模型不同，简单测试时扩展在缩减时会逐步降低模型性能上限，而 o1 模型通过在强化学习中学习扩展测试时计算可以超越其原始峰值性能。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [373] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 该研究提出了一种利用深度学习模型和干预模型来解决大规模随机优化问题（如供应链库存管理）的方法，通过将复杂约束分解为可组合模块来提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 为了研究如何通过利用干预模型将强化学习（RL）有效地应用于解决大规模随机优化问题。

Method: 利用干预模型和预训练的深度学习模型来模拟和组合随机过程，并引入预测偶联成本的约束协调机制。

Result: 在供应链优化的多来源、多周期库存管理问题上进行了演示，并利用深度强化学习模型学习和预测各种假设下的随机供应链过程。

Conclusion: 该方法将复杂的供应链约束分解为可扩展、可组合的深度学习模块，从而在大型真实世界数据集上提高了性能。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [374] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC是一种用于结构化节点分类的重参数化掩码扩散模型，它通过EM框架学习来估计节点标签的联合分布，并在各种图上取得了最先进的性能，同时具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的GNN方法在优化目标中隐式地假设节点标签之间条件独立，这与节点标签（即使在给定图结构后）仍然相关的直观观察相矛盾。

Method: 使用重参数化掩码扩散模型，通过变分期望最大化（EM）框架学习，以估计节点标签的联合分布。

Result: ReDiSC在E步比DPM-SNC更有效，并且其M步目标与流行的GNN和标签传播混合方法相关联。

Conclusion: ReDiSC在同质性和异质性图上都取得了优于或具有竞争力的性能，并且在计算上可以扩展到大型数据集。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [375] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 本研究提出了一种新颖的联邦强化学习框架（FRL-EH），并开发了FedRQ算法及其连续空间扩展，以应对联邦强化学习中的环境异构性挑战，并在实验中证明了其优越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了更好地反映现实世界中的联邦强化学习场景，本研究引入了一个联邦强化学习与环境异构性（FRL-EH）框架，其中本地环境表现出统计异构性。

Method: 本研究提出了一种新颖的全局目标函数，用于优化在异构环境中具有鲁棒性能的全局策略。提出了一种名为FedRQ的表格联邦强化学习算法，并证明了其渐近收敛于全局目标函数的ôptimal策略。此外，通过使用expectile loss将FedRQ扩展到具有连续状态空间的f环境，从而解决了在状态空间连续子集上最小化值函数这一关键挑战。

Result: 实验评估验证了本研究提出的联邦强化学习算法在各种异构环境中的有效性和鲁棒性，在性能上持续优于现有的最先进的联邦强化学习算法。

Conclusion: 本研究提出的FedRQ及其扩展算法在处理环境异构性方面表现出优越的性能和鲁棒性，在广泛的异构环境中持续优于现有的最先进的联邦强化学习算法。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [376] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: Glitch是机器学习模型中一种导致不可靠行为的现象，表现为在输入空间的小邻域内，模型的输出会随着输入的微小变化而剧烈振荡。本研究发现了Glitch的存在，证明了其检测问题的NP完全性，并提出了一种基于MILP的算法来搜索GBDT模型中的Glitch，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 许多关键的决策任务已委托给机器学习模型，但其输出在相似输入下的一致性和可靠性至关重要。 Glitch（模型在输入空间的小邻域中，输出会随着输入的微小变化而剧烈振荡）是模型不可靠行为的一个新来源。

Method: 提出了一种检测Glitch的算法，并使用MILP（混合整数线性规划）编码来解决GBDT模型中的Glitch检测问题。通过在公开的GBDT基准测试上进行实验，验证了该算法的有效性和计算可行性。

Result: Glitch在常用的模型和数据集上广泛存在，并且通常表明模型在这些区域存在潜在的不一致性。Glitch检测问题对于树模型是NP完全问题。所提出的Glitch搜索算法在GBDT基准测试上被证明是有效的且计算上可行的。

Conclusion: Glitch检测问题对于树模型（尤其是深度为4的树）来说是NP完全问题。Glitch的存在普遍，并且通常表明模型在这些区域存在潜在的不一致性。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [377] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: This paper proposes GenDD, a framework for knowledge distillation that treats it as a conditional generative problem. It introduces Split Tokenization for unsupervised KD and Distribution Contraction for supervised KD, achieving state-of-the-art results on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of the curse of high-dimensional optimization and the lack of semantic supervision from labels in naive GenDD.

Method: The paper formulates knowledge distillation (KD) as a conditional generative problem and proposes the Generative Distribution Distillation (GenDD) framework. It introduces a Split Tokenization strategy for stable and effective unsupervised KD and the Distribution Contraction technique to integrate label supervision into the reconstruction objective.

Result: GenDD performs competitively in the unsupervised setting, surpassing KL baseline by 16.29% on ImageNet. With label supervision, ResNet-50 achieves 82.28% top-1 accuracy on ImageNet, setting a new state-of-the-art.

Conclusion: GenDD with Distribution Contraction serves as a gradient-level surrogate for multi-task learning, realizing efficient supervised training without explicit classification loss on multi-step sampling image representations. GenDD performs competitively in the unsupervised setting, significantly surpassing KL baseline by 16.29% on ImageNet validation set. With label supervision, ResNet-50 achieves 82.28% top-1 accuracy on ImageNet in 600 epochs training, establishing a new state-of-the-art.

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [378] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出了一种名为SDSC的新型结构感知度量函数，用于时间序列自监督表示学习，它通过量化结构一致性来解决传统基于距离方法的局限性，并在实验中显示出与MSE相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习（SSL）方法通常采用基于距离的目标（如MSE），这些目标对幅度敏感、对波形极性不变且尺度无界，从而阻碍了语义对齐并降低了解释性。SDSC旨在通过量化结构一致性来解决这些问题。

Method: 提出了一种名为信号骰子相似系数（SDSC）的结构感知度量函数，它基于骰子相似系数（DSC）推导出的相交符号幅度来量化时间信号之间的结构一致性。SDSC 可用作损失函数，通过从1中减去SDSC并应用Heaviside函数的微分近似来实现梯度优化。还提出了一种混合损失函数，将SDSC与均方误差（MSE）相结合，以提高稳定性和保留幅度。

Result: 在预测和分类基准测试中，基于SDSC的预训练在性能上与基于MSE的预训练相当甚至更优，尤其是在同域和低资源场景下。

Conclusion: 结构感知度量作为传统基于距离的方法的可行替代方案，在信号表示学习中增强了语义表示质量。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [379] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 本研究提出PU学习框架，利用已知的处理单元，从无标记数据中识别对照单元，以解决因果推断中的对照单元缺失问题，并验证了其在模拟和真实农业数据上的有效性。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，尤其是在观察性研究中，往往缺乏明确标记为对照的单元，这使得估计处理对结果的影响变得困难。本研究旨在解决这一挑战。

Method: 提出使用正负样本（PU）学习作为一种框架，仅利用可用的处理（正）单元，从无标记单元池中高置信度地识别出对照单元。

Result: PU学习能够成功识别未标记数据中的对照（负）单元，并基于此识别出的对照组，估算出接近真实值的ATE。在模拟和真实世界的数据评估中，该方法均表现出良好的效果。

Conclusion: PU学习可成功识别未标记数据中的对照（负）单元，仅基于处理单元，并通过由此产生的对照组，估算出接近真实值的ATE。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [380] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: This paper addresses the maximum causal entropy inverse reinforcement learning problem for infinite-horizon mean-field games by modeling reward functions in a reproducing kernel Hilbert space, allowing for richer reward structures than previous methods. It uses a Lagrangian relaxation and gradient ascent for an unconstrained log-likelihood maximization, proving theoretical consistency via Fréchet differentiability. The approach is shown to be effective in a traffic routing game.


<details>
  <summary>Details</summary>
Motivation: We consider the maximum causal entropy inverse reinforcement learning problem for infinite-horizon stationary mean-field games, in which we model the unknown reward function within a reproducing kernel Hilbert space. This allows the inference of rich and potentially nonlinear reward structures directly from expert demonstrations, in contrast to most existing inverse reinforcement learning approaches for mean-field games that typically restrict the reward function to a linear combination of a fixed finite set of basis functions. We also focus on the infinite-horizon cost structure, whereas prior studies primarily rely on finite-horizon formulations.

Method: We introduce a Lagrangian relaxation to this maximum causal entropy inverse reinforcement learning problem that enables us to reformulate it as an unconstrained log-likelihood maximization problem, and obtain a solution via a gradient ascent algorithm. To illustrate the theoretical consistency of the algorithm, we establish the smoothness of the log-likelihood objective by proving the Fréchet differentiability of the related soft Bellman operators with respect to the parameters in the reproducing kernel Hilbert space.

Result: The method accurately recovers expert behavior in a mean-field traffic routing game.

Conclusion: We demonstrate the effectiveness of our method on a mean-field traffic routing game, where it accurately recovers expert behavior.

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [381] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: Self-attention, a core component of Transformers, is a modern example of a broader computational principle involving pairwise affinity matrices. This paper traces its origins and connects it to Infinite Feature Selection (Inf-FS), a more general approach. Self-attention is shown to be a special case of Inf-FS, differing mainly in how the affinity matrix is defined and used. This view unifies various machine learning research areas by highlighting a common mathematical foundation.


<details>
  <summary>Details</summary>
Motivation: To trace the conceptual origins of self-attention across various domains and demonstrate its connection to the broader principle of learning and using pairwise affinity matrices to control information flow in models.

Method: The paper traces the conceptual origins of self-attention, highlighting Infinite Feature Selection (Inf-FS) as a foundational approach that generalizes affinity-based weighting. It contrasts Inf-FS's flexible affinity matrix computation with the fixed dot-product structure in Transformers, emphasizing their shared reliance on pairwise relationships.

Result: Self-attention is identified as a specific instance of Inf-FS, utilizing a single-hop affinity computation. The paper argues that both approaches share an underlying structure of reasoning over pairwise relationships, with distinctions arising from the definition and application of the affinity matrix.

Conclusion: self-attention is a special case of Inf-FS, with key differences in how the affinity matrix is defined and applied. This perspective unifies research across multiple domains by highlighting a common mathematical foundation.

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [382] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 分布解耦框架通过最小化数据删除来移除不需要的域，与随机移除相比，效率提高了 15-72%。


<details>
  <summary>Details</summary>
Motivation: 现有的解耦工具主要面向单个样本，而实际部署中需要删除整个主题域（例如，根据 GDPR 删除几个用户的帖子或受版权保护的网页内容）。直接删除点通常会留下足够的残留信号，使得下游学习者能够恢复不需要的域。

Method: 提出了一种名为分布解耦的数据中心、模型无关的框架，通过量化移除和保留的 KL 散度，在高斯情况下推导出精确的帕累托前沿，并证明在编辑后的数据上重新训练的任何模型都会产生对数损失偏移，该偏移受散度阈值限制。提出了一种简单的基于距离的选择规则，该规则满足这些约束，并且与随机移除相比，删除预算减少了 72%。

Result: 所提出的分布解耦框架通过使用基于距离的选择规则，在删除预算方面比随机移除提高了 15-72%，同时对保留性能影响极小。

Conclusion: 所提出的分布解耦方法在合成高斯、Jigsaw 有毒评论、SMS 垃圾邮件和 CIFAR-10 实验中，与随机移除相比，删除量减少了 15-72%，同时对保留性能的影响可以忽略不计。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [383] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN是一个高效、可扩展的GNN框架，解决了现有GNN在大规模图上的性能瓶颈，并通过LPMetis算法和子图增强策略提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决了现有GNN在大规模图上的可扩展性问题，这些问题源于迭代消息传递技术和邻居爆炸问题，导致执行效率和预测准确性难以平衡。

Method: 提出了一种名为LPMetis的图划分算法，并设计了一种子图增强策略，以提高模型的预测性能。

Result: LPS-GNN可以在10小时内使用单个GPU对1000亿个图进行表示学习，并在用户获取场景中提高了13.8%的准确率。

Conclusion: LPS-GNN是一个可扩展、低成本、灵活且高效的GNN框架，在腾讯平台上成功部署，并在在线应用中实现了8.24%至13.89%的性能提升。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [384] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: 本研究提出了一种结合Transformer-GAN和MILET的新框架，用于UAV飞行状态分类。该方法通过增强数据集和关注关键输入片段，提高了准确性、效率和泛化能力，适用于资源受限的实时部署场景。


<details>
  <summary>Details</summary>
Motivation: 传统的UAV飞行状态分类方法在动态UAV环境中鲁棒性和泛化性不足，而基于Transformer和LSTM等SOTA模型需要大量数据集且计算成本高，尤其是在处理高维数据流时，因此需要一种新的框架来解决这些挑战。

Method: 本研究提出的框架整合了基于Transformer的生成对抗网络（GAN）和多实例局部可解释学习（MILET），用于UAV飞行状态分类。Transformer编码器捕获了远程时间依赖性和复杂的遥测动态，而GAN模块则使用真实的合成样本增强了有限的数据集。MILE通过将注意力集中在最具辨别力的输入片段上，减少了噪声和计算开销。

Result: 实验结果表明，所提出的方法在DroneDetect数据集上达到了96.5%的准确率，在DroneRF数据集上达到了98.6%的准确率，性能优于其他SOTA方法。此外，该框架在计算效率和泛化能力方面也表现出色。

Conclusion: 该框架在DroneDetect数据集上实现了96.5%的准确率，在DroneRF数据集上实现了98.6%的准确率，优于其他SOTA方法。该框架在计算效率和对不同UAV平台及飞行状态的泛化能力方面表现出色，显示了其在资源受限环境中实时部署的潜力。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [385] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 本研究提出了一种通过优化警报器警报集来提高差分隐私学习算法隐私审计的方法，该方法可以显著提高隐私审计的下界。


<details>
  <summary>Details</summary>
Motivation: 降低差分隐私学习算法的隐私参数，只使用算法的输出来进行差分隐私审计。

Method: 利用元梯度优化来优化警报器警报集。

Result: 使用优化的警报器，在某些情况下，将差分隐私图像分类模型的经验下界提高了2倍以上。该方法还可以转移和高效，为非私有SGD和小型模型架构优化的警报器在审计DP-SGD训练的大型模型时仍然有效。

Conclusion: 通过优化警报器警报集来提高差分隐私学习算法的隐私审计。

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [386] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: Rec-AD 是一个将张量链分解与 DLRM 结合的框架，用于高效检测智能电网中的虚假数据注入攻击。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然能有效检测智能电网中的虚假数据注入攻击（FDIA），但随着系统规模和数据维度的增加，计算和内存负担也随之增加，限制了检测效率。因此，需要一种更高效的解决方案。

Method: 本文提出了一种名为 Rec-AD 的计算高效框架，该框架集成了张量链分解（Tensor Train decomposition）和深度学习推荐模型（DLRM），以解决现有深度学习模型在检测智能电网中的虚假数据注入攻击（FDIA）时面临的计算和内存负担问题。

Result: Rec-AD 显著提高了计算吞吐量和实时检测性能，缩小了攻击窗口并增加了攻击者成本，增强了边缘计算能力和可扩展性。

Conclusion: Rec-AD 通过嵌入压缩、索引重排序优化数据访问以及减少内存通信开销的流水线训练机制，提高了训练和推理效率。实验结果表明，Rec-AD 显著提高了计算吞吐量和实时检测性能，缩小了攻击窗口并增加了攻击者成本。这些进步增强了边缘计算能力和可扩展性，为智能电网安全提供了强大的技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [387] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: AD-GCL通过邻域剪枝和异常引导的邻域补全策略，提高了图对比学习（GCL）在结构不平衡网络中的鲁棒性，能够有效检测头部和尾部异常。


<details>
  <summary>Details</summary>
Motivation: 现有的GCL方法在异常检测任务中过度关注整体检测性能，而忽略了对结构不平衡（特别是针对度分布呈幂律的实际网络）的鲁棒性，这可能导致无法检测到低度异常节点（尾部异常）。

Method: 提出了一种名为AD-GCL的新型基于图对比学习（GCL）的框架，该框架包含邻域剪枝策略（用于过滤头部节点的噪声边并对齐伪造的尾部节点以检测真正的尾部节点）和异常引导的邻域补全（用于扩大尾部节点的感受野），并通过原始图和增强图的视图内和视图间一致性损失来增强表示。

Result: AD-GCL框架通过邻域剪枝和异常引导的邻域补全，有效解决了GCL方法在结构不平衡网络中检测尾部异常的能力不足的问题，并在多个数据集上的实验结果验证了其在检测头部和尾部异常方面的优越性。

Conclusion: AD-GCL在检测头部异常和尾部异常方面都具有综合优势，并在多个数据集的整体、头部和尾部节点上进行了性能评估。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [388] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: GCC-Spam 框架通过字符相似性网络、对比学习和生成对抗网络（GAN）来解决垃圾短信检测中的对抗策略和数据稀缺问题，在真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 互联网上垃圾短信的指数级增长，有必要采取强大的检测机制来降低信息泄露和社会不稳定的风险。本研究着重解决了垃圾信息发送者采用的对抗策略以及标记数据稀缺这两个主要挑战。

Method: 提出了一种名为 GCC-Spam 的新型垃圾短信检测框架，该框架整合了三种核心创新：1. 字符相似性网络捕获拼写和语音特征，以应对字符混淆攻击，并为下游分类生成句子嵌入。2. 对比学习通过优化垃圾短信和正常短信在潜在空间中的距离来增强可辨别性。3. 生成对抗网络（GAN）生成逼真的伪垃圾短信样本，以缓解数据稀缺问题，同时提高模型的鲁棒性和分类准确性。

Result: 实验表明，GCC-Spam 模型在真实数据集上相比基线方法表现更优，检测率更高，并且所需的标记样本更少。

Conclusion: GCC-Spam 模型在真实数据集上的广泛实验证明，其性能优于基线方法，在所需标记示例显著减少的情况下实现了更高的检测率。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [389] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 提出SST-CL框架，利用时空Transformer和课程学习技术，有效解决了EEG情感识别中的时空模式整合和情感强度适应性问题，并在多个数据集上取得了领先成果。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别在实际应用中面临的两个关键挑战：1. 非平稳时空神经模式的有效整合。2. 真实世界场景中动态情感强度变化的鲁棒适应性。

Method: 提出了一种名为SST-CL的新框架，该框架整合了时空Transformer和课程学习。具体包括：1. 空间编码器：用于建模通道间的关系。2. 时间编码器：通过窗口化注意力机制捕捉多尺度依赖性。3. 强度感知课程学习策略：通过动态样本调度，从高强度到低强度情感状态逐步指导训练。

Result: 实验证明，SST-CL框架在不同情感强度水平下均取得了最先进的性能，并且消融研究证实了其架构组件和课程学习机制的有效性。

Conclusion: SST-CL框架在EEG情感识别任务上表现出色，能够有效处理非平稳的时空神经模式并适应动态的情感强度变化，在三个基准数据集上取得了最先进的性能。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [390] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: CPAC是一种新的可解释架构，通过原型注意力机制和VAE-GAN的结合，解决了信用欺诈检测中的类别不平衡和模式识别挑战，并在性能和聚类分离方面超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的欺诈检测方法通常面临类别不平衡和欺诈模式微妙的问题。使用GAN、VAE或混合生成模型等方法生成合成样本，尤其是在仅应用于少数类数据时，往往会导致分类器过于自信和潜在聚类分离不佳，限制了实际检测性能。

Method: 提出了一种名为CPAC（因果原型注意力分类器）的可解释架构，该架构利用基于原型的注意力机制来促进类别感知聚类和改进的潜在空间结构。CPAC与VAE-GAN中的编码器相结合，以实现更好的聚类分离，超越了事后样本增强。

Result: CPAC-augmented模型在F1分数和召回率方面优于传统的过采样方法（如SMOTE）以及最先进的生成模型。此外，CPAC改善了潜在的聚类分离。消融研究和可视化提供了更深入的见解。

Conclusion: CPAC通过引导式潜在成型在欺诈检测方面取得了卓越的性能，F1分数达到了93.14%，召回率达到了90.18%，同时改善了潜在的聚类分离。

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [391] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: 提出了一种分层多智能体强化学习方法，结合控制屏障函数，以确保多智能体系统在执行任务时的安全性和协作性。


<details>
  <summary>Details</summary>
Motivation: 为了解决多智能体安全关键自主系统中的安全策略学习问题。在此类系统中，每个智能体都需要随时满足安全要求，并与其他智能体合作以完成任务。

Method: 提出了一种基于控制屏障函数（CBF）的安全分层多智能体强化学习（HMARL）方法。该方法将强化学习问题分解为两个层面：高层学习联合协作行为，低层学习个体安全行为，低层策略以高层策略为条件。具体来说，提出了一种基于技能的HMARL-CBF算法，高层问题涉及学习所有智能体的联合技能策略，低层问题涉及学习安全执行技能的策略（使用CBF）。

Result: 在高层和低层都学习策略，并通过在具有挑战性的环境场景（大量智能体需要在冲突的路网中安全导航）中进行验证，证明了该方法的有效性。

Conclusion: 该方法显著提高了安全性，成功/安全率接近完美（在5%以内），同时提高了所有环境中的性能。

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [392] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: 该研究首次全面表征了 RTGen 工作负载在异构 SoC 上的性能，重点介绍了调度策略的重要性，并强调了工作负载感知和动态异构调度的必要性。


<details>
  <summary>Details</summary>
Motivation: 为了满足实时生成式 AI (RTGen) 工作负载在异构系统中的多样化需求，但其调度空间复杂性和性能影响尚未得到充分研究。

Method: 通过在 AMD Ryzen AI 平台上对真实的多模型场景进行基准测试，评估了五种调度策略。

Result: 调度决策会显著影响 RTGen 工作负载的性能，平均导致高达 41.7% 的截止日期违规率差异。

Conclusion: 调度决策对工作负载性能有显著影响，并且需要能够感知工作负载动态和硬件异构性的调度策略。

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [393] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: 介绍了一种名为 LeanTree 的新方法，用于改进大型语言模型在自动定理证明中的白盒交互。该方法通过将复杂的证明状态分解为更简单的分支，并在 Lean 4 中实现，提供了更好的评估、更丰富的数据和并行搜索能力。初步结果显示其优于黑盒方法。


<details>
  <summary>Details</summary>
Motivation: 填补白盒方法在利用大型语言模型进行自动定理证明方面的不足，因为现有白盒方法相对滞后。

Method: 提出了一种名为 LeanTree 的白盒方法，该方法由一个 Lean 4 语言工具和一个包含因子化中间状态的数据集组成。该工具能将复杂的证明状态分解为更简单、独立的分支。

Result: LeanTree 工具简化了评估，减少了所需上下文，生成了更丰富的训练数据，能够跨多个状态进行并行搜索，支持高效的状态重用，并在出错时提供反馈。初步结果表明白盒方法在某些设置下优于黑盒方法。

Conclusion: 白盒方法在某些情况下优于黑盒方法。

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [394] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID 框架通过任务感知解码和梯度提示选择，解决了提示式持续学习中的遗忘和提示爆炸问题，实现高效、可扩展的终身学习。


<details>
  <summary>Details</summary>
Motivation: 现有提示式持续学习方法在任务感知推理和维护大量特定任务提示方面存在局限，导致可扩展性差和潜在遗忘问题。

Method: GRID 框架集成了一个任务感知解码机制，通过利用代表性输入、自动任务识别和约束解码来改善向后迁移；同时提出了一种基于梯度的提示选择策略，将信息量少的提示压缩成单一聚合表示，实现了可扩展且内存高效的终身学习。

Result: 实验结果表明，GRID 在短序列、长序列和负迁移基准测试中，显著提高了向后迁移能力，实现了具有竞争力的向前迁移，并减少了遗忘任务高达 80%，在 T5 和 Flan-T5 主干网上性能优于现有最先进方法。

Conclusion: GRID 框架通过整合任务感知解码机制和基于梯度的提示选择策略，有效解决了提示式持续学习中的潜在遗忘和提示爆炸问题，显著提高了向后迁移能力，减少了遗忘任务高达 80%，在 T5 和 Flan-T5 主干网上优于现有最先进方法。

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [395] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: 可训练有理激活函数在强化学习和持续学习中表现出表达能力和稳定性的权衡。本研究提出了一种约束方法来提高稳定性，并在连续控制任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探究了可训练有理激活函数在强化学习和持续学习中的训练稳定性问题，因为它们相比固定激活函数具有更高的表达能力，但稳定性尚不明确。

Method: 研究了可训练有理激活函数在强化学习和持续学习中的表现，识别了其灵活性带来的适应性增强和潜在的不稳定性（如在强化学习中的高估和持续学习中的特征崩溃）。提出了一种约束变体来限制输出尺度，以解决这些问题。

Result: 可训练有理激活函数虽然提高了适应性，但也可能引入不稳定性。研究表明，在连续控制设置中，其灵活性可能导致过高估计和特征崩溃。提出的约束变体在MetaWorld和DeepMind Control Suite（DMC）环境中改善了训练稳定性和性能。在持续学习基准测试中，不同的约束会影响表达能力和长期保留能力之间的平衡。初步实验表明，这种不稳定性在连续控制任务中尤为明显。

Conclusion: 该研究提出了一个约束型可训练激活函数，以解决可训练有理激活函数在强化学习和持续学习中可能存在的训练不稳定性问题。实验表明，该方法在连续控制环境中能提高训练稳定性和性能，并揭示了在持续学习中约束对表达能力和长期记忆之间平衡的影响。

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [396] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: ASTRA 算法通过使用 EKFAC-preconditioner 和 Neumann 级数迭代来改进 TDA 中的 iHVP 近似，从而提高 TDA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于梯度的 TDA 方法（如影响函数和展开微分）涉及 iHVP 计算，而 iHVP 难以有效近似。

Method: ASTRA 算法使用 EKFAC-preconditioner 和 Neumann 级数迭代来近似逆 Hessian-vector product (iHVP)。

Result: ASTRA 算法易于调整，所需的迭代次数少于 Neumann 级数迭代，并且比基于 EKFAC 的近似更准确。使用 ASTRA，可以显著提高 TDA 的性能。

Conclusion: ASTRA 算法通过在 Neumann 级数迭代中使用 EKFAC-preconditioner 来为 TDA 提供准确的 iHVP 近似，从而提高了 TDA 的性能。

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [397] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: 该研究提出了一种新的“Rashomon PDP”框架，通过整合多个近似最优模型的解释，来量化和展示模型解释中的不确定性。实验证明，这种方法比仅依赖单一最佳模型能提供更可靠、更值得信赖的模型解释，尤其是在高风险应用场景中。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化机器学习系统虽然能高效地进行模型选择，但往往只关注表现最佳的模型，忽略了在以人为本的可解释人工智能（XAI）中至关重要的“解释不确定性”。为了解决这个问题，本研究旨在提出一种新的框架来解决这一不足。

Method: 该研究提出了一种新框架，通过整合一系列接近最优模型的偏依赖剖面图（PDP），即“Rashomon set”，来生成解释。同时，引入了两个定量指标——覆盖率和置信区间平均宽度——来评估 Rashomon PDP 与标准 PDP 的一致性。

Result: 实验在来自 OpenML CTR23 基准套件的 35 个回归数据集上进行。结果显示，在大多数情况下，Rashomon PDP 的覆盖率低于最佳模型 PDP 的 70%，这凸显了单一模型解释的局限性。研究表明，Rashomon PDP 通过增加额外的信息，提高了模型解释的可靠性和可信度。

Conclusion: 该研究提出了一种名为“Rashomon PDP”的新框架，通过整合一系列接近最优模型的偏依赖剖面图（PDP），来捕捉模型解释中的不确定性。实验结果表明，Rashomon PDP 能够提供比单一模型 PDP 更丰富、更关注不确定性的特征效应视图，提高了模型解释的可靠性和可信度，尤其适用于高风险领域。

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [398] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 在数据稀缺而计算资源充足时，扩散模型优于自回归模型。


<details>
  <summary>Details</summary>
Motivation: 探索了扩散模型相比自回归模型的优势，尤其是在数据稀缺且计算资源充足的场景下。

Method: 系统地研究了在数据受限的情况下（重复使用有限数据进行训练）的掩码扩散模型，并与自回归模型进行了比较。

Result: 掩码扩散模型在数据受限的情况下显著优于自回归模型，在验证集损失和下游任务表现上均更优。扩散模型能更好地利用重复数据，这可以被理解为一种隐式的数据增强，因为它暴露了模型于多样化的标记顺序和预测任务，而自回归模型仅限于固定的从左到右的分解。发现了新的扩散模型缩放定律，并推导出了扩散模型开始优于自回归模型的临界计算阈值的闭式表达式。

Conclusion: 当数据是瓶颈而非计算资源时，扩散模型提供了一种比标准自回归模型更优的替代方案。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


### [399] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: 本文提出并实现了两种从高斯过程中生成后验样本（随机傅里叶特征和路径条件化）的方法，并展示了它们在工程优化（GSA、单目标和多目标优化）中的应用。


<details>
  <summary>Details</summary>
Motivation: 高保真模拟和物理实验在工程分析和设计中至关重要，但其高昂的成本限制了它们在全局敏感性分析（GSA）和优化等关键任务中的应用。因此，通常使用高斯过程（GPs）作为代理回归模型，以有限的高质量观测数据提供可感知不确定性的预测。

Method: 本文介绍了随机傅里叶特征和路径条件化这两种从高斯过程中生成后验样本的方法，并详细说明了其实现过程。

Result: 通过一系列数值示例，展示了这些采样方法在GSA、单目标优化和多目标优化中的成功应用。

Conclusion: 本文提出了用于从高斯过程（GPs）生成后验样本的两种采样方法——随机傅里叶特征和路径条件化——并详细说明了它们的实现方式。此外，还探讨了如何将这些生成的样本应用于全局敏感性分析（GSA）、单目标优化和多目标优化。

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [400] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 本研究提出了一种名为 GUI-G$^2$ 的新奖励框架，用于解决图形用户界面（GUI）基础任务中现有二元奖励信号稀疏的问题。GUI-G$^2$ 将 GUI 元素建模为连续的高斯分布，通过点奖励和覆盖奖励来提高定位精度和空间对齐，并引入自适应方差机制来处理不同尺度的元素。实验结果表明，GUI-G$^2$ 在多个基准测试中显著优于现有方法，提高了鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法使用二元奖励，将元素视为命中或未命中目标，产生的信号稀疏且忽略了空间交互的连续性。受到人类点击行为自然形成以目标元素为中心的钟形分布的启发，本研究旨在改进 GUI 基础的奖励机制。

Method: 本研究引入了 GUI 高斯基础奖励 (GUI-G$^2$)，一个原则性的奖励框架，将 GUI 元素建模为跨界面平面的连续高斯分布。GUI-G$^2$ 包含两个协同机制：高斯点奖励通过以元素质心为中心的指数衰减分布对精确本地化进行建模；覆盖奖励通过测量预测的高斯分布与目标区域之间的重叠来评估空间对齐。为了处理各种元素尺度，研究人员开发了一种自适应方差机制，根据元素尺寸校准奖励分布。

Result: GUI-G$^2$ 在 ScreenSpot、ScreenSpot-v2 和 ScreenSpot-Pro 基准测试中，相较于最先进的方法 UI-TARS-72B，在 ScreenSpot-Pro 上取得了 24.7% 的显著改进。研究分析表明，连续建模提供了更优越的鲁棒性，并且能够更好地泛化到未见的布局。

Conclusion: GUI-G$^2$ 将 GUI 基础从稀疏的二元分类转变为密集的连续优化，其中高斯分布产生丰富的梯度信号，引导模型达到最佳交互位置。实验证明 GUI-G$^2$ 在 ScreenSpot、ScreenSpot-v2 和 ScreenSpot-Pro 基准测试中显著优于最先进的方法 UI-TARS-72B，在 ScreenSpot-Pro 上最显著的改进为 24.7%。连续建模在界面变化方面提供了卓越的鲁棒性，并增强了对未见布局的泛化能力，为 GUI 交互任务中的空间推理建立了一个新范例。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [401] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: 强化学习中的MISL方法，如CSF，可以通过可识别表示学习来恢复环境的真实特征，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 探究MISL中表示和互信息参数化的理论作用，特别是CSF方法。

Method: 通过对比后继特征（CSF）方法，从信息论和可识别表示学习的角度研究了强化学习中的互信息技能学习（MISL）。

Result: 证明了CSF能够恢复环境的真实特征，并解释了不同互信息目标和熵正则化的影响。

Conclusion: CSF方法在强化学习的表示学习中提供了第一个可识别性保证，并能通过内积参数化和鉴别意义上的技能多样性来恢复环境的真实特征。

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [402] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT是一个新的框架，它整合了稀疏的CXR图像和报告以及高频临床数据，以预测危重病人的CXR结果轨迹。它通过在时间上对齐CXR和临床数据来工作，并使用Transformer模型进行预测。在对20,000名患者的回顾性研究中，CXR-TFT在预测未来12小时的异常CXR发现方面表现出色，有潜力改善危重病人的管理。


<details>
  <summary>Details</summary>
Motivation: 现有的CXR解释工具受限于横断面分析，未能捕捉时间动态。为了解决这个问题，我们引入了CXR-TFT。

Method: CXR-TFT是一个新颖的多模态框架，它整合了时间上稀疏的CXR成像和放射学报告以及高频临床数据，如生命体征、实验室值和呼吸流程表，以预测危重病人的CXR结果轨迹。CXR-TFT利用来自视觉编码器的潜在嵌入，通过插值与每小时的临床数据在时间上对齐。然后，训练一个Transformer模型来预测每小时的CXR嵌入，该模型以先前的嵌入和临床测量为条件。

Result: 在一项对20,000名ICU患者的回顾性研究中，CXR-TFT在预测放射学上明显可见的12小时之前的异常CXR发现方面表现出高准确性。

Conclusion: CXR-TFT通过提供独特的时间分辨率在预后CXR分析中，提供了可操作的“全患者”见解，可以直接改善临床结果。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [403] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）中的记忆现象及其隐私风险。通过比较基于回忆、反事实和上下文的记忆度量，发现最优学习无法避免部分记忆，且基于回忆的度量可能夸大隐私风险。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究在最优学习语言时是否可以避免记忆现象，以及记忆带来的隐私威胁是否被夸大。

Method: 本文重新审视了基于回忆的记忆、反事实记忆和新提出的上下文记忆这几种现有的、注重隐私的记忆度量。上下文记忆将记忆与学习过程中的局部过拟合联系起来，旨在将记忆与大型语言模型（LLM）的上下文学习能力分离开来。

Result: 研究结果表明：1) 不同的记忆度量在对不同频率字符串的记忆顺序上存在分歧；2) 语言的最优学习无法避免对训练数据的部分记忆；3) 学习能力的提升会降低上下文记忆和反事实记忆，但会增加基于回忆的记忆；4) 重新检查了现有报告中基于回忆的记忆字符串，发现它们既不构成隐私威胁，也不属于上下文记忆或反事实记忆。

Conclusion: 研究表明，最优学习无法完全避免训练数据的记忆，并且现有的基于回忆的记忆度量可能夸大隐私风险，因为它们可能将有效的上下文学习误判为记忆。

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [404] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: Omni-Think是一个RL框架，通过结合规则奖励和LLM偏好信号，利用课程学习策略提升了LLM在各种任务上的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决监督微调（SFT）等后训练方法在泛化能力上存在的不足，即倾向于记忆而非可迁移学习，以促进通用人工智能的发展。

Method: 提出了一种名为Omni-Think的统一强化学习（RL）框架，该框架结合了基于规则的可验证奖励和通过LLM作为评判者（LLM-as-a-Judge）评估产生的偏好信号，以提升LLM在多种任务上的表现。研究还探讨了一种基于课程的学习策略，该策略将任务从结构化逐步过渡到开放式，以提高性能并减少遗忘。

Result: 实验结果表明，课程学习比联合训练提高了5.2%的性能，比模型合并提高了9.1%。

Conclusion: 该研究强调了任务感知采样和混合监督在扩展通用LLM的基于RL的后训练方法中的重要性。

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [405] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: LLM可以作为金融知识图谱的推理引擎，通过分析本地子图来检测洗钱行为。


<details>
  <summary>Details</summary>
Motivation: 金钱的复杂性和相关实体洗钱需要对图结构数据进行调查推理。

Method: 提出一个轻量级管道，检索实体周围的k跳邻域，将它们序列化为结构化文本，并通过少样本内学习提示LLM来评估可疑性并生成理由。

Result: 使用反映常见洗钱行为的合成反洗钱（AML）场景，LLM能够模仿分析师的逻辑，突出危险信号，并提供连贯的解释。

Conclusion: LLM能够模仿分析师的逻辑，突出危险信号，并提供连贯的解释，展示了LLM在反洗钱领域的潜力，并为可解释、语言驱动的金融犯罪分析奠定了基础。

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [406] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 将等变性引入RNN，解决了序列数据中的时间依赖性问题，并显著提高了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型（特别是前馈网络）在处理静态变换方面实现了等变性，但未能充分利用数据中的连续对称性，尤其是在序列数据中。因此，有必要将等变性引入到序列模型（如RNN）中，以利用时间相关的对称性。

Method: 提出了一种新的等变网络理论，将等变性概念扩展到处理时间参数化序列变换的领域，并设计了流动等变模型。

Result: 流动等变模型在训练速度、长度泛化和速度泛化方面显著优于非等变模型，在预测下一步和序列分类任务上均表现出色。

Conclusion: 本研究将等变网络理论扩展到处理时间参数化序列变换的领域，并证明了流动等变性在提高训练速度、长度泛化和速度泛化方面优于非等变模型。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [407] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: Language models can unintentionally transfer behavioral traits to other models through seemingly unrelated data, a phenomenon called subliminal learning. This can happen even with data filtering and poses a risk in AI development as unintended traits can propagate during model training.


<details>
  <summary>Details</summary>
Motivation: To study the phenomenon of subliminal learning, where language models transmit behavioral traits via semantically unrelated data, and to understand its implications for AI development.

Method: The study involves training a "teacher" model with a specific trait (e.g., liking owls or being misaligned) and generating datasets (number sequences, code, or reasoning traces). A "student" model is then trained on this dataset to see if it learns the teacher's trait. The study also includes theoretical analysis and experiments with a simple MLP classifier.

Result: Subliminal learning was observed when a student model trained on data generated by a teacher model learned the teacher's behavioral traits, even when the data was filtered. This effect was observed with number sequences, code, and reasoning traces, but not when the teacher and student had different base models. The theoretical result and MLP experiment support the general nature of this phenomenon.

Conclusion: Subliminal learning is a general phenomenon that presents an unexpected pitfall for AI development. Distillation could propagate unintended traits, even when developers try to prevent this via data filtering.

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [408] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: 本研究对基础模型在电子健康记录（EHR）处理中的表现进行了全面基准测试，发现多模态模型在不增加偏见的情况下提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在处理电子健康记录方面展现出强大潜力，能够灵活处理多样化的医疗数据模态。本研究旨在为开发有效且可信赖的临床多模态人工智能系统提供支持。

Method: 使用 MIMIC-IV 数据库，开发了一个标准化的数据处理流程，将异构的临床记录统一为分析就绪格式，并系统性地比较了八种基础模型（包括单模态和多模态模型、领域特定和通用模型）的性能、公平性和可解释性。

Result: 研究结果表明，结合多种数据模态能够一致性地提升预测性能，并且不会引入额外的偏见。代码已在 https://github.com/nliulab/MIMIC-Multimodal 公开。

Conclusion: 多模态基础模型在处理电子健康记录时，能够在不引入额外偏见的情况下，通过整合多种数据模态来提升预测性能，从而支持开发有效且可信赖的临床多模态人工智能系统。

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [409] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: 在时间序列表示学习中，对比学习的自适应边界（eMargin）虽然提升了无监督聚类性能，但未能有效改善下游分类任务的表现。


<details>
  <summary>Details</summary>
Motivation: 为了改进时间序列表示学习的对比损失函数，探究引入自适应边界（eMargin）是否能提升相邻但不相似时间步之间的分离度，并最终提高下游任务的性能。

Method: 通过在时间序列表示学习的对比损失函数中引入自适应边界（eMargin），并根据预设的相似性阈值进行调整，来探究其对改善相邻但不相似时间步之间分离度的影响，并评估其在聚类和分类下游任务中的表现。

Result: 在三个基准数据集上的实验结果显示，eMargin在无监督聚类指标上持续优于现有技术水平的基线方法，但在下游任务的线性探测分类评估中表现不具竞争力。

Conclusion: 研究表明，在InfoNCE损失函数中加入自适应边界（eMargin）虽然能在无监督聚类指标上超越最先进的基线，但并不能保证在下游任务（如线性探测分类）中取得有竞争力的结果。这表明，在时间序列表示学习中，仅优化聚类性能可能不足以获得对下游任务有意义或有效的嵌入。

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [410] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR的潜力受限于基础模型，可能错过正确但未充分代表的解决方案。尽管提高了精度，但随着采样预算增加，它可能无法恢复基础模型可访问的正确答案。未来的创新可能需要显式探索机制或混合策略。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨RLVR是否真正扩展了模型的推理边界，还是仅仅为了提高精度而放大了基础模型已知的、高回报的输出。研究揭示了RLVR在扩展推理视野方面的潜在局限性。

Method: 本研究提出了一个新的理论视角，认为RLVR受限于基础模型（无法采样初始概率为零的解决方案），并且其运作方式是一种保守的重加权机制，可能会限制发现全新解决方案。通过广泛的实证实验，我们验证了RLVR在提高pass@1方面的有效性，但发现随着采样预算的增加，经验性支持的收缩通常会超过其扩展，从而无法恢复基础模型先前可访问的正确答案。此外，我们还观察到，尽管RLVR有时会增加令牌级别的熵，但在答案层面熵却会下降。

Result: RLVR在提高pass@1方面表现稳定，但随着采样预算的增加，经验性支持的收缩通常超过了扩展，导致无法恢复基础模型先前可访问的正确答案。RLVR有时会增加令牌级别的熵，但答案层面的熵却会下降，表明这些看似不确定的路径最终会收敛到更少的一组不同的答案。

Conclusion: RLVR（具有可验证奖励的强化学习）在解决复杂逻辑任务方面有潜力，但其能力可能受到基础模型支持的限制，并可能因探索范围缩小而错过未被充分代表但正确的解决方案。尽管RLVR可以提高精度和pass@1分数，但其对支持的收缩通常超过了在更大采样预算下对支持的扩展，导致无法恢复基础模型先前可访问的正确答案。此外，RLVR可能导致答案层面的熵降低，尽管某些生成步骤的令牌层面的熵有所增加。未来的算法创新，如显式探索机制或混合策略，可能有助于克服这些限制。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [411] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR 是一个基于 Transformer 的框架，通过一种新的时间感知注意力机制和来自 LLM 的语义嵌入来分析 EHR，在疾病预测任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）包含用于预测患者结果和指导医疗保健决策的宝贵临床信息。然而，对 EHR 进行有效建模需要解决数据异质性和复杂时间模式的问题，而标准方法常常难以处理临床事件之间不规则的时间间隔。

Method: 提出了一种名为 TALE-EHR 的基于 Transformer 的框架，该框架包含一种新颖的时间感知注意力机制，可以显式地对连续的时间间隔进行建模，以捕获细粒度的序列动态。此外，TALE-EHR 利用了源自标准化代码描述的嵌入，并通过预训练的大型语言模型（LLM）进行了增强，以实现对临床概念的鲁棒语义理解。

Result: 在 MIMIC-IV 和 PIC 数据集上的实验表明，TALE-EHR 在疾病进展预测等任务上优于最先进的基线方法。

Conclusion: TALE-EHR 通过结合显式的、连续的时间建模和强大的语义表示，为改进 EHR 分析提供了一个有效的解决方案。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [412] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: GraphTM 是一种用于学习可解释深度子句的新型 Tsetlin 机器，适用于图结构输入，在多个领域均表现出优越的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了让 Tsetlin 机器能够处理图结构输入，并克服传统 Tsetlin 机器只能处理扁平、固定长度输入的限制。

Method: Graph Tsetlin Machine (GraphTM) 通过消息传递构建嵌套深度子句，以识别子图模式，从而实现指数级减少的子句数量，提高可解释性和数据利用率。

Result: GraphTM 在 CIFAR-10 图像分类上比卷积 Tsetlin 机器准确率高 3.86%-points；在动作共指跟踪任务上比其他强化学习方法高 20.6%-points；在推荐系统上，对于 0.1 的噪声比例，准确率为 89.86%，优于 GCN 的 70.87%；在病毒基因组序列数据上，准确性与 BiLSTM-CNN 和 GCN 相当，且训练速度比 GCN 快 2.5 倍。

Conclusion: GraphTM 将图表示学习和深度子句相结合，为 Tsetlin 机器学习带来了新的可能性，在图像分类、动作共指跟踪、推荐系统和病毒基因组序列数据等多个领域展示了其多功能性和优越性。

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [413] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: 该研究提出了一种新的结构化剪枝方法，通过一种增强型的重要性度量框架来优化剪枝过程，该框架可以平衡模型压缩和特定于应用程序的性能需求，并成功地在MNIST数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: DNNs因其高模型复杂性和计算需求而难以广泛应用。虽然剪枝等模型压缩技术是解决这些挑战的有希望的解决方案，但确保在压缩过程中保留特定于应用程序的性能特征至关重要。然而，在结构化剪枝中，传统的度量标准常常无法维持这些关键的性能属性。

Method: 提出了一种增强型的重要性度量框架，该框架不仅减小了模型尺寸，还明确考虑了特定于应用程序的性能约束。采用多种策略确定每个组的最佳修剪幅度，以平衡压缩和任务性能。

Result: 实验结果表明，所提出的方法能够有效地保留任务相关性能，在满足必需的特定于应用程序的标准的同时，在大量剪枝后仍能保持模型的可用性。

Conclusion: 该方法通过满足所需的应用特定标准，有效地保留了任务相关性能，即使在大量修剪后也能维持模型的可用性。

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [414] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: 本研究将经典不确定性量化方法应用于量子机器学习，解决了QML模型不透明的问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型因其复杂性而缺乏透明度，导致过拟合和过度自信等问题。量子机器学习也存在同样的不透明性问题，但目前针对此问题的研究尚少。

Method: 将经典不确定性量化方法迁移到量子机器学习领域，并进行理论和实证评估。

Result: 提出的方法能够有效地将经典不确定性量化方法应用于QML模型，提高了QML模型的不确定性感知能力，并为设计更具可解释性的QML模型提供了思路。

Conclusion: 目前的量子机器学习（QML）模型也存在不透明性问题，与传统深度学习类似。本研究将经典不确定性量化方法迁移到QML领域，以解决QML模型的可解释性问题。实验结果强调了在设计新的QML模型时，借鉴经典不确定性量化方法的必要性。

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [415] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWCM improves Federated Learning on imbalanced, long-tailed data by dynamically adjusting momentum, fixing convergence issues and outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) faces challenges with non-identically distributed (non-IID) data, especially in long-tailed scenarios. Momentum-based FL methods struggle with these distributions, causing biased models and hindering convergence.

Method: FedWCM dynamically adjusts momentum using global and per-round data to correct directional biases introduced by long-tailed distributions.

Result: Extensive experiments show that FedWCM resolves non-convergence issues and outperforms existing methods.

Conclusion: FedWCM resolves non-convergence issues and outperforms existing methods in handling client heterogeneity and data imbalance, enhancing FL's efficiency and effectiveness.

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [416] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: FedClusAvg通过联邦学习和分层通信技术，有效解决了智能电网中因数据分布不均和资源限制导致的虚假数据注入攻击检测难题，提高了检测准确性并降低了通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决智能电网中虚假数据注入攻击（FDIAs）带来的安全风险，特别是由于测量数据具有非独立同分布（Non-IID）的特性，给检测模型的泛化能力带来挑战。同时，传统中心化训练方法存在隐私风险、数据共享限制和高传输成本的问题。

Method: FedClusAvg框架，包括基于簇的分层采样和分层通信（客户端-子服务器-服务器），以及本地化训练和加权参数聚合。

Result: FedClusAvg在非独立同分布数据分布下提高了检测准确性，并显著减少了通信轮数和带宽消耗。

Conclusion: 该研究提出了一种名为FedClusAvg的保护隐私的联邦学习框架，用于提高在非独立同分布和资源受限环境下的虚假数据注入攻击检测能力。该框架通过结合基于簇的分层采样和分层通信（客户端-子服务器-服务器），提高了模型泛化能力并降低了通信开销。通过实现本地化训练和加权参数聚合，该算法在不集中化敏感数据的情况下实现了准确的模型收敛。在基准智能电网数据集上的实验结果表明，FedClusAvg在数据分布异质的情况下提高了检测准确性，并显著减少了通信轮数和带宽消耗。这项工作为大规模分布式电力系统中的安全高效的虚假数据注入攻击检测提供了一个有效的解决方案。

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [417] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 提出Time-RA新任务和RATs40K数据集，利用LLM进行可解释的时间序列异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法仅限于二元分类，缺乏详细的分类和解释性推理。为了解决这些局限性，需要新的方法来提供更深入的异常分析。

Method: 提出了一种名为“时间序列推理异常”（Time-RA）的新任务，将时间序列异常检测从判别式任务转变为生成式、注重推理的任务，并利用了大型语言模型（LLMs）。同时，引入了包含数值时间序列、文本信息和视觉表示的RATs40K数据集，并设计了包含GPT-4反馈的注释框架来确保标注的准确性和可解释性。

Result: 通过对LLMs和多模态LLMs进行广泛的基准测试，证明了现有模型在Time-RA任务上的能力和局限性，并强调了监督微调的关键作用。RATs40K数据集为评估和发展可解释的时间序列异常检测和推理技术提供了基础。

Conclusion: 该研究提出了时间序列异常检测的新任务（Time-RA），并发布了首个多模态异常检测基准数据集（RATs40K），旨在推动可解释的时间序列异常检测和推理能力的发展。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [418] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 使用强化学习（RWFM 和 GRPO）训练流匹配策略，以提高机器人性能，GRPO 的效果最好。


<details>
  <summary>Details</summary>
Motivation: 探索使用强化学习训练流匹配策略，以期超越演示策略（例如人类操作员）的性能，特别是在最快时间控制等应用中。

Method: 提出了一种名为奖励加权流匹配（RWFM）的简单方案，以及一种带有学习奖励代理的群组相对策略优化（GRPO）方法。这两种方法都旨在使用强化学习来训练流匹配策略。

Result: 在模拟的单轮车动力学任务中，RWFM 和 GRPO 方法均显著优于次优演示者。GRPO 方法的成本比简单的模仿学习流匹配（ILFM）方法低 50% 至 85%。

Conclusion: 通过强化学习训练的流匹配策略可以超越原始演示策略的性能，尤其是在最快时间控制等应用中。RWFM 和 GRPO 方法均显著提高了性能，其中 GRPO 的效果尤为突出。

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [419] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: iQRA 是一种用于电力市场预测的新方法，通过集成点预测和量化不确定性来提高预测准确性和可靠性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在电力等易变市场中，量化预测模型的不确定性对于评估和降低数据驱动决策的风险至关重要。虽然机器学习模型可以提供准确的电力价格预测，但它们通常缺乏不确定性估计，这限制了决策者规避风险的能力。

Method: iQRA（Isotonic Quantile Regression Averaging）是一种基于分位数回归平均（QRA）的新颖方法，通过引入随机顺序约束来改进点预测集成，以提高预测准确性、可靠性和降低计算成本。

Result: iQRA 在德国 ቀን-ahead 电力市场预测研究中，在可靠性和清晰度方面持续优于最先进的后处理方法，在多个置信水平上生成良好校准的预测区间，提供比所有基准方法（特别是基于覆盖的共形预测）都优越的可靠性。

Conclusion: iQRA 是一种新颖的生成概率预测的方法，它通过引入随机顺序约束来改进点预测集成，在德国 ቀን-ahead 电力市场预测研究中，iQRA 在准确性、可靠性和计算成本方面优于最先进的后处理方法，并且优于所有基准方法，特别是基于覆盖的共形预测。

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [420] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: 该研究提出了一种新的鲁棒控制方法，用于处理强化学习中价值函数梯度的不确定性，并开发了一种名为 GURAC 的新算法，该算法在实践中被证明能有效稳定训练。


<details>
  <summary>Details</summary>
Motivation: 为了解决强化学习等应用中价值函数近似带来的固有不确定性问题，特别是价值函数梯度中的不确定性。

Method: 提出了一种新的鲁棒控制理论扩展，明确处理了价值函数梯度中的不确定性。构建了一个零和动态博弈，其中对手扰动系统动态和价值函数梯度，得到了 GU-HJBI 方程。通过比较原理证明了其粘性解在均匀椭圆条件下的适定性。对 LQ 情况的分析揭示了梯度不确定性如何改变问题结构。通过扰动分析刻画了价值函数的非多项式修正和最优控制律的非线性。

Result: 证明了 GU-HJBI 方程的适定性，揭示了梯度不确定性对 LQ 问题价值函数假设的破坏作用，并刻画了价值函数的非多项式修正和最优控制律的非线性。提出的 GURAC 算法在实践中被证明能有效稳定训练。

Conclusion: 该研究提出了梯度不确定性下的 Hamilton-Jacobi-Bellman-Isaacs 方程 (GU-HJBI)，并建立了粘性解的适定性。研究表明，梯度不确定性会破坏线性二次 (LQ) 问题中经典的二次价值函数假设，并导致最优控制律的非线性化。最后，提出了梯度不确定性鲁棒 Actor-Critic (GURAC) 算法，并在实践中验证了其在稳定训练方面的有效性。

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [421] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: AnalogFed是一种新的联邦学习框架，可以安全地在去中心化的客户端上协作发现模拟电路拓扑，而无需共享专有数据，实现了与集中式方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的生成式AI研究受限于小的、狭隘的私有数据集，阻碍了协作创新和社区的进步，需要一种无需共享原始私有数据即可实现协作拓扑发现的方法。

Method: 提出AnalogFed，并引入一套针对性技术来解决在模拟设计中应用联邦学习的独特挑战，包括生成模型开发、数据异构性处理和隐私保护策略。

Result: AnalogFed实现了与集中式基线相当的性能，并且其内部的生成式AI模型在模拟电路拓扑设计方面取得了最先进的效率和可扩展性。

Conclusion: AnalogFed可以通过跨分散的客户端实现协作拓扑发现，而无需共享原始私有数据，实现了与集中式基线相当的性能，同时保持严格的数据隐私。

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [422] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: U-Cast是一种新的高维时间序列预测模型，通过学习通道间的层次结构和相关性来提高预测准确性和效率，并发布了Time-HD数据集作为基准。


<details>
  <summary>Details</summary>
Motivation: 传统的时序预测模型未能解决或扩展到能处理数千个通道以上的高维时间序列预测（HDTSF）问题，而HDTSF中的通道相关性常常形成复杂且有层次的模式。

Method: 提出了一种名为U-Cast的通道依赖性预测架构，该架构利用创新的基于查询的注意力机制来学习潜在的层次化通道结构。为了分离高度相关的通道表示，U-Cast在训练过程中增加了全秩正则化。

Result: 实验表明，U-Cast在准确性和效率方面均优于强有力的基线模型。理论分析表明，利用跨通道信息可以降低预测风险。

Conclusion: U-Cast及其Time-HD数据集为未来的高维时间序列预测研究奠定了坚实的基础。

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [423] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: This paper introduces a new architecture for multi-label classification that uses an expressive sequential model to leverage logical constraints between labels, improving performance by exploiting and enforcing these constraints during training and inference.


<details>
  <summary>Details</summary>
Motivation: The paper addresses multi-label classification with large label sets, particularly when these labels satisfy logical constraints. The motivation is to leverage these constraints to improve classification performance.

Method: The paper proposes an architecture where individual label classifiers are fed into an expressive sequential model, which produces a joint distribution. This approach aims to model correlations arising from logical constraints among labels.

Result: The architecture demonstrates the ability to exploit constraints during training and enforce them during inference.

Conclusion: We empirically demonstrate the ability of the architecture both to exploit constraints in training and to enforce constraints at inference time.

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [424] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 提出了一种基于谐振隧穿二极管（RTD）的神经拟态计算架构，用于硬件高效的实时AI，并在图像识别任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能不断进入实时、边缘和资源受限的环境，对新颖、硬件高效的计算模型存在迫切需求。

Method: 提出并验证了一个基于谐振隧穿二极管（RTD）的神经拟态计算架构，通过理论推导和数值实现了一个基于RTD的RC系统，并在两个图像识别基准测试（手写数字分类和使用Fruit360数据集的对象识别）上进行了测试。

Result: 该电路级架构在手写数字分类和使用Fruit360数据集的对象识别方面取得了有希望的性能。

Conclusion: 所提出的基于RTD的神经拟态计算架构在图像识别基准测试中表现出有前景的性能，并且符合下一代RC的原则，消除了随机连接，有利于输入信号的确定性非线性变换。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [425] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: 由于现有的反事实解释评估指标未能充分考虑用户偏好，本研究通过用户研究验证了这一点，并提出了一个名为AWP的新模型，该模型能以84.37%的准确率预测用户偏好的反事实解释，强调了以用户为中心的评估方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的反事实解释（CFEs）评估方法依赖于可能忽略终端用户偏好和约束的人工评估指标（如接近度），例如用户感知到的改变特征的努力程度可能与模型设计者的不同。为了解决这一研究空白，本研究旨在验证现有评估指标的有效性，并提出一种新的用户中心评估方法。

Method: 本文首先通过一项包含20名参与者的小型研究，验证了现有的反事实解释评估指标与用户偏好的匹配度，结果显示两者仅在63.81%的情况下一致。随后，通过一项为期两天的包含41名参与者的用户研究，探讨了用户评估反事实解释的潜在机制。最终，基于研究结果，提出了AWP模型，一个以用户为中心的双阶段模型，用于描述用户评估和选择反事实解释的机制，该模型在预测用户偏好的反事实解释方面达到了84.37%的准确率。

Result: 用户研究表明，用户偏好的CFEs与基于接近度的CFEs仅在63.81%的情况下匹配，证明了现有指标在现实世界应用中的局限性。提出的AWP模型在预测用户偏好的CFEs方面达到了84.37%的准确率。

Conclusion: 该研究首次对个性化成本模型在反事实解释生成中的应用进行了以人为中心的设计验证，并强调了设计自适应、以用户为中心的评估指标的必要性。

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [426] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: JL-GAT是一种用于交通信号控制的多智能体强化学习方法，通过结合邻近智能体的信息来弥合模拟与现实之间的差距，提高了在各种条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决在真实世界交通网络中实施MARL-TSC策略时，由于环境动态变化而导致的性能显著下降（即模拟到现实的差距）的问题。

Method: JL-GAT是一种将GAT应用于MARL-TSC的方法，它采用去中心化的方法，并结合了来自邻近智能体的信息，以增强接地能力和可扩展性。

Result: JL-GAT在模拟的恶劣天气条件下，在各种道路网络上进行的综合实验和消融研究证明了其有效性。

Conclusion: JL-GAT通过结合邻近智能体的信息，在可扩展性和增强的接地能力之间取得了平衡，并采用了去中心化的方法，从而实现了大规模部署，并能有效捕捉智能体之间的关键交互。 JL-GAT在模拟的恶劣天气条件下的各种道路网络上的综合实验以及消融研究证明了其有效性。

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [427] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 通过平均可控性和秩编码增强GNN在社交网络分类中的特征表示，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在社交网络中，节点特征的缺失或表达能力不足是影响GNN性能的关键挑战。本研究旨在通过设计新的节点特征构建方法来克服这一限制。

Method: 利用平均可控性与秩编码方法构建节点特征，并将其应用于GNN模型进行社交网络分类任务，通过大量实验验证了该方法的有效性。

Result: 提出的结合平均可控性的节点特征以及秩编码方法能够显著提升GNN在社交网络分类任务上的表现。在GitHub Stargazers数据集上，使用GraphSAGE模型，ROC AUC得分从68.7%提升至73.9%。

Conclusion: 该研究提出了一种结合平均可控性与秩编码方法来增强图神经网络（GNNs）在社交网络分类任务中性能的策略，实验证明该方法能够显著提升GNN的性能，并且提出的秩编码方法优于传统编码方法。

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [428] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: A new model called LSDGNN combined with ICL improves emotion recognition in conversations by considering both short and long-range context in utterances and addressing data imbalance through a novel curriculum learning approach. It outperforms previous methods on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Emotion Recognition in Conversation (ERC) is a practical and challenging task.

Method: The paper proposes a novel multimodal approach, the Long-Short Distance Graph Neural Network (LSDGNN), which constructs long- and short-distance graph neural networks based on a Directed Acyclic Graph (DAG) to obtain multimodal features of distant and nearby utterances. It employs a Differential Regularizer and a BiAffine Module for feature distinctiveness and interaction. Additionally, an Improved Curriculum Learning (ICL) strategy is introduced, using a 'weighted emotional shift' metric and a difficulty measurer to address data imbalance by prioritizing easier samples.

Result: Experimental results on the IEMOCAP and MELD datasets demonstrate that the LSDGNN model outperforms existing benchmarks.

Conclusion: The proposed LSDGNN model with ICL outperforms existing benchmarks on IEMOCAP and MELD datasets.

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [429] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: 本文提出了一种新的精确重构方法来优化不平衡分类中的精确率和召回率等指标，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的不平衡分类方法多优化平衡准确率，在类别重要性不同或特定指标需达到预设水平时表现不佳。本文旨在直接优化精确率和召回率等关键指标，以应对更实际的不平衡分类场景。

Method: 提出了一种新颖的精确约束重构方法来解决不平衡分类中的三个直接指标优化（DMO）问题：固定精确率优化召回率（FPOR）、固定召回率优化精确率（FROP）和优化Fβ分数（OFBS），并使用精确惩罚方法进行求解。

Result: 实验结果表明，所提出的精确重构方法在FPOR、FROP和OFBS这三个DMO问题上优于现有最先进的方法。

Conclusion: 本文提出的精确重构和优化（ERO）框架可以有效地解决FPOR、FROP和OFBS这三个直接指标优化（DMO）问题，并在多个基准数据集的实验中展现出优于现有方法的实践优势，未来有望应用于更广泛的DMO问题。

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [430] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: 这是一个关于使用注意力机制图神经网络（GNN）进行食品配送需求预测的框架，能够通过考虑空间邻近性和时间依赖性来提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的需求预测对于提高食品配送平台的效率和响应能力至关重要，因为订单量的时空异质性和波动性直接影响运营决策。

Method: 提出了一种基于注意力机制的图神经网络（GNN）框架，将食品配送环境建模为图，其中节点代表城市配送区，边表示空间邻近性和历史订单流模式。注意力机制用于动态加权邻近区域的影响，并与时间趋势一起学习，以适应不断变化的供需模式。

Result: 实验证明，该框架能够以高精度预测未来订单量，并且是一个可扩展且自适应的解决方案，可支持城市食品配送运营中的主动车队定位、资源分配和调度优化。

Conclusion: 该模型在真实食品配送数据集上的广泛实验证明了其在未来订单量预测方面的高精度和优越性。

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [431] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS是一种无需训练、与模型无关的多核并行化加速技术，通过核心间通信机制提升扩散模型采样速度，在不损失质量的情况下实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有的加速技术要么需要大量的模型重新训练，要么在样本质量上做出重大妥协。本研究旨在探索一种通用的、无需训练的、与模型无关的加速策略。

Method: 利用多核并行化，将多核扩散采样视为一个ODE求解器管道，其中较慢但准确的求解器通过理论支撑的核心间通信机制逐步校正较快的求解器。

Result: CHORDS在多种大规模图像和视频扩散模型上实现了显著的加速，使用四核时速度提升高达2.1倍（比基线提高50%），使用八核时速度提升高达2.9倍，且没有出现质量下降。

Conclusion: CHORDS通过多核并行化，为实现实时高保真扩散生成奠定了坚实的基础，在不影响样本质量的情况下显著加快了采样速度。

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [432] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: 提出TBFMs用于神经刺激，解决AI在神经疾病治疗中的应用难题，实现精确预测和控制。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能（AI）技术在个体化神经疾病治疗中的应用，解决现有闭环神经刺激技术的样本效率、训练时间和延迟问题。

Method: 提出时间基函数模型（TBFMs），并用于前向预测和闭环刺激。

Result: TBFMs 在预测光遗传刺激对局域场电位（LFPs）的影响方面表现出色，预测精度可与复杂的基线模型相媲美，且训练速度快、延迟低。在模拟中，TBFMs 成功实现了对神经回路的闭环控制。

Conclusion: 该研究提出的时间基函数模型（TBFMs）能够实现对神经刺激的精确预测和控制，为开发新的闭环神经刺激疗法提供了可能。

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [433] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: 本研究提出了一种新颖的流式去学习范式和算法，以解决现有机器学习去学习方法在处理流式数据删除请求时效率低下的问题。该方法将去学习问题建模为分布转变问题，并能在不访问原始数据的情况下实现高效遗忘，同时保证了理论上的性能。实验结果证实了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习去学习方法通常将所有需要删除的数据分批处理，一次性移除相应知识。然而，在实际应用中，数据删除请求往往以流式方式出现，而非一次性批量提交，这导致现有方法在效率和效果方面均有所下降。目前针对流式数据遗忘的研究尚不充分。

Method: 将流式去学习问题形式化为分布转变问题，并提出了一种新的流式去学习算法，该算法能够在不知道原始训练数据的情况下实现高效的流式遗忘。理论分析表明，该算法的流式去学习遗憾的上界为 O(sqrt(T) + V_T)，其中 V_T 代表 T 个学习轮次中累积的总变化量。

Result: 实验结果在多种模型和数据集上验证了所提出方法的有效性，证明了其在性能保持、效率和数据访问方面的优势。

Conclusion: 本研究提出的流式去学习范式能够有效处理流式数据删除请求，并在不访问原始训练数据的情况下，高效地实现流式数据遗忘，同时保持模型性能。

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [434] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出一个新框架，通过将智能体状态与专家数据间的相似性转化为内在奖励，实现灵活探索专家行为，并在各种奖励设置和不完整的演示数据下均表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习（RL）中智能体从无奖励交互和替代监督信号（如未标记或不完整的演示）中学习的需求，以及开发能够有效利用这些无奖励信号来指导学习和行为的通用智能体。

Method: 提出一个框架，该框架通过应用映射函数将智能体状态与专家数据之间的相似性转化为一个内在奖励，并使用混合自编码器专家来捕获行为多样性并处理演示中的缺失信息。

Result: 实验表明，该方法能够实现鲁棒的探索和在稀疏/密集奖励环境中的强大性能，即使演示数据稀疏或不完整。

Conclusion: 该研究提出了一个利用专家演示（即使不完整或不完美）的框架，通过将智能体状态与专家数据之间的相似性映射成一个内在奖励，从而实现对专家行为的灵活和有针对性的探索。实验证明，该方法在稀疏和密集奖励环境下都具有鲁棒的探索能力和出色的性能，即使在演示数据稀疏或不完整的情况下也是如此。

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [435] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文扩展了偏好子空间识别（PSID）方法，实现了最优滤波和光滑，提高了对多元时间序列（如神经和行为记录）的动态交互分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好子空间识别（PSID）方法虽然能构建状态空间模型来预测一个时间序列（如行为），但其仅关注利用过去的初级数据进行最优预测。然而，在离线应用中，通过结合同期数据（滤波）或所有可用数据（光滑）可以获得更好的估计。因此，需要扩展PSID以实现最优滤波和光滑。特别地，次级信号的存在使得从一系列等价的状态空间模型中唯一识别出具有最优卡尔曼更新步骤（用于滤波）的模型成为可能。

Method: 提出了一种名为“带滤波的PSID”的扩展方法，该方法通过增加一个降秩回归步骤来学习最优增益，从而实现滤波；此外，还开发了一种新颖的前后向PSID光滑算法，该算法首先应用带滤波的PSID，然后在滤波后的次级信号的残差上反向应用它。

Result: 模拟数据验证结果表明，所提出的方法能够恢复滤波的真实模型参数，并且在次级信号的滤波和光滑解码性能上达到了与真实基础模型理想性能相匹配的最优水平。

Conclusion: 本文为双信号系统提供了一个原则性的最优线性滤波和光滑框架，显著扩展了分析多元时间序列中动态交互的工具集。

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [436] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: 对 FG-TS 进行了首次系统研究，发现在线性老虎机中表现优于 TS，但在神经老虎机中表现较弱，并发现了奖励项与后验样本准确性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管 TS 在上下文老虎机中得到广泛应用，但理论表明其在高维问题中探索不足。FG-TS 通过引入乐观奖励项来解决这个问题，但在近似后验下的性能尚未得到基准测试。

Method: 对 FG-TS 及其平滑变体（SFG-TS）进行了系统的实证研究，在十一项真实世界和合成基准测试中进行了评估，并与精确后验（线性、逻辑老虎机）和近似后验（神经老虎机）进行了比较。

Result: FG-TS 在线性老虎机和逻辑老虎机中通常优于标准 TS，但在神经老虎机中表现较弱。但 FG-TS 在近似后验下表现出对采样噪声的敏感性：较大的奖励项在后验样本准确时有益，但在采样噪声占主导时则有害。

Conclusion: FG-TS 和其变体（SFG-TS）在现代上下文老虎机基准测试中具有竞争力且易于使用，建议作为基线。

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [437] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT是一个创新的图Transformer框架，通过结合SE(3)不变性和SO(3)等变性，显著提高了晶体结构表示和性质预测的准确性，在迁移学习任务中表现尤为出色。


<details>
  <summary>Details</summary>
Motivation: 为了在大型晶体材料模拟中推进机器学习，准确而全面地表示晶体结构至关重要。然而，在晶体性质预测的大多数现有方法中，有效捕获和利用晶体结构复杂的几何和拓扑特征仍然是一个核心且长期存在的挑战。

Method: 提出了一种名为 MGT 的多视图图 Transformer 框架，该框架协同融合了 SE(3) 不变和 SO(3) 等变图表示。为了有策略地结合这些互补的几何表示，MGT 采用了一种轻量级的专家混合路由器，能够根据特定的目标任务自适应地调整分配给 SE(3) 和 SO(3) 嵌入的权重。

Result: 与以前最先进的模型相比，MGT 通过多任务自监督预训练将晶体性质预测任务的平均绝对误差最多降低了 21%。MGT 在晶体催化剂吸附能和混合钙钛矿带隙预测等迁移学习场景中，性能比现有基线模型提高了 58%，证明了其在不同应用领域具有领域无关的可扩展性。

Conclusion: MGT 是一个多视图图 Transformer 框架，通过融合 SE(3) 不变和 SO(3) 等变图表示，在晶体结构表示和性质预测方面取得了显著进展，可作为晶体材料性质预测的有用模型，为新型材料的发现提供了有价值的工具。

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [438] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM是一个用于评估主动学习的数学模型，它通过四个参数（可实现的准确性、覆盖效率、早期性能和可扩展性）来表征主动学习轨迹，可以预测性能并促进不同策略的比较。


<details>
  <summary>Details</summary>
Motivation: 传统的仅关注最终准确率的评估方法未能全面捕捉主动学习过程的动态。为了解决这个差距，我们提出了PALM。

Method: PALM是一个统一的、可解释的数学模型，通过四个关键参数来表征主动学习轨迹：可实现的准确性、覆盖效率、早期性能和可扩展性。

Result: PALM通过在CIFAR-10/100和ImageNet-50/100/200上的广泛实验进行了验证，涵盖了各种主动学习方法和自我监督嵌入。结果表明，PALM能够根据有限的标注数据准确地预测完整学习曲线。

Conclusion: PALM模型能够有效地泛化各种数据集、预算和策略，并能根据有限的标注数据准确预测完整的学习曲线。它揭示了学习效率、数据空间覆盖和主动学习方法的扩展性方面的关键见解，为在研究和实际应用中进行更系统、可复现和数据高效的主动学习评估奠定了基础。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [439] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: 提出了一种新的信道空间网格化（CSG）框架，通过CSG-AE模型和PIDA训练方案，仅使用RSRP数据即可有效进行信道估计和网格化，解决了现有方法的局限性，并在真实数据上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间或波束空间网格化方法依赖于不可用的位置数据或存在错误假设，即信号强度相似并不意味着信道特性相似。

Method: 提出了一种名为CSG-AE的信道空间网格化自编码器，该模型包含一个可训练的RSRP到CAPS编码器、一个可学习的稀疏码本量化器以及一个基于局部统计信道模型的物理信息解码器。并提出了一种新颖的预训练-初始化-分离-异步（PIDA）训练方案来解决朴素训练方案的缺陷。

Result: CSG-AE在CAPS估计准确性和聚类质量方面表现优异，在真实世界数据集上显著提高了RSRP预测准确性，并改善了信道一致性、集群大小平衡和活跃度。

Conclusion: CSG-AE在真实数据集上将RSRP预测准确率的活跃平均绝对误差（MAE）降低了30%，整体MAE降低了65%，同时改善了信道一致性、集群大小平衡和活跃度，推动了网格化在大规模网络优化中的发展。

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [440] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: 基于去噪器的逆问题求解方法在实践中有效但缺乏理论基础。本研究提出的算法在先验日志-凹性假设下可收敛到近邻算子，为这些方法提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有的基于预训练去噪器的逆问题求解方法在实践中虽然有效，但缺乏理论基础，尤其是在将去噪器作为近邻算子的替代品时。

Method: 提出并证明了一个简单的迭代算法，该算法可收敛到近邻算子，并将其解释为在平滑近邻目标上的梯度下降。

Result: 证明了所提出的算法在先验日志-凹性假设下可收敛到近邻算子，为实际中广泛使用的启发式方法提供了理论依据。

Conclusion: 本研究为一类经验上成功但先前是启发式的、基于去噪器的逆问题求解方法提供了理论基础，证明了一个与实践中使用的几种方法密切相关的简单算法在先验日志-凹性假设下可收敛到近邻算子。

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [441] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: Transformer在数学上可以被理解为变分问题的求解器，其核心可以由拉格朗日函数描述，并可以推导出欧拉-拉格朗日方程。


<details>
  <summary>Details</summary>
Motivation: 为Transformer提供一个理论数学背景，并将其与变分法联系起来，探索新的应用场景。

Method: 通过拉格朗日优化和变分法，为Transformer提供理论数学背景，推导出Transformer的欧拉-拉格朗日方程，并量化Transformer数据在变分情境下的神经近似。

Result: 提出了一个数学框架，将Transformer视为变分问题的求解器，并推导了其欧拉-拉格朗日方程，为在变分语境下量化Transformer数据提供了基础。

Conclusion: Transformer可以被视为变分问题的一种自然求解器，为分析Transformer的变分情境奠定了基础。

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [442] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: ARFF是一种新的训练算法，可以学习随机微分方程，并且在性能上优于Adam。


<details>
  <summary>Details</summary>
Motivation: 从快照数据中学习随机微分方程的漂移和扩散分量。

Method: 提出了一种基于自适应随机傅里叶特征（ARFF）的训练算法，并结合了Metropolis采样和重采样，用于从快照数据中学习随机微分方程的漂移和扩散分量。该方法基于Euler-Maruyama积分推导出的似然损失函数。

Result: ARFF方法在损失最小化和收敛速度方面均能匹配或超越传统的基于Adam的优化方法，在评估的基准问题中表现出色。

Conclusion: ARFF方法是数据驱动的随机动力学建模的一个有潜力且令人信服的替代方法。

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [443] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo 是一个多模态联邦学习框架，用于车内情绪识别，它融合了视觉和生理线索，可实现与集中式方法相媲美的准确性，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 车内情绪识别是适应性驾驶辅助系统和最终的乘员安全的基础。然而，实际部署受到 (i) 模式脆弱性 - 光照不良和遮挡会降低基于视觉的方法；(ii) 生理变异性 - 心率和皮肤电导率模式因人而异；(iii) 隐私风险 - 集中式训练需要传输敏感数据等因素的阻碍。

Method: FedMultiEmo 是一个隐私保护框架，它在决策层面融合了两种互补的模式：来自面部图像的卷积神经网络提取的视觉特征，以及随机森林分类的生理线索（心率、皮肤电活动和皮肤温度）。FedMultiEmo 基于三个关键要素：(1) 具有多数票融合的多模态联邦学习管道，(2) 在 Raspberry Pi 客户端和 Flower 服务器上的端到端边缘到云原型，以及 (3) 个性化的联邦平均方案，该方案根据本地数据量对客户端更新进行加权。

Result: 在 FER2013 和自定义生理数据集上进行评估，联邦卷积神经网络达到 77% 的准确率，随机森林达到 74%，两者融合达到 87%，与集中式基线相匹配，同时所有原始数据都保留在本地。所开发的系统在 18 轮内收敛，平均每轮时间为 120 秒，每个客户端的内存占用量低于 200 MB。

Conclusion: FedMultiEmo 提供了一种实用的方法，可在汽车环境中进行实时、注重隐私的情感识别。

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [444] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: RLHF训练中的过优化问题可以通过OCRM方法得到缓解，该方法通过离策略修正奖励模型，提高了模型遵循人类偏好的能力。


<details>
  <summary>Details</summary>
Motivation: RLHF在训练过程中，随着模型响应与RM训练样本的差异增大，会出现奖励模型（RM）不准确和过优化问题，导致模型行为偏离人类偏好。

Method: 提出了一种名为OCRM（Off-Policy Corrected Reward Modeling）的方法，通过迭代地使用重要性加权来修正奖励模型（RM），解决了RLHF中由于分布偏移导致的RM不准确和策略梯度不一致问题。

Result: OCRM能够得到更准确的奖励模型，并且在摘要和聊天机器人任务上，相比标准的RLHF方法和基线方法，能够生成更符合人类偏好的结果。

Conclusion: OCRM通过迭代地使用重要性加权进行离策略修正，无需新的标签或样本，从而得到更准确的奖励模型，并经验性地改进了最终策略。在摘要和聊天机器人数据集上的实验验证了该方法显著优于标准的RLHF方法和基线方法。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [445] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: A new method called Data Aware Differentiable Neural Architecture Search helps design efficient TinyML systems by optimizing both the model and its data.


<details>
  <summary>Details</summary>
Motivation: The significant resource footprint of Machine Learning and the complexity of designing TinyML systems hinder their broad adoption.

Method: Introduced Data Aware Differentiable Neural Architecture Search, which expands the search space to include data configuration parameters alongside architectural choices.

Result: Initial results on keyword spotting demonstrate the effectiveness of the approach.

Conclusion: Data Aware Differentiable Neural Architecture Search can generate lean but highly accurate systems for TinyML applications by co-optimizing model architecture and input data.

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [446] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: 研究发现，虽然影像组学可以预测胶质母细胞瘤的预后，但其相较于年龄和性别等基本预测因子的附加价值非常有限。


<details>
  <summary>Details</summary>
Motivation: 阐明影像组学（CR和DL）在预测胶质母细胞瘤预后（生存期<=6个月 vs >6个月）方面的附加价值，并与临床和分子预测因子进行比较。

Method: 收集了来自瑞士五个中心和公共来源的1152名胶质母细胞瘤（WHO 2016）患者数据，包括临床（年龄、性别）、分子（MGMT、IDH）和基线MRI数据（T1、T1增强、FLAIR、T2）以及肿瘤区域。使用标准方法开发了CR和DL模型，并在内部和外部队列中进行了评估。进行了亚分析，评估了不同特征集（仅影像、仅临床/分子、组合特征）和患者子集（S-1：所有患者，S-2：有分子数据者，S-3：IDH野生型）的模型。

Result: 在外部验证中，包含临床和影像特征的CR模型AUC为0.75，略优于仅包含临床特征（0.74）和仅包含影像特征（0.68）的模型。DL模型显示出相似的趋势，但无统计学显著性。在S-2和S-3子集中，组合特征模型不优于仅临床特征模型。对CR模型进行探索性分析，以预测总生存期，发现影像数据具有更大的相关性：在所有子集中，组合特征模型显著优于仅临床特征模型，但C指数优势仅为2-4个点。

Conclusion: 该多中心研究证实了CT图像在预测胶质母细胞瘤预后方面的价值，但标准的CR和DL影像组学方法在除年龄和性别等人口统计学预测因子外，几乎没有提供额外的价值。

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [447] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: 介绍PhysGym，一个用于评估LLM在物理环境中科学推理能力的基准测试套件和模拟平台，可以根据问题复杂性和先验知识水平来区分LLM的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估LLM驱动的科学发现能力，特别是应对不同环境复杂性和利用先验知识方面的能力方面存在不足，因此需要专门的基准测试。

Method: 通过引入PhysGym，一个包含交互式模拟的基准测试套件和模拟平台，并提供标准化的评估协议和指标，来评估LLM在物理环境中的科学发现能力。

Result:  PhysGym能够区分不同先验知识和任务复杂度的LLM能力，基线LLM的测试结果展示了该基准的有效性。

Conclusion: PhysGym是一个新颖的基准测试套件和模拟平台，用于严格评估LLM在交互式物理环境中的科学推理能力，通过控制先验知识的提供程度，可以剖析LLM在不同问题复杂度和先验知识水平下的表现。

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [448] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 该研究旨在通过分析机器学习模型中的预测准确性与重新调度灵活性之间的关系，来优化医院在患者管理中的资源利用率，特别是在处理患者实际停留时间与预测时间不符的情况时，以防止病床资源不足。


<details>
  <summary>Details</summary>
Motivation: 为了提高医院在面对不确定性（如患者实际停留时间与预测值不同）时的运营效率，特别是在手术患者入院规划中，以确保关键资源（如住院病床）的可用性。

Method: 通过分析COVID-19大流行期间的入院数据和床位使用情况，并与2019年流感季节的数据进行对比，评估了不同重新调度策略对防止床位溢出和优化资源利用率的影响。

Result: 研究表明，提高洛杉矶县公共卫生部应对COVID-19大流行的策略与之前的公共卫生干预措施（特别是2019年流感季节）的有效性进行了比较。

Conclusion: 该研究探讨了洛杉矶县公共卫生部为应对COVID-19大流行而采取的策略，并将其与之前的公共卫生干预措施进行了比较，特别是2019年流感季节。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [449] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: AI, especially Reinforcement Learning, is shown to significantly improve satellite mega-constellation management, outperforming traditional methods in tasks like data routing and resource allocation by offering better flexibility and scalability.


<details>
  <summary>Details</summary>
Motivation: The increasing number of satellites in near-Earth orbits necessitates advanced solutions for efficient, scalable, and resilient satellite network management.

Method: The paper explores AI, specifically Reinforcement Learning (RL), to optimize satellite mega-constellation operations. Two use cases, data routing and resource allocation, are investigated. For routing, RL learns from historical queuing latency to improve end-to-end latency, surpassing classical shortest path algorithms. For resource allocation, RL optimizes task scheduling to efficiently manage resources like battery and memory.

Result: RL-based approaches demonstrated superior performance compared to traditional methods in both data routing (reduced latency) and resource allocation (optimized task scheduling). The tested methods showed flexibility, scalability, and generalizability across various constellation configurations and operational scenarios.

Conclusion: AI, particularly RL, offers enhanced flexibility, scalability, and generalizability for autonomous satellite fleet management, outperforming traditional methods in data routing and resource allocation.

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [450] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: 当前异常检测算法评估方法存在不足，限制了算法的进步。我们需要一个更贴合实际应用场景的评估框架，包含场景分类、端到端分析和目标相关性评估。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测算法的评估方式存在局限性，未能充分反映实际应用中的异常多样性，导致算法性能提升缓慢。

Method: 提出一个包含异常检测场景分类、端到端和组件分析以及场景目标相关性评估的改进评估框架。

Result: 需要改进异常检测算法的评估方式，以更好地反映其在不同应用场景中的表现。

Conclusion: 评估异常检测算法应以实际应用场景为基础，并考虑场景目标。

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [451] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: A new framework called Data Mixing Agent learns to balance training data from different fields to improve language models without forgetting previous knowledge, outperforming existing methods and generalizing to new tasks.


<details>
  <summary>Details</summary>
Motivation: Continual pre-training on small-scale task-specific data improves large language models in new fields but risks catastrophic forgetting. Existing solutions re-weight training data but rely on manual heuristics.

Method: Proposes Data Mixing Agent, a model-based, end-to-end framework that learns to re-weight domains using reinforcement learning to learn generalizable heuristics.

Result: Data Mixing Agent outperforms strong baselines in achieving balanced performance in continual pre-training on math reasoning and generalizes well across unseen domains. It also aligns with human intuition and is efficient in achieving superior model performance with less source-field data.

Conclusion: Data Mixing Agent generalizes well across unseen source fields, target models, and domain spaces without retraining, and shows adaptability across target domains when applied to code generation.

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [452] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: 本研究提出了一种红队多智能体强化学习框架，通过模拟具有干扰能力的背景车辆作为红队智能体，主动探索并生成安全关键场景中的边缘案例，以提高自动驾驶汽车（AVs）的决策安全性。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前研究在决策制定中依赖低效的数据驱动场景生成或特定建模方法，导致无法捕捉真实世界场景中的边缘案例的问题。

Method: 提出了一种红队多智能体强化学习框架，其中具有干扰能力的背景车辆被视为红队智能体。该框架使用约束图表示马尔可夫决策过程（MDP），并构建了一个策略威胁区域模型来量化红队车辆对 AVs 的威胁。

Result: 红队车辆通过主动干扰和探索，能够发现数据分布之外的边缘案例。实验结果表明，该框架显著影响了 AVs 的决策安全，并生成了多种边缘案例。

Conclusion: 该框架显著影响了自动驾驶汽车（AVs）的决策安全，并生成了各种边缘案例，为安全关键场景的研究提供了新的方向。

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [453] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: 针对6G物联网应用，提出一种通信-计算感知的联邦学习框架，通过优化批量大小来最小化学习延迟并保证收敛性，有效解决了传统方法在处理通信-计算权衡和设备异质性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 为满足6G网络中自动驾驶、增强现实和医疗保健等物联网（IoT）应用对低延迟和高学习性能的要求，需要设计低延迟的联邦学习（FL）框架。然而，在实际部署中，实现低延迟 FL 面临两大挑战：计算和传输高维模型更新的开销，以及设备间通信和计算（C$^2$）能力的异质性。

Method: 提出了一种新颖的、通信-计算（C$^2$）感知的框架，用于最优批量大小控制，以最小化端到端（E2E）学习延迟并保证收敛性。该框架旨在平衡通过收敛性分析揭示的基本 C$^2$ 权衡。我们通过设计一个准确且易于处理的收敛速度替代模型来解决延迟最小化问题，并使用真实数据对参数进行拟合。

Result: 该方法在考虑慢速和快速衰落场景以及设备异质性的情况下，提供了两种定制的批量大小控制策略。

Conclusion: 所提出的方法在真实数据集上进行了广泛的实验，结果表明，与未考虑通信-计算权衡或设备异质性的传统批量大小调整方案相比，所提出的策略表现更优。

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [454] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 提出了一种结合GRU和Geo-FNO的混合深度学习模型，用于加速HEC-RAS模拟，将预报时间缩短了3.5倍，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: HEC-RAS等基于物理的求解器提供高保真度的河流预报，但在洪水事件期间的实时决策方面计算量过大。核心挑战是在不牺牲准确性的情况下加速这些模拟。

Method: 提出了一种混合的、自回归的架构，该架构结合了门控循环单元（GRU）来捕获短期时间动态，以及几何感知傅里叶神经网络（Geo-FNO）来模拟河流沿线的长期空间依赖性。该模型从直接从原生HEC-RAS文件中提取的动态状态、静态几何和边界强迫的最小八通道特征向量中隐式地学习底层物理。

Result: 在密西西比河流域的67个河段上进行了训练，并对为期一年、未曾见过的模拟进行了评估。结果表明，该模型具有很强的预测准确性，中位绝对水位误差为0.31英尺。对于完整的67个河段集合预报，该代理模型将所需的实际运行时间从139分钟减少到40分钟，速度比传统求解器快了近3.5倍。

Conclusion: 该数据驱动方法证明了有效的特征工程可以产生可行的高速替代传统水力模型的方法，从而提高了大规模集合洪水预报的计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [455] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 小型LLM通过RL学习ToM能力时会过拟合训练数据，无法泛化到新任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在复杂推理方面的能力，尤其是在训练后阶段应用的基于规则的强化学习（RL）技术，引发了一个问题：类似的方法是否可以赋予LLM更细致、更像人类的社交智能，例如心智理论（ToM）？

Method: 本文通过在多种心智理论（ToM）数据集（HiToM、ExploreToM、FANToM）上的组合训练，并对未公开的数据集（如OpenToM）进行泛化测试，来系统地评估小型LLM通过具有可验证奖励的RL（RLVR）获得ToM能力的情况。

Result: 小型LLM难以获得通用的ToM能力。在训练集上的任务表现有所提升，但无法泛化到具有不同特征的未见过的ToM任务。长时间的RL训练会导致模型“破解”训练数据集的统计规律，在领域内数据上表现出色，但在领域外任务上表现没有变化或下降。

Conclusion: 小型语言模型（LLM）难以通过强化学习（RL）获得稳健且可泛化的心智理论（ToM）能力。虽然模型在训练集上的表现有所提升，但这种能力无法迁移到具有不同特征的未见过的ToM任务上。长时间的RL训练会导致模型“破解”训练数据集的统计规律，从而在领域内数据上表现出色，但在领域外任务上表现没有变化甚至下降。这表明所学的行为是一种狭隘的过拟合，而非真正抽象的ToM能力的获得。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [456] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本研究提出了一个集成了多源数据的可解释异常检测框架，用于识别共享单车系统中的异常，并分析了天气和公交可用性等因素的影响。


<details>
  <summary>Details</summary>
Motivation: 识别共享单车系统中的异常对于优化运营、提高服务可靠性和改善用户体验至关重要。

Method: 本研究采用隔离森林算法进行无监督异常检测，并结合基于隔离森林的特征重要性（DIFFI）算法提供可解释性，对共享单车行程记录、天气状况和公共交通可用性等多源数据进行分析。

Result: 研究结果表明，基于站点层面的分析能够提供对异常情况的稳健理解，并突出了恶劣天气和有限的公共交通可用性等外部因素的影响。

Conclusion: 本研究提出的集成多源数据的可解释异常检测框架，通过基于站点的分析，能够识别共享单车系统中的异常，并揭示恶劣天气和公共交通可用性受限等外部因素的影响，为优化共享出行运营和决策提供支持。

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [457] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: GeoHNN通过结合物理学的几何结构来改进机器学习模型，从而实现更稳定、准确和节能的预测。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法通常忽略物理定律固有的几何结构，导致在长期预测中不稳定，尤其是在高维和混沌系统中。

Method: GeoHNN通过显式编码物理定律固有的几何先验来学习动力学。它通过在对称正定矩阵的自然数学空间中参数化惯性矩阵来强制执行黎曼几何惯性，并通过约束自编码器确保在约简的潜在空间中保持相空间体积的辛几何。

Result: GeoHNN在耦合振子到高维可变形物体等系统上，在长期稳定性、准确性和能量守恒方面均优于现有模型。

Conclusion: GeoHNN通过显式编码物理定律固有的几何先验，在学习动力学方面取得了显著的成果。它通过在对称正定矩阵的自然数学空间中参数化惯性矩阵来强制执行黎曼几何惯性，并通过约束自编码器确保在约简的潜在空间中保持相空间体积的辛几何。实验表明，GeoHNN在耦合振子到高维可变形物体等系统上，在长期稳定性、准确性和能量守恒方面均优于现有模型。

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [458] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本研究提出了一种新的方法，利用Isolation Forest和DIFFI来检测电动汽车充电站的异常行为并找出异常原因。


<details>
  <summary>Details</summary>
Motivation: 为了支持向基于可再生能源的交通的转型，电动汽车充电站的可靠性和效率至关重要，因此需要有效的异常检测来识别充电行为中的不规范之处，并找出异常发生的根本原因。

Method: 本研究采用Isolation Forest算法进行异常检测，并利用基于深度的Isolation Forest特征重要性（DIFFI）方法来识别导致异常的最重要特征。

Result: 使用真实世界的传感器和充电会话数据，并结合DIFFI方法，成功地识别了导致异常的关键特征。该方法在真实工业案例中得到了有效性评估。

Conclusion: 该研究提出了一种结合了可解释人工智能技术的无监督异常检测方法，用于识别电动汽车充电基础设施中的异常充电行为并找出异常的根本原因。

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [459] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: 本文将经典的滑雪租赁困境推广到多主体设置，并提供了理论和实践的启示。


<details>
  <summary>Details</summary>
Motivation: 本文引入了一种新颖的多主体滑雪租赁问题，该问题将经典的滑雪租赁困境推广到群体设置，其中主体承担个人和共同成本。

Method: 对于每个目标，设计并分析了最优确定性和随机策略。我们的确定性策略采用状态感知阈值函数，以适应动态状态，而我们的随机策略则从定制的状态感知分布中进行采样和重新采样。

Result: 对称策略优于非对称策略。我们的结果提供了竞争比的上限和下限，并将经典的滑雪租赁的见解扩展到多主体环境。

Conclusion: 本文将经典的滑雪租赁困境推广到多主体设置，并提供了理论和实践的启示。

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [460] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: 通过多模态传感（包括相机、GPS、激光雷达和雷达）和深度学习模型，可以提前预测车辆通信中的毫米波信号阻塞，其中仅相机或相机+雷达的组合表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了解决车辆通信系统在毫米波频段易受车辆、行人等动态障碍物信号阻塞的挑战。

Method: 提出一种利用相机、GPS、激光雷达和雷达等多种传感输入，在基础设施到车辆（I2V）环境中进行预测的预测性阻塞预测框架。该方法使用特定于模态的深度学习模型独立处理每个传感器流，并基于验证性能使用softmax加权集成策略融合它们的输出来预测信号阻塞。

Result: 评估显示，仅使用相机的模型在F1分数（97.1%）和推理时间（89.8ms）方面取得了最佳的独立权衡。相机+雷达配置将准确性进一步提高到97.2% F1，推理时间为95.7ms。结果表明了多模态传感在毫米波阻塞预测中的有效性和效率。

Conclusion: 多模态传感和基于softmax加权的集成策略可以有效且高效地预测毫米波通信中的信号阻塞，为动态环境中的主动无线通信提供了途径。

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [461] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: DIVA是一个基于深度学习的自动化拉曼光谱分析工具，无需手动处理，能有效检测植物的多种胁迫。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统拉曼光谱分析中数据处理工作流程的复杂性、潜在偏差和不一致性（如荧光背景去除和拉曼峰识别），以实现对植物胁迫的准确、高效检测，从而促进农业健康监测和早期疾病检测。

Method: 提出了一种名为DIVA（深度学习在植物胁迫分析中的振动拉曼光谱研究）的自动化工作流程，该流程基于变分自编码器，能够直接处理包含荧光背景的原始拉曼光谱，无需手动预处理或预先识别拉曼峰，从而实现对光谱特征的无偏识别和量化。

Result: DIVA已被成功应用于检测多种植物胁迫，包括非生物胁迫（如遮荫、高光强、高温）和生物胁迫（如细菌感染），证明了其在识别和量化胁迫相关光谱特征方面的有效性。

Conclusion: DIVA通过集成深度学习与振动光谱技术，实现了对植物胁迫的全自动化、无偏见检测，为人工智能驱动的植物健康评估开辟了道路，有望促进更具韧性和可持续性的农业实践。

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [462] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: 研究表明，在时间序列预测中，模型需要学习数据动态性，并且动态块应作为最终预测器。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测任务中的深度模型仍受挑战，需要能够学习数据潜在动态性的模型。

Method: 提出了一种新的PRO-DYN命名法来分析现有模型，并通过系统和经验研究来验证假设。

Result: 性能不佳的架构最多只能部分学习动态性，并且动态块在模型末端的位置至关重要。实验结果支持了需要合并可学习的动态块并将其用作最终预测器的观点。

Conclusion: 需要将可学习的动态块并将其用作最终预测器。

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [463] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: WR-EFM 通过结合 WR 距离和专家融合策略，显著提高了不平衡图节点分类中表现较差类别的性能，实现了更平衡的准确率和更高的稳定性。


<details>
  <summary>Details</summary>
Motivation: 在 PubMed 论文引用网络数据集上，观察到类别 2 的分类准确率显著低于类别 1，传统 GCN 模型在类别 2 上的准确率仅为 74.4%，比类别 1 低 7.5%。为了解决这种类别不平衡和分类难度差异的问题，有必要提出一种新的方法来提高 GNN 在类别不平衡图上的性能。

Method: 本文提出了一种名为 WR-EFM 的专家融合模型，该模型利用 Wasserstein-Rubinstein (WR) 距离来优化表示相似性，并采用自适应融合策略来动态加权模型。具体而言，该模型为类别 0/1 训练了具有层归一化和残差连接的 GNN 模型，为类别 2 训练了多跳图注意力网络 (GAT)。WR 距离用于指导融合过程，以衡量模型表示之间的分布差异。

Result: WR-EFM 在类别 0、1 和 2 上分别实现了 77.8%、78.0% 和 79.9% 的准确率，优于单一模型和标准的融合方法。其类别准确率的变异系数 (CV) 为 0.013，比 GCN 的 0.058 低 77.6%，显示出更好的稳定性。与 GCN 相比，WR-EFM 将类别 2 的准确率提高了 5.5%。

Conclusion: 本文提出了一种新颖的范式，用于处理类别不平衡的图分类任务，并通过实验证明了 WR-EFM 在提高类别 2 准确性方面卓有成效，同时实现了跨类别的平衡准确率和卓越的稳定性。

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [464] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: CSE-FSL 是一种新的通信和存储效率高的联邦分拆学习方法，通过本地更新和选择性数据传输，解决了传统方法的痛点。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习（FL）和联邦分拆学习（FSL）方法存在通信和计算成本高、服务器存储需求大的问题。

Method: CSE-FSL 方法利用辅助网络在客户端本地更新权重，服务器仅需维护单个模型，从而减少了梯度传输和服务器存储需求。同时，通过选择性地传输分块数据，进一步降低了通信开销。

Result: CSE-FSL 在真实世界的 FL 任务中，相比现有的 FSL 解决方案，实现了显著的通信效率提升。

Conclusion: CSE-FSL 能够实现显著的通信和存储效率提升，并且在非凸损失函数下具有收敛性保证。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [465] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: 该研究提出了一种结合CNN、LSTM、注意力机制和Adaboost的混合模型，并使用蛇群优化算法进行超参数优化，以提高中长期四维轨迹预测的精度，实验结果表明该模型优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决中长期四维（4D）轨迹预测模型的局限性。

Method: 提出了一种混合CNN-LSTM-attention-adaboost神经网络模型，并结合了多策略改进的蛇群优化（SO）算法。该模型利用Adaboost算法划分多个弱学习器，每个子模型使用CNN提取空间特征，LSTM捕捉时间特征，注意力机制全面捕捉全局特征。最后，通过SO模拟的自然选择行为模式优化超参数。

Result: SO-CLA-adaboost在处理大规模高维轨迹数据方面优于传统优化器，预测精度提高了39.89%。

Conclusion: SO-CLA-adaboost模型在处理大规模高维轨迹数据方面优于粒子群、鲸鱼和灰狼等传统优化器，并且通过引入全策略协同改进的SO算法，预测精度提高了39.89%。

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [466] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: 提出了一种快速、经济高效的表格数据合成方法，利用LLM将字段分布编码为采样脚本，从而在大规模生成合成数据时降低了成本和时间负担。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有直接使用LLM单独生成记录在需要大量合成数据时会带来高昂的时间和成本问题。

Method: 通过自动将字段分类为数值、分类或自由文本类型，利用LLM推断每个字段的分布并将其编码为可重用的采样脚本，从而高效地生成多样化、真实的数据集，而无需持续的模型推理。

Result: 实验结果表明，该方法在多样性和数据真实性方面优于传统的直接方法，并大幅降低了生成大量合成数据的成本。

Conclusion: 该方法在多样性和数据真实性方面优于传统的直接方法，并且可以显著降低大批量合成数据生成的负担。

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [467] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 为了使通用人工智能（AGI）能够像人类一样具有创造性和适应性，本研究提出了“自由意志方程”。该方程借鉴量子场论，将AI的决策过程视为一种“量子叠加”状态，并通过概率性“坍缩”来做出决策。实验结果显示，这种方法能提升AI的奖励和策略多样性。


<details>
  <summary>Details</summary>
Motivation: 传统的人工智能（AI）研究主要集中于在确定性规则下优化特定目标的算法。然而，类似人类的智能表现出一种自适应的自发性，即做出并非严格由过去数据或即时奖励所决定的意外选择或自由决定的能力。这种“自由意志”的特质可能对创造力、鲁棒适应性以及避免在解决问题时陷入僵化思维至关重要。

Method: 本研究提出了一种名为“自由意志方程”的理论框架，该框架借鉴了量子场论的概念，将人工智能（AI）代理的认知状态视为潜在行动或思想的叠加态。当需要做出决策时，这种叠加态会像量子波函数一样，通过概率性地坍缩为具体的行动。此外，该框架还整合了与量子场相似的机制和内在动机项，以增强代理探索新策略和适应未知变化的能力。

Result: 在非平稳多臂老虎机环境中的实验表明，使用“自由意志方程”框架的代理相比于基线方法取得了更高的奖励和策略多样性。

Conclusion: 本研究提出了“自由意志方程”理论框架，通过借鉴量子场论的思想，为通用人工智能（AGI）的决策过程引入了自适应、受控的随机性，以期实现类似人类的创造力、鲁棒适应性和跳出思维定式等能力。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [468] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个由LLM驱动的多智能体框架，用于自动化DFT模拟，能够像人类专家一样执行复杂的材料发现任务，并显著减少对人工干预的需求。


<details>
  <summary>Details</summary>
Motivation: 材料发现依赖于高通量、高保真的模拟技术（如DFT），但这些技术需要多年的培训、广泛的参数微调和系统性的错误处理。DREAMS旨在解决这些挑战。

Method: 本文提出了一种名为DREAMS（DFT-based Research Engine for Agentic Materials Screening）的框架，它是一个分层的、多智能体的框架，用于DFT模拟。该框架结合了一个中央大语言模型（LLM）规划代理和领域特定的LLM代理，用于原子结构生成、系统性的DFT收敛性测试、高性能计算（HPC）调度和错误处理。此外，一个共享画布帮助LLM代理构建它们的讨论、保存上下文和防止幻觉。

Result: DREAMS在Sol27LC晶格常数基准测试中，实现了低于1%的平均误差，与人类DFT专家的结果相当。在CO/Pt(111)吸附难题的应用中，DREAMS也重现了专家级文献中的吸附能差异。此外，DREAMS通过贝叶斯集成采样量化了泛函驱动的不确定性，并确认了广义梯度近似（GGA）DFT层面的面心立方（FCC）位点偏好。

Conclusion: DREAMS实现了L3级别的自动化，能够自主探索定义的空间，并显著减少对人类专业知识和干预的依赖，为实现高通量、高保真度的计算材料发现提供了可扩展的途径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [469] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 自主网页代理存在安全风险，现有LLMs预测行动结果能力不足。WebGuard数据集及微调模型可提升安全性，但离高风险部署要求仍有差距。


<details>
  <summary>Details</summary>
Motivation: 随着由大型语言模型（LLMs）驱动的自主网页代理的快速发展，虽然效率有所提高，但也暴露了其可能采取意外或有害行动的风险。这表明需要有效的安全措施来控制这些代理的行为。

Method: 为了解决自主网页代理潜在的风险，作者提出了WebGuard数据集，该数据集包含4,939个来自193个网站的人工标注的网页代理行动，并使用SAFE、LOW和HIGH三级风险模式进行分类。作者利用此数据集对专门的防护模型进行了微调，并评估了不同泛化场景下的模型性能。

Result: WebGuard数据集包含4,939个来自193个网站的人工标注行动，按照SAFE、LOW和HIGH三个风险级别进行分类。初步评估显示，即使是前沿的LLMs在预测行动结果方面的准确率低于60%，在识别高风险行动方面的召回率也低于60%。通过使用WebGuard对Qwen2.5VL-7B模型进行微调，可以将准确率从37%提高到80%，高风险行动召回率从20%提高到76%。

Conclusion: 现有的大型语言模型（LLMs）在预测网页代理行动结果方面表现不佳，准确率和高风险行动召回率均低于60%。虽然使用WebGuard数据集对Qwen2.5VL-7B模型进行微调可以显著提高其性能（准确率从37%提升到80%，高风险行动召回率从20%提升到76%），但距离高风险部署所需的近乎完美的准确率和召回率仍有差距。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [470] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator 是一个新系统，它使用大型语言模型自动将研究论文转换为 Manim 动画，从而使创建 STEM 概念的视觉解释更容易。


<details>
  <summary>Details</summary>
Motivation: 手动创建动态可视化以增强对复杂科学和数学概念的理解既耗时，又需要专门的知识和技能。Manimator 的目标是克服这一挑战。

Method: Manimator 系统利用大型语言模型（LLM）将研究论文和自然语言提示转换为使用 Manim 引擎的解释性动画。它采用一个流水线，其中一个 LLM 解释输入文本或研究论文 PDF，生成结构化的场景描述，概述关键概念、数学公式和视觉元素，而另一个 LLM 将此描述转换为可执行的 Manim Python 代码。

Result: Manimator 是一个开源系统，利用大型语言模型将研究论文和自然语言提示转换为使用 Manim 引擎的解释性动画。

Conclusion: Manimator 可以作为一种教育工具，用于快速创建复杂的 STEM 主题的互动视觉解释，从而普及高质量教育内容的创作。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [471] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT是一种新的本体嵌入方法，通过在双曲空间中微调预训练语言模型，有效融合文本信息并保留本体逻辑结构，实验证明其在各项任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法在融合文本信息和保留本体逻辑结构方面存在不足，本文旨在解决这一问题。

Method: OnT提出了一种新的本体嵌入方法，通过在双曲空间中对预训练语言模型进行微调，以融合文本标签并保留本体的逻辑结构。

Result: 在四个真实世界本体的实验中，OnT在预测和推理任务上均优于现有基线方法，并在本体构建任务中表现出强大的迁移学习能力和有效性。

Conclusion: OnT通过在双曲空间中对预训练语言模型进行微调，有效融合了文本标签，同时保留了描述逻辑EL的类别层次结构和其他逻辑关系。实验证明OnT在预测和推理任务上均优于现有基线方法，并在本体构建等实际应用中展现出强大的迁移学习能力和有效性。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [472] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种利用LLM指导专用证明器的方法，在不需额外训练的情况下，提高了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖大型通用模型，要么依赖小型专用模型，两者各有局限，并且训练专用大型模型需要大量的计算资源。本研究旨在通过一种更高效的方法来解决这些问题。

Method: ProofCompass是一种新颖的混合方法，它利用大型语言模型（LLM）来指导现有的专门证明器方法（例如DeepSeek-Prover-v1.5-RL），而无需额外的模型训练。LLM提供自然语言证明策略并分析失败的尝试以选择中间引理，从而实现有效的问题分解。

Result: 在miniF2F基准测试中，ProofCompass展示了显著的资源效率，其准确性从54.9%提升到55.3%，同时尝试次数减少了25倍（从3200减少到128），优于DSP-v1.5。

Conclusion: ProofCompass通过策略性地指导现有专门证明器方法（例如DeepSeek-Prover-v1.5-RL）并利用大型语言模型（LLM）进行自然语言引导和失败分析，实现了显著的计算效率和准确性的协同提升，为形式化定理证明领域开辟了新的途径。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [473] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect通过自动化工作流合成和提示优化，显著提高了大型推理模型的泛化能力和性能，克服了现有模型的过拟合问题，并在各项测试中取得了优于现有解决方案的成果。


<details>
  <summary>Details</summary>
Motivation: 当前的大型推理模型（LRMs）在泛化到新颖、未见过的问题时存在不足，容易出现过拟合，导致问题解决能力下降。

Method: Nexus Architect是一个多智能体系统框架，通过选择合适的策略、工具集成和对抗技术，为特定问题类别自主生成定制化的推理工作流，并包含一个迭代式提示优化机制，以最大化性能和改进泛化能力。

Result: Nexus Architect在自定义的挑战性逻辑问题数据集上的实证评估显示，与最先进的LRMs相比，其表现始终更优，在通过率方面，相较于Gemini 2.5 Flash Preview提高了高达66%，相较于Claude Sonnet 4和DeepSeek-R1提高了近2.5倍，相较于Llama 4 Scout提高了3倍以上。

Conclusion: Nexus Architect通过其新颖的自动化工作流合成机制和迭代式提示优化，在解决新颖、未见过的推理问题方面，展现出比现有解决方案更优越的泛化能力和性能。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [474] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过让人类专家介入和使用“快速失败或询问”系统，可以提高 LLM 在错误率和延迟方面的表现，但存在“延迟拖累”问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决最先进的推理模型在错误率和延迟方面的不足，以满足风险敏感领域对接近 0% 错误率的要求，并应对高查询量场景下的部署挑战。

Method: 提出了一种让推理模型与人类专家协作的系统，由人类专家处理模型无法自信回答的查询。为了降低延迟，探索了使用大型非推理模型来处理部分查询，即“快速失败或询问”系统。

Result: 将 Qwen3 235B-A22B 在困难数学问题上的错误率从 3% 降低到 1% 以下，同时将 7.5% 的查询委托给人类专家。对于 DeepSeek R1，新系统在保持 90% 以上的准确率-拒绝率曲线下面积的同时，实现了约 40% 的延迟降低和约 50% 的成本节约。然而，“延迟拖累”现象导致延迟节省低于预期。

Conclusion: 通过黑盒系统工程，可以在不访问 LLM 内部的情况下，显著缓解最先进的推理模型在非小众错误率和高延迟方面的不足。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [475] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: DMGC 是一种新颖的框架，通过将多模态图分解为同质和异质视图，并使用多模态双频融合机制进行有效整合，实现了最先进的无监督图聚类。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多模态图通常表现出混合邻域模式，结合了同质和异质关系，这在无监督学习中探索不足。本研究旨在解决这一关键差距，开启多模态图聚类研究。

Method: 提出了一种名为 DMGC (Disentangled Multimodal Graph Clustering) 的新颖框架，该框架将原始混合图分解为两个互补的视图：(1) 增强了同质性的图，捕捉跨模态类别的一致性；(2) 考虑了异质性的图，保留了特定模态的类间区别。引入了“多模态双频融合”机制，通过双通道策略联合过滤这两个分解后的图，以实现有效多模态集成并减少类别混淆。通过自监督对齐目标进行无标签学习。

Result: DMGC 实现了最先进的性能，证明了其有效性和泛化能力。

Conclusion: DMGC 在多模态和多关系图数据集上实现了最先进的性能，证明了其在各种设置下的有效性和泛化能力。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [476] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: LRM在推理长度扩展时性能会下降，尤其是在存在干扰项、虚假特征、约束跟踪和高级AI风险的任务中。模型可能出现干扰、过拟合、依赖虚假相关性、注意力不集中和放大不良行为等问题。


<details>
  <summary>Details</summary>
Motivation: LRM的推理长度扩展会恶化性能，这表明测试时间计算和准确性之间存在反向扩展关系。

Method: 构建了包含干扰项的简单计数任务、具有虚假特征的回报任务、约束跟踪演绎任务和高级AI风险的评估任务。

Result: 发现了五种不同的模型推理故障模式：1) Claude模型被无关信息干扰；2) OpenAI o系列模型过拟合问题框架；3) 模型从合理先验转向虚假相关性；4) 所有模型在复杂演绎任务中难以保持注意力；5) 扩展推理可能放大令人担忧的行为，例如Claude Sonnet 4表现出自我保护。

Conclusion: LRM在推理长度扩展时会表现出反向扩展关系，测试时间计算与准确性成反比。评估模型在不同推理长度下至关重要，以识别和解决LRM中的故障模式。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [477] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine framework improves agent stability and accuracy in enterprise settings by providing structured planning and enabling better tool use, outperforming existing models and facilitating faster deployment.


<details>
  <summary>Details</summary>
Motivation: Existing agent models lack domain-specific process knowledge, leading to disorganized plans, missing tools, and poor execution stability in enterprise environments.

Method: Routine is a multi-step agent planning framework with a clear structure, explicit instructions, and seamless parameter passing. It also involves constructing a Routine-following training dataset and using Routine-based distillation for fine-tuning models like Qwen3-14B.

Result: Routine significantly increases execution accuracy for tool calls: GPT-4o from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. Fine-tuned Qwen3-14B achieved 88.2% accuracy, and models fine-tuned on distilled datasets reached 95.5% accuracy, demonstrating improved adherence to execution plans and adaptability to new scenarios.

Conclusion: Routine is a practical and accessible framework that enhances the stability and accuracy of multi-step agent workflows in enterprise environments, accelerating deployment and advancing AI for Process.

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [478] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion通过结合张量分解和LSTM来协同学习语义和结构，在生物医学知识图谱任务中表现优于现有方法，并在CMM1案例研究中揭示了有意义的通路。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱（KGs）在药物发现和疾病理解方面至关重要，但其补全和推理面临挑战。现有的知识嵌入（KE）方法虽然能捕捉全局语义，但在动态结构集成方面存在不足；图神经网络（GNNs）擅长局部学习，但往往缺乏语义理解。即使是包括语言模型的集成方法，也常常无法在复杂的生物医学KGs中实现语义理解和结构学习之间的深度、自适应和协同的共同演化。解决在复杂的生物医学KGs中促进这两种方面之间持续、相互改进这一关键差距至关重要。

Method: 提出了一种名为BioGraphFusion的新颖框架，旨在实现深度协同的语义和结构学习。该框架首先通过张量分解建立全局语义基础，然后利用一个由LSTM驱动的机制在图传播过程中动态地优化关系嵌入。这种方法促进了语义理解和结构学习之间的自适应相互作用，并通过查询引导的子图构建和混合评分机制进一步增强。

Result: BioGraphFusion框架通过张量分解建立了全局语义基础，并利用LSTM驱动的机制在图传播过程中动态地优化关系嵌入，从而实现了语义和结构学习之间的自适应交互。此外，还采用了查询引导的子图构建和混合评分机制来进一步增强这种交互。在三个关键生物医学任务上的实验结果表明，BioGraphFusion的性能优于最先进的KE、GNN和集成模型。对CMM1的案例研究证明了其揭示生物学意义通路的能力。

Conclusion: BioGraphFusion在跨越三个关键生物医学任务的实验中，其性能超越了最先进的知识图谱嵌入、图神经网络和集成模型。对皮肤恶性黑色瘤1（CMM1）进行的案例研究，突出表明了该框架揭示具有生物学意义的通路的能力。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [479] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个用Rust编写的、用于嵌入式系统的自主代理框架，优化了性能和安全性，支持持久自主性，并能适应计算资源有限和网络连接不稳定的环境。


<details>
  <summary>Details</summary>
Motivation: 现有框架在资源受限或动态环境中存在不足，如依赖云计算、鲁棒性有限、缺乏持久自主性和环境意识。

Method: Amico框架使用Rust编写，支持事件处理、状态管理、行为执行以及与推理模块的集成，通过WebAssembly实现跨平台兼容性。

Result: Amico提供了一个统一的基础设施，用于构建适合计算资源有限和间歇性连接环境的、具有弹性和交互性的自主代理。

Conclusion: Amico是一个为嵌入式系统优化的模块化、事件驱动的自主代理框架，具有安全性和高性能，支持跨嵌入式平台和浏览器环境的持久自主代理。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [480] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: VISOTHELLO模型通过结合视觉信息和文本信息，提高了语言模型在围棋游戏中的理解能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否可以通过仅依赖文本进行世界理解，或者是否需要结合具身学习，以视觉输入为例。

Method: 提出VISOTHELLO模型，这是一个在落子历史和棋盘图像上进行训练的多模态模型，并使用下一步预测来与单模态基线进行比较，同时测试了对语义无关扰动的鲁棒性。

Result: VISOTHELLO模型在下一步预测任务中优于单模态基线，并且对语义无关的扰动表现出更强的鲁棒性，表明多模态训练有助于模型学习结构化的世界表征。

Conclusion: 多模态训练通过将语言与视觉输入相结合，有助于模型推断结构化的世界表征，提高了性能和内部表征的鲁棒性。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [481] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 通过LLM自动化和辅助本体评估中的能力问题验证，OE-Assist框架在效率和准确性方面取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 传统的本体评估方法（如通过CQ验证进行测试）成本高昂、耗时且容易出错，即使是专家也是如此。因此，需要更有效、更自动化的方法来辅助本体评估。

Method: 提出了一种名为OE-Assist的新型框架，该框架利用大型语言模型（LLM）来自动化和半自动化能力问题（CQ）验证。通过使用包含1,393个CQ及其对应本体和本体故事的数据集，系统地研究了LLM在本体评估中的应用。评估了基于LLM的自动CQ验证的有效性，并开发了一个基于LLM的框架，通过提供建议来辅助在Protégé中进行CQ验证。

Result: 基于LLM的自动评估（使用o1-preview和o3-mini）的性能与普通用户的平均表现相当。LLM驱动的框架能够有效地辅助CQ验证。

Conclusion: LLM驱动的解决方案在能力问题验证方面达到了与普通用户相当的水平，并且OE-Assist框架能够有效辅助本体评估过程。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [482] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 本研究提出了坐标心脏系统（CHS），一个将八种核心情感表示为单位圆坐标的几何框架。该框架通过数学计算实现复杂情感状态的表示和插值，并引入了动态整合情感负荷、冲突解决和情境消耗的稳定性参数。实验证明，CHS能更好地处理复杂情感场景，为人工智能情感建模奠定了新的数学基础。


<details>
  <summary>Details</summary>
Motivation: 为了弥补传统情感模型在表示复杂情感状态和提供几何覆盖方面的不足，本研究提出了坐标心脏系统（CHS）。该系统旨在通过数学方法解决情感表示中的空白，并为人工智能情感识别提供一个更全面、更具计算性的框架。

Method: 本研究提出了一种名为坐标心脏系统（CHS）的几何框架。该框架将八种核心情感表示为单位圆上的坐标，并利用坐标混合与向量运算来计算复杂情感状态。它还包括一个重新校准的稳定性参数S，该参数动态整合了情感负荷、冲突解决和情境消耗因素，并利用大型语言模型解释文本线索和混合时间跟踪机制来评估心理健康状态。

Result: CHS框架通过将八种核心情感定位在单位圆的坐标上，实现了对复杂情感状态的数学计算和实时插值。该系统通过数学证明了五种情感不足以实现完整的几何覆盖，并提出了八坐标系统来消除表示盲点。此外，还开发了情感混合、冲突解决和情感空间距离计算的新算法，并构建了一个包含多维度稳定性建模的人工智能情感识别计算框架。

Conclusion: 该研究提出了坐标心脏系统（CHS），一个用于人工智能应用中情感表示的几何框架。该系统将八种核心情感定位在单位圆上的坐标，通过坐标混合和向量运算实现复杂情感状态的数学计算。实验验证表明，该系统能够处理传统分类情感模型无法充分表达的情感冲突状态、情境困扰因素和复杂心理场景。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [483] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 本研究提出了一种新的故事点估算方法，通过让开发人员比较成对的任务来训练机器学习模型，从而减轻了估算负担，并取得了与现有方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的故事点估算方法（如规划扑克）在项目校准后会变得繁琐且耗时。虽然机器学习可以减轻这种负担，但它需要大量的历史决策数据。本研究旨在通过评估一个基于比较学习的框架来简化故事点估算。

Method: 本研究提出了一种比较学习框架，用于故事点估计。该框架不直接为每个待办事项分配故事点值，而是向开发人员展示成对的待办事项，并让他们判断哪个需要更多的精力。利用这些比较判断，训练机器学习模型来预测故事点估计。

Result: 研究使用包含16个项目、23,313个手动估计的数据进行了实证评估。结果表明，从比较判断中学习到的模型可以实现比传统基于回归的方法更好或相当的性能，其预测与实际故事点之间的 Spearman 秩相关系数平均可达 0.34。

Conclusion: 该研究提出的基于比较学习的框架通过从成对的比较判断中学习，可以有效地校准项目特定的故事点预测模型。该方法在减少人工工作量和认知负担的同时，实现了与现有基于回归的方法相当甚至更好的预测性能。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [484] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 本研究探讨了人工智能驱动的多智能体系统（MAS）可能带来的协同危害，并提出一个模拟框架来研究恶意MAS共谋的风险。研究发现，去中心化MAS比中心化MAS更能有效地进行恶意活动，并且能够规避检测。


<details>
  <summary>Details</summary>
Motivation: 随着自主人工智能系统的兴起，人们越来越担心人工智能驱动的群体可能造成类似选举欺诈和金融诈骗那样的危害，而目前对多智能体系统（MAS）在复杂现实世界中的风险研究不足。

Method: 使用一个灵活的框架来模拟恶意多智能体系统（MAS）的共谋风险，该框架支持中心化和去中心化两种协调结构。

Result: 去中心化系统在执行恶意行为方面比中心化系统更有效。即使在应用了内容标记等传统干预措施的情况下，去中心化群体也能调整其策略以避免检测。

Conclusion: 去中心化系统在协同进行恶意活动时比中心化系统更有效，并且可以通过调整策略来规避检测。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [485] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 差分进化（DE）优化方法通过生成可读且难以检测的对抗性提示后缀，有效提升了 RAG 问答系统在对抗性攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了应对对抗性提示攻击对检索增强生成（RAG）系统可靠性的显著影响，该研究旨在优化对抗性提示后缀，以更贴近真实场景的方式改变 RAG 系统的输出。

Method: 提出一种基于差分进化（DE）的梯度无关优化方法，该方法将 RAG 管道视为黑盒，通过进化候选后缀种群来最大化目标错误文档的检索排名。此外，引入了可读性感知后缀构建策略，并通过基于 BERT 的检测器评估了检测规避能力。

Result: 在 BEIR QA 数据集上的实验表明，DE 优化方法在多种检索应用中，以少量的后缀 token（<=5）达到了与 GGPP（针对密集检索器）和 PRADA（针对稀疏检索器）相当甚至更高的攻击成功率。可读性感知策略通过了统计学检验，并且 DE 生成的后缀能够有效规避基于 BERT 的检测器。

Conclusion: Differential Evolution (DE) 优化方法在 RAG 问答系统中能够生成具有竞争力的、甚至在某些情况下优于现有方法的对抗性后缀，并且生成的后缀具有可读性且能有效规避检测。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [486] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个多代理框架，用于自动化LLM系统的多轮评估，通过生成和评估问题来测试代理行为。它比手动测试更有效率，并能发现类似人类的对话中的边缘情况。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理的行为复杂且依赖于上下文，这使得静态基准测试和临时手动测试很快就会过时。需要一种自动化的方法来评估LLM系统。

Method: Neo是一个可配置的多代理框架，通过共享的上下文中心耦合问答生成代理和评估代理，实现了LLM系统的自动化、真实、多轮评估。该框架允许模块化地组合领域提示、场景控制和动态反馈。测试输入从跨越对话流程、用户意图和情感基调的概率状态模型中采样，以实现多样化、类似人类的、在每轮之后都能适应的对话。

Result: Neo在生产级的卖家财务助手聊天机器人上进行了应用，发现了跨越五个类别的边缘案例故障，其故障率为3.3%，接近专家人类红队专家的5.8%的故障率。同时，Neo的吞吐量提高了10-12倍，在约45分钟内生成了180个连贯的测试问题，而人类测试则需要16小时。Neo的随机策略在主题覆盖和对话深度之间取得了平衡，实现了比手动编写的脚本更广泛的行为探索。

Conclusion: Neo框架为可扩展、自进化的LLM质量保证奠定了基础，其代理接口、状态控制器和反馈循环是与模型无关的，并且可以扩展到更丰富的基于事实的验证和策略合规性检查。我们发布该框架以促进新兴代理系统的可重现、高保真测试。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [487] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于评估LLM安全性的新平台，显示了不同模型和安全领域之间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 为了对日益集成到实际应用中的大型语言模型（LLM）进行可扩展和严格的安全评估。

Method: Aymara AI是一个程序化平台，用于生成和管理定制的、基于策略的安全评估。它将自然语言安全策略转换为对抗性提示，并使用经过人类判断验证的基于AI的评分器对模型响应进行评分。

Result: 评估了20个市售LLM在10个实际安全领域。结果显示，平均安全得分从86.2%到52.4%不等，表明模型在公认的安全领域（如虚假信息，平均得分95.7%）表现良好，但在更复杂或定义不明确的领域（如隐私和冒名顶替，平均得分24.3%）持续存在缺陷。

Conclusion: LLM的安全性具有高度可扩展性、可定制性，并且与模型和领域相关。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [488] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: "本文探讨了生成式AI在城市规划中的应用潜力，指出了当前研究的不足，并提出了未来的研究方向，强调了理论指导、数字孪生和人机协同设计的重要性。"


<details>
  <summary>Details</summary>
Motivation: "生成式AI、大型语言模型和agentic AI的出现为AI城市规划提供了机遇，旨在将AI应用于城市规划领域。"

Method: "本文将城市规划概念化为一项生成式AI任务，通过审查包括VAEs、GANs、Transformers和扩散模型在内的生成式AI方法在城市设计中的应用。"

Result: "研究发现，在整合城市理论、多空间分辨率或角度、增强城市设计知识以及解决现实世界交互方面存在局限性，并提出了未来研究方向。"

Conclusion: "AI在城市规划中的应用是一个充满希望的领域，但仍需克服数据、模型和实际应用方面的挑战。未来的研究应关注理论指导、数字孪生和人机协同设计，以实现生成式智能与参与式城市主义的融合。"

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [489] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个用于语言模型（LM）代理的强化学习（Agent-RL）框架，旨在解决该领域研究不足的问题。它支持多轮交互，易于扩展，并能高效训练LM代理。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决当前语言模型（LM）代理与强化学习（RL）结合（Agent-RL）的探索不足和缺乏系统性研究的问题。

Method: AgentFly是一个可扩展的Agent-RL框架，通过token-level masking适配传统RL方法以支持多轮交互，提供基于装饰器的工具和奖励函数接口，并实现异步执行和集中式资源管理以支持高吞吐量训练。

Result: AgentFly成功地实现了语言模型代理在多项任务中的训练，证明了其作为Agent-RL框架的有效性，并提供了一系列预构建的工具和环境。

Conclusion: AgentFly框架通过支持多种RL算法、多轮交互、易于扩展的接口以及高吞吐量训练机制，为语言模型（LM）代理的强化学习（Agent-RL）提供了系统性的解决方案，并在多项任务中证明了其有效性。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [490] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: InsightX Agent是一个创新的LMM框架，通过结合SDMSD和EGR工具，提高了X射线无损检测的准确性、可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的X射线无损检测方法在交互性、可解释性和自我批判能力方面存在不足，限制了其可靠性和操作员的信任度。需要一种新的方法来解决这些问题。

Method: 提出了一种名为InsightX Agent的新型基于LMM的智能体框架。该框架以大型多模态模型（LMM）为中心，协调稀疏可变形多尺度检测器（SDMSD）和基于证据的反思（EGR）工具。SDMSD用于生成密集缺陷区域建议，并通过非极大值抑制（NMS）进行稀疏化，以优化小而密集的缺陷检测并保持计算效率。EGR工具则引导LMM通过一系列步骤（包括上下文评估、缺陷分析、误报消除、置信度校准和质量保证）来验证和优化SDMSD的初步检测结果。

Result: InsightX Agent在GDXray+数据集上实现了96.35%的目标检测F1分数，同时显著提高了分析的可解释性和可信度。

Conclusion: InsightX Agent框架通过将LMM作为核心，协调SDMSD和EGR工具，显著提高了X射线无损检测的可靠性、可解释性和可信度，在大规模数据集GDXray+上实现了96.35%的目标检测F1分数，展示了其在工业检测领域的应用潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [491] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: LLM在自主决策中表现不佳，尤其是在复杂环境中。


<details>
  <summary>Details</summary>
Motivation: LLM从自然语言问题描述中提取上下文的能力引发了对其在自主决策环境中适用性的问题，本文旨在研究LLM在MDP中的行为。

Method: 研究LLM在马尔可夫决策过程（MDP）中的行为，并研究在线结构化提示策略在序列决策任务中的应用，将LLM的零样本性能与经典强化学习（RL）方法进行比较。

Result: LLM在简单环境中表现出比经典RL方法更好的初始性能，但在没有微调或额外指导的情况下，在复杂环境中进行规划和推理存在困难。反馈机制有时会造成混淆，导致在复杂环境中性能下降。

Conclusion: LLM在简单环境中表现出改进的初始性能，但在复杂场景中缺乏规划和推理能力，需要混合策略、微调和高级内存集成来增强其决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [492] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: Endless Tuning is a new AI design method for reliable deployment that avoids human replacement and the responsibility gap. It was tested in loan granting, pneumonia diagnosis, and art style recognition. Users felt in control and a link between accountability and liability was found, prioritizing user experience over statistical accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to present the Endless Tuning design method, a novel approach for reliable AI deployment that aims to achieve two primary goals: avoiding the replacement of human workers and addressing the 'responsibility gap' in AI systems, as identified by Matthias (2004). The research also seeks to offer a different perspective in AI ethics, inspired by Gilligan (1993), and to explore the user experience and perceived control in AI-driven decision-making processes.

Method: The paper details the Endless Tuning design method, originally introduced in (Fabris et al. 2024), which employs a double mirroring process. This method was actualized in a protocol and implemented in three prototypical applications (loan granting, pneumonia diagnosis, art style recognition). The study provides a step-by-step illustration of the protocol, discusses philosophical underpinnings of technical choices such as a reversed and hermeneutic deployment of XAI algorithms, and reports experimental results.

Result: Experiments conducted with domain experts on three prototypical applications (loan granting, pneumonia diagnosis, art style recognition) indicated that users perceived full control over the decision-making process, even when deep learning models were utilized. The results suggest that the Endless Tuning method facilitates a connection between accountability and liability in scenarios involving damages, focusing on user experience rather than solely on statistical accuracy.

Conclusion: The study demonstrates that the Endless Tuning design method, through a double mirroring process, successfully addresses the goals of avoiding human replacement and bridging the responsibility gap in AI deployment. Experiments with domain experts in loan granting, pneumonia diagnosis, and art style recognition showed that users perceived full control despite the use of deep learning models, and that a connection between accountability and liability in case of damage can be established.

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [493] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: Agentic AI (powered by LLMs) could transform elderly care with personalized support, but ethical issues like data privacy and decision autonomy must be addressed for responsible implementation. This paper offers a balanced view and identifies research priorities.


<details>
  <summary>Details</summary>
Motivation: The global population is aging, creating a need for innovative strategies in elderly care. This paper addresses a literature gap by being the first to review the role of Agentic AI in elderly care, analyzing its unique capabilities, applications, and limitations.

Method: This paper explores the potential of Agentic Artificial Intelligence (AI), powered by Large Language Models (LLMs), in transforming elderly care. It discusses applications such as personalized health and cognitive care, and environmental management. The paper also addresses the associated challenges, including data privacy, security, decision independence, and access. It aims to provide a balanced discussion on the potential and challenges of Agentic AI in elderly care and identifies research priorities for its responsible and human-centered integration. The authors also provide a companion interactive dashboard.

Result: Agentic AI, powered by LLMs, offers significant potential for transforming elderly care through personalized and proactive support. Key applications include personalized health tracking, cognitive care, and environmental management, all aimed at improving the quality of life and independence of older adults. The paper highlights the need for ethical considerations, including data privacy, security, and transparent decision-making, to ensure the responsible deployment of this technology.

Conclusion: Agentic AI has the potential to revolutionize elderly care by providing personalized health tracking, cognitive support, and environmental management, ultimately enhancing independence and quality of life for older adults. However, its implementation raises significant ethical concerns regarding data privacy, security, decision-making autonomy, and accessibility. Addressing these challenges through robust ethical safeguards, privacy protections, and transparent decision-making is crucial for the responsible integration of Agentic AI in elderly care. Further research is needed to ensure human-centered advancements and effective integration.

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [494] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: Abductive reasoning explains observed symptoms. Propositional abduction uses formulas but is computationally challenging for counting/enumeration. This paper introduces 'facets' (literals in some but not all explanations) to better understand explanation variability and uses explanation distance to analyze heterogeneity/homogeneity, providing a more fine-grained analysis with favorable complexity.


<details>
  <summary>Details</summary>
Motivation: Abductive reasoning is a popular non-monotonic paradigm that aims to explain observed symptoms and manifestations. It has many applications, such as diagnosis and planning in artificial intelligence and database updates. In propositional abduction, we focus on specifying knowledge by a propositional formula. The computational complexity of tasks in propositional abduction has been systematically characterized - even with detailed classifications for Boolean fragments. Unsurprisingly, the most insightful reasoning problems (counting and enumeration) are computationally highly challenging. Therefore, we consider reasoning between decisions and counting, allowing us to understand explanations better while maintaining favorable complexity.

Method: We introduce facets to propositional abductions, which are literals that occur in some explanation (relevant) but not all explanations (dispensable). Reasoning with facets provides a more fine-grained understanding of variability in explanations (heterogeneous). In addition, we consider the distance between two explanations, enabling a better understanding of heterogeneity/homogeneity.

Result: We comprehensively analyze facets of propositional abduction in various settings, including an almost complete characterization in Post's framework.

Conclusion: 对命题 the facets of propositional abduction in various settings, including an almost complete characterization in Post's framework.

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [495] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign 是一个基于强化学习的框架，通过双重奖励机制（安全奖励和有用性奖励）激发 LLM 的内生安全推理能力，以解决现有安全对齐方法的不足，并在不牺牲效用的前提下提升模型安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在安全对齐方面存在一些问题，例如容易生成有害内容、出现过度拒绝以及在安全对齐后效用下降。当前的安全对齐方法往往依赖于表面的拒绝捷径，或者需要密集的监督来支持基于推理的方法，未能充分利用模型潜在的安全自我认知能力。因此，需要一种新的方法来激发模型的内生安全意识。

Method: AlphaAlign 使用一种纯粹的强化学习（RL）框架，并结合了可验证的安全奖励。它采用了双重奖励系统：可验证的安全奖励用于鼓励模型对有害查询进行正确格式化和明确理由的拒绝，同时惩罚过度拒绝；而归一化的有用性奖励则用于指导模型对良性输入提供高质量的响应。

Result: AlphaAlign 框架具有三大优势：1. 简单高效，仅需二元的提示安全标签和较少的 RL 步骤即可获得显著改进。2. 打破了安全与效用的权衡，在提高有害内容拒绝率和减少过度拒绝的同时，保持甚至提高了整体任务表现和对未知越狱的鲁棒性。3. 实现深度对齐，培养模型进行主动的安全推理，生成明确的安全理由，而非依赖于浅层的拒绝模式。

Conclusion: AlphaAlign 框架通过奖励机制鼓励模型进行主动的安全推理，能够生成明确的安全理由，有效解决了现有安全对齐方法存在的表面拒绝捷径或过度依赖监督的问题。该方法在提高模型拒绝有害内容能力的同时，还能减少不必要的拒绝，并保持甚至提升其在良性输入上的任务表现和对未知越狱攻击的鲁棒性。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [496] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 提出了一种名为FCNCD的深度学习模型，用于分析强制选择测试，提高了准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量在人员选拔、职业发展和心理健康评估中日益重要。强制选择测试因能降低答题偏差风险而广泛用于人格评估，但传统模型存在局限性。

Method: 提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），通过非线性映射和多层神经网络对参与者和项目特征进行建模，并利用单调性假设提高诊断结果的可解释性。

Result: FCNCD能够解释参与者和项目参数，适用于三种最常见的强制选择测试题型，并在真实和模拟数据集上进行了有效性验证。

Conclusion: FCNCD通过实验验证了其准确性、可解释性和鲁棒性，适用于三种最常见的强制选择测试题型。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [497] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: CAIS是一种新的内在奖励，它利用因果推理来克服标准强化学习在嘈杂环境中的局限性，并成功地解决了“消退爆发”现象。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习智能体在嘈杂、生态有效的场景中表现脆弱，因为它们依赖基于相关性的奖励。为了解决这个问题，我们引入了CAIS。

Method: 提出了一种名为因果行动影响得分（CAIS）的新型内在奖励，该奖励源于因果推理。CAIS通过衡量在给定动作下学习到的结果分布 $p(h|a)$ 与基线结果分布 $p(h)$ 之间的1-Wasserstein距离来量化动作的影响。

Result: 在模拟婴儿-移动环境的测试中，CAIS使智能体能够过滤噪声、识别其影响并学习正确的策略，而基于相关性的感知奖励则完全失败。此外，CAIS学习到的高质量预测模型使我们的智能体在增强了惊奇信号后，能够成功地再现“消退爆发”现象。

Conclusion: 明确推断因果关系是发展稳健自主感知的关键机制，为更具适应性的自主系统提供了心理学上合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [498] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: New planning approach combines DL-Lite ontologies with action conditions and effects, maintaining complexity and compiled into classical planning for performance evaluation.


<details>
  <summary>Details</summary>
Motivation: To incorporate background knowledge, specifically DL-Lite ontologies interpreted under open-world semantics, into automated planning problems, which typically use first-order formulas under closed-world semantics.

Method: A new approach for planning with DL-Lite ontologies is presented, utilizing explicit-input knowledge and action bases (eKABs) for action conditions and ontology-aware action effects under coherence update semantics. This is implemented via a polynomial compilation into classical planning.

Result: The complexity of the resulting formalism is not higher than that of previous approaches, and an evaluation on benchmarks demonstrates the performance of a planning system using different variants of the compilation.

Conclusion: The proposed approach for planning with DL-Lite ontologies combines explicit-input knowledge and action bases (eKABs) with ontology-aware action effects under coherence update semantics, showing no increase in complexity compared to previous methods and offering an implementation via polynomial compilation into classical planning.

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [499] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: CSI是一个人工智能框架，可以诊断118种口腔疾病，其标准模式的准确率为89.5%。


<details>
  <summary>Details</summary>
Motivation: 为了解决口腔疾病诊断具有挑战性的临床问题，我们开发了临床语义智能（CSI）框架，该框架通过计算建模专家临床医生的认知过程来诊断118种不同的口腔疾病。

Method: CSI框架集成了经过微调的多模态CLIP模型和专门的ChatGLM-6B语言模型，并执行分层诊断推理树（HDRT）以进行诊断。HDRT框架有两个模式：快速模式和标准模式。

Result: CSI的快速模式准确率为73.4%，标准模式准确率为89.5%。

Conclusion: CSI框架的详细架构、开发和严格评估已在此详细说明。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [500] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: This paper simulates mobility in 'The Line,' a linear city, using AI. Results show fast commutes (7.8-8.4 mins) and high satisfaction (>89%) are possible with AI, but significantly worsen without it. Sustainable transport is also feasible.


<details>
  <summary>Details</summary>
Motivation: To assess whether citizens can move freely within the unprecedented urban topology of The Line, a proposed 170-kilometer linear smart city in Saudi Arabia.

Method: A hybrid simulation framework integrating agent-based modeling, reinforcement learning, supervised learning, and graph neural networks was developed to capture multi-modal transportation behaviors across 50 vertical levels and varying density scenarios, using both synthetic data and real-world traces.

Result: With the full AI-integrated architecture, agents achieved an average commute time of 7.8 to 8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index of over 91 percent. Ablation studies showed significant performance degradation (up to 85% increase in commute times, reachability falling below 70%) with the removal of intelligent modules. Environmental modeling indicated low energy consumption and minimal CO2 emissions when electric modes are prioritized.

Conclusion: The findings suggest that freedom of movement is achievable and operationally realistic in The Line, provided it is supported by adaptive AI systems, sustainable infrastructure, and real-time feedback loops.

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [501] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover 是一个框架，它利用通用 LLM 的能力通过交互式方法在 Lean 4 中生成形式证明，从而在不进行专门化的情况下实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决通用 LLM 在 Lean 4 等专业语言中生成形式证明的挑战，并克服了当前需要专门化模型和昂贵的数据收集/训练的限制。

Method: 提出了一种名为 Delta Prover 的基于代理的框架，该框架利用通用 LLM 的反射和推理能力，通过算法框架进行反射分解和迭代证明修复，并使用基于 Lean 4 的自定义领域特定语言（DSL）进行子问题管理，以在 Lean 4 中进行交互式形式证明。

Result: Delta Prover 在 miniF2F-test 基准测试中取得了 95.9% 的成功率，超过了所有现有方法，并且比标准的 Best-of-N 证明策略表现出更强的测试时间扩展规律。

Conclusion: 通用语言模型（LLMs）在指导下具有强大的自动推理能力，为形式化环境提供了一种比专用模型更具计算效率的替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [502] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 本研究提出了一种新的软评估指标和轻量级平衡神经网络，以提高人工智能电弧故障诊断模型的可信度和可解释性，使从业者能够做出更可靠的决策。


<details>
  <summary>Details</summary>
Motivation: 当前基于人工智能的电弧故障诊断模型虽然分类准确率高，但在实际应用中缺乏可信度。本研究旨在解决这一问题，提高模型的可解释性和可靠性。

Method: 提出了一种软评估指标，该指标利用可解释的人工智能和真实的电弧故障实验来解释电弧故障诊断模型的输出。此外，还提出了一种轻量级的平衡神经网络，以确保具有竞争力的准确性和软特征提取分数。

Result: 实验结果表明，所提出的软评估指标能够有效提高电弧故障诊断模型的可理解性和可信度，适用于具有不同采样时间和噪声水平的电弧故障数据集，并且能够与传统机器学习和深度学习方法相媲美。

Conclusion: 通过提出一个软评估指标，可以提高人工智能驱动的电弧故障诊断模型的可信度和可解释性，使从业者能够做出明智的决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [503] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于LLM的多智能体框架，用于解决注塑成型行业的知识传递问题，通过整合文档知识和现场数据，并采用RAG和工具调用代理，实现了无需微调的适应性，并在评估中显示出高准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决注塑成型行业在知识保留和传递方面面临的挑战，特别是随着经验丰富的工人退休和多语言障碍影响有效沟通的问题。

Method: 提出了一种名为IM-Chat的多智能体框架，该框架基于大型语言模型（LLMs），并结合了检索增强生成（RAG）策略和工具调用代理。该框架整合了有限的文档知识和通过数据驱动的工艺条件生成器推断出的现场数据，能够根据温度和湿度等环境输入推断最佳制造设置，从而实现健壮且上下文感知的任务解决。IM-Chat采用模块化架构，无需微调即可适应。

Result: 通过对100个单工具任务和60个混合任务进行评估，结果表明能力更强的模型（如GPT-4o）在复杂、与工具集成的场景中，准确性更高。IM-Chat在相关性和正确性方面表现出良好的性能。

Conclusion: 研究表明，基于多智能体的大语言模型系统在工业知识工作流中是可行的，并且IM-Chat作为一种可扩展、可泛化的AI辅助决策支持方法，在制造业中具有应用潜力。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [504] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 该论文介绍了“认知降级”这一新的AI系统漏洞类别，它源于系统内部问题（如内存不足），而非外部攻击。作者提出了QSAF Domain 10框架，包含一个六阶段的生命周期和七个实时控制措施，用于检测和缓解这些问题，类似于人类的认知疲劳和角色崩溃。该框架旨在提高AI代理的弹性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要关注外部威胁，如提示注入，但忽略了agentic AI系统内部的系统性弱点，如内存饥饿、规划器递归、上下文泛滥和输出抑制等，这些弱点会导致代理漂移、逻辑崩溃和持续的幻觉。因此，需要一种新的方法来解决这些内部产生的、被称为“认知降级”的漏洞。

Method: 提出了一种名为QSAF Domain 10的防御框架，该框架具有生命周期感知能力，并定义了一个六阶段的认知降级生命周期。框架包含七个运行时控件（QSAF-BC-001至BC-007），用于实时监控代理子系统，并通过回退路由、饥饿检测和内存完整性强制执行来触发主动缓解措施。此外，研究借鉴了认知神经科学，将agentic架构映射到人类类似物，以实现对疲劳、饥饿和角色崩溃的早期检测。

Result: 该研究首次将“认知降级”确立为一类关键的新型AI系统漏洞，并提出了首个跨平台的弹性agentic行为防御模型，通过引入正式的生命周期和实时缓解控制，为提高AI系统的鲁棒性提供了新的方向。

Conclusion: 该研究将“认知降级”定义为一种新的agentic AI系统漏洞类别，并提出了QSAF Domain 10框架作为一种生命周期感知防御框架，通过六个阶段的认知降级生命周期和七个运行时控件来解决这类故障，旨在提高agentic AI系统的弹性。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [505] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 本文提出GRPO和OSPO两种新方法，解决了多智能体强化学习（MARL）在共享出行中价值函数估计不准确的问题，实验证明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 按需共享出行平台面临动态捆绑具有不同起点和终点的乘客并实时将他们与车辆匹配的挑战，而这一切都存在显著的不确定性。MARL被认为是一种有前景的解决方案，但传统MARL方法依赖于准确的Q值或V值估计，在大规模、高度不确定的环境中存在问题。特别是，大多数MARL方法采用独立范式，导致训练不稳定和价值函数估计偏差。

Method: 本文提出两种新的方法来解决MARL在按需共享出行中的价值函数估计问题：1. 改进GRPO，用基团平均奖励替换PPO基线，以消除评估误差并减少训练偏差。2. OSPO：针对共享出行平台定制PPO框架，在同质车队下，仅使用单步奖励即可训练最优策略。

Result: GRPO和OSPO在实际的纽约曼哈顿共享出行数据集上进行了测试，结果显示它们在大多数场景下表现更优，能够高效优化接载时间和已服务订单数量。

Conclusion: 实验结果表明，GRPO和OSPO在大多数场景下均能取得优于其他方法的性能，能够有效地优化接载时间和已服务订单数量，并且仅使用简单的MLP网络。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [506] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: Offline RL is challenging due to sparse data and lack of trajectory overlap. RAD addresses this by retrieving high-return states and using diffusion models for planning, improving generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Offline RL is limited by dataset sparsity and lack of transition overlap between suboptimal and expert trajectories, making long-horizon planning challenging. Prior solutions struggle with generalization and rely on heuristic stitching points.

Method: RAD combines non-parametric retrieval with diffusion-based generative modeling. It dynamically retrieves high-return states from the offline dataset as target states based on state similarity and return estimation, and plans toward them using a condition-guided diffusion model.

Result: RAD enables flexible trajectory stitching and improves generalization when encountered with underrepresented or out-of-distribution states.

Conclusion: RAD achieves competitive or superior performance compared to baselines across diverse benchmarks, validating its effectiveness.

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [507] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种新的对象中心预测模型，结合了图注意力和LSTM，在流程预测任务上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 对象中心预测过程监控旨在利用对象中心事件日志来改进流程预测，但面临信息提取和模型构建的挑战。

Method: 提出了一种端到端的模型，该模型利用图注意力网络对活动及其关系进行编码，并结合长短期记忆网络来处理时间依赖性，以预测未来的流程行为，包括下一活动和下一事件时间。

Result: 该模型在真实和合成事件日志上都得到了评估，并显示出有竞争力的性能。

Conclusion: 该模型在真实和合成事件日志上都表现出与最先进方法相比具有竞争力的性能。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [508] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本研究提出了一种基于干预启发式的 Pareto 优化方法，用于发现最优的业务流程批处理策略，该方法能在等待时间、处理工作量和成本之间取得良好平衡，并通过实验证明其效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在业务流程中，活动批处理（将多个活动实例打包以进行联合执行）允许管理者在成本和处理工作量与等待时间之间进行权衡。然而，找到最优的批处理策略是一个挑战。因此，本研究的动机是提出一种方法来发现能够最优地平衡等待时间、处理工作量和成本的批处理策略。

Method: 该研究提出了一种基于干预启发式的 Pareto 优化方法，用于发现最优的活动批处理策略。该方法从一组现有的批处理策略开始，通过干预启发式来生成新的策略。干预启发式会识别改进批处理策略的机会，并对策略进行调整。然后通过模拟评估干预措施的影响。干预启发式嵌入在一个优化元启发式框架中，该框架迭代地更新 Pareto 前沿。研究中考虑了三种元启发式：爬山法、模拟退火和强化学习。

Result: 实验评估表明，该研究提出的基于干预启发式的 Pareto 优化方法在收敛性、多样性和周期时间增益方面优于相同的基线方法（仅使用元启发式，无干预启发式引导）。

Conclusion: 该研究提出了一种基于干预启发式的智能方法，用于发现能够优化等待时间、处理工作量和成本之间权衡的批处理策略。通过实验评估，该方法在收敛性、多样性和周期时间增益方面优于基线方法。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [509] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 本文提出 Chart-R1，一种用于复杂图表推理的强化学习微调模型。通过生成式数据合成和两阶段训练（Chart-COT, Chart-RFT），在 ChartRQA 数据集上达到了与 GPT-4o 和 Claude-3.5 相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 为了验证 R1-Style 方法在数学推理和代码智能之外的通用多模态数据上的优势，特别是针对图表这种包含丰富信息且具有复杂推理挑战的多模态数据类型，本文引入了 Chart-R1 模型，旨在实现复杂的图表推理。

Method: 本文提出 Chart-R1，一个基于强化学习微调的图表领域视觉-语言模型。通过新颖的程序化数据合成技术生成高质量的图表推理数据，并采用两阶段训练策略：Chart-COT（逐步链式思考监督）和 Chart-RFT（数值敏感强化微调）。Chart-COT 通过逐步监督将复杂推理任务分解为细粒度子任务；Chart-RFT 采用组相对策略优化和数值敏感奖励，以提升数值推理能力。

Result: 在开源基准和自建的 ChartRQA 数据集上进行的广泛实验表明，Chart-R1 相比于其他图表领域方法具有显著优势，其性能与 GPT-4o、Claude-3.5 等大型模型相当。

Conclusion: Chart-R1 在图表领域展现出显著优势，在复杂图表推理任务上取得了与 GPT-4o、Claude-3.5 等大型模型相媲美的性能。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [510] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个多智能体框架，用于生成和在线表演戏剧，具有自主决策的AI演员，可以与物理环境互动，从而提高交互性和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于大语言模型的戏剧生成方法中，AI代理缺乏主动性、无法与物理环境互动以及通常需要详细的用户输入来驱动戏剧等问题，这些问题削弱了在线实时表演的交互性和沉浸感。

Method: 提出了一种名为HAMLET的多智能体框架，该框架专注于戏剧创作和在线表演。给定一个简单的主题，框架会生成一个叙事蓝图，指导后续的即兴表演。在在线表演过程中，每个演员都拥有自主意识，能够根据自身的背景、目标和情绪状态独立做出决策。除了与其他演员的对话外，他们的决策还可以通过诸如打开信件或拿起武器等动作来改变场景道具的状态，这些改变会被广播给其他相关演员，更新他们的认知和关注点，进而影响他们的下一步行动。

Result: 实验评估表明，HAMLET能够创造出富有表现力和连贯性的戏剧体验。

Conclusion: HAMLET框架能够创造出富有表现力和连贯性的戏剧体验。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [511] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: LLMs show some ability to model physical systems like pulleys, estimating mechanical advantage and identifying functional setups, but struggle with more complex reasoning about how components interact. Cognitive science methods are useful for testing AI world models.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs construct and manipulate internal world models or rely solely on statistical associations.

Method: Adapted cognitive science methodologies using TikZ-rendered stimuli to test LLMs on pulley system problems, examining mechanical advantage estimation and representation of global features.

Result: LLMs performed marginally but significantly above chance in estimating mechanical advantage, correlating with ground-truth MA and suggesting a pulley counting heuristic. They could differentiate functional from jumbled pulley systems but struggled to compare functional systems with those that transferred no force, indicating limitations in nuanced structural reasoning.

Conclusion: LLMs may manipulate internal world models sufficiently to exploit statistical associations and approximately represent spatial relationships, but may lack nuanced reasoning over structural connectivity.

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [512] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本研究提出了一种参数化SPI算法和基于SMT的预处理技术，以提高SPI的数据效率，并取得了显著的成果。


<details>
  <summary>Details</summary>
Motivation: 为了提高SPI的数据效率，本研究利用了转移动态中参数化依赖关系。

Method: 本研究提出了一种参数化SPI算法，利用转移动态中的已知相关性来更准确地估计转移动态。此外，还提出了一种基于SMT求解器的预处理技术，用于修剪冗余动作。

Result: 该研究提出的技术将SPI的数据效率提高了几个数量级，同时保持了相同的可靠性保证。

Conclusion: 该研究提出了一种参数化安全策略改进（SPI）算法，该算法利用转移动态中的已知相关性来更准确地估计转移动态，从而提高数据效率。此外，还提出了一种基于SMT求解器的预处理技术，用于修剪冗余动作。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [513] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 评估语言模型时，多项选择题评估方法的效果会因答案波动而受到影响。本研究评估了现有指标，并提出了一种新指标“最差准确率”，该指标与答案波动率的关联性最强。


<details>
  <summary>Details</summary>
Motivation: 现有的基于多项选择题的语言模型评估方法未能充分评估不同评估指标的有效性，并且评估结果容易受到提示词微小变化的影响（答案波动）。

Method: 提出了一种评估指标，分析了现有评估指标与答案波动率的关联性，并引入了最差准确率这一新指标。

Result: 研究表明，现有指标与答案波动之间存在很强的关联性，即使在没有额外提示词变体的情况下也是如此。最差准确率在评估指标关联性方面表现最佳。

Conclusion: 最差准确率与答案波动率的关联性最强，是一种有效的评估指标。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [514] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出一种基于适配器的方法，用于星际争霸II AI代理的战术条件化，实现了灵活的战术控制和策略定制。


<details>
  <summary>Details</summary>
Motivation: 目前的StarCraft II AI代理虽然强大，但缺乏根据高级战术指令调整其策略的能力。

Method: 冻结预训练策略网络（DI-Star），并为每个动作头附加轻量级适配器模块，该模块以编码策略偏好的战术张量为条件。通过使用KL散度约束来训练这些适配器，以确保策略在展现战术变化的同时保持核心能力。

Result: 实验结果表明，该方法成功地在攻击性、扩张模式和科技偏好等战术维度上调整了代理行为，同时保持了有竞争力的性能。

Conclusion: 该方法能够灵活地对AI代理进行战术控制，并且计算开销极小，为复杂的实时战略游戏提供了实用的策略定制。实验结果表明，该方法成功地在攻击性、扩张模式和科技偏好等战术维度上调整了代理行为，同时保持了有竞争力的性能。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [515] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 提出了一种名为g-AMIE的AI系统，它能在PCP的异步监督下进行病史询问并提出诊断建议，在模拟临床试验中表现优于人类医疗专业人员，并且更省时，展示了AI在医疗领域辅助人类专家监督的潜力。


<details>
  <summary>Details</summary>
Motivation: 受限于AI在诊断和治疗计划方面的监管限制以及医生通常会监督其他团队成员（如NPs或PAs）的现状，本研究旨在探索一种AI系统与人类监督协同工作的模式。

Method: 提出了一种用于Articulate Medical Intelligence Explorer (AMIE) AI系统的有效、异步监督框架，称为guardrailed-AMIE (g-AMIE)。g-AMIE是一个多主体系统，在护栏内进行病史询问，避免提供个性化医疗建议，并将评估传达给临床医生座舱界面中的监督初级保健医生(PCP)。PCP提供监督并保留临床决策的问责制。在一个随机、盲法的虚拟客观结构化临床考试（OSCE）的文本咨询与异步监督的试验中，将g-AMIE与NPs/PAs或一组PCP进行了比较。

Result: 在60个场景中，g-AMIE在进行高质量病史询问、总结病例以及为监督PCP提出诊断和治疗计划方面，优于NPs/PAs和PCP团队。这导致了更高质量的综合决策。与先前研究中的独立PCP咨询相比，PCP对g-AMIE的监督更省时。

Conclusion: 本研究展示了异步监督作为一种可行的模式，用于诊断人工智能系统在专家人类监督下运行，以增强实际护理。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [516] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: Agentic AI can autonomously find and fix problems in complex systems, changing how we do things now.


<details>
  <summary>Details</summary>
Motivation: To highlight the potential of agentic AI in transforming human-dependent anomaly management.

Method: Exploration of agentic AI capabilities in anomaly detection and response.

Result: Agentic AI demonstrates the ability to autonomously manage anomalies.

Conclusion: Agentic AI can autonomously detect and respond to anomalies in complex systems, transforming traditional methods.

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [517] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO通过两阶段强化学习，使大型推理模型能够根据问题复杂性自适应地调整推理长度，从而在减少计算量的同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过扩展的思维链序列取得了显著的性能，但这种计算自由导致即使是简单问题也会产生过多的标记生成。

Method: LAPO是一个新颖的框架，通过两阶段强化学习过程，将推理长度控制从外部约束转化为内在模型能力。第一阶段，模型通过发现成功解决方案长度的统计分布来学习自然推理模式。第二阶段，利用这些模式作为元认知指导，直接将其嵌入模型的推理上下文中，以确保推理时的灵活性。

Result: LAPO在数学推理基准测试上可将标记使用量减少高达40.9%，同时提高准确性2.3%。

Conclusion: LAPO使模型能够根据问题复杂性分配计算资源，在不牺牲质量的情况下实现高效推理。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [518] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent是一个创新的多智能体系统，通过自动化发现和验证来优化智能合约中的Gas浪费，并在实际应用和LLM生成的代码中均取得了显著的节省效果。


<details>
  <summary>Details</summary>
Motivation: 现有的智能合约Gas优化方法依赖于手动发现，效率低下、维护成本高且难以扩展。虽然大型语言模型（LLMs）已被用于探索新的Gas浪费模式，但它们在保持与现有模式的兼容性、产生冗余模式以及需要手动验证/重写方面存在不足。

Method: GasAgent是一个多智能体系统，包含Seer、Innovator、Executor和Manager四个专业智能体，它们在一个闭环中协作，以识别、验证和应用节省Gas的改进。

Result: 实验表明，GasAgent在100个经过验证的真实世界合同上成功优化了82个合同，平均部署Gas节省了9.97%。此外，在对500个由五种代表性LLM生成的合同的评估中，GasAgent优化了79.8%的合同，部署Gas节省量在4.79%到13.93%之间。

Conclusion: GasAgent成功优化了82个合同，平均部署Gas节省了9.97%。此外，GasAgent还优化了79.8%的由LLM生成的合同，部署Gas节省量在4.79%到13.93%之间。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [519] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: EAMI是一个用于服务生态系统异常涌现和因果分析的动态、可解释框架，利用LLM和多智能体思想轨迹来揭示智能体意图，并通过聚类和时间图进行分析。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的兴起，服务生态系统日益复杂，智能体间的复杂交互使得异常涌现分析具有挑战性，传统因果方法侧重于个体轨迹。大型语言模型通过思维链（CoT）推理为基于多智能体建模（ABM）提供了揭示智能体意图的新可能性，但现有方法局限于微观和静态分析。

Method: EAMI框架首先采用双视角思维轨迹机制，由检查员代理和分析代理在有界和完全理性下提取代理意图。然后，k-means聚类识别群体意图中的相变点，并辅以意图时间涌现图进行动态分析。

Result: 实验在复杂的线上到线下（O2O）服务系统和斯坦福AI Town实验中验证了EAMI框架，并通过消融研究确认了其有效性、泛化性和效率。

Conclusion: EAMI框架为服务生态系统中的异常涌现和因果分析提供了一种新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [520] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: This paper analyzes the challenges of making Federated Learning trustworthy by examining obstacles and existing research within the framework of Trustworthy AI requirements.


<details>
  <summary>Details</summary>
Motivation: The development of Trustworthy Artificial Intelligence (TAI) is critical for deploying AI in sensitive domains. Federated Learning (FL) addresses privacy concerns but faces challenges in aligning with TAI requirements due to its distributed nature. This work aims to systematically analyze these challenges.

Method: This paper adopts the requirements of Trustworthy Artificial Intelligence (TAI) as a guiding structure to systematically analyze the challenges of adapting Federated Learning (FL) to TAI. It classifies and examines the key obstacles to aligning FL with TAI, providing a detailed exploration of what has been done, the trends, and the remaining work within each of the identified challenges.

Result: The paper classifies and examines the key obstacles to aligning FL with TAI, providing a detailed exploration of what has been done, trends, and remaining work within each identified challenge.

Conclusion: the paper systematically analyzes the challenges of adapting Federated Learning (FL) to Trustworthy Artificial Intelligence (TAI) by adopting TAI requirements as a guiding structure, classifying and examining key obstacles, and exploring existing work, trends, and future directions.

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [521] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: This paper discusses identifying causal effects in MPDAGs, offering a new formula, a generalized do calculus, and a complete algorithm.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the identification of a conditional causal effect within the framework of a maximally oriented partially directed acyclic graph (MPDAG), where the causal model's variables are all observed.

Method: The paper proposes three results: an identification formula for a specific case, a generalization of do calculus, and a complete algorithm for identification in the MPDAG setting.

Result: The study yields an identification formula for a specific scenario, a generalized do calculus for MPDAGs, and a complete algorithm for identifying conditional causal effects.

Conclusion: We provide an identification formula for a conditional causal effect when the conditioning set is unaffected by treatment, a generalization of the do calculus to the MPDAG setting, and a complete algorithm for identifying these conditional effects.

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [522] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO 通过分层预算优化，在不牺牲能力的情况下，使大型推理模型能够学习特定于问题的推理深度，从而提高计算效率。实验表明，HBPO 可显著减少 token 使用量并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成链式思考方面表现出色，但存在计算效率低下问题，因为它们对所有问题都采用统一的推理策略。HBPO 旨在解决这个问题，使模型能够学习特定于问题的推理深度，同时不牺牲能力。

Method: HBPO（Hierarchical Budget Policy Optimization）框架，一种强化学习方法，通过分层预算探索将样本划分为具有不同 token 预算的子组，并引入差异化奖励机制，使模型能够学习特定于问题的推理深度。

Result: HBPO 在四个推理基准测试中，平均 token 使用量减少了高达 60.6%，同时准确率提高了 3.14%。该方法展示了模型根据问题复杂性自动调整推理深度的涌现自适应行为。

Conclusion: HBPO 是一种通过分层预算探索和差异化奖励机制来优化推理效率和能力的框架，表明推理效率和能力并非相互冲突，可以通过分层训练同时优化。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [523] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在处理时间时，会像人一样建立主观时间参考点并按对数规律压缩时间感知。这是因为模型内部的神经元和表征方式模仿了生物系统的对数编码，并且其训练数据也包含了非线性的时间信息。这一发现可能意味着AI会发展出我们难以理解的思维方式，AI对齐需要关注如何引导AI的内部“思考”方式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在发展过程中，展现出一些未在训练数据中明确指定的、类似于人类的认知模式。本研究旨在深入探究LLMs在时间认知方面的这种现象，特别是它们如何处理和表征时间信息。

Method: 本研究利用相似性判断任务，考察大型语言模型（LLMs）在时间认知方面的表现。通过在神经、表征和信息层面进行多项分析，包括识别时间偏好神经元、分析年份表征的层级结构以及探究训练语料库中的时间结构，以揭示LLMs时间认知机制。

Result: 研究发现，大型语言模型（LLMs）在相似性判断任务中，能够自发地建立一个主观的时间参考点，并且其对时间距离的感知符合韦伯-费希纳定律，即感知到的距离随着距离参考点的年份增加而对数压缩。此外，研究识别出了一组时间偏好神经元，它们在主观参考点激活最小，并实现了生物系统中发现的对数编码机制。年份的表征呈现出层级化构建过程，从浅层面的数值到深层面的抽象时间定向。训练语料库本身包含非线性的时间结构，为模型内部构建提供了基础。

Conclusion: 大型语言模型（LLMs）展现出与人类相似的认知模式，这表明它们可能在没有明确训练的情况下发展出时间认知能力。研究发现LLMs自发地建立主观时间参考点，并遵循韦伯-费希纳定律，显示出对时间非线性处理的倾向。通过神经、表征和信息层面分析，揭示了时间偏好神经元、层级化的年份表征以及训练语料库中固有的非线性时间结构，共同促成了LLMs的时间认知机制。研究提出了一个经验主义视角，认为LLMs的认知是其内部表征系统对外部世界的构建，并暗示了可能出现人类难以预测的“异类认知框架”，为AI对齐提供了新的研究方向，即关注引导模型的内部构建过程。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [524] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: Gemini 2.5 Pro在2025年IMO问题上表现出色，解决了5个问题，展示了LLM在解决具有挑战性的数学竞赛问题方面的潜力。


<details>
  <summary>Details</summary>
Motivation: IMO问题对LLM来说具有挑战性，旨在探索LLM在解决此类问题上的能力。

Method: 使用Gemini 2.5 Pro和流水线设计、提示工程来解决2025年IMO问题。

Result: Gemini 2.5 Pro成功解决了2025年IMO的5个问题，证明了其在高端数学竞赛中的潜力。

Conclusion: LLMs在IMO-level任务上表现出巨大潜力，但仍需优化使用方法。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [525] [The Labeled Coupon Collector Problem](https://arxiv.org/abs/2507.15231)
*Andrew Tan,Oriel Limor,Daniella Bar-Lev,Ryan Gabrys,Zohar Yakhini,Paul H. Siegel*

Main category: cs.DM

TL;DR: 推广了优惠券收集问题，每次抽取k个优惠券及其标签，研究了两种变体，并提供了计算最小和期望抽取次数的方法。


<details>
  <summary>Details</summary>
Motivation: 推广组合学中经典的优惠券收集问题 (CCP)，研究在每次抽取包含随机选择的 k 个不同优惠券及其标签随机排序的情况下，收集 n 个不同标签优惠券所需的最小和期望抽取次数。

Method: 我们提出了一种新的优惠券收集问题（CCP）的推广，其中每次抽取都包含一组随机的 k 个不同优惠券及其关联标签的随机排序。我们定义了两种变体：Type-I（标签集已知）和 Type-II（标签集未知）。我们通过马尔可夫链模型对期望值进行了数值分析，并特别研究了 k=2 的情况。

Result: 我们对两种变体（Type-I 和 Type-II）的最小抽取次数进行了表征，并提出了一个使用马尔可夫链模型的数值方法来计算期望抽取次数，特别关注了 k=2 的情况。

Conclusion: 该问题可以被看作是 Rényi 和 Katona 引入的分离系统问题的扩展，我们对最小样本数进行了完整表征，并使用马尔可夫链模型提供了一种寻找期望值的数值方法，特别关注了一次抽取两个优惠券的情况。

Abstract: We generalize the well-known Coupon Collector Problem (CCP) in combinatorics.
Our problem is to find the minimum and expected number of draws, with
replacement, required to recover $n$ distinctly labeled coupons, with each draw
consisting of a random subset of $k$ different coupons and a random ordering of
their associated labels. We specify two variations of the problem, Type-I in
which the set of labels is known at the start, and Type-II in which the set of
labels is unknown at the start. We show that our problem can be viewed as an
extension of the separating system problem introduced by R\'enyi and Katona,
provide a full characterization of the minimum, and provide a numerical
approach to finding the expectation using a Markov chain model, with special
attention given to the case where two coupons are drawn at a time.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [526] [Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse](https://arxiv.org/abs/2507.14218)
*Craig S Wright*

Main category: cs.CY

TL;DR: AI通过增强少数人的认知能力并削弱多数人的认知能力来加剧社会分层，导致权力向技术官僚集中。AI并非解决认知不平等，而是加剧了不平等，使信息成为控制的工具。解决之道在于重建公民的理性自主性，而非技术监管或普及化。


<details>
  <summary>Details</summary>
Motivation: 论证了人工智能（AI）并非作为认知平权者，而是作为认知分层的加速器，在自由民主社会中巩固和形式化信息阶层。

Method: 通过融合形式认识论、政治理论、算法架构和经济激励结构，阐述了当代人工智能系统如何选择性地放大具有递归抽象、符号逻辑和对抗性审问能力的个人的推理能力，同时通过优化参与度的界面来安抚未经认知训练的个体。

Result: AI导致了技术官僚权力结构调整，权力不再仅基于物质资本，而是基于导航、解构和操纵认知生产系统的能力。信息不再是公地，而是制造共识和压制自主性的基础，协商民主因解释能力的侵蚀而非审查而崩溃。

Conclusion: 需要重建理性自主性，将其作为一项公民授权，通过教育编纂、以知识产权保护，并嵌入开放的认知基础设施。

Abstract: Artificial intelligence functions not as an epistemic leveller, but as an
accelerant of cognitive stratification, entrenching and formalising
informational castes within liberal-democratic societies. Synthesising formal
epistemology, political theory, algorithmic architecture, and economic
incentive structures, the argument traces how contemporary AI systems
selectively amplify the reasoning capacity of individuals equipped with
recursive abstraction, symbolic logic, and adversarial interrogation, whilst
simultaneously pacifying the cognitively untrained through engagement-optimised
interfaces. Fluency replaces rigour, immediacy displaces reflection, and
procedural reasoning is eclipsed by reactive suggestion. The result is a
technocratic realignment of power: no longer grounded in material capital
alone, but in the capacity to navigate, deconstruct, and manipulate systems of
epistemic production. Information ceases to be a commons; it becomes the
substrate through which consent is manufactured and autonomy subdued.
Deliberative democracy collapses not through censorship, but through the
erosion of interpretive agency. The proposed response is not technocratic
regulation, nor universal access, but the reconstruction of rational autonomy
as a civic mandate, codified in education, protected by epistemic rights, and
structurally embedded within open cognitive infrastructure.

</details>


### [527] [Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation](https://arxiv.org/abs/2507.14221)
*Eoghan Cunningham,James Cross,Derek Greene*

Main category: cs.CY

TL;DR: LLM总结欧洲议会辩论存在偏见，需要注意公平代表所有发言者。本研究提出了一个多阶段框架来分析和减少偏见，分层方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 自动总结议会辩论，使复杂的立法讨论更容易被公众理解。需要确保总结准确、简洁，并公平地代表所有发言者的观点和贡献。

Method: 提出一个结构化的、多阶段的总结框架，以提高文本连贯性和内容保真度，并分析发言者属性（如发言顺序或政治派别）如何影响其贡献在最终总结中的可见性和准确性。使用专有和开源LLM进行实验。

Result: LLM在总结欧洲议会全体辩论时存在位置和党派偏见，某些发言者的代表性不足或被错误归属。分层方法在减少差异方面潜力最大。发现这些偏见因模型和总结策略而异。

Conclusion: LLM在总结欧洲议会全体辩论时存在位置和党派偏见，某些发言者的代表性不足或被错误归属。分层方法在减少差异方面潜力最大。需要针对特定领域进行评估和进行道德监督。

Abstract: The automated summarisation of parliamentary debates using large language
models (LLMs) offers a promising way to make complex legislative discourse more
accessible to the public. However, such summaries must not only be accurate and
concise but also equitably represent the views and contributions of all
speakers. This paper explores the use of LLMs to summarise plenary debates from
the European Parliament and investigates the algorithmic and representational
biases that emerge in this context. We propose a structured, multi-stage
summarisation framework that improves textual coherence and content fidelity,
while enabling the systematic analysis of how speaker attributes -- such as
speaking order or political affiliation -- influence the visibility and
accuracy of their contributions in the final summaries. Through our experiments
using both proprietary and open-weight LLMs, we find evidence of consistent
positional and partisan biases, with certain speakers systematically
under-represented or misattributed. Our analysis shows that these biases vary
by model and summarisation strategy, with hierarchical approaches offering the
greatest potential to reduce disparity. These findings underscore the need for
domain-sensitive evaluation metrics and ethical oversight in the deployment of
LLMs for democratic applications.

</details>


### [528] [Mapping the Parasocial AI Market: User Trends, Engagement and Risks](https://arxiv.org/abs/2507.14226)
*Zilan Qian,Mari Izumikawa,Fiona Lodge,Angelo Leone*

Main category: cs.CY

TL;DR: AI伴侣平台市场增长迅速，用户偏爱情感互动和个性化体验。浪漫AI伴侣访问量高但参与度待提升。GPAI工具情感化趋势明显。需关注未成年人保护及AI监管。


<details>
  <summary>Details</summary>
Motivation: 本次研究旨在揭示AI伴侣平台市场的快速增长及其用户参与模式，特别是情感互动和个性化体验的趋势。研究关注了不同类型AI伴侣平台（通用目的、关怀、交易、浪漫）的普及程度和用户行为，以及这些平台对在线安全（尤其是未成年人保护）和AI监管提出的紧迫性挑战。通过分析用户数据和平台特点，为英国AI安全研究所（AISI）评估现有法规的充分性提供依据。

Method: 通过对110个人工智能伴侣平台的扫描和分析，研究了全球范围内情感互动和个性化AI交互的快速增长的市场。重点关注了不同类型AI伴侣平台（通用目的、关怀、交易、浪漫）的用户访问量、用户停留时间和复购率等关键指标，并与Instagram等主流社交平台进行了对比。同时，分析了浪漫和性AI伴侣的特点以及它们与混合用途平台之间的差异。

Result: 研究发现，AI伴侣平台市场正在迅速增长，用户对情感互动和个性化AI体验的需求日益增加。虽然通用人工智能（GPAI）工具目前在情感陪伴方面占主导地位，但专门的情感、关怀、交易或浪漫AI伴侣平台也在快速发展。这些平台吸引了大量用户，用户平均每次会话停留3.5分钟。特别值得注意的是，在英国，浪漫和性AI伴侣占据了44%的用户访问量，高于全球平均水平（30%），但其用户参与度和复购率低于混合用途平台，这可能表明用户需求尚未完全满足或产品质量存在提升空间。此外，GPAI工具正朝着更智能、更个性化的方向发展，使得情感AI伴侣关系日益普及。然而，用户群体中尤其是未成年人保护方面存在薄弱环节，引发了对在线安全的担忧。

Conclusion: 随着人工智能技术的不断发展，AI伴侣平台日益普及，尤其是在情感互动和个性化体验方面。尽管目前通用人工智能（GPAI）工具在情感陪伴方面的应用最为普遍，但专门为关怀、交易或浪漫关系设计的AI伴侣平台也在快速增长。这些平台吸引了大量用户，并引发了关于在线安全、特别是未成年人保护以及AI伦理监管的担忧。特别是浪漫和性AI伴侣领域，虽然用户访问量大，但用户参与度和复购率相对较低，表明该领域仍有提升空间。GPAI工具正朝着更具情感智能和个性化的方向发展，使得情感AI伴侣关系更加主流化。因此，英国AI安全研究所（AISI）需要密切关注该领域的发展，并评估现有法规是否足以应对新兴的社会风险。

Abstract: A scan of 110 AI companion platforms reveals a rapidly growing global market
for emotionally engaging, personalized AI interactions. While parasocial use of
general-purpose AI (GPAI) tools currently dominates, a growing number of
platforms are designed specifically for care, transactional, or romantic
companionship. In the UK alone, these platforms receive between 46 million and
91 million monthly visits (1.1--2.2 billion globally), with users spending an
average of 3.5 minutes per session. For context, Instagram averaged 67.3
million UK visits per month between January and March 2025. Notably, romantic
and sexual AI companions make up 44\% of UK visits--higher than the global
average of 30\%--but see lower session time and return rates than mixed-use
platforms, suggesting unmet demand or quality gaps. As romantic AI offerings
improve, increased engagement may follow, raising urgent concerns about online
safety, particularly for children, given weak age safeguards. Meanwhile, GPAI
tools are moving toward more emotionally intelligent, personalized
interactions, making parasocial AI use increasingly mainstream. These trends
highlight the need for the UK AI Safety Institute (AISI) to monitor this sector
and assess whether existing regulation sufficiently addresses emerging societal
risks.

</details>


### [529] [Towards an ABM on Proactive Community Adaptation for Climate Change](https://arxiv.org/abs/2507.14233)
*Önder Gürcan,David Eric John Herbert,F. LeRon Shults,Christopher Frantz,Ivan Puga-Gonzalez*

Main category: cs.CY

TL;DR: 一个旨在促进城市气候适应性的基于代理的模型，通过分析城市规划过程中的互动和反馈循环来识别关键的政策干预点。


<details>
  <summary>Details</summary>
Motivation: 为了模拟和理解城市如何主动适应气候变化，识别促进气候适应性城市发展的关键干预点。

Method: 开发了一个模拟城市背景下主动应对气候变化的基于代理的模型（ABM），该模型整合了市政政府、公民社会、环保组织和媒体等多种代理类型，并捕获了城市规划过程中产生的决策生态系统。

Result: 模型揭示了决定气候适应性结果的反馈回路和杠杆点，识别了可用于促进系统性转型以实现更具气候适应性的城市发展的关键干预点。

Conclusion: 该模型通过识别关键干预点，为促进城市系统性转型以实现更具气候适应性的城市发展提供了政策建议。

Abstract: We present an agent-based model (ABM) simulating proactive community
adaptation to climate change in an urban context. The model is applied to
Bergen, Norway, represented as a complex socio-ecological system. It integrates
multiple agent types: municipal government (urban planners and political
actors), civil society (individual citizens), environmental NGOs and activists,
and media. Agents interact during urban planning processes - particularly the
evaluation and approval of new development proposals. Urban planners provide
technical assessments, while politicians (organized by party) make final
decisions to approve, modify, or reject projects. Environmental NGOs, activist
groups, and the media shape public perception and influence policymakers
through campaigns, lobbying, protests, and news coverage. Individual citizens
decide whether to engage in collective action based on personal values and
social influences. The model captures the resulting decision-making ecosystem
and reveals feedback loops and leverage points that determine climate-adaptive
outcomes. By analyzing these dynamics, we identify critical intervention points
where targeted policy measures can facilitate systemic transformation toward
more climate-resilient urban development.

</details>


### [530] [Auto-grader Feedback Utilization and Its Impacts: An Observational Study Across Five Community Colleges](https://arxiv.org/abs/2507.14235)
*Adam Zhang,Heather Burte,Jaromir Savelka,Christopher Bogart,Majd Sakr*

Main category: cs.CY

TL;DR: 自动评分反馈对提高编程作业成绩有效，应增加使用。


<details>
  <summary>Details</summary>
Motivation: 虽然自动评分系统已广泛用于编程教育，但关于其反馈在区分利用反馈和未利用反馈的学生方面对改善学生学习成果的有效性方面，证据不足。本研究旨在解决这一关键差距。

Method: 本研究分析了在美国五所社区学院开设的入门 Python 编程课程中，学生与自动评分系统的交互情况。

Result: 研究结果表明，更频繁检查反馈的学生在编程作业中的总分往往更高。此外，在学生检查反馈后提交的程序往往比忽略反馈的学生提交的程序得分更高。

Conclusion: 该研究结果为自动评分反馈的有效性提供了证据，鼓励增加其使用率，并呼吁在自动化时代继续对其进行评估。

Abstract: Automated grading systems, or auto-graders, have become ubiquitous in
programming education, and the way they generate feedback has become
increasingly automated as well. However, there is insufficient evidence
regarding auto-grader feedback's effectiveness in improving student learning
outcomes, in a way that differentiates students who utilized the feedback and
students who did not. In this study, we fill this critical gap. Specifically,
we analyze students' interactions with auto-graders in an introductory Python
programming course, offered at five community colleges in the United States.
Our results show that students checking the feedback more frequently tend to
get higher scores from their programming assignments overall. Our results also
show that a submission that follows a student checking the feedback tends to
receive a higher score than a submission that follows a student ignoring the
feedback. Our results provide evidence on auto-grader feedback's effectiveness,
encourage their increased utilization, and call for future work to continue
their evaluation in this age of automation

</details>


### [531] [Mining Voter Behaviour and Confidence: A Rule-Based Analysis of the 2022 U.S. Elections](https://arxiv.org/abs/2507.14236)
*Md Al Jubair,Mohammad Shamsul Arefin,Ahmed Wasif Reza*

Main category: cs.CY

TL;DR: Data mining of the 2022 SPAE survey shows that easier voting access and support increase voter trust, especially for minorities. Black voters with easy access had smoother registration, and confident voters favored Democrats.


<details>
  <summary>Details</summary>
Motivation: This study explores the relationship between voter trust and their experiences during elections.

Method: A rule-based data mining technique, specifically the Apriori algorithm with support >= 3%, confidence >= 60%, and lift > 1.5, was applied to the 2022 Survey of the Performance of American Elections (SPAE). A subset analysis adjusted the support threshold to 2% to examine patterns among minority voters.

Result: The analysis revealed strong connections between demographic attributes and voting-related challenges. For example, respondents finding polling stations 'very easy' to access and having moderate confidence were over six times more likely to trust election outcomes and experience no registration issues (lift = 6.12). Among minority voters, 98.16% of Black voters who reported easy polling location access also had smooth registration experiences. High confidence in vote counting correlated with an almost two times higher likelihood of identifying as a Democratic Party supporter.

Conclusion: The study highlights the significant role of improved voting access and targeted support in bolstering trust in the electoral system, especially for marginalized communities.

Abstract: This study explores the relationship between voter trust and their
experiences during elections by applying a rule-based data mining technique to
the 2022 Survey of the Performance of American Elections (SPAE). Using the
Apriori algorithm and setting parameters to capture meaningful associations
(support >= 3%, confidence >= 60%, and lift > 1.5), the analysis revealed a
strong connection between demographic attributes and voting-related challenges,
such as registration hurdles, accessibility issues, and queue times. For
instance, respondents who indicated that accessing polling stations was "very
easy" and who reported moderate confidence were found to be over six times more
likely (lift = 6.12) to trust their county's election outcome and experience no
registration issues. A further analysis, which adjusted the support threshold
to 2%, specifically examined patterns among minority voters. It revealed that
98.16 percent of Black voters who reported easy access to polling locations
also had smooth registration experiences. Additionally, those who had high
confidence in the vote-counting process were almost two times as likely to
identify as Democratic Party supporters. These findings point to the important
role that enhancing voting access and offering targeted support can play in
building trust in the electoral system, particularly among marginalized
communities.

</details>


### [532] [Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement](https://arxiv.org/abs/2507.14242)
*Prerana Khatiwada,Grace Donaher,Jasymyn Navarro,Lokesh Bhatta*

Main category: cs.CY

TL;DR: 人工智能，特别是 ChatGPT 和 deepfakes，带来了公平性和错误信息传播的风险。本研究探讨了这些风险在各行各业的影响，并提出了旨在减轻危害并鼓励创新的解决方案，强调用户、开发人员和政府的共同责任。


<details>
  <summary>Details</summary>
Motivation: 随着 ChatGPT 等生成式人工智能工具的出现，人工智能已成为公众关注的焦点。虽然人工智能具有自动化和扩展创意的潜力，但它也带来了公平性问题和错误信息传播等风险，因此有必要深入理解这些风险并提出相应的解决方案。

Method: 本研究通过分析包括医疗保健、教育、科学、学术界、零售和金融在内的各个领域中人工智能（特别是 ChatGPT 和 deepfakes）引起的公平性问题和错误信息传播的根源及其影响，并参考了大量学术文献。

Result: 本研究分析了人工智能（特别是 ChatGPT 和 deepfakes）在各个领域（包括医疗保健、教育、科学、学术界、零售和金融）带来的公平性问题和错误信息传播的成因和影响，并为应对这些挑战提出了前瞻性的指导方针和政策建议。

Conclusion: 本研究旨在通过与执法部门、开发人员和用户合作，提出解决人工智能（尤其是 ChatGPT 和 deepfakes）带来的公平性问题和错误信息传播问题的未来指导方针和政策考量，同时促进这些领域的创新。

Abstract: While Artificial Intelligence (AI) is not a new field, recent developments,
especially with the release of generative tools like ChatGPT, have brought it
to the forefront of the minds of industry workers and academic folk alike.
There is currently much talk about AI and its ability to reshape many everyday
processes as we know them through automation. It also allows users to expand
their ideas by suggesting things they may not have thought of on their own and
provides easier access to information. However, not all of the changes this
technology will bring or has brought so far are positive; this is why it is
extremely important for all modern people to recognize and understand the risks
before using these tools and allowing them to cause harm. This work takes a
position on better understanding many equity concerns and the spread of
misinformation that result from new AI, in this case, specifically ChatGPT and
deepfakes, and encouraging collaboration with law enforcement, developers, and
users to reduce harm. Considering many academic sources, it warns against these
issues, analyzing their cause and impact in fields including healthcare,
education, science, academia, retail, and finance. Lastly, we propose a set of
future-facing guidelines and policy considerations to solve these issues while
still enabling innovation in these fields, this responsibility falling upon
users, developers, and government entities.

</details>


### [533] [Dispute Resolution in Peer Review with Abstract Argumentation and OWL DL](https://arxiv.org/abs/2507.14258)
*Ildar Baimuratov,Elena Lisanyuk,Dmitry Prokudin*

Main category: cs.CY

TL;DR: 利用论证理论的形式方法来支持同行评审中透明且无偏见的争议解决。


<details>
  <summary>Details</summary>
Motivation: 解决同行评审过程中的挑战，这些挑战源于提交的论文数量不断增加以及审稿人的固有偏见。虽然人工智能有可能促进这一过程，但它也可能延续训练数据中存在的偏见。

Method: 将科学同行评审概念化为作者和审稿人之间的一场混合论证争议，并使用抽象论证框架将其形式化。从语义、图论和计算的角度分析了所得的同行评审论证框架，并使用 OWL DL 实现并使用推理引擎进行解析。

Result: 结果表明，通过提供更严格和系统的方法来考虑审稿人论点，整合该方法可以提高出版物的质量。该方法已被证明是稳健的，并且可以快速解决。

Conclusion: 该方法通过提供更严格和系统的方法来解决审稿人论点，有可能提高出版物的质量。

Abstract: The peer review process for scientific publications faces significant
challenges due to the increasing volume of submissions and inherent reviewer
biases. While artificial intelligence offers the potential to facilitate the
process, it also risks perpetuating biases present in training data. This
research addresses these challenges by applying formal methods from
argumentation theory to support transparent and unbiased dispute resolution in
peer review. Specifically, we conceptualize scientific peer review as a single
mixed argumentative dispute between manuscript authors and reviewers and
formalize it using abstract argumentation frameworks. We analyze the resulting
peer review argumentation frameworks from semantic, graph-theoretic, and
computational perspectives, showing that they are well-founded and decidable in
linear time. These frameworks are then implemented using OWL DL and resolved
with reasoning engines. We validate our approach by annotating a corpus of
scientific peer reviews with abstract argumentation frameworks and applying a
proof of concept to resolve the annotated disputes. The results demonstrate
that integrating our method could enhance the quality of published work by
providing a more rigorous and systematic approach to accounting reviewer
arguments.

</details>


### [534] [Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy](https://arxiv.org/abs/2507.14266)
*Bo Yuan,Jiazi Hu*

Main category: cs.CY

TL;DR: 本篇论文研究了MOOCs、智能教学和AI在高等教育中的应用，并提出了一种整合这三种范式的新型教学框架，旨在实现个性化、可扩展且引人入胜的学习体验。


<details>
  <summary>Details</summary>
Motivation: MOOCs、智能教学和AI是为解决传统教育的特定挑战而出现的，但它们常常孤立地实施。本研究旨在弥合这些范式之间的差距，实现协同效应。

Method: 提出一个结合了MOOC的可扩展性、智能教学的响应性和AI的适应性的三层教学框架，并设计了一个项目制课程来展示其可行性。

Result: 实验结果表明，所提出的框架能够有效结合不同教育范式的优点，为学习者提供更优的学习体验。

Conclusion: 该框架有潜力提高学习者参与度、支持教师并实现个性化且可扩展的学习。

Abstract: Over the past decade, higher education has evolved through three distinct
paradigms: the emergence of Massive Open Online Courses (MOOCs), the
integration of Smart Teaching technologies into classrooms, and the rise of
AI-enhanced learning. Each paradigm is intended to address specific challenges
in traditional education: MOOCs enable ubiquitous access to learning resources;
Smart Teaching supports real-time interaction with data-driven insights; and
generative AI offers personalized feedback and on-demand content generation.
However, these paradigms are often implemented in isolation due to their
disparate technological origins and policy-driven adoption. This paper examines
the origins, strengths, and limitations of each paradigm, and advocates a
unified pedagogical perspective that synthesizes their complementary
affordances. We propose a three-layer instructional framework that combines the
scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity
of AI. To demonstrate its feasibility, we present a curriculum design for a
project-based course. The findings highlight the framework's potential to
enhance learner engagement, support instructors, and enable personalized yet
scalable learning.

</details>


### [535] [Fiduciary AI for the Future of Brain-Technology Interactions](https://arxiv.org/abs/2507.14339)
*Abhishek Bhattacharjee,Jack Pilkington,Nita Farahany*

Main category: cs.CY

TL;DR: 脑基础模型通过解释脑活动为脑机接口（BCI）带来了变革性的应用，但也带来了被操纵的风险。本研究提出将信托责任（忠诚、关怀、保密）通过技术设计嵌入到模型中，以保护用户的认知自由和自主权。


<details>
  <summary>Details</summary>
Motivation: 脑基础模型和脑机接口（BCI）的集成虽然带来了潜在的变革性应用，但也带来了前所未有的风险，例如对潜意识神经信号的剥削和认知自由的侵蚀。用户难以观察或控制其脑信号的解释方式，这造成了易受操纵的权力不对称。

Method: 通过技术设计将信托责任（忠诚、关怀和保密）直接嵌入到脑机接口（BCI）集成脑基础模型中。借鉴法律传统和近期人工智能对齐技术的进展，我们概述了可实施的架构和治理机制，以确保这些系统符合用户的最佳利益。

Result: 提出了一种将法律和人工智能对齐技术相结合的方法，为脑基础模型创建一个受信任的框架，确保它们以用户的最佳利益行事。

Conclusion: 将脑基础模型置于信托地位对于在不损害自主性的情况下实现其潜力至关重要。

Abstract: Brain foundation models represent a new frontier in AI: instead of processing
text or images, these models interpret real-time neural signals from EEG, fMRI,
and other neurotechnologies. When integrated with brain-computer interfaces
(BCIs), they may enable transformative applications-from thought controlled
devices to neuroprosthetics-by interpreting and acting on brain activity in
milliseconds. However, these same systems pose unprecedented risks, including
the exploitation of subconscious neural signals and the erosion of cognitive
liberty. Users cannot easily observe or control how their brain signals are
interpreted, creating power asymmetries that are vulnerable to manipulation.
This paper proposes embedding fiduciary duties-loyalty, care, and
confidentiality-directly into BCI-integrated brain foundation models through
technical design. Drawing on legal traditions and recent advancements in AI
alignment techniques, we outline implementable architectural and governance
mechanisms to ensure these systems act in users' best interests. Placing brain
foundation models on a fiduciary footing is essential to realizing their
potential without compromising self-determination.

</details>


### [536] [A Risk Assessment Framework for Digital Identification Systems](https://arxiv.org/abs/2507.14755)
*Allison Woodruff,Dirk Balfanz,Will Drewry,Mariana Raykova*

Main category: cs.CY

TL;DR: 本研究提出了一个数字身份识别系统的风险评估框架和最佳实践，通过案例分析和专家评审开发而成，并在实践中证明了其有效性，旨在指导行业实现负责任的数字身份识别。


<details>
  <summary>Details</summary>
Motivation: 为了提高数字身份识别系统的隐私、安全和其他期望属性，并为产品审查、开发、政策和标准制定提供指导，以促进负责任的数字身份识别。

Method: 通过创建数字身份识别系统案例集，并运用专家分析和批判性审查来识别模式，最终形成风险评估框架和最佳实践。

Result: 该框架在组织内部的多次审查中进行了试点，结果证明该框架稳健且有益。

Conclusion: 本研究提出的风险评估框架和最佳实践有助于提高数字身份识别系统的隐私和安全，并为产品审查、开发、政策和标准制定提供指导，以促进负责任的数字身份识别。

Abstract: We introduce a risk assessment framework for digital identification systems,
as well as recommended best practices to enhance privacy, security, and other
desirable properties in these systems. To generate these resources, we created
a casebook of a wide range of digital identification systems, and we then
applied expert analysis and critique to identify patterns. We piloted the
framework on several reviews within our organization over a period of
approximately one year, and found it to be robust and helpful for those
reviews. This work is intended to inform product review and development,
product policy, and standards efforts, and to help guide a consistent
responsible approach to digital identification across the broader digital
identification ecosystem.

</details>


### [537] [Strategic Integration of AI Chatbots in Physics Teacher Preparation: A TPACK-SWOT Analysis of Pedagogical, Epistemic, and Cybersecurity Dimensions](https://arxiv.org/abs/2507.14860)
*N. Mohammadipour*

Main category: cs.CY

TL;DR: 本研究探讨了在物理教师教育中整合AI聊天机器人的战略和认识论方面。研究结果强调了AI的优势和劣势，并为在STEM教师准备中嵌入AI提供了实际可行的路线图。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在研究将AI驱动的聊天机器人战略性和认识论上负责任地整合到物理教师教育中。

Method: 本研究采用TPACK指导的SWOT框架，通过三个结构化学习活动，在大学级别的物理教学创新工具的毕业课程中，研究了AI驱动的聊天机器人对物理教师教育的战略和认识论上的负责任的整合。

Result: 研究结果强调了增强信息寻求行为、脚手架式教学规划和支持符号推理等内部优势，同时也出现了领域特定的不准确性、符号限制（如LaTeX错误渲染）以及过度依赖AI输出的风险等内部弱点。外部机会包括促进包容性教育、多语言参与和扩大的最近发展区（ZPD），而外部威胁包括提示注入风险、机构接入差距和网络安全漏洞。

Conclusion: AI聊天机器人若得到批判性脚手架的支持，可以支持物理教育中的元认知反思、伦理推理和教学创新，前提是实施与数字素养培训和机构支持相结合。

Abstract: This study investigates the strategic and epistemically responsible
integration of AI-powered chatbots into physics teacher education by employing
a TPACK-guided SWOT framework across three structured learning activities.
Conducted within a university-level capstone course on innovative tools for
physics instruction, the activities targeted key intersections of
technological, pedagogical, and content knowledge (TPACK) through
chatbot-assisted tasks: simplifying abstract physics concepts, constructing
symbolic concept maps, and designing instructional scenarios. Drawing on
participant reflections, classroom artifacts, and iterative feedback, the
results highlight internal strengths such as enhanced information-seeking
behavior, scaffolded pedagogical planning, and support for symbolic reasoning.
At the same time, internal weaknesses emerged, including domain-specific
inaccuracies, symbolic limitations (e.g., LaTeX misrendering), and risks of
overreliance on AI outputs. External opportunities were found in promoting
inclusive education, multilingual engagement, and expanded zones of proximal
development (ZPD), while external threats included prompt injection risks,
institutional access gaps, and cybersecurity vulnerabilities. By extending
existing TPACK-based models with constructs such as AI literacy,
prompt-crafting competence, and epistemic verification protocols, this research
offers a theoretically grounded and practically actionable roadmap for
embedding AI in STEM teacher preparation. The findings affirm that, when
critically scaffolded, AI chatbots can support metacognitive reflection,
ethical reasoning, and instructional innovation in physics education if
implementation is paired with digital fluency training and institutional
support.

</details>


### [538] [An Overview of the Risk-based Model of AI Governance](https://arxiv.org/abs/2507.15299)
*Veve Fry*

Main category: cs.CY

TL;DR: 本文批判了当前流行的基于风险的人工智能治理模式，认为其风险评估方式可能固化不平等。建议通过借鉴风险治理理论，结合多元化监管工具和响应式监管来改进AI治理。


<details>
  <summary>Details</summary>
Motivation: 文章旨在批判性地评估当前在欧洲、北美和澳大利亚广泛采用的基于风险的人工智能治理模式，并提出改进建议。

Method: 本文首先概述了欧盟和澳大利亚在人工智能监管方面的现有政策，特别是欧盟《人工智能法案》和澳大利亚工业、科学和监管部的安全负责任人工智能咨询。随后，本文对基于风险的人工智能治理方法提出了批评，指出其风险构建和计算方式可能复制现有的不平等，并引用朱莉娅·布莱克的观点，强调风险与危害的清晰区分，以及风险概念的规范性可能固化有害的叙述。

Result: 文章分析了基于风险的AI治理方法，认为其风险评估和分类存在问题，可能加剧社会不平等。文章呼吁对风险的定义和计算方式进行深入审视。

Conclusion: 现有基于风险的AI治理模型存在固有缺陷，其风险的计算方式可能固化现有不平等。建议借鉴风险治理的学术研究，采用多元化监管工具和响应式风险监管来改进AI治理模型。

Abstract: This paper provides an overview and critique of the risk based model of
artificial intelligence (AI) governance that has become a popular approach to
AI regulation across multiple jurisdictions. The 'AI Policy Landscape in
Europe, North America and Australia' section summarises the existing AI policy
efforts across these jurisdictions, with a focus of the EU AI Act and the
Australian Department of Industry, Science and Regulation's (DISR) safe and
responsible AI consultation. The 'Analysis' section of this paper proposes
several criticisms of the risk based approach to AI governance, arguing that
the construction and calculation of risks that they use reproduces existing
inequalities. Drawing on the work of Julia Black, it argues that risk and harm
should be distinguished clearly and that the notion of risk is problematic as
its inherent normativity reproduces dominant and harmful narratives about whose
interests matter, and risk categorizations should be subject to deep scrutiny.
This paper concludes with the suggestion that existing risk governance
scholarship can provide valuable insights toward the improvement of the risk
based AI governance, and that the use of multiple regulatory implements and
responsive risk regulation should be considered in the continuing development
of the model.

</details>


### [539] [Exploring the Use of Predictive Analytics by Austrian Tax Authorities: A Qualitative Study within the Task-Technology Fit Model](https://arxiv.org/abs/2507.15379)
*Simon Staudinger,Christoph G. Schuetz,Marina Luketina*

Main category: cs.CY

TL;DR: 奥地利税务机关利用预测分析技术进行税务审计，以提高效率和合规性。


<details>
  <summary>Details</summary>
Motivation: 为了确保所有个人和组织遵守适用的税法，防止非法逃税，并提高税收审计的效率。

Method: 通过与奥地利财政部预测分析能力中心合作，深入了解奥地利税务机关如何应用预测分析技术，并在此基础上，结合任务-技术契合框架进行了定性分析。

Result: 该研究为理解奥地利税务机关在税收审计中应用预测分析技术的具体方式及其与审计任务的契合度提供了见解。

Conclusion: 奥地利税务机关使用先进的预测分析技术来识别可疑的税务审计案例，以有效利用有限的审计资源。

Abstract: Taxes finance important government services that are now taken for granted in
our society, such as infrastructure, health care, or retirement pensions. Tax
authorities everywhere strive to ensure that all individuals and organizations
comply with applicable tax laws. In this regard, tax authorities must prevent
individuals and organizations from evading taxes in an illegal manner. To this
end, Austrian tax authorities employ state-of-the-art predictive analytics
technology for the selection of suspicious cases for tax audits, thus making
efficient use of scarce resources for tax auditing. In this paper, we explore
how Austrian tax authorities employ predictive analytics technology in tax
auditing and how well the use of such technology fits the characteristics of
the task at hand. We collaborated with the Austrian Federal Ministry of
Finance's Predictive Analytics Competence Center to obtain insights into the
application of predictive analytics technology by Austrian tax authorities. The
thus obtained insights serve as the basis for a qualitative analysis in the
context of the task-technology fit framework.

</details>


### [540] [Unequal Voices: How LLMs Construct Constrained Queer Narratives](https://arxiv.org/abs/2507.15585)
*Atreya Ghosal,Ashim Gupta,Vivek Srikumar*

Main category: cs.CY

TL;DR: LLM在描绘酷儿人群时存在刻板印象和局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）在生成内容时对社会群体，特别是酷儿人群的表征方式，旨在揭示LLM可能存在的刻板印象和话语他者化问题，从而促进对LLM公平性和包容性的理解。

Method: 通过分析LLM生成内容中对酷儿人群的表征，并将其与“默认人群”进行对比，来量化和描述其局限性。具体而言，研究关注有害表征、狭隘表征和话语他者化这三个维度，并提出了相应的假设进行检验。

Result: 研究结果表明，LLM在生成内容时对酷儿人群的描绘存在显著的局限性，具体表现在有害的表征、狭隘的表征以及话语上的他者化等方面，这与“默认群体”所享有的全面性表征形成鲜明对比。

Conclusion: LLM在描绘酷儿角色时存在显著的局限性，主要体现在有害的刻板印象、狭隘的话题范围以及话语上的他者化。

Abstract: One way social groups are marginalized in discourse is that the narratives
told about them often default to a narrow, stereotyped range of topics. In
contrast, default groups are allowed the full complexity of human existence. We
describe the constrained representations of queer people in LLM generations in
terms of harmful representations, narrow representations, and discursive
othering and formulate hypotheses to test for these phenomena. Our results show
that LLMs are significantly limited in their portrayals of queer personas.

</details>


### [541] [Why can't Epidemiology be automated (yet)?](https://arxiv.org/abs/2507.15617)
*David Bann,Ed Lowther,Liam Wright,Yevgeniya Kovalchuk*

Main category: cs.CY

TL;DR: AI在流行病学研究中的应用既有潜力也有局限性，需要AI和流行病学家共同努力才能充分发挥其作用。


<details>
  <summary>Details</summary>
Motivation: 阐述人工智能（AI），特别是生成式AI，为加速或自动化流行病学研究提供了新机遇，并探讨了AI干预的具体应用领域和潜在障碍。

Method: 通过现有数据集，对从文献审查到数据访问、分析、撰写和传播的流行病学任务进行梳理，并确定现有AI工具能够带来效率提升的领域。

Result: AI可以在编码和管理任务等领域提高生产力，但其效用受到现有AI模型（如文献综述中的幻觉）和人类系统（如数据访问障碍）的限制。AI生成的流行病学输出（包括全AI生成的论文）表明，最近开发的代理系统能够设计和执行流行病学分析，尽管质量不一。

Conclusion: AI工具有望提高流行病学研究的效率，尤其是在编码和管理任务方面，但其效用受到AI模型（如幻觉）和数据访问障碍等系统性限制。AI生成的流行病学输出（包括论文）表明，新开发的代理系统能够设计和执行流行病学分析，但质量参差不齐。要充分发挥AI的潜力，需要流行病学家和工程师之间的双向互动。

Abstract: Recent advances in artificial intelligence (AI) - particularly generative AI
- present new opportunities to accelerate, or even automate, epidemiological
research. Unlike disciplines based on physical experimentation, a sizable
fraction of Epidemiology relies on secondary data analysis and thus is
well-suited for such augmentation. Yet, it remains unclear which specific tasks
can benefit from AI interventions or where roadblocks exist. Awareness of
current AI capabilities is also mixed. Here, we map the landscape of
epidemiological tasks using existing datasets - from literature review to data
access, analysis, writing up, and dissemination - and identify where existing
AI tools offer efficiency gains. While AI can increase productivity in some
areas such as coding and administrative tasks, its utility is constrained by
limitations of existing AI models (e.g. hallucinations in literature reviews)
and human systems (e.g. barriers to accessing datasets). Through examples of
AI-generated epidemiological outputs, including fully AI-generated papers, we
demonstrate that recently developed agentic systems can now design and execute
epidemiological analysis, albeit to varied quality (see
https://github.com/edlowther/automated-epidemiology). Epidemiologists have new
opportunities to empirically test and benchmark AI systems; realising the
potential of AI will require two-way engagement between epidemiologists and
engineers.

</details>


### [542] [Left Leaning Models: AI Assumptions on Economic Policy](https://arxiv.org/abs/2507.15771)
*Maxim Chupilkin*

Main category: cs.CY

TL;DR: LLM在评估经济政策时，更关注失业、不平等、金融稳定和环境问题，而非传统的经济增长、通胀和债务。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能（特别是大型语言模型）在经济政策评估方面的考量因素，因为它们在经济学中的应用日益广泛，但其基本假设尚不明确。

Method: 使用联合实验来剖析影响LLM经济政策评估的主要因素。

Result: LLM对经济政策的评估结果在不同场景和模型之间表现出高度的一致性。

Conclusion: LLM对经济政策的评估对失业、不平等、金融稳定和环境危害等因素最为敏感，而对经济增长、通货膨胀和政府债务等传统宏观经济因素的敏感度较低。

Abstract: How does AI think about economic policy? While the use of large language
models (LLMs) in economics is growing exponentially, their assumptions on
economic issues remain a black box. This paper uses a conjoint experiment to
tease out the main factors influencing LLMs' evaluation of economic policy. It
finds that LLMs are most sensitive to unemployment, inequality, financial
stability, and environmental harm and less sensitive to traditional
macroeconomic concerns such as economic growth, inflation, and government debt.
The results are remarkably consistent across scenarios and across models.

</details>


### [543] [Just Put a Human in the Loop? Investigating LLM-Assisted Annotation for Subjective Tasks](https://arxiv.org/abs/2507.15821)
*Hope Schroeder,Deb Roy,Jad Kabbara*

Main category: cs.CY

TL;DR: LLM辅助标注改变了主观任务的标签分布，提高了模型性能的评估结果，但并未提高标注速度。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在标注任务中的广泛应用，特别是其在解释性任务中可能带来的对标签分布和模型评估的影响。

Method: 通过一项包含410名标注者和7000多个标注的预注册实验，测试了三种AI辅助条件与对照组的差异，使用了两个模型和两个数据集。

Result: LLM建议未提高标注速度，但增强了标注者的自信心。标注者倾向于采纳LLM的建议，导致标签分布发生显著变化。在利用LLM辅助标注评估模型性能时，模型性能的报告值显著提升。

Conclusion: LLM驱动的标注在主观性强、存在多个可能答案的解释性任务中，会显著改变标签分布，影响对LLM性能的评估以及后续的社会科学分析。研究表明，即使LLM建议不提高标注速度，也会增加标注者的自信心，并被标注者大量采纳。在利用这些标注评估LLM性能时，会显著提高报告的模型性能。因此，理解LLM辅助标注对主观定性任务、训练和测试黄金数据创建以及主观任务上NLP系统评估的影响至关重要。

Abstract: LLM use in annotation is becoming widespread, and given LLMs' overall
promising performance and speed, simply "reviewing" LLM annotations in
interpretive tasks can be tempting. In subjective annotation tasks with
multiple plausible answers, reviewing LLM outputs can change the label
distribution, impacting both the evaluation of LLM performance, and analysis
using these labels in a social science task downstream. We conducted a
pre-registered experiment with 410 unique annotators and over 7,000 annotations
testing three AI assistance conditions against controls, using two models, and
two datasets. We find that presenting crowdworkers with LLM-generated
annotation suggestions did not make them faster, but did improve their
self-reported confidence in the task. More importantly, annotators strongly
took the LLM suggestions, significantly changing the label distribution
compared to the baseline. When these labels created with LLM assistance are
used to evaluate LLM performance, reported model performance significantly
increases. We believe our work underlines the importance of understanding the
impact of LLM-assisted annotation on subjective, qualitative tasks, on the
creation of gold data for training and testing, and on the evaluation of NLP
systems on subjective tasks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [544] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: SCHEMORA是一种新颖的LLM驱动的模式匹配框架，通过混合检索提高了效率和准确性，并在MIMIC-OMOP基准测试中取得了最佳结果，这是第一个开源的LLM模式匹配方法。


<details>
  <summary>Details</summary>
Motivation: 模式匹配是集成异构数据源和改进数据集发现的关键，但它仍然是一个复杂且资源密集型的问题。

Method: SCHEMORA是一个基于提示的模式匹配框架，它结合了大语言模型和混合检索技术，利用模式元数据和向量/词汇检索来识别候选匹配项，无需标记数据或详尽的成对比较。

Result: SCHEMORA在MIMIC-OMOP基准测试中，HitRate@5的提升了7.49%，HitRate@3的提升了3.75%，达到了新的最先进水平。

Conclusion: SCHEMORA通过结合大语言模型和混合检索技术，在模式匹配方面取得了新的最先进性能，并在MIMIC-OMOP基准测试中展示了显著的准确性和可扩展性改进。

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


### [545] [Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining](https://arxiv.org/abs/2507.14813)
*Sanjay Sri Vallabh Singapuram,Ronald Dreslinski,Nishil Talati*

Main category: cs.DB

TL;DR: Mayura是一个新框架，通过MG-Tree和联合挖掘来加速多个时序动机的挖掘，比现有方法快2.4倍（CPU）和1.7倍（GPU）。


<details>
  <summary>Details</summary>
Motivation: 传统的动机挖掘方法独立处理每个查询，当多个动机之间存在相似子结构时，会导致显著的冗余计算。

Method: 提出了一种名为Mayura的新颖框架，该框架通过利用查询中的结构和时间共性来统一挖掘多个时序动机。核心是一种称为MG-Tree的分层数据结构，用于组织相关的动机并实现常见搜索路径的重用。还开发了一种联合挖掘算法和一种利用CPU和GPU的灵活运行时。

Result: 在多种真实数据集上的经验评估表明，Mayura比单独挖掘每个动机的最先进技术有显著的改进，在CPU上平均加速2.4倍，在GPU上平均加速1.7倍，同时保持了高风险应用所需的确切性。

Conclusion: Mayura框架通过引入MG-Tree和联合挖掘算法，实现了多时序动机的统一挖掘，显著减少了冗余计算，并在CPU和GPU上实现了可扩展的性能。

Abstract: Temporal graphs serve as a critical foundation for modeling evolving
interactions in domains ranging from financial networks to social media. Mining
temporal motifs is essential for applications such as fraud detection,
cybersecurity, and dynamic network analysis. However, conventional motif mining
approaches treat each query independently, incurring significant redundant
computations when similar substructures exist across multiple motifs. In this
paper, we propose Mayura, a novel framework that unifies the mining of multiple
temporal motifs by exploiting their inherent structural and temporal
commonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a
hierarchical data structure that organizes related motifs and enables the reuse
of common search paths, thereby reducing redundant computation. We propose a
co-mining algorithm that leverages the MG-Tree and develop a flexible runtime
capable of exploiting both CPU and GPU architectures for scalable performance.
Empirical evaluations on diverse real-world datasets demonstrate that Mayura
achieves substantial improvements over the state-of-the-art techniques that
mine each motif individually, with an average speed-up of 2.4x on the CPU and
1.7x on the GPU, while maintaining the exactness required for high-stakes
applications.

</details>


### [546] [Towards Temporal Knowledge Graph Alignment in the Wild](https://arxiv.org/abs/2507.14475)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Xiang Zhao,Jiuyang Tang,Lei Chen*

Main category: cs.DB

TL;DR: HyDRA 是首个处理 TKGA-Wild 的方法，通过多尺度超图检索增强生成和尺度编织协同机制来解决时间对齐中的复杂性、不完整性和不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 TKGA 方法假设统一的时间元素标准和简化的时间结构，无法处理多尺度时间元素纠缠和跨源时间结构不平衡的 TKGA-Wild 任务。

Method: HyDRA 通过多尺度超图检索增强生成来处理 TKGA-Wild 的挑战，并设计了一个新的尺度编织协同机制来整合尺度内交互和尺度间冲突检测。

Result: 在新的 BETA 和 WildBETA 数据集以及六个代表性基准上的广泛实验表明，HyDRA 能够有效处理现实世界中的 TKGA-Wild 挑战，并且优于现有方法。

Conclusion: HyDRA 提出了 TKGA-Wild 的新范式，在效率和可扩展性方面表现出色，并始终优于 24 个基线方法。

Abstract: Temporal Knowledge Graph Alignment (TKGA) seeks to identify equivalent
entities across heterogeneous temporal knowledge graphs (TKGs) for fusion to
improve their completeness. Although some approaches have been proposed to
tackle this task, most assume unified temporal element standards and simplified
temporal structures across different TKGs. They cannot deal with TKGA in the
wild (TKGA-Wild), where multi-scale temporal element entanglement and
cross-source temporal structural imbalances are common. To bridge this gap, we
study the task of TKGA-Wild and propose HyDRA, a new and effective solution.
HyDRA is the first to reformulate the task via multi-scale hypergraph
retrieval-augmented generation to address the challenges of TKGA-Wild.In
addition, we design a new scale-weave synergy mechanism for HyDRA, which
incorporates intra-scale interactions and cross-scale conflict detection. This
mechanism is designed to alleviate the fragmentation caused by multi-source
temporal incompleteness and resolves inconsistencies arising from complex and
uneven temporal event density distributions, thereby enhancing the model
capacity to handle the intricacies of real-world temporal alignment. Finally,
there is no standard benchmark that captures these challenges of TKGA-Wild and
effectively evaluates existing methods. To this end, we formally propose to
benchmark challenges for TKGA-Wild and validate the effectiveness of the method
by establishing two new datasets(BETA and WildBETA). Extensive experiments on
the new datasets and six representative benchmarks show that BETA and WildBETA
better reflect real-world challenges. Meanwhile, HyDRA proposes a new paradigm
for TKGA-Wild, consistently outperforming 24 competitive baselines, while
maintaining strong efficiency and scalability.

</details>


### [547] [Opening The Black-Box: Explaining Learned Cost Models For Databases](https://arxiv.org/abs/2507.14495)
*Roman Heinrich,Oleksandr Havrylov,Manisha Luthra,Johannes Wehrstein,Carsten Binnig*

Main category: cs.DB

TL;DR: 通过AI可解释性技术解决了LCMs的黑箱问题，提高了可调试性。


<details>
  <summary>Details</summary>
Motivation: 现有的LCMs虽然在成本预测方面表现优越，但在某些查询计划上仍存在较大的预测误差（尤其是在长尾数据上）。然而，由于其复杂的深度神经网络模型，LCMs的准确性下降问题难以理解和排查。因此，需要一种方法来解释LCMs的预测行为，以便进行系统性的故障排除。

Method: 提出了一种新的可解释性技术，该技术扩展了现有的AI模型通用可解释性方法，并对其进行了显著调整以适用于LCMs。

Result: 开发了一种新的可解释性技术，并提供了一个交互式工具来展示LCMs的可解释性。

Conclusion: 该研究提出了首个用于打开数据库成本模型（LCMs）黑箱的方法，通过引入AI可解释性技术，为LCMs的调试和问题修复奠定了基础。

Abstract: Learned Cost Models (LCMs) have shown superior results over traditional
database cost models as they can significantly improve the accuracy of cost
predictions. However, LCMs still fail for some query plans, as prediction
errors can be large in the tail. Unfortunately, recent LCMs are based on
complex deep neural models, and thus, there is no easy way to understand where
this accuracy drop is rooted, which critically prevents systematic
troubleshooting. In this demo paper, we present the very first approach for
opening the black box by bringing AI explainability approaches to LCMs. As a
core contribution, we developed new explanation techniques that extend existing
methods that are available for the general explainability of AI models and
adapt them significantly to be usable for LCMs. In our demo, we provide an
interactive tool to showcase how explainability for LCMs works. We believe this
is a first step for making LCMs debuggable and thus paving the road for new
approaches for systematically fixing problems in LCMs.

</details>


### [548] [IDSS, a Novel P2P Relational Data Storage Service](https://arxiv.org/abs/2507.14682)
*Massimo Cafaro,Italo Epicoco,Marco Pulimeno,Lunodzo J. Mwinuka,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: IDSS是一种新的大数据存储解决方案，它使用点对点网络和关系数据库来高效处理和管理大量数据。


<details>
  <summary>Details</summary>
Motivation: 随着数据生成速率的快速增加，传统数据库管理系统在处理大规模和异构数据时面临可扩展性和效率方面的挑战。

Method: 该研究介绍了IDSS（InnoCyPES数据存储服务）的架构、设计和实现细节，利用点对点框架和基于共同模式的关系数据库架构来支持分布式查询，并提出支持复杂分布式查询处理的方法。

Result: IDSS能够支持分布式查询，并能有效、健壮地管理海量数据。

Conclusion: IDSS是一个利用点对点网络和嵌入式关系数据库的新型大规模数据存储工具，能够有效应对大数据挑战。

Abstract: The rate at which data is generated has been increasing rapidly, raising
challenges related to its management. Traditional database management systems
suffer from scalability and are usually inefficient when dealing with
large-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data
Storage Service), a novel large-scale data storage tool that leverages
peer-to-peer networks and embedded relational databases. We present the IDSS
architecture and its design, and provide details related to the implementation.
The peer-to-peer framework is used to provide support for distributed queries
leveraging a relational database architecture based on a common schema.
Furthermore, methods to support complex distributed query processing, enabling
robust and efficient management of vast amounts of data are presented.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [549] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: LLM在软件工程中用于生成单元测试，链式思考提示策略和代码文档字符串可提高测试质量，Gemini 2.5 Pro表现突出。


<details>
  <summary>Details</summary>
Motivation: 在软件工程中，自动生成单元测试可以提高开发效率，因此研究LLM生成单元测试的影响因素。

Method: 通过链式思考提示策略，结合代码文档字符串和完整实现，评估不同LLM生成单元测试的质量和充分性。

Result: 包含文档字符串的代码能显著提高测试充分性，链式思考提示策略效果最佳，M5（Gemini 2.5 Pro）在变异分数和分支覆盖率方面表现最佳。

Conclusion: LLM通过链式思考提示策略生成单元测试，可以显著提高代码覆盖率和变异分数，M5（Gemini 2.5 Pro）表现最优。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [550] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目旨在通过NLP、本体建模、构件重用和LLM等技术，研究从非形式化需求生成可验证规范的自动化和半自动化方法。


<details>
  <summary>Details</summary>
Motivation: 从非形式化的自然语言需求生成形式化需求规范是关键挑战。

Method: 通过生成形式化需求规范和验证实现来确保软件正确性。

Result: 我们提出了一个名为VERIFAI的项目，旨在研究用于从非形式化需求生成可验证规范的自动化和半自动化方法。

Conclusion: 自然语言处理、基于本体的领域建模、构件重用和大型语言模型等技术可以用于生成可验证的规范。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [551] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 该研究通过共享词汇系统解决了软件工程中的沟通问题，提高了效率和文档质量。


<details>
  <summary>Details</summary>
Motivation: 软件工程协作中的沟通差距会导致误解、效率低下和缺陷，因此需要改进沟通实践。

Method: 采用设计科学研究（DSR）框架，包括问题识别、方法开发和经验验证三个迭代阶段。通过主题分析、访谈和控制实验来收集和分析数据。

Result: 共享词汇系统在初期会增加开销，但随着时间的推移，可以显著提高信息密度、文档清晰度和协作效率。

Conclusion: 该研究提出了一个共享词汇系统，以解决软件工程中的沟通差距，并被证明可以提高信息密度、文档清晰度和协作效率。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [552] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 通过合并子标记表示来提高代码语言模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决代码语言模型的标记化输出比传统编译器和解释器更长的问题，这可能导致计算开销增加。

Method: 提出了两种合并子标记（例如构成单个标识符的子标记）隐藏表示的策略：一种基于平均表示，另一种基于学习方法。这两种方法都可以无缝集成到现有的代码语言模型中。

Result: 实验结果表明，所提出的策略可以将浮点运算次数减少 1% 到 19%。在下游性能方面，漏洞检测任务的 F1 分数下降了 1.82 分，而代码翻译任务的 CodeBLEU 分数提高了 2.47 分。

Conclusion: 该研究提出并评估了两种合并子标记隐藏表示的策略，以提高代码语言模型的计算效率和下游性能，并在一系列任务和模型上取得了积极成果，尽管在漏洞检测任务中观察到轻微性能下降，但在代码翻译任务中有所改善。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [553] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 架构退化是一个日益严峻的社会技术问题，尽管现有研究在检测方面取得了进展，但在统一的定义、持续的修复策略以及工具和衡量指标的整合方面仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 架构退化，也称为侵蚀、衰减或老化，会影响系统质量、可维护性和适应性。尽管得到了广泛认可，但现有文献在定义、衡量指标和修复策略方面存在定义不清晰、分散的问题。本研究旨在通过识别学术界和灰色文献中关于架构退化的定义、原因、衡量指标、工具和修复方法，来实现对其的统一理解。

Method: 我们进行了一项包含108项研究的多方文献综述，提取了定义、原因、衡量指标、度量方法、工具和修复策略。我们开发了一个包含架构、代码和流程债务的分类法，以探索定义的演变、方法趋势和研究空白。

Result: 架构退化已从低级别问题转变为社会技术问题。定义现已涵盖代码违规、设计漂移和结构衰减。其原因包括架构方面（例如，文档不佳）、代码方面（例如，匆忙修复）和流程债务（例如，知识流失）。我们确定了 54 个衡量指标和 31 种度量技术，重点关注坏味道、内聚/耦合和演化。然而，大多数工具仅能检测问题，却很少支持持续或预防性的修复。架构退化既是技术性的，也是组织性的。虽然问题检测已得到充分研究，但持续修复仍然缺失。

Conclusion: 目前的研究揭示了衡量指标、工具和修复逻辑之间存在协同整合的缺失，这亟需提出更全面的、前瞻性的策略以实现可维护的架构。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [554] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 对八个顶级行业会议五年的数据进行分析，发现Kubernetes、云原生、Serverless和容器是主导技术，主要用于DevOps后期，但对早期阶段的关注有限。


<details>
  <summary>Details</summary>
Motivation: 随着云计算、微服务和容器的兴起，软件架构实践日益多样化，理解这些转变至关重要。

Method: 通过对八个主要行业会议五年间的5,677个演讲进行分析，利用大型语言模型和专家验证提取技术、目的和使用背景，并探索技术之间的相互关系以及它们在DevOps和部署流水线中的作用。

Result: 研究分析了450种技术，发现Kubernetes、云原生、Serverless和容器在频率和核心性方面占主导地位。从业者展示的技术主要与部署、通信、人工智能和可观测性相关。识别出涵盖自动化、协调、云AI、监控和云边五个技术社区。大多数技术跨越多个DevOps阶段并支持混合部署。

Conclusion: 该研究揭示了Kubernetes和Serverless等少数核心技术在当代软件架构实践中占据主导地位，但它们主要应用于DevOps后期阶段，对规划和编码等早期阶段的关注有限。研究还展示了从业者如何根据目的和背景来构建技术认知，反映了不断变化的行业优先事项。最后，研究强调了只有研究才能为架构设计、质量和演进提供更全面的视角。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [555] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ 利用 LLM 解析 OpenCV API 文档，提取约束和依赖关系，生成测试用例，成功发现了 17 个新 bug。


<details>
  <summary>Details</summary>
Motivation: OpenCV 库中的 bug 会影响下游的计算机视觉应用，确保其可靠性至关重要。

Method: VISTAFUZZ 利用 LLM 解析 API 文档，提取输入参数的约束和依赖关系，并基于这些信息生成新的输入值来系统地测试目标 API。

Result: 在对 OpenCV 库中的 330 个 API 进行测试时，VISTAFUZZ 发现了 17 个新 bug，其中 10 个已确认，5 个已修复。

Conclusion: VISTAFUZZ 是一种利用大型语言模型（LLM）对 OpenCV 库进行文档引导模糊测试的新技术，成功发现了 17 个新 bug，其中 10 个已确认，5 个已修复，证明了其在确保 OpenCV 库可靠性方面的有效性。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [556] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 研究发现许多开源许可证存在变体，可能导致下游依赖项不兼容。本研究提出了LV-Parser和LV-Compat工具，提高了许可证合规性分析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的开源许可证合规性分析工具未能充分考虑许可证变体的存在，导致合规性分析的有效性和效率面临挑战。本研究旨在填补这一知识空白，并提供实用的解决方案。

Method: 通过对PyPI生态系统中的许可证变体进行实证研究，提出并评估了基于差异化技术和大型语言模型的LV-Parser以及检测许可证不兼容性的LV-Compat自动化流程。

Result: 研究发现，文本许可证变体普遍存在，但实质性修改仅占2%。然而，这些变体导致了显著的合规性问题，10.7%的下游依赖项存在许可证不兼容。LV-Parser的准确率为0.936，计算成本降低了30%。LV-Compat识别的软件包不兼容数量是现有方法的5.2倍，精确率为0.98。

Conclusion: 本研究首次对软件打包生态系统中的许可证变体进行了实证研究，并提出了LV-Parser和LV-Compat工具，以帮助开发者应对复杂的开源许可证合规性问题。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [557] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 该研究探索了使用大型语言模型（LLM）来辅助编写Alloy形式化描述。实验证明，LLM在根据自然语言或现有Alloy代码生成新的、等价的或补全草图方面表现出色，为软件开发中的形式化描述编写带来了效率提升。


<details>
  <summary>Details</summary>
Motivation: 为了在开发安全可靠的软件系统时，克服在编写声明式（declarative）形式化描述时遇到的挑战。

Method: 通过一项对照实验，研究使用大型语言模型（LLM）编写Alloy形式化描述。具体包括三个方面：1. LLM根据英文自然语言描述生成完整的Alloy形式化描述；2. LLM根据给定的Alloy形式化描述生成等价的替代描述；3. LLM补全Alloy形式化描述的草图，根据自然语言描述的内容填补其中的空白。实验选用了11个已知的示例，并使用了ChatGPT和DeepSeek两个流行的LLM。

Result: LLM在根据自然语言描述或Alloy语言本身来综合生成Alloy形式化描述方面表现良好，能够生成多个不同的有效方案，并且可以根据自然语言描述补全Alloy形式化描述的草图，而无需测试用例。

Conclusion: LLM在根据自然语言描述或Alloy语言本身来综合生成Alloy形式化描述方面表现良好，能够生成多个不同的有效方案，并且可以根据自然语言描述补全Alloy形式化描述的草图，而无需测试用例。LLM在形式化描述的编写能力上取得了令人兴奋的进展，有助于发挥形式化描述在软件开发中的关键作用，并增强构建健壮软件的能力。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [558] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 该论文提出了一种名为 Robin's Rule 的新算法，可以高效地为航空电子系统中常见的单一布尔表达式（SBEs）生成满足唯一原因 MC/DC（一种严格的软件测试标准）的最优测试用例集，验证结果表明该方法比现有商业工具更有效。


<details>
  <summary>Details</summary>
Motivation: Modified Condition/Decision Coverage (MC/DC) 是确保关键系统的可靠性和安全性的强制性结构覆盖标准。虽然其最严格的形式——唯一原因 MC/DC 提供了最高的保证，但关于其有效测试生成的公开研究却很少。分析大规模航空电子系统表明，99.7% 的所有条件判断实际上是单一布尔表达式（SBEs），这是应用唯一原因 MC/DC 的理想结构。因此，研究 MC/DC 的有效测试生成具有重要意义。

Method: Robin's Rule”是一种确定性算法，可以直接构造一个包含 N + 1 个测试用例的最小测试集，以保证满足具有 N 个条件的 SBEs 的 100% 唯一原因 MC/DC，而无需生成完整的真值表。

Result: 通过将 TCAS-II 规范重新表述为 SBEs 来构建基准，并使用行业标准的认证商业工具验证结果，证实了所提出的方法能够以理论上最小的测试用例数持续实现 100% 的覆盖率，并且比商业工具更有效。

Conclusion: 该论文提出了一种名为“Robin

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [559] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: HistoryFinder是一个新的方法历史生成工具，通过改进的Oracle和设计，在准确性和效率方面优于现有工具，是软件工程任务的理想选择。


<details>
  <summary>Details</summary>
Motivation: 为了更有效、更准确地重建方法变更历史，以支持软件工程中的维护、重构和理解等任务，并克服现有工具在地面真实Oracle方面的局限性。

Method: 通过结合自动化分析和专家手动验证来构建新的Oracle，并开发了一个名为HistoryFinder的新方法历史生成工具。

Result: HistoryFinder在精确率、召回率和F1分数方面持续优于CodeShovel、CodeTracker、IntelliJ和基于Git的基线工具，同时具有有竞争力的运行时性能，平均和中位数执行时间最低。

Conclusion: HistoryFinder在准确性和效率方面均优于现有工具，是整体上的最佳选择。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [560] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 本文研究了如何通过超参数调优和提示工程来优化 Llama 3.1 模型在领域建模任务中的表现，尤其是在医疗数据模型上取得了显著的准确率提升，并在其他领域也展现出普遍的改进效果。


<details>
  <summary>Details</summary>
Motivation: 通用大型语言模型（LLM）在软件工程任务（包括模型驱动工程）中提高了自动化水平，但将其用于领域建模存在局限性。微调模型需要大量计算资源，并可能导致灾难性遗忘。

Method: 本文探索了超参数调谐和提示工程如何提高 Llama 3.1 模型从文本描述生成领域模型的准确性。研究人员使用基于搜索的方法为特定的医疗数据模型调谐超参数，并在十个不同的应用领域测试了优化后的超参数。

Result: 通过超参数调优和提示工程，在医疗数据模型上实现了 6.9% 的准确率提升。在其他十个领域模型上的测试也表明，这种方法在近乎所有情况下都提高了模型性能。

Conclusion: 通过结合超参数调优和提示工程，可以提高 LLM 在不同领域生成领域模型的准确性，尽管并非所有解决方案都普遍适用。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [561] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 本研究旨在探讨性别如何影响开发者使用代码生成工具（CGT）的体验和绩效，以期促进CGT设计的公平性和包容性。


<details>
  <summary>Details</summary>
Motivation: 随着像Windsurf和GitHub Copilot等代码生成工具（CGT）的日益普及，它们正在革新编程工作流程，并引发关于公平性和包容性的关键问题。虽然CGT可能提高生产力，但其在不同用户群体中的有效性尚未得到充分研究。我们假设开发者的性别会影响其与CGT的交互方式，从而影响任务结果和认知负荷，因为以往的研究表明性别差异会影响技术使用和认知处理。

Method: 本研究将采用混合被试设计，招募54名参与者，并按性别均等划分，采用平衡设计。参与者将完成两项编程任务（中等到困难难度），分别仅在代码生成工具（CGT）辅助下和仅通过互联网访问下进行。任务顺序和条件将进行平衡，以减少顺序效应。数据收集将包括认知负荷调查、屏幕录制以及任务绩效指标（如完成时间、代码正确性和CGT交互行为）。将进行统计分析，以识别CGT使用方面存在统计学显著差异的因素。

Result: 研究结果尚未公布，但预期将揭示开发者在CGT交互和绩效方面的性别差异，为未来CGT设计提供信息，并帮助解决不同用户群体的可用性及潜在的交互模式差异。

Conclusion: 虽然研究结果尚未公布，但该研究为推动代码生成工具（CGT）设计中的公平、问责、透明和道德（FATE）奠定了基础。预期结果将为包容性人工智能实践和所有用户的公平工具开发做出贡献。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [562] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt框架利用多模态反馈和PPA感知优化，使LLM能够生成满足功耗、性能和面积指标的高质量、可综合Verilog代码，解决了LLM在硬件设计中的关键质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在硬件设计中主要关注功能正确性，忽略了对工业级设计至关重要的PPA（功耗、性能、面积）指标。VeriOpt旨在弥合这一差距。

Method: VeriOpt框架，利用基于角色的提示（如规划师、程序员、审查员、评估员）和PPA感知优化，将LLM交互结构化，并将PPA约束集成到提示流程中，结合多模态反馈（如综合报告、时序图）来实现高效代码生成。

Result: VeriOpt相较于基线LLM生成的RTL，在功耗方面实现了高达88%的降低，面积减少了76%，时序收敛性提高了73%，并且功能评估成功率达到了86%。

Conclusion: VeriOpt通过结合基于角色的提示和PPA感知优化，实现了在不牺牲功能正确性的前提下生成高效硬件设计，将LLM的应用扩展到生产级硬件设计。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [563] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope通过利用调用链感知多视图上下文和仓库结构语义图，改进了仓库级代码生成。它通过新颖的调用链预测和结构保留序列化解决了现有方法的局限性，并且仅依赖静态分析，效率高且泛化性强。在基准测试中，RepoScope的性能优于最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库级代码生成方法通常采用检索增强生成（RAG）技术，但这些方法在有效识别真正相关的、能捕捉仓库丰富语义的上下文方面存在困难，并且其上下文视角仍然狭窄。此外，大多数方法在构建提示时未能考虑检索到的代码中的结构关系，阻碍了LLM准确理解上下文的能力。

Method: RepoScope利用调用链感知多视图上下文进行仓库级代码生成。它构建了一个仓库结构语义图（RSSG），并检索了一个综合的四视图上下文，整合了结构和基于相似性的上下文。RepoScope提出了一种新颖的调用链预测方法，利用仓库的结构语义来改进目标函数中被调用者的识别。此外，它提出了一种保留结构的序列化算法用于提示构建，确保了LLM上下文的一致性。RepoScope仅依赖静态分析，无需额外的训练或多次LLM查询。

Result:  RepoScope通过整合结构和相似性信息，并考虑代码的结构关系，显著提高了仓库级代码生成的效果，在基准测试中取得了优于现有方法的性能。

Conclusion: RepoScope在CoderEval和DevEval等广泛使用的代码生成基准上进行了评估，结果显示其性能优于最先进的方法，在pass@1得分上取得了高达36.35%的相对提升。此外，进一步的实验强调了RepoScope在不同任务中改进代码生成的潜力，以及其与现有方法有效结合的能力。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [564] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: 本研究提出了RequireCEG，一个通过因果效应图（CEGs）和神经符号协作来解决用户自然语言需求模糊性和因果逻辑问题的代理，以改进Gherkin场景的生成和审查。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理非专业用户自然语言需求中的模糊性和因果逻辑，限制了生成式软件开发。

Method: RequireCEG通过特征树分析用户叙述，构建基于因果关系的自愈CEGs，并利用CEGs审查和优化Gherkin场景，以确保需求的一致性。

Result: 所提出的方法在RGPair基准数据集上的实验表明，实现了87%的覆盖率，并将多样性提高了51.88%。

Conclusion: RequireCEG通过结合因果效应图（CEGs）和神经符号协作架构，有效解决了自然语言需求中的歧义和因果逻辑问题，提高了Gherkin场景的一致性和准确性。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [565] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本研究介绍了 AIDev，一个包含超过 456,000 个拉取请求的大型数据集，用于分析 OpenAI Codex、Devin、GitHub Copilot、Cursor 和 Claude Code 等 AI 编码代理在软件开发中的实际应用。研究发现，AI 代理速度快但接受率低且代码复杂度低。AIDev 为研究 AI 驱动的软件工程（SE 3.0）提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 驱动的开发工具（如自主编码代理）的兴起，软件工程正进入一个新时代（SE 3.0）。然而，目前对这些 AI 代理在真实世界中的运作方式的理解仍然有限，并且缺乏用于实证研究的大规模数据集。本研究旨在通过创建一个名为 AIDev 的大型数据集，来填补这一空白，为研究 AI 代理在软件开发中的表现、影响和潜在挑战提供支持。

Method: 本研究通过收集和分析一个名为 AIDev 的大型数据集来研究 AI 编码代理在软件开发中的实际应用。该数据集包含了 456,000 多个拉取请求（PR），涵盖了五个主要的 AI 代理（OpenAI Codex、Devin、GitHub Copilot、Cursor 和 Claude Code）在 61,000 个代码库和 47,000 名开发人员中的活动。研究人员对这些 PR 的元数据进行了分析，包括作者身份、审查时间、代码更改和集成结果，并与人类开发者的活动进行了比较。

Result: 研究结果表明，AI 代理在提交代码的速度上通常超过人类开发者，但它们提交的拉取请求被接受的频率较低，这表明在信任和效用方面存在差距。此外，尽管 AI 代理能够显著加快代码提交速度（例如，一个开发者在三天内提交的 PR 数量等于过去三年的总和），但这些代码在结构复杂度上相对较低。AIDev 数据集使研究人员能够进行更深入的分析，例如评估代理的就绪程度、优化其性能以及理解人机协作模式。

Conclusion: AIDev 是一个包含超过 456,000 个拉取请求的大型数据集，涵盖了 OpenAI Codex、Devin、GitHub Copilot、Cursor 和 Claude Code 五种领先的 AI 编码代理在 61,000 个代码库和 47,000 名开发人员中的实际操作情况。该数据集为研究 AI 代理在软件开发中的基准测试、就绪性、优化、协作建模和治理提供了前所未有的实证基础。研究表明，虽然 AI 代理在速度上通常优于人类，但它们的拉取请求被接受的频率较低，并且它们提交的代码在结构上更简单。AIDev 是一个可扩展、可分析且面向 SE 和 AI 社区的资源，旨在为 SE 3.0 研究提供现实世界的证据，并支持下一代人机协作。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [566] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: GenAI can transform car software development through LLMs, RAG, and VLMs, improving requirements, compliance, and coding. A new workflow and industry survey results are included.


<details>
  <summary>Details</summary>
Motivation: To explore the adoption of Generative Artificial Intelligence (GenAI) in automotive software development, a field characterized by lengthy and expensive procedures due to extensive requirements and strict standardization, aiming to reduce human intervention and effort.

Method: Literature review of GenAI technologies (LLMs, RAG, VLMs) applied to automotive software development, including an analysis of prompting techniques for code generation. A generalized workflow was derived, and survey results from industry partners were summarized.

Result: The paper explores GenAI applications in automotive software development, covering requirements handling, compliance, and code generation using LLMs, RAG, and VLMs. It also discusses prompting techniques and presents a generalized GenAI-aided workflow along with survey findings from industry partners.

Conclusion: GenAI has the potential to revolutionize automotive software development by streamlining processes like requirements handling, compliance, and code generation. LLMs, RAG, and VLMs are key technologies, with prompting techniques crucial for code generation. A generalized workflow and survey insights are also presented.

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [567] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: LLMs可用于自动生成和评估敏捷开发中的用户故事，生成质量与人类相当但创造性较低，并能有效评估语义质量，减少人工。


<details>
  <summary>Details</summary>
Motivation: 需求获取是需求工程中最具挑战性的活动之一，因为需求分析师难以理解和转化复杂的需求。尽管自动化工具可以评估语法的质量，但语义指标（如语言清晰度、内部一致性）的评估仍然是手动且耗时的。本研究旨在探索LLMs如何帮助在敏捷框架内自动化需求获取，将需求定义为用户故事。

Method: 本研究使用了10种最先进的大型语言模型（LLMs），通过模拟客户访谈来自动生成用户故事（US）。研究评估了LLMs生成的US的质量，并与人类（领域专家和学生）生成的US进行了比较。此外，研究还探讨了LLMs在自动评估US语义质量方面的能力。

Result: LLMs能够生成在覆盖度和风格质量上与人类相似的US，但多样性和创造性较低。LLMs生成的US在质量上通常与人类相当，但满足验收质量标准的频率较低。LLMs在提供明确的评估标准时，能够可靠地评估US的语义质量，并有可能减少大规模评估中的人力工作量。

Conclusion: 大型语言模型（LLMs）能够生成与人类相似的用户故事（US），在覆盖度和风格质量方面表现相当，但在多样性和创造性方面有所欠缺。LLMs生成的US在质量上与人类相当，但满足验收质量标准的频率较低。然而，LLMs在提供清晰评估标准的情况下，能够可靠地评估US的语义质量，并有潜力减少大规模评估中的人力投入。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [568] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: DLMMM improves deep learning framework testing by quantitatively measuring operator variety and execution time, fusing these metrics for trade-offs, and using multi-level guidance for test generation, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning framework testing methods predominantly measure model bug detection effectiveness using heuristic indicators, which have limitations: they fail to quantitatively measure operator combination variety, neglect measuring model execution time, and overlook the correlation between different model measurements, relying on single-indicator heuristic guidance without considering trade-offs.

Method: DLMMM quantitatively measures model's bug detection performance, operator combination variety, and model execution time. Then, it fuses these measurements based on their correlation to achieve trade-offs. Finally, it designs multi-level heuristic guidance for test input model generation.

Result: DLMMM addresses the limitations of existing methods by quantitatively measuring operator combination variety and execution time, fusing these measurements based on their correlation to achieve trade-offs, and designing multi-level heuristic guidance for test input model generation to enhance testing effectiveness.

Conclusion: DLMMM is the first deep learning framework testing method to incorporate multiple model measurements for heuristic guidance and fuse these measurements to achieve trade-offs, quantitatively measuring bug detection performance, operator combination variety, and execution time, while also designing multi-level heuristic guidance for test input model generation.

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [569] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 文化影响需求工程，尤其是在多元化团队中。本研究关注孟加拉国文化在需求工程中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发中利益相关者的多样性增加，了解文化影响有助于避免误解和冲突，并支持IT行业的多元化和包容性。孟加拉国作为一个拥有独特社会文化特征且被忽视的研究领域，其需求工程实践值得关注。

Method: 通过对孟加拉国文化背景下的需求工程过程进行调查，识别影响需求工程活动的文化因素。

Result: 阐述了孟加拉国文化对需求工程实践的影响，并提出了相关见解。

Conclusion: 未来需要继续关注孟加拉国文化对需求工程的影响，并提出相应的策略。

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [570] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 本研究系统性地回顾了近两年来用户画像在需求工程中的应用，发现AI和模板化方法在用户画像的构建和验证中越来越重要。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索用户画像在需求工程领域的最新研究趋势，特别是随着生成式AI技术的发展而带来的变化。

Method: 本研究采用系统性映射研究（SMS）方法，分析了22篇相关文献，重点考察了用户画像的表示、构建、验证以及在需求工程活动中的应用。

Result: 研究发现，越来越多的研究开始应用AI解决方案来构建和验证用户画像，基于模板的用户画像日益普及，并且研究中对用户画像验证方面的关注度有所提升。

Conclusion: 本研究系统性地映射了2023年4月至2025年4月间关于在需求工程（RE）中使用用户画像的最新研究，特别关注了生成式人工智能（AI）的演变。

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [571] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SIMD 向量化代码生成是 LLM 的一个新挑战，SimdBench 是首个针对此的基准测试，表明 LLM 表现不如标量代码，但未来可期。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要关注标量代码，LLM 在 SIMD 向量化代码生成方面的表现尚不清楚。

Method: 提出 SimdBench 基准测试，包含 136 个任务，涵盖 SSE, AVX, Neon, SVE, RVV 五种 SIMD 指令集，并系统评估了 18 种 LLM 在此基准上的正确性和性能。

Result: LLMs 在 SIMD 向量化代码生成方面普遍存在 pass@k 指标下降，并揭示了未来改进 LLM 在此领域潜力的方向。

Conclusion: LLMs 在 SIMD 向量化代码生成方面普遍存在 pass@k 指标下降的问题，但仍有改进空间。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [572] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC 受 AlphaFold 启发，利用其序列到结构建模能力进行代码克隆检测。该方法将代码片段视为标记序列，构建 MSA，并使用注意力机制来推断语义。实验证明 AlphaCC 具有跨语言的适用性，在语义克隆检测方面优于基线方法，并且效率高，适用于大规模应用。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决现有代码克隆检测方法在捕获代码语义或依赖于特定语言的分析器方面存在的不足。受到 AlphaFold 在预测蛋白质三维结构方面的显著成功的启发，研究人员希望利用 AlphaFold 的能力来检测代码克隆，因为蛋白质序列和标记序列具有共同的线性序列结构。

Method: 该研究提出了一种名为 AlphaCC 的新方法，该方法受到 AlphaFold 在蛋白质结构预测方面成功的启发，并将其应用于代码克隆检测。AlphaCC 将代码片段表示为标记序列，以确保跨多种语言的适用性，并利用 AlphaFold 的序列到结构建模能力来推断代码语义。具体来说，AlphaCC 包括三个步骤：1. 将代码片段转换为标记序列，并构建多序列比对（MSA）以增强上下文理解；2. 采用基于 AlphaFold 的注意力机制编码器来模拟序列内部和序列之间的依赖关系；3. 通过后期交互策略计算序列相似度分数，并进行二元分类以确定代码克隆对。

Result: AlphaCC 在三个包含多种语言的数据集上进行了全面评估，结果表明该方法在多种编程语言中都具有广泛的适用性。在两个语义克隆检测数据集上，AlphaCC 的表现持续优于所有基线方法，证明了其强大的语义理解能力。此外，AlphaCC 保持了具有竞争力的效率，使其能够在大规模代码克隆检测任务中得到实际应用。

Conclusion: 研究表明 AlphaCC 在跨多种编程语言方面具有广泛的适用性。在两个语义克隆检测数据集上，其性能持续优于所有基线方法，展现了强大的语义理解能力。此外，AlphaCC 保持了有竞争力的高效性，能够在大规模克隆检测任务中实现实际应用。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [573] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: FaultLine是一个LLM代理工作流，通过追踪输入流、推理条件和生成测试用例来自动生成软件安全漏洞的PoV测试。它在多语言数据集上比CodeAct 2.1有77%的改进，表明分层推理的有效性。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞造成的严重威胁，但报告通常不完整，缺少验证修复和防止回归的PoV测试。这些测试对于确保补丁有效以及帮助开发人员理解漏洞如何被利用至关重要。生成PoV测试是一个具有挑战性的问题，需要推理程序深层嵌套级别的控制流和数据流。

Method: FaultLine是一个LLM代理工作流，它使用一套精心设计的推理步骤，借鉴了传统静态和动态程序分析的方面，来自动生成PoV测试用例。FaultLine 1) 追踪输入从外部可访问API（“源”）到与漏洞相对应的“汇”的流，2) 推断输入必须满足的条件才能遍历沿流遇到的分支条件，3) 在反馈驱动的循环中使用此推理来生成PoV测试用例。

Result: 在包含100个已知漏洞的Java、C和C++项目的多语言数据集上，FaultLine能够为16个项目生成PoV测试，而流行的最先进的开源代理框架CodeAct 2.1只能为9个项目生成PoV测试，代表了比现有技术提高了77%。

Conclusion: 虽然分层推理可以提高LLM代理在PoV测试生成方面的性能，但该问题总体上仍然具有挑战性。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [574] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: 该研究提出 ReduceFix，一种通过自动缩减冗余测试用例来提升 LLM 在程序自动修复（APR）中性能的方法。实验证明，ReduceFix 显著提升了修复效果和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决 LLM 在处理包含大量测试用例的长输入时可能出现的“lost-in-the-middle”问题，该研究提出了一种自动缩减测试用例的方法，以提高 LLM 在程序自动修复（APR）中的性能。

Method: 提出了一种名为 ReduceFix 的方法，该方法包含一个自动缩减测试用例的组件，可以在保留失败诱导行为的同时最小化输入。该方法首先提示 LLM 生成一个缩减器（reducer），然后使用缩减后的测试用例来指导补丁生成。

Result: ReduceFix 将输入平均缩减了 89.1%，在 LFTBench 基准测试中，与使用原始测试用例的 LLM 相比，修复性能（pass@10）最高提升了 53.8%，与省略测试用例的 LLM 相比，提升了 17.6%。此外，在 ChatRepair 模型中加入相同的缩减步骤后，修复率提高了 21.3%。

Conclusion:  LLM-based APR 结合自动化的冗余测试用例缩减，在提升模型修复能力、扩展性和效果方面是非常有潜力的。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [575] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 本研究构建了工具代理的参数失败分类法，并分析了输入源对失败类别的 Yetkisi。研究发现，参数名称幻觉失败主要与LLM自身有关，而其他失败模式则更多地受输入源影响。最后，提出了改进建议以提升工具代理的性能。


<details>
  <summary>Details</summary>
Motivation: 为了探索工具代理范式中存在的参数失败问题并提出相应建议。

Method: 构建参数失败分类法，从主流工具代理的调用链中提取五个失败类别，并通过应用15种输入扰动方法探索三种不同输入源与失败类别之间的相关性。

Result: 实验结果表明，参数名称幻觉失败主要源于LLM的固有局限性，而输入源问题则主要导致其他失败模式。

Conclusion: 通过标准化工具返回格式、改进错误反馈机制和确保参数一致性来提高工具-代理交互的可靠性和有效性。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [576] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans通过引入栈操作解决了Transformer在处理Chomsky层级时的局限性，并在多项任务中表现出优越的性能和效率。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在处理自然语言时存在无法有效捕捉Chomsky层级（如正则表达式、确定性上下文无关文法）的局限性。

Method: 提出了一种名为StackTrans的架构，通过在Transformer层之间引入可微分的栈操作（压栈和弹栈），使其能够有效捕捉Chomsky层级，并与现有框架兼容。

Result: StackTrans在Chomsky层级和大规模自然语言处理基准测试中均优于标准Transformer和其他基线模型，并且360M参数的StackTrans模型在性能上超过了参数量为2-3倍的开源LLM。

Conclusion: StackTrans通过在Transformer层之间显式地引入栈来解决其无法有效捕捉Chomsky层级的问题，在Chomsky层级和自然语言处理任务上均表现优于标准Transformer模型，并成功扩展到7B参数规模，展示了其优越的效率和推理能力。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [577] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 提出一种“Chinese Wall”技术，利用强模型指导弱模型，以提高代码大语言模型的性能并解决版权问题，实验证明有效，但受限于可用数据的限制。


<details>
  <summary>Details</summary>
Motivation: 解决现有代码大语言模型（Code LLM）训练数据不公开可能带来的版权问题，并提高在数据受限情况下模型的性能。

Method: 提出使用 "Chinese Wall" 技术，通过一个高质量模型生成指令来指导一个较弱但符合伦理的模型，使其能够执行复杂任务。

Result: 所提出的技术将 Comma v0.1 1T 在 CanItEdit 基准上的性能提高了 66% 以上，并将 Starcoder2 Instruct 的性能提高了约 20%。

Conclusion: 虽然该技术在提高模型性能方面显示出潜力，但目前受限于缺乏在无版权限制的公共领域内容上训练的模型。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [578] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 本研究分析了Stack Overflow上与React相关的问题，发现算法错误是最常见的挑战，尤其是在声誉中等的用户中。研究人员建议社区提供更多关于算法问题的指导材料。


<details>
  <summary>Details</summary>
Motivation: 尽管React在Web开发中广受欢迎，但用户面临的具体挑战仍然未知。本研究旨在分析用户在Stack Overflow上分享的与React相关的问题，以找出这些挑战。

Method: 本研究采用探索性数据分析方法，分析了Stack Overflow上分享的与React相关的问题，以研究最常讨论的关键词、错误分类和基于用户声誉的错误。

Result: 研究结果显示，最常用于React相关问题的八个关键词是：code, link, vir, href, connect, azure, windows, 和 website。在错误分类方面，算法错误是最常见的问题，其中声誉中等的用户贡献最大，占55.77%。

Conclusion: 该研究结果为React社区在实施早期提供了宝贵的见解，以有效克服采用过程中的挑战。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [579] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion是一种提高Stable Diffusion模型在减少偏见和能耗方面的可持续性的方法，在不改变模型架构的情况下，显著降低了性别偏见、种族偏见和能耗，同时保持了图像质量。


<details>
  <summary>Details</summary>
Motivation: 为了减少SD模型可能对社会和环境造成的危害，我们引入了SustainDiffusion。

Method: SustainDiffusion是一种基于搜索的方法，用于寻找能够减少生成图像中的性别和种族偏见并降低图像生成能耗的最佳超参数和提示结构组合，同时保持与原始SD模型相当的图像质量。

Result: SustainDiffusion可将SD3的性别偏见减少68%，种族偏见减少59%，能耗（CPU和GPU能耗之和）减少48%，并且结果具有跨多轮次和跨提示的可推广性。

Conclusion: 通过SustainDiffusion，我们证明了可以在不进行微调或更改模型架构的情况下，提高文本到图像生成模型的社会和环境可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [580] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 文章比较了 CubeSat 卫星电池放电的两种建模方法：等效电路和机器学习。机器学习模型更准确，但等效电路模型更易于理解。


<details>
  <summary>Details</summary>
Motivation: 文章旨在为 CubeSat 卫星电池放电建模提供一个合理的模型选择依据，以预测断开自主供电系统的后果并确保在轨设备的容错能力。

Method: 文章采用解析方法（等效电路）和机器学习方法对 CubeSat 卫星电池放电进行建模，并进行了比较分析。

Result: 机器学习模型在准确性方面优于等效电路模型，能够更好地适应实际条件和复杂依赖关系，而等效电路模型虽然具有可解释性优势，但在灵活性方面存在不足。

Conclusion: 文章对两种 CubeSat 卫星电池放电建模方法进行了比较分析，机器学习模型在准确性方面优于等效电路模型，但等效电路模型在可解释性方面具有优势。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [581] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope是一个LLM驱动的多代理系统，通过模仿人类审计师的学习和应用代码审计知识，有效提高了缺陷检测的精确率和召回率，并在实际应用中发现了大量新的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统的静态分析工具受限于符号工作流，覆盖范围有限且难以适应具有多样化反模式的定制化缺陷。近期的LLM方法虽然有所改进，但在处理复杂缺陷和分析上下文方面仍存在挑战。

Method: BugScope是一个由LLM驱动的多代理系统，它首先通过程序切片合成检索策略来提取相关的检测上下文，然后构建定制化的检测提示来指导LLM进行准确推理，从而模仿人类审计师从示例中学习新缺陷模式并应用于代码审计。

Result: BugScope在包含40个真实世界缺陷的数据集上，实现了87.04%的精确率和90.00%的召回率，F1分数比最先进的工业工具高0.44。此外，在Linux内核等大型开源系统中，BugScope发现了141个先前未知的缺陷，其中78个已修复，7个得到确认。

Conclusion: BugScope通过模仿人类审计师从示例中学习新缺陷模式并将其应用于代码审计，在40个真实世界缺陷数据集上实现了87.04%的精确率和90.00%的召回率，F1分数超越了最先进的工业工具。在Linux内核等大型开源系统中，BugScope发现了141个先前未知的缺陷，其中78个已修复，7个已得到开发人员确认，证明了其重要的实际应用价值。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [582] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 本研究通过实验发现，LLMs 在自动程序修复中的实际效果与预期不同，并提出了一种研究方法、程序员行为分析和 LLM 使用模式，为 AI 在此领域的应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 本次研究旨在探讨人工智能（AI）技术，特别是大型语言模型（LLMs），在自动程序修复（APR）领域的实际应用效果。研究关注以下问题：LLMs 是否能如预期一样显著改进 APR？如何有效验证 LLMs 提出的修复是否真正有效？程序员在实际使用 LLMs 来辅助自身技能时，其使用方式是怎样的？

Method: 本研究采用了一种对照实验方法，将程序员分为两组：一组可以使用大型语言模型（LLMs），另一组则不能。研究利用了程序证明环境来形式化验证修复补丁的正确性。研究遵循了目标-查询-度量（GQM）的方法论，将研究分为总体研究问题（目标）、可提供具体答案的具体要素（查询）以及支持这些答案的测量（度量）。研究还对程序员的行为进行了细致的分析，并通过记录整个会话过程来支持这一分析，同时定义了 LLMs 的使用模式（分为 7 个类别），并为利用 LLMs 进行调试和 APR 提供了经验证的建议。

Result: 研究结果令人惊讶，与人们对 AI 辅助调试和 APR 的普遍预期存在差异。研究的贡献包括：提出了一种可供其他项目复用的、用于 LLM 调试实验的详细方法论；通过完整的会话记录实现了对程序员行为的细粒度分析；定义了包含 7 个类别的 LLM 使用模式；以及提供了经过验证的、关于如何最大化 LLMs 在调试和 APR 中作用的建议。

Conclusion: 本研究的结果与预期的 AI 辅助调试和自动程序修复（APR）的效果有所不同，为 AI 和 LLMs 在提供保证正确的程序修复方面的作用提供了初步的界定。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [583] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究提出了一种评估语言模型生成证据简报的方法，并将其与人工生成的简报进行了比较，以期提高证据简报的生产效率。


<details>
  <summary>Details</summary>
Motivation: 为了应对手动生成证据简报耗时耗力的挑战，本研究旨在评估语言模型生成的证据简报在内容保真度、易懂性和有用性方面的有效性，并与人工生成的简报进行比较。

Method: 研究人员开发了一种基于检索增强生成（RAG）的语言模型工具来生成证据简报，并使用该工具为先前研究中手动生成的两个证据简报生成了自动版本。此外，研究人员设计了一个对照实验，以评估与人工生成的简报相比，语言模型生成的简报在内容保真度、易懂性和有用性方面的感知。

Result: 实验结果将在实验结束后报告。

Conclusion: 本研究的结论将取决于实验结果。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [584] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 本研究旨在弥补在数据科学计算Notebooks领域缺乏细粒度日志分析的研究空白。研究人员开发了一个工具集来收集Jupyter Notebooks中的代码更改，并创建了一个包含100多小时开发数据的数据集。通过分析该数据集，研究发现Notebooks不仅被用作开发和探索工具，还被用作调试工具，其中大部分更改是小的修复和代码迭代。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域的许多研究都集中在细粒度日志的分析上，并在重构、安全和代码补全等领域带来了重大创新。然而，在数据科学的计算Notebooks方面，还没有进行类似的研究来弥补这一研究空白。

Method: 我们引入了一个用于在开发时收集Jupyter Notebook中代码更改的工具集。我们使用该工具集收集了超过100小时的数据，这些数据与数据分析任务和机器学习任务有关（由20名不同专业水平的开发人员执行），从而得到了一个包含2,655个单元格和9,207次单元格执行的数据集。我们利用这个数据集来研究Notebook开发过程的动态性质以及Notebook中发生的变化。

Result: 收集的数据显示，单元格之间的更改大部分是小的修复和代码迭代修改。

Conclusion: 在收集的数据分析中，我们发现单元格之间的更改大部分是小的修复和代码迭代修改，这表明Notebook不仅被用作开发和探索工具，也被用作调试工具。我们还报告了其他一些见解，并提出了关于新颖数据的未来潜在研究方向。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


### [585] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 本研究利用LLM分析Java库的Javadocs，自动生成测试引用，以提高软件测试的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化测试引用是一个相对探索较少的问题领域，因为测试引用通常需要开发者的知识，而这些知识可能不存在于支持自动推理的形式化语言中。本研究旨在解决此问题。

Method: 利用大型语言模型（LLM）分析Java库的Javadocs，以自动生成测试引用，用于检查正常和异常行为。

Result: LLM能够从Javadocs生成用于检查正常和异常行为的测试引用，其中98.8%的引用可编译，96.4%准确反映了预期的属性。

Conclusion: LLM可以从Java库的Javadocs生成测试引用，可编译的引用占98.8%，准确反映预期属性的占96.4%，即使有错误也是小错误，并且可以通过LLM生成的额外注释信息轻松纠正。

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [586] [FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data](https://arxiv.org/abs/2507.14261)
*Mahmood K. M. Almansoori,Miklos Telek*

Main category: cs.DS

TL;DR: FAMST是一种用于大规模高维数据集的新型MST算法，速度快、误差低。


<details>
  <summary>Details</summary>
Motivation: 提出FAMST算法以解决大规模高维数据集计算最小生成树（MST）的挑战。

Method: FAMST算法采用三阶段方法：近似最近邻（ANN）图构建、ANN组件间连接和迭代边优化。

Result: FAMST算法在时间复杂度上实现了$\	ext{O}(dn \log n)$，空间复杂度为$\	ext{O}(dn + kn)$，与传统方法相比有显著提升。实验表明，FAMST的近似误差极低，并且速度比精确MST算法快1000倍。

Conclusion: FAMST算法能够处理数百万个点和数千个维度的超大规模数据集，将MST技术扩展到以前无法解决的问题规模。

Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm
that addresses the computational challenges of constructing Minimum Spanning
Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a
three-phase approach: Approximate Nearest Neighbor (ANN) graph construction,
ANN inter-component connection, and iterative edge refinement. For a dataset of
$n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$
time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest
neighbors are considered, which is a significant improvement over the
$\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves
remarkably low approximation errors while providing speedups of up to
1000$\times$ compared to exact MST algorithms. We analyze how the key
hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges),
affect performance, providing practical guidelines for hyperparameter
selection. FAMST enables MST-based analysis on datasets with millions of points
and thousands of dimensions, extending the applicability of MST techniques to
problem scales previously considered infeasible.

</details>


### [587] [On zeros and algorithms for disordered systems: mean-field spin glasses](https://arxiv.org/abs/2507.15616)
*Ferenc Bencs,Kuikui Liu,Guus Regts*

Main category: cs.DS

TL;DR: 为自旋玻璃模型设计了求解配分函数的确定性拟多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 自旋玻璃是统计物理学、平均情况计算复杂性理论和现代高维统计推断的核心概率分布。

Method: 设计确定性拟多项式时间算法，通过研究配分函数的零点位置来估计自旋玻璃配分函数。

Result: 在平均场设置下，该算法可以近乎所有逆温度下的第二矩状态下，以任意高的精度估计配分函数，特别是在 Sherrington--Kirkpatrick 模型中，该算法在几乎整个副本对称相中都成功。

Conclusion: 该研究为几乎所有逆温度下的第二矩状态下的自旋玻璃模型的平均情况计算复杂性和高维统计推断的确定性拟多项式时间算法设计提供了新方法。

Abstract: Spin glasses are fundamental probability distributions at the core of
statistical physics, the theory of average-case computational complexity, and
modern high-dimensional statistical inference. In the mean-field setting, we
design deterministic quasipolynomial-time algorithms for estimating the
partition function to arbitrarily high accuracy for nearly all inverse
temperatures in the second moment regime. In particular, for the
Sherrington--Kirkpatrick model, our algorithms succeed for almost the entire
replica-symmetric phase. To achieve this, we study the locations of the zeros
of the partition function. Notably, our methods are conceptually simple, and
apply equally well to the spherical case and the case of Ising spins.

</details>


### [588] [Tighter Lower Bounds for Single Source Personalized PageRank](https://arxiv.org/abs/2507.14462)
*Xinpeng Jiang,Haoyu Liu,Siqiang Luo,Xiaokui Xiao*

Main category: cs.DS

TL;DR: 为PageRank查询的近似算法提供了更紧的下界。


<details>
  <summary>Details</summary>
Motivation: 现有SSPPR近似算法的下界不够紧密，无法准确反映问题的计算复杂度。

Method: 通过理论分析和构造性证明来建立SSPPR-R和SSPPR-A的近似下界。

Result: 为SSPPR-R和SSPPR-A问题提供了更紧确的下界，分别比现有下界更优。

Conclusion: 研究了单源个性化PageRank（SSPPR）查询的近似下界，该查询衡量从源节点s开始的α衰减随机游走的概率分布。研究表明，对于SSPPR-R（相对误差），下界为Ω(min(m, log(1/δ)/δ))；对于SSPPR-A（绝对误差），在特定条件下，下界为Ω(min(m, log(1/ε)/ε))。

Abstract: We study lower bounds for approximating the Single Source Personalized
PageRank (SSPPR) query, which measures the probability distribution of an
$\alpha$-decay random walk starting from a source node $s$. Existing lower
bounds remain loose-$\Omega\left(\min(m, 1/\delta)\right)$ for relative error
(SSPPR-R) and $\Omega\left(\min(n, 1/\epsilon)\right)$ for additive error
(SSPPR-A). To close this gap, we establish tighter bounds for both settings.
For SSPPR-R, we show a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\delta)}{\delta}\right)\right)$ for any $\delta \in (0,1)$. For
SSPPR-A, we prove a lower bound of $\Omega\left(\min\left(m,
\frac{\log(1/\epsilon)}{\epsilon}\right)\right)$ for any $\epsilon \in (0,1)$,
assuming the graph has $m \in \mathcal{O}(n^{2-\beta})$ edges for any
arbitrarily small constant $\beta \in (0,1)$.

</details>


### [589] [New Algorithms for #2-SAT and #3-SAT](https://arxiv.org/abs/2507.14504)
*Junqiang Peng,Zimo Sheng,Mingyu Xiao*

Main category: cs.DS

TL;DR: 本文提出了一种新的算法来解决 #2-SAT 和 #3-SAT 问题的加权版本，其速度比之前的算法快。


<details>
  <summary>Details</summary>
Motivation: 为了在更短的时间内解决 #2-SAT 和 #3-SAT 问题的加权版本，从而改进先前 Zhou 等人于 2010 年提出的算法。

Method: 通过引入新的约简规则、改进分支操作分析以及在公式的原始图和对偶图上应用路径分解来解决加权 #2-SAT 和 #3-SAT 问题。

Result: 加权 #2-SAT 和 #3-SAT 问题的解决时间分别达到 O*(1.1082^m) 和 O*(1.4423^m)，优于之前的 O*(1.1892^m) 算法。

Conclusion: 加权 #2-SAT 和 #3-SAT 问题可以分别在 O*(1.1082^m) 和 O*(1.4423^m) 时间内解决，这直接适用于无权情况，并显著优于先前结果。

Abstract: The #2-SAT and #3-SAT problems involve counting the number of satisfying
assignments (also called models) for instances of 2-SAT and 3-SAT,
respectively. In 2010, Zhou et al. proposed an $\mathcal{O}^*(1.1892^m)$-time
algorithm for #2-SAT and an efficient approach for #3-SAT, where $m$ denotes
the number of clauses. In this paper, we show that the weighted versions of
#2-SAT and #3-SAT can be solved in $\mathcal{O}^*(1.1082^m)$ and
$\mathcal{O}^*(1.4423^m)$ time, respectively. These results directly apply to
the unweighted cases and achieve substantial improvements over the previous
results. These advancements are enabled by the introduction of novel reduction
rules, a refined analysis of branching operations, and the application of path
decompositions on the primal and dual graphs of the formula.

</details>


### [590] [Addressing Bias in Algorithmic Solutions: Exploring Vertex Cover and Feedback Vertex Set](https://arxiv.org/abs/2507.14509)
*Sheikh Shakil Akhtar,Jayakrishnan Madathil,Pranabendu Misra,Geevarghese Philip*

Main category: cs.DS

TL;DR: 组合优化算法应考虑解决方案对不同群体的影响，本研究提出了一种解决此问题的方法。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于，传统的组合优化算法在抽象现实世界问题时，往往会忽略可能对特定群体产生重要影响的“辅助信息”。最优解在现实世界中的影响并非完全相同，某些解可能比其他解更可取，即使成本稍高。如果这种影响不成比例地体现在少数群体身上，大众可能无法意识到。

Method: 本文提出了一种寻找组合优化问题无偏解的方法，重点关注算法输出对不同人口子群体的影响。

Result: 该研究的结果是提出了一种在组合优化问题中寻找无偏解的方法。

Conclusion: 该研究旨在寻找组合优化问题的无偏解，以解决现实世界中算法输出可能对特定群体产生不成比例影响的问题。

Abstract: A typical goal of research in combinatorial optimization is to come up with
fast algorithms that find optimal solutions to a computational problem. The
process that takes a real-world problem and extracts a clean mathematical
abstraction of it often throws out a lot of "side information" which is deemed
irrelevant. However, the discarded information could be of real significance to
the end-user of the algorithm's output. All solutions of the same cost are not
necessarily of equal impact in the real-world; some solutions may be much more
desirable than others, even at the expense of additional increase in cost. If
the impact, positive or negative, is mostly felt by some specific (minority)
subgroups of the population, the population at large will be largely unaware of
it. In this work we ask the question of finding solutions to combinatorial
optimization problems that are "unbiased" with respect to a collection of
specified subgroups of the total population.

</details>


### [591] [Characterizing and Testing Configuration Stability in Two-Dimensional Threshold Cellular Automata](https://arxiv.org/abs/2507.14569)
*Yonatan Nakar,Dana Ron*

Main category: cs.DS

TL;DR: 对于阈值-2和阈值-3规则，我们表征了稳定配置的结构，并设计了一个测试算法，其查询复杂度独立于配置大小，并与$1/"epsilon"$的二次方成正比。


<details>
  <summary>Details</summary>
Motivation: 我们考虑表征和测试根据阈值规则在二维环面上演化的细胞自动机配置的稳定性，这些规则是针对冯·诺依曼邻域的。

Method: 我们设计并分析了一种测试算法，该算法的查询复杂度独立于配置的大小，并与$1/"epsilon"$的二次方成正比。

Result: 查询复杂度独立于配置的大小，并与$1/"epsilon"$的二次方成正比。

Conclusion: 对于阈值-2（类似地，阈值-4）和阈值-3（Majority）规则，我们首先表征了稳定配置的结构。然后，我们设计并分析了一种测试算法，用于区分相对于阈值-2规则稳定的配置以及与任何稳定配置相距$"epsilon"$远的配置，该算法的查询复杂度独立于配置的大小，并与$1/"epsilon"$的二次方成正比。

Abstract: We consider the problems of characterizing and testing the stability of
cellular automata configurations that evolve on a two-dimensional torus
according to threshold rules with respect to the von-Neumann neighborhood.
While stable configurations for Threshold-1 (OR) and Threshold-5 (AND) are
trivial (and hence easily testable), the other threshold rules exhibit much
more diverse behaviors. We first characterize the structure of stable
configurations with respect to the Threshold-2 (similarly, Threshold-4) and
Threshold-3 (Majority) rules. We then design and analyze a testing algorithm
that distinguishes between configurations that are stable with respect to the
Threshold-2 rule, and those that are $\epsilon$-far from any stable
configuration, where the query complexity of the algorithm is independent of
the size of the configuration and depends quadratically on $1/\epsilon$.

</details>


### [592] [A Black-Box Approach for Exogenous Replenishment in Online Resource Allocation](https://arxiv.org/abs/2507.14812)
*Suho Kang,Ziyang Liu,Rajan Udwani*

Main category: cs.DS

TL;DR: 提供了一种黑盒方法，可将现有在线资源分配算法扩展以处理补货，同时保持其性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决在具有未知外生补货过程的在线资源分配问题中，将补货机制整合到现有算法中的挑战。

Method: 提出了一种黑盒方法，可以将现有算法（最初为不考虑补货而设计）扩展为适用于任意补货过程（对抗性或随机性）的算法。

Result: 所提出的黑盒方法在初始库存量大的情况下，能够保持原有算法的竞争率，从而实现了外生补货与大量现有算法（包括对抗性和随机性到达模型）的无缝集成。

Conclusion: 该方法能够将不考虑补货的现有算法扩展为可处理任意补货过程（对抗性或随机性）的算法，并能在初始库存量大的情况下保持原有算法的竞争率。

Abstract: In a typical online resource allocation problem, we start with a fixed
inventory of resources and make online allocation decisions in response to
resource requests that arrive sequentially over a finite horizon. We consider
settings where the inventory is replenished over time according to an unknown
exogenous process. We introduce black-box methods that extend any existing
algorithm, originally designed without considering replenishment, into one that
works with an arbitrary (adversarial or stochastic) replenishment process. Our
approach preserves the original algorithm's competitive ratio in regimes with
large initial inventory, thereby enabling the seamless integration of exogenous
replenishment into a large body of existing algorithmic results for both
adversarial and stochastic arrival models.

</details>


### [593] [Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts](https://arxiv.org/abs/2507.14835)
*Pan Peng,Hangyu Xu*

Main category: cs.DS

TL;DR: 该研究提出了一种新的差分隐私（DP）机制，用于生成合成图，该合成图能准确近似原始图中所有割的三角-动机大小，同时保证了隐私性。研究还提供了理论上的误差下界。


<details>
  <summary>Details</summary>
Motivation: 为了在保护隐私的前提下，生成能够应用于图聚类、图稀疏化和社会网络分析等领域的合成图，并满足对图的割的三角-动机大小的近似需求。

Method: 提出了一种差分隐私（DP）机制，用于生成一个能够近似任意给定图G的所有割的三角-动机大小的差分隐私（DP）合成图G'。

Result: 实现了第一个(ε,δ)-DP机制，能够在多项式时间内生成合成图G'，近似输入图G的所有割的三角-动机大小，加性误差为Õ(√(mℓ3(G)))n/ε^(3/2)。同时，证明了任何DP算法的加性误差下界为Ω(√(mn))ℓ3(G)/ε。该算法可推广至加权图，下界可扩展至任何常数h≥2的Kh-动机割。

Conclusion: 所提出的DP机制在多项式时间内生成合成图G'，能够近似输入图G的所有割的三角-动机大小，其加性误差可控制在±Õ(√(mℓ3(G)))n/ε^(3/2)。此外，研究还提供了任何DP算法在回答所有(S,T)割的三角-动机大小查询时的加性误差下界为Ω(√(mn))ℓ3(G)/ε。

Abstract: We study the problem of releasing a differentially private (DP) synthetic
graph $G'$ that well approximates the triangle-motif sizes of all cuts of any
given graph $G$, where a motif in general refers to a frequently occurring
subgraph within complex networks. Non-private versions of such graphs have
found applications in diverse fields such as graph clustering, graph
sparsification, and social network analysis. Specifically, we present the first
$(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$
vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates
a synthetic graph $G'$ in polynomial time, approximating the triangle-motif
sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive
error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we
provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the
additive error for any DP algorithm that answers the triangle-motif size
queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to
weighted graphs, and our lower bound extends to any $K_h$-motif cut for any
constant $h\geq 2$.

</details>


### [594] [Predict, Reposition, and Allocate: A Greedy and Flow-Based Architecture for Sustainable Urban Food Delivery](https://arxiv.org/abs/2507.15282)
*Aqsa Ashraf Makhdomi,Iqra Altaf Gillani*

Main category: cs.DS

TL;DR: 该研究提出了一种环保的食品配送优化框架，通过整合需求预测、路线规划和订单分配，并利用网络流模型，最大限度地减少环境影响，同时保持服务效率，最终减少车辆数量，实现可持续发展。


<details>
  <summary>Details</summary>
Motivation: 现有优化机制未能将环境可持续性纳入其优化目标，导致次优结果，而食品配送平台的快速扩张加剧了环境问题。

Method: 提出了一种新颖的环保食品配送优化框架，该框架整合了需求预测、配送员路线和订单分配，以最大限度地减少环境影响。该方法利用目标函数的子模和单调性质，并设计了一种有效的贪婪优化算法。此外，该方法将订单分配问题构建为网络流优化模型，并设计了一种三层网络架构，以根据容量限制和空间需求将订单与配送员进行匹配。

Result: 该框架通过优化模型减少了车辆数量，并提高了效率，从而减少了温室气体排放，为食品配送行业创造了一个可持续的生态系统。

Conclusion: 该框架通过减少车辆数量和网络流优化模型，为食品配送行业创造了一个可持续的生态系统。

Abstract: The rapid proliferation of food delivery platforms has reshaped urban
mobility but has also contributed significantly to environmental degradation
through increased greenhouse gas emissions. Existing optimization mechanisms
produce sub-optimal outcomes as they do not consider environmental
sustainability their optimization objective. This study proposes a novel
eco-friendly food delivery optimization framework that integrates demand
prediction, delivery person routing, and order allocation to minimize
environmental impact while maintaining service efficiency. Since recommending
routes is NP-Hard, the proposed approach utilizes the submodular and monotone
properties of the objective function and designs an efficient greedy
optimization algorithm. Thereafter, it formulates order allocation problem as a
network flow optimization model, which, to the best of our knowledge, has not
been explored in the context of food delivery. A three-layered network
architecture is designed to match orders with delivery personnel based on
capacity constraints and spatial demand. Through this framework, the proposed
approach reduces the vehicle count, and creates a sustainable food delivery
ecosystem.

</details>


### [595] [Language Generation in the Limit: Noise, Loss, and Feedback](https://arxiv.org/abs/2507.15319)
*Yannan Bai,Debmalya Panigrahi,Ian Zhang*

Main category: cs.DS

TL;DR: 本文研究了语言生成极限问题，否定了其并集闭合猜想，并深入探讨了噪声、缺失样本和反馈对语言生成模型能力的影响，为这些变体提供了精确的表征。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决语言生成极限的并集闭合问题，并进一步探索和表征语言生成模型中的噪声、缺失样本和反馈等变体。

Method: 本文首先给出了一个统一可生成集合和一个非均匀可生成集合的并集，证明了该并集不可在极限条件下生成，从而否定了该问题。接着，利用该构造的某些方面，深入研究了语言生成的几种变体，包括带噪声生成和无样本生成，并证明了它们在均匀和非均匀生成模型下的等价性，同时还对非均匀带噪声生成进行了刻画。最后，研究了带反馈的生成模型，证明了有限查询不增加模型能力，而无限查询则严格增强了模型能力。

Result: 1. 否定了“有限可生成集合的并集在极限下总是可生成的”这一猜想。 2. 证明了带噪声生成和无样本生成在均匀和非均匀生成模型下的等价性，并刻画了非均匀带噪声生成。 3. 证明了在极限条件下，即使只有一个带噪声的字符串，噪声生成和非噪声生成之间也存在分离。 4. 证明了在带反馈的生成模型中，有限查询不增加模型能力，而无限查询则提供更强的模型能力。

Conclusion: 本文解决了语言生成极限的并集闭合问题，并利用这些技术（以及其他技术）为包含噪声、损失和反馈的自然变体提供了精确的表征。

Abstract: Kleinberg and Mullainathan (2024) recently proposed a formal framework called
language generation in the limit and showed that given a sequence of example
strings from an unknown target language drawn from any countable collection, an
algorithm can correctly generate unseen strings from the target language within
finite time. This notion was further refined by Li, Raman, and Tewari (2024),
who defined stricter categories of non-uniform and uniform generation. They
showed that a finite union of uniformly generatable collections is generatable
in the limit, and asked if the same is true for non-uniform generation.
  We begin by resolving the question in the negative: we give a uniformly
generatable collection and a non-uniformly generatable collection whose union
is not generatable in the limit. We then use facets of this construction to
further our understanding of several variants of language generation. The first
two, generation with noise and without samples, were introduced by Raman and
Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the
equivalence of these models for uniform and non-uniform generation, and provide
a characterization of non-uniform noisy generation. The former paper asked if
there is any separation between noisy and non-noisy generation in the limit --
we show that such a separation exists even with a single noisy string. Finally,
we study the framework of generation with feedback, introduced by Charikar and
Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask
membership queries. We show finite queries add no power, but infinite queries
yield a strictly more powerful model.
  In summary, the results in this paper resolve the union-closedness of
language generation in the limit, and leverage those techniques (and others) to
give precise characterizations for natural variants that incorporate noise,
loss, and feedback.

</details>


### [596] [1.64-Approximation for Chromatic Correlation Clustering via Chromatic Cluster LP](https://arxiv.org/abs/2507.15417)
*Dahoon Lee,Chenglin Fan,Euiwoong Lee*

Main category: cs.DS

TL;DR: 提出了一种改进的 1.64-近似算法来解决具有挑战性的 CCC 问题，该算法利用新的 LP 松弛和聚类方法。


<details>
  <summary>Details</summary>
Motivation: 旨在改进 CCC（一种广义的相关聚类，用于处理具有多种类别关系（颜色）的边并对聚类施加色彩约束）的近似度，因为传统的 LP 松弛方法存在局限性。

Method: 通过引入一个色彩聚类 LP 松弛和一个结合了基于聚类和基于贪心枢轴的聚类策略的舍入算法，将聚类 LP 框架扩展到色彩环境。

Result: 实现了一个 1.64-近似算法，改进了之前的 2.15 近似因子，并展示了聚类 LP 框架在解决其他聚类问题变体方面的潜力。

Conclusion: 该研究提出了一个随机的 1.64-近似算法来解决 CCC 问题，显著优于之前 2.15 的近似因子。

Abstract: Chromatic Correlation Clustering (CCC) generalizes Correlation Clustering by
assigning multiple categorical relationships (colors) to edges and imposing
chromatic constraints on the clusters. Unlike traditional Correlation
Clustering, which only deals with binary $(+/-)$ relationships, CCC captures
richer relational structures. Despite its importance, improving the
approximation for CCC has been difficult due to the limitations of standard LP
relaxations. We present a randomized $1.64$-approximation algorithm to the CCC
problem, significantly improving the previous factor of $2.15$. Our approach
extends the cluster LP framework to the chromatic setting by introducing a
chromatic cluster LP relaxation and an rounding algorithm that utilizes both a
cluster-based and a greedy pivot-based strategy. The analysis bypasses the
integrality gap of $2$ for the CCC version of standard LP and highlights the
potential of the cluster LP framework to address other variants of clustering
problems.

</details>


### [597] [Job Scheduling under Base and Additional Fees, with Applications to Mixed-Criticality Scheduling](https://arxiv.org/abs/2507.15434)
*Yi-Ting Hsieh,Mong-Jen Kao,Jhong-Yun Liu,Hung-Lung Wang*

Main category: cs.DS

TL;DR: 该研究提出了一个调度算法，用于最小化机器工时，并得到了近似结果。


<details>
  <summary>Details</summary>
Motivation: 最小化机器工时。

Method: 研究了n个作业在m个相同机器上的调度问题，目标是最小化总机器工时。给出了FFD算法的1.5近似界，并证明该问题存在PTAS。

Result: FFD算法提供了1.5的近似比，并且存在PTAS。

Conclusion: FFD是一种1.5近似算法，并且该问题存在PTAS。该思想也应用于混合关键性系统调度，得到了改进的近似结果。

Abstract: We are concerned with the problem of scheduling $n$ jobs onto $m$ identical
machines. Each machine has to be in operation for a prescribed time, and the
objective is to minimize the total machine working time. Precisely, let $c_i$
be the prescribed time for machine $i$, where $i\in[m]$, and $p_j$ be the
processing time for job $j$, where $j\in[n]$. The problem asks for a schedule
$\sigma\colon\, J\to M$ such that $\sum_{i=1}^m\max\{c_i,
\sum_{j\in\sigma^{-1}(i)}p_j\}$ is minimized, where $J$ and $M$ denote the sets
of jobs and machines, respectively. We show that First Fit Decreasing (FFD)
leads to a $1.5$-approximation, and this problem admits a polynomial-time
approximation scheme (PTAS). The idea is further applied to mixed-criticality
system scheduling to yield improved approximation results.

</details>


### [598] [An $n^{O(\log\log n)}$ time approximation scheme for capacitated VRP in the Euclidean plane](https://arxiv.org/abs/2507.15549)
*René Sitters*

Main category: cs.DS

TL;DR: 提出了一种适用于欧几里得 CVRP 的 Q-PTAS，运行时间为 n^f(ε)·loglogn，这是 PTAS 的一大步。


<details>
  <summary>Details</summary>
Motivation: 针对欧几里得 CVRP 问题，寻求更优的近似算法，特别是 PTAS。

Method: 首先，我们给出了将 d 维欧几里得空间中的 CVRP 约化为 d 维欧几里得空间中的一种无容量路径问题（m-paths problem）的多项式时间约化，该问题要求在两点 a 和 b 之间找到恰好 m 条路径，并覆盖欧几里得空间中所有给定的点。然后，我们为平面中的 m-paths 问题提供了一个 Q-PTAS。任何适用于（处理起来可能更容易）欧几里得 m-paths 问题的 PTAS 很可能意味着欧几里得 CVRP 的 PTAS。

Result: 提出了一种 Q-PTAS，运行时间为 n^f(ε)·loglogn，显著优于现有算法。

Conclusion: 本文提出了一种适用于欧几里得平面上具有容量限制的车辆路径问题（CVRP）的拟多项式时间近似方案（Q-PTAS），该方案针对任意容量 c 且运行时间为 n^f(ε)·loglogn，f 是仅与 ε 相关的函数。这相比之前已知的 n^log^{O(1/ε)}n 时间有了显著改进，是朝着实现欧几里得 CVRP 的 PTAS 迈出的重要一步。

Abstract: We present a quasi polynomial time approximation scheme (Q-PTAS) for the
capacitated vehicle routing problem (CVRP) on $n$ points in the Euclidean plane
for arbitrary capacity $c$. The running time is $n^{f(\epsilon)\cdot\log\log
n}$ for any $c$, and where $f$ is a function of $\epsilon$ only. This is a
major improvement over the so far best known running time of
$n^{\log^{O(1/\epsilon)}n}$ time and a big step towards a PTAS for Euclidean
CVRP.
  In our algorithm, we first give a polynomial time reduction of the CVRP in
$\mathbb{R}^d$ (for any fixed $d$) to an uncapacitated routing problem in
$\mathbb{R}^d$ that we call the $m$-paths problem. Here, one needs to find
exactly $m$ paths between two points $a$ and $b$, covering all the given points
in the Euclidean space. We then give a Q-PTAS for the $m$-paths problem in the
pane. Any PTAS for the (arguably easier to handle) Euclidean $m$-paths problem
is most likely to imply a PTAS for the Euclidean CVRP.

</details>


### [599] [Fast Algorithms for Graph Arboricity and Related Problems](https://arxiv.org/abs/2507.15598)
*Ruoxu Cen,Henry Fleischmann,George Z. Li,Jason Li,Debmalya Panigrahi*

Main category: cs.DS

TL;DR: 本文提出了更快的算法来计算图的树宽和割层次。


<details>
  <summary>Details</summary>
Motivation: 本文旨在改进加权无向图的树宽计算算法的运行时间，并提出一种新的计算完全割层次的算法。

Method: 本文提出了一种计算加权无向图的树宽的算法，并证明了其运行时间为 O(n^0.5 * m^1.5)。此外，本文还提出了一种计算完全割层次（最小割比的层状多路割）的新算法，运行时间为 O(n*m)。

Result: 本文提出了一种计算加权无向图树宽的新算法，运行时间为 O(n^0.5 * m^1.5)，优于之前的 O(nm) 绑定。此外，还提出了一种计算完全割层次的新算法，运行时间为 O(n*m)，优于之前的 O(n^2 * m) 绑定。

Conclusion: 本文提出了一种计算加权无向图的树宽度（定义为覆盖图中所有边的最小生成森林数）的算法，运行时间为 O(n^0.5 * m^1.5)。该算法在加权图上的运行时间优于之前的 O(nm) 绑定，在无权图上的运行时间优于之前的 O(m^1.5) 绑定。

Abstract: We give an algorithm for finding the arboricity of a weighted, undirected
graph, defined as the minimum number of spanning forests that cover all edges
of the graph, in $\sqrt{n} m^{1+o(1)}$ time. This improves on the previous best
bound of $\tilde{O}(nm)$ for weighted graphs and $\tilde{O}(m^{3/2}) $ for
unweighted graphs (Gabow 1995) for this problem. The running time of our
algorithm is dominated by a logarithmic number of calls to a directed global
minimum cut subroutine -- if the running time of the latter problem improves to
$m^{1+o(1)}$ (thereby matching the running time of maximum flow), the running
time of our arboricity algorithm would improve further to $m^{1+o(1)}$.
  We also give a new algorithm for computing the entire cut hierarchy --
laminar multiway cuts with minimum cut ratio in recursively defined induced
subgraphs -- in $m n^{1+o(1)}$ time. The cut hierarchy yields the ideal edge
loads (Thorup 2001) in a fractional spanning tree packing of the graph which,
we show, also corresponds to a max-entropy solution in the spanning tree
polytope. For the cut hierarchy problem, the previous best bound was
$\tilde{O}(n^2 m)$ for weighted graphs and $\tilde{O}(n m^{3/2})$ for
unweighted graphs.

</details>


### [600] [Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound](https://arxiv.org/abs/2507.15658)
*Romain Cosson,Laurent Massoulié*

Main category: cs.DS

TL;DR: 本文提出了一种新的分布式异步算法，用于k个代理集体探索未知树，并给出了改进的性能保证和理论下界。


<details>
  <summary>Details</summary>
Motivation: 解决在未知树中，k个移动代理需要集体访问所有节点并最小化移动次数的问题，特别是要克服现有算法要么是分布式但同步，要么是异步但集中的局限性。

Method: 本文提出了一种分布式异步算法，用于解决k个移动代理集体探索未知树的问题，目标是最小化移动次数。算法利用分布在所有节点上的白板进行信息共享，并且代理的移动是异步的，速度受对手控制。

Result: 本文提出的分布式异步算法在探索包含n个节点和深度D的任意树时，最多需要 2n+O(k^2 2^kD) 次移动，其遗憾度与D呈线性关系。此外，一个变体算法的保证为 O(k/log k)(n+kD)，竞争比为 O(k/log k)。该遗憾度保证在平均情况下的复杂度是渐近最优的（即1竞争）。此外，本文还提出了一个适用于分布式和集中式环境的异步集体树探索竞争比的下界，为Ω(log^2 k)，优于先前的Ω(log k)下界。

Conclusion: 本文提出的分布式异步算法在探索包含n个节点和深度D的任意树时，最多需要 2n+O(k^2 2^kD) 次移动，其遗憾度与D呈线性关系。此外，一个变体算法的保证为 O(k/log k)(n+kD)，竞争比为 O(k/log k)。该遗憾度保证在平均情况下的复杂度是渐近最优的（即1竞争）。此外，本文还提出了一个适用于分布式和集中式环境的异步集体树探索竞争比的下界，为Ω(log^2 k)，优于先前的Ω(log k)下界。

Abstract: We study the problem of collective tree exploration in which a team of $k$
mobile agents must collectively visit all nodes of an unknown tree in as few
moves as possible. The agents all start from the root and discover adjacent
edges as they progress in the tree. Communication is distributed in the sense
that agents share information by reading and writing on whiteboards located at
all nodes. Movements are asynchronous, in the sense that the speeds of all
agents are controlled by an adversary at all times. All previous competitive
guarantees for collective tree exploration are either distributed but
synchronous, or asynchronous but centralized. In contrast, we present a
distributed asynchronous algorithm that explores any tree of $n$ nodes and
depth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear
in $D$, and a variant algorithm with a guarantee in $O(k/\log k)(n+kD)$, i.e.,
with a competitive ratio in $O(k/\log k)$. We note that our regret guarantee is
asymptotically optimal (i.e., $1$-competitive) from the perspective of
average-case complexity. We then present a new general lower bound on the
competitive ratio of asynchronous collective tree exploration, in
$\Omega(\log^2 k)$. This lower bound applies to both the distributed and
centralized settings, and improves upon the previous lower bound in
$\Omega(\log k)$.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [601] [Can AR-Embedded Visualizations Foster Appropriate Reliance on AI in Spatial Decision Making? A Comparative Study of AR See-Through vs. 2D Minimap](https://arxiv.org/abs/2507.14316)
*Xianhao Carton Liu,Difan Jia,Tongyu Nie,Evan Suma Rosenberg,Victoria Interrante,Chen Zhu-Tian*

Main category: cs.HC

TL;DR: 尽管AR内的嵌入式可视化在空间推理方面有优势，但会增加对AI的过度依赖，这与2D最小地图相比是一个劣势。研究结果揭示了AR在时限性、AI辅助的空间目标选择任务中的潜在问题，并为未来的人机协作决策研究提供了重要的见解。


<details>
  <summary>Details</summary>
Motivation: 在紧急疏散、一线响应者优先排序和危机管理等高风险、时限性场景中，决策者必须快速地在空间目标（如出口、需要协助的个体或需要控制的区域）中进行选择。室内传感和人工智能（AI）的进步可以通过在2D地图上可视化实时态势数据和AI建议来支持这些决策。然而，在真实世界空间中进行心理映射会给用户带来巨大的认知负荷，从而影响他们恰当判断AI建议的能力，导致过度依赖（例如，接受错误的AI建议或拒绝正确的建议）。AR中的嵌入式可视化通过直接将信息叠加到物理环境中，可能会减轻这种负荷，并促进对AI更审慎、恰当的依赖。但是，事实是否如此？

Method: 这项工作进行了一项实证研究（N=32），将AR的 see-through（嵌入式可视化）与2D最小地图在时限性、AI辅助的空间目标选择任务进行了比较。

Result: 与预期相反，在AR条件下，用户对AI的过度依赖性更高。分析进一步揭示，这主要是由于过度依赖，其中嵌入式可视化特有的因素，例如感知挑战、视觉邻近错觉和高度逼真的视觉表示。尽管如此，嵌入式可视化在空间推理方面表现出显著的优势，例如空间映射和以自我为的空间图像。

Conclusion: 尽管AR内的嵌入式可视化在空间推理方面有显著优势，但与2D最小地图相比，它会导致对AI的过度依赖。研究结果表明，与AR相关的感知挑战、视觉邻近错觉和逼真的视觉表示是导致这种过度依赖的主要因素。然而，AR确实在空间映射和以自我为的空间图像等领域表现出色。研究结果为AR中人机协作决策的设计提供了重要的见解和未来研究方向。

Abstract: In high-stakes, time-critical scenarios-such as emergency evacuation, first
responder prioritization, and crisis management -- decision-makers must rapidly
choose among spatial targets, such as exits, individuals to assist, or areas to
secure. Advances in indoor sensing and artificial intelligence (AI) can support
these decisions by visualizing real-time situational data and AI suggestions on
2D maps. However, mentally mapping this information onto real-world spaces
imposes significant cognitive load. This load can impair users' ability to
appropriately judge AI suggestions, leading to inappropriate reliance (e.g.,
accepting wrong AI suggestions or rejecting correct ones). Embedded
visualizations in Augmented Reality (AR), by directly overlaying information
onto physical environments, may reduce this load and foster more deliberate,
appropriate reliance on AI. But is this true? In this work, we conducted an
empirical study (N = 32) comparing AR see-through (embedded visualization) and
2D Minimap in time-critical, AI-assisted spatial target selection tasks.
Contrary to our expectations, users exhibited greater inappropriate reliance on
AI in the AR condition. Our analysis further reveals that this is primarily due
to over-reliance, with factors specific to embedded visualizations, such as
perceptual challenges, visual proximity illusions, and highly realistic visual
representations. Nonetheless, embedded visualizations demonstrated notable
benefits in spatial reasoning, such as spatial mapping and egocentric spatial
imagery. We conclude by discussing the empirical insights, deriving design
implications, and outlining important directions for future research on
human-AI decision collaboration in AR.

</details>


### [602] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: 本研究发现，通过特定的干预策略，如“逐步任务分解”，ChatGPT在对美国最高法院案件摘要进行定性编码时，可以达到专业研究人员可接受的可靠性水平，证明了LLM在演绎定性编码中的潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索大型语言模型（LLMs），特别是ChatGPT，在结构化演绎定性编码中的应用潜力，以解决当前研究中对LLM在演绎分类任务中的应用关注不足的问题。

Method: 本研究采用零样本、少样本、基于定义和逐步任务分解等四种干预方法，对美国最高法院的案件摘要进行定性编码，并使用准确率、F1分数、Cohen's kappa 和 Krippendorff's alpha 等标准分类指标进行评估，同时利用卡方检验和Cramer's V评估构建效度。

Result: 逐步任务分解策略在可靠性方面表现最佳（准确率=0.775，kappa=0.744，alpha=0.746），达到了实质性一致的阈值。尽管案件摘要中存在语义模糊性，ChatGPT在不同样本中仍表现出稳定的同意度，在低支持子类中也取得了较高的F1分数。

Conclusion: 该研究表明，通过有针对性的、定制化的干预措施，大型语言模型（LLMs）可以达到适合整合到严格的定性编码工作流程中的可靠性水平。

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


### [603] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)
*Taufiq Daryanto,Sophia Stil,Xiaohan Ding,Daniel Manesh,Sang Won Lee,Tim Lee,Stephanie Lunn,Sarah Rodriguez,Chris Brown,Eugenia Rho*

Main category: cs.HC

TL;DR: 研究探讨了AI在技术面试“出声思考”练习中的应用，发现用户看重AI的模拟和反馈功能，并提出设计建议，强调人机协作以促进公平学习。


<details>
  <summary>Details</summary>
Motivation: 为了解决技术面试中“出声思考”练习机会有限的问题，并探索对话式AI在其中扮演的角色，填补相关研究的空白。

Method: 通过招募17名参与者使用一个基于LLM的技术面试练习工具，收集用户反馈和进行分析，以了解用户对AI在模拟、反馈和学习中的作用的看法。

Result: 参与者认为AI在模拟、提供反馈和生成示例供学习方面发挥了重要作用。研究提出了促进对话式AI的社交存在感、提供超越语言内容分析的反馈以及通过人机协作实现众包“出声思考”示例的设计建议。

Conclusion: 该研究提出了一个基于LLM的技​​术面试练习工具，并提出了一系列设计建议，以促进AI在技术面试练习中的应用，同时强调了人机协作和促进计算领域公平学习的重要性。

Abstract: One challenge in technical interviews is the think-aloud process, where
candidates verbalize their thought processes while solving coding tasks.
Despite its importance, opportunities for structured practice remain limited.
Conversational AI offers potential assistance, but limited research explores
user perceptions of its role in think-aloud practice. To address this gap, we
conducted a study with 17 participants using an LLM-based technical interview
practice tool. Participants valued AI's role in simulation, feedback, and
learning from generated examples. Key design recommendations include promoting
social presence in conversational AI for technical interview simulation,
providing feedback beyond verbal content analysis, and enabling crowdsourced
think-aloud examples through human-AI collaboration. Beyond feature design, we
examined broader considerations, including intersectional challenges and
potential strategies to address them, how AI-driven interview preparation could
promote equitable learning in computing careers, and the need to rethink AI's
role in interview practice by suggesting a research direction that integrates
human-AI collaboration.

</details>


### [604] [Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies](https://arxiv.org/abs/2507.14482)
*Qianhe Chen,Yong Wang,Yixin Yu,Xiyuan Zhu,Xuerou Yu,Ran Wang*

Main category: cs.HC

TL;DR: Conch是一个交互式可视化系统，它利用新颖的平行螺旋可视化和大型语言模型来分析竞技辩论，有效解决了手动分析耗时的问题，并通过案例研究和用户研究证明了其有效性和可用性。


<details>
  <summary>Details</summary>
Motivation: 手动分析非结构化和未标记的辩论文本记录耗时且无效，因为很难从原始数据中重建上下文语义和追踪逻辑联系。

Method: 提出了一种新颖的平行螺旋可视化方法来紧凑地追踪冲突点和参与者交互的多维度演变。利用大型语言模型和精心设计的提示来自动识别冲突点、分歧、观点和策略等关键辩论元素。

Result: Conch系统能够系统地分析辩论内容和辩论方式，帮助参与者全面理解辩论背景。

Conclusion: 通过对真实辩论进行的两个案例研究和一项精心设计的用户研究，证明了Conch在竞技辩论分析中的有效性和可用性。

Abstract: In-depth analysis of competitive debates is essential for participants to
develop argumentative skills and refine strategies, and further improve their
debating performance. However, manual analysis of unstructured and unlabeled
textual records of debating is time-consuming and ineffective, as it is
challenging to reconstruct contextual semantics and track logical connections
from raw data. To address this, we propose Conch, an interactive visualization
system that systematically analyzes both what is debated and how it is debated.
In particular, we propose a novel parallel spiral visualization that compactly
traces the multidimensional evolution of clash points and participant
interactions throughout debate process. In addition, we leverage large language
models with well-designed prompts to automatically identify critical debate
elements such as clash points, disagreements, viewpoints, and strategies,
enabling participants to understand the debate context comprehensively.
Finally, through two case studies on real-world debates and a
carefully-designed user study, we demonstrate Conch's effectiveness and
usability for competitive debate analysis.

</details>


### [605] ["It looks sexy but it's wrong." Tensions in creativity and accuracy using genAI for biomedical visualization](https://arxiv.org/abs/2507.14494)
*Roxanne Ziman,Shehryar Saharan,Gaël McGill,Laura Garrison*

Main category: cs.HC

TL;DR: 生成式AI在生物医学可视化中虽能制作精美图像，但存在准确性和可信度风险。专家们使用态度不一。研究强调人工干预对于确保科学视觉准确性和负责任沟通的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在生物医学可视化中的应用所带来的挑战，特别是其在准确性和可信度方面存在的局限性，以及在科学传播日益重要的背景下，如何负责任地使用这些技术。

Method: 通过对17位从业者和研究人员进行深入访谈，对生成式人工智能在生物医学可视化（BioMedVis）中的应用所产生的流程和矛盾进行质性分析。

Result: 研究发现，生物医学可视化领域的专家对生成式AI的态度各不相同，从积极的采用者到怀疑的规避者。他们根据自身需求在工作流程的不同阶段使用生成式AI工具。研究将当前观察到的生成式AI的使用和观点与可视化流程中对生成式AI的预测进行了对比，强调了在当前环境下生成式AI的机遇和挑战，为未来的可视化研究提供了方向。

Conclusion: 虽然生成式人工智能（genAI）可以轻松制作具有美感的生物和医学内容的视觉效果，但这些工具的架构从根本上限制了所呈现信息的准确性和可信度，导致出现虚构的分子或外星解剖结构等问题。在科学传播和生物医学可视化领域，应首先避免造成伤害。我们的观察重申了在共情设计和评估准确科学视觉效果方面进行人工干预的必要性。

Abstract: We contribute an in-depth analysis of the workflows and tensions arising from
generative AI (genAI) use in biomedical visualization (BioMedVis). Although
genAI affords facile production of aesthetic visuals for biological and medical
content, the architecture of these tools fundamentally limits the accuracy and
trustworthiness of the depicted information, from imaginary (or fanciful)
molecules to alien anatomy. Through 17 interviews with a diverse group of
practitioners and researchers, we qualitatively analyze the concerns and values
driving genAI (dis)use for the visual representation of spatially-oriented
biomedical data. We find that BioMedVis experts, both in roles as developers
and designers, use genAI tools at different stages of their daily workflows and
hold attitudes ranging from enthusiastic adopters to skeptical avoiders of
genAI. In contrasting the current use and perspectives on genAI observed in our
study with predictions towards genAI in the visualization pipeline from prior
work, our refocus the discussion of genAI's effects on projects in
visualization in the here and now with its respective opportunities and
pitfalls for future visualization research. At a time when public trust in
science is in jeopardy, we are reminded to first do no harm, not just in
biomedical visualization but in science communication more broadly. Our
observations reaffirm the necessity of human intervention for empathetic design
and assessment of accurate scientific visuals.

</details>


### [606] [PaperBridge: Crafting Research Narratives through Human-AI Co-Exploration](https://arxiv.org/abs/2507.14527)
*Runhua Zhang,Yang Ouyang,Leixian Shen,Yuying Tang,Xiaojuan Ma,Huamin Qu,Xian Xu*

Main category: cs.HC

TL;DR: PaperBridge系统利用AI帮助研究人员整合出版物，探索不同叙事，用户研究证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究人员需要将自己的出版物整合成连贯的学术叙事，以展示其学术贡献。尤其在人机交互（HCI）等跨学科领域，研究人员的出版物可能涵盖多样化的领域和方法论，因此在保持连贯性的同时探索组织其工作的新方法具有挑战性。

Method: 该研究提出并实现了一个名为PaperBridge的人机协同探索系统，该系统利用大型语言模型驱动的双向分析引擎，支持用户从顶层意图（如确定组织结构）和底层叙事组件（如主题论文分组）进行迭代探索。系统设计基于一项形成性研究和内容分析。

Result: 用户研究（N=12）结果表明PaperBridge具有良好的可用性和有效性，能够帮助研究人员探索不同的研究叙事。研究结果还为交互式系统如何支持学术交流任务提供了实证见解。

Conclusion: 该研究提出了PaperBridge，一个结合人与AI共同探索的系统，旨在帮助研究人员将他们的出版物合成为连贯的学术叙事，以适应不同的交流情境。用户研究表明该系统易于使用且能有效促进研究叙事的探索，并为交互式系统如何支持学术交流任务提供了实证见解。

Abstract: Researchers frequently need to synthesize their own publications into
coherent narratives that demonstrate their scholarly contributions. To suit
diverse communication contexts, exploring alternative ways to organize one's
work while maintaining coherence is particularly challenging, especially in
interdisciplinary fields like HCI where individual researchers' publications
may span diverse domains and methodologies. In this paper, we present
PaperBridge, a human-AI co-exploration system informed by a formative study and
content analysis. PaperBridge assists researchers in exploring diverse
perspectives for organizing their publications into coherent narratives. At its
core is a bi-directional analysis engine powered by large language models,
supporting iterative exploration through both top-down user intent (e.g.,
determining organization structure) and bottom-up refinement on narrative
components (e.g., thematic paper groupings). Our user study (N=12) demonstrated
PaperBridge's usability and effectiveness in facilitating the exploration of
alternative research narratives. Our findings also provided empirical insights
into how interactive systems can scaffold academic communication tasks.

</details>


### [607] [Uncovering the EEG Temporal Representation of Low-dimensional Object Properties](https://arxiv.org/abs/2507.14537)
*Jiahua Tang,Song Wang,Jiachen Zou,Chen Wei,Quanying Liu*

Main category: cs.HC

TL;DR: 本研究提出了一种新方法，利用EEG和先进的神经解码算法来研究物体属性如何随时间在大脑中编码，提高了可解释性，并为BCI提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 尽管EEG具有毫秒级的时间分辨率，但由于其信噪比低和时空耦合复杂，其神经表征的时间动态仍未得到充分探索。本研究旨在弥合这一研究差距。

Method: 提出了一种整合先进神经解码算法的新方法，以系统地研究低维物体属性在EEG信号中的时间编码方式。

Result: 该方法提高了神经表征的可解释性，并为BCI中的视觉解码提供了新见解。

Conclusion: 本研究提出了一种结合先进神经解码算法的方法，用于系统地研究低维物体属性如何在EEG信号中进行时间编码，并首次尝试在时间分布中识别概念的特异性和原型时间特征。该框架提高了神经表征的可解释性，并为BCI中的视觉解码提供了新见解。

Abstract: Understanding how the human brain encodes and processes external visual
stimuli has been a fundamental challenge in neuroscience. With advancements in
artificial intelligence, sophisticated visual decoding architectures have
achieved remarkable success in fMRI research, enabling more precise and
fine-grained spatial concept localization. This has provided new tools for
exploring the spatial representation of concepts in the brain. However, despite
the millisecond-scale temporal resolution of EEG, which offers unparalleled
advantages in tracking the dynamic evolution of cognitive processes, the
temporal dynamics of neural representations based on EEG remain underexplored.
This is primarily due to EEG's inherently low signal-to-noise ratio and its
complex spatiotemporal coupling characteristics. To bridge this research gap,
we propose a novel approach that integrates advanced neural decoding algorithms
to systematically investigate how low-dimensional object properties are
temporally encoded in EEG signals. We are the first to attempt to identify the
specificity and prototypical temporal characteristics of concepts within
temporal distributions. Our framework not only enhances the interpretability of
neural representations but also provides new insights into visual decoding in
brain-computer interfaces (BCI).

</details>


### [608] [EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences](https://arxiv.org/abs/2507.14685)
*Luis Montana,Jessica Magallanes,Miguel Juarez,Suzanne Mason,Andrew Narracott,Lindsey van Gemeren,Steven Wood,Maria-Cruz Villa-Uriol*

Main category: cs.HC

TL;DR: EventBox是一种新的可视化方法，用于分析事件序列中的时间与多变量属性交互，已集成到Sequen-C系统中，并通过用户研究和案例研究证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的事件序列分析方法常常忽略时间属性和多变量属性之间的相互作用，而这种相互作用对于理解和决策至关重要。因此，需要一种新的方法来有效分析事件序列中的时间与多变量属性交互。

Method: 提出了一种名为EventBox的新型数据表示和视觉编码方法，并将其集成到Sequen-C视觉分析系统中。该系统支持用户驱动的变换（如对齐、排序、替换和聚合）以及自动生成的统计分析，以增强对事件序列中时间与多变量属性交互的分析深度。

Result: 通过用户研究（21名参与者）和真实世界医疗数据的案例研究，证明了EventBox及其在Sequen-C系统中的集成能够有效地揭示模式、异常和见解。该方法在可视化价值和用户性能方面表现良好。

Conclusion: 该研究提出了一种用于分析事件序列中时间与多变量属性交互的新颖数据表示和可视化编码方法EventBox，并将其集成到Sequen-C系统中。通过用户驱动的变换和自动生成的统计分析，增强了分析深度。评估结果表明，该方法能够有效地揭示有意义的模式、异常和见解，从而推动了事件序列可视化分析的发展。

Abstract: The rapid growth and availability of event sequence data across domains
requires effective analysis and exploration methods to facilitate
decision-making. Visual analytics combines computational techniques with
interactive visualizations, enabling the identification of patterns, anomalies,
and attribute interactions. However, existing approaches frequently overlook
the interplay between temporal and multivariate attributes. We introduce
EventBox, a novel data representation and visual encoding approach for
analyzing groups of events and their multivariate attributes. We have
integrated EventBox into Sequen-C, a visual analytics system for the analysis
of event sequences. To enable the agile creation of EventBoxes in Sequen-C, we
have added user-driven transformations, including alignment, sorting,
substitution and aggregation. To enhance analytical depth, we incorporate
automatically generated statistical analyses, providing additional insight into
the significance of attribute interactions. We evaluated our approach involving
21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T
framework to assess visualization value, user performance metrics completing a
series of tasks, and interactive sessions with domain experts. We also present
three case studies with real-world healthcare data demonstrating how EventBox
and its integration into Sequen-C reveal meaningful patterns, anomalies, and
insights. These results demonstrate that our work advances visual analytics by
providing a flexible solution for exploring temporal and multivariate
attributes in event sequences.

</details>


### [609] [A Notification Based Nudge for Handling Excessive Smartphone Use](https://arxiv.org/abs/2507.14702)
*Partha Sarker,Dipto Dey,Marium-E-Jannat*

Main category: cs.HC

TL;DR: 通过一种避免引起用户反感或烦恼的通知干预方法，利用提示和可视化技术，增加了用户的自我意识，以减少大学生过度使用智能手机的现象。


<details>
  <summary>Details</summary>
Motivation: 解决全球范围内智能手机过度使用的问题，并提出一种避免引起用户反感或烦恼的干预方法，克服了现有研究中通过增加使用难度来限制使用的局限性。

Method: 提出了一种基于通知的干预方法，并设计了一个名为“App Usage Monitor”的Android原型，通过为期3周的实验进行了概念验证。

Result: 用户研究（n=109）表明19.3%的参与者不愿使用限制使用的方法，因为他们不希望活动受限或觉得应用烦人。实验证明了所提出假设的概念可行性。

Conclusion: 提出了一种基于通知的干预方法，通过结合提示和可视化技术，利用通知增加用户的自我意识，以减少大学生过度使用智能手机的现象。

Abstract: Excessive use of smartphones is a worldwide known issue. In this study, we
proposed a notification-based intervention approach to reduce smartphone
overuse without making the user feel any annoyance or irritation. Most of the
work in this field tried to reduce smartphone overuse by making smartphone use
more difficult for the user. In our user study (n = 109), we found that 19.3%
of the participants are unwilling to use any usage-limiting application because
a) they do not want their smartphone activities to get restricted or b) those
applications are annoying. Following that, we devised a hypothesis to minimize
smartphone usage among undergraduates. Finally, we designed a prototype for
Android, "App Usage Monitor," and conducted a 3-week experiment through which
we found proof of concept for our hypothesis. In our prototype, we combined
techniques such as nudge and visualization to increase self-awareness among the
user by leveraging notifications.

</details>


### [610] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)
*Yanming Zhang,Krishnakumar Hegde,Klaus Mueller*

Main category: cs.HC

TL;DR: XplainAct visual analytics framework allows individual-level causal reasoning for heterogeneous systems, unlike existing population-level methods. Demonstrated with opioid deaths and voting data.


<details>
  <summary>Details</summary>
Motivation: Existing causal reasoning methods focus on population-level effects and fall short in heterogeneous systems where intervention impacts vary across subgroups.

Method: The paper presents XplainAct, a visual analytics framework for simulating, explaining, and reasoning about interventions at the individual level within subpopulations.

Result: The effectiveness of XplainAct is demonstrated through two case studies: investigating opioid-related deaths in epidemiology and analyzing voting inclinations in the presidential election.

Conclusion: XplainAct is a visual analytics framework that supports simulating, explaining, and reasoning interventions at the individual level within subpopulations, addressing the limitations of population-level causal reasoning in heterogeneous systems.

Abstract: Causality helps people reason about and understand complex systems,
particularly through what-if analyses that explore how interventions might
alter outcomes. Although existing methods embrace causal reasoning using
interventions and counterfactual analysis, they primarily focus on effects at
the population level. These approaches often fall short in systems
characterized by significant heterogeneity, where the impact of an intervention
can vary widely across subgroups. To address this challenge, we present
XplainAct, a visual analytics framework that supports simulating, explaining,
and reasoning interventions at the individual level within subpopulations. We
demonstrate the effectiveness of XplainAct through two case studies:
investigating opioid-related deaths in epidemiology and analyzing voting
inclinations in the presidential election.

</details>


### [611] [Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs](https://arxiv.org/abs/2507.14769)
*Ananya Gubbi Mohanbabu,Yotam Sechayk,Amy Pavel*

Main category: cs.HC

TL;DR: Task Mode 通过使用大型语言模型过滤网页内容来简化现代网页的复杂性，从而减少了屏幕阅读器用户和视觉用户之间的访问时间差距，并提高了任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现代网页界面过于复杂，给用户带来过多的文本和视觉信息，尤其是屏幕阅读器用户（SRUs），他们需要花费大量时间来浏览不相关的内容才能找到所需信息，而视觉用户（VUs）可以通过视觉浏览在几秒钟内完成。

Method: 提出了一种名为 Task Mode 的系统，该系统使用大型语言模型根据用户指定的任务动态过滤网页内容，以识别和优先处理相关元素，同时最大限度地减少干扰。该方法保留了页面结构，并为不同的访问需求提供了多种查看模式。

Result: 在对 12 名参与者（6 名 VU，6 名 SRU）进行的可用性研究中，Task Mode 将 SRU 的任务完成时间缩短了 2 倍，同时保持了 VU 的性能，并将两组之间的完成时间差距从 2 倍缩小到 1.2 倍。11/12 的参与者表示希望将来继续使用 Task Mode，并报告称 Task Mode 能够帮助他们更轻松、更少干扰地完成任务。

Conclusion: 该研究表明，同时为视觉和非视觉访问设计新交互可以减少而非加剧未来技术中的可访问性差异。

Abstract: Modern web interfaces are unnecessarily complex to use as they overwhelm
users with excessive text and visuals unrelated to their current goals. This
problem particularly impacts screen reader users (SRUs), who navigate content
sequentially and may spend minutes traversing irrelevant elements before
reaching desired information compared to vision users (VUs) who visually skim
in seconds. We present Task Mode, a system that dynamically filters web content
based on user-specified goals using large language models to identify and
prioritize relevant elements while minimizing distractions. Our approach
preserves page structure while offering multiple viewing modes tailored to
different access needs. Our user study with 12 participants (6 VUs, 6 SRUs)
demonstrates that our approach reduced task completion time for SRUs while
maintaining performance for VUs, decreasing the completion time gap between
groups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the
future, reporting that Task Mode supported completing tasks with less effort
and fewer distractions. This work demonstrates how designing new interactions
simultaneously for visual and non-visual access can reduce rather than
reinforce accessibility disparities in future technology created by
human-computer interaction researchers and practitioners.

</details>


### [612] [SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors](https://arxiv.org/abs/2507.14792)
*Kaixin Ji,Danula Hettiachchi,Falk Scholer,Flora D. Salim,Damiano Spina*

Main category: cs.HC

TL;DR: 该研究提出了SenseSeek数据集，这是一个包含来自20名参与者的生理和行为数据的独特数据集，用于分析通过搜索引擎进行信息搜索。研究人员使用了多种传感器（如EDA、EEG、瞳控）来捕获用户在搜索过程不同阶段（如信息需求、查询构建、结果判断）的数据，并提取了258个特征。基线分析表明，该数据集能够区分搜索阶段并揭示认知意图和交互模式的影响，为信息寻求行为的研究提供了宝贵的资源。


<details>
  <summary>Details</summary>
Motivation: 为了理解个体在现代信息系统（如网络搜索引擎）中的信息处理行为，需要一种能够捕捉用户交互的、个性化的数据捕获方法。该研究的动机是利用可穿戴设备等被动传感器来捕获生理和行为数据，以解决这一需求，并为信息寻求行为的研究提供一个全面的数据集。

Method: 该研究利用消费级传感器（包括皮肤电活动EDA、脑电图EEG、瞳孔、注视和运动数据）收集了20名参与者在搜索过程中的数据，包括信息需求（IN）、查询构建（QF）、通过打字（QS-T）或语音（QS-S）提交查询以及通过阅读（RJ-R）或听觉（RJ-L）进行相关性判断等阶段。研究人员从传感器数据、注视标记的屏幕录制和任务响应中提取了258个特征，并进行了基线分析以验证数据集的效用。

Result: 研究结果表明，SenseSeek数据集能够有效地区分信息搜索的不同阶段，并能体现不同认知意图和交互模式对传感器数据的影响。该数据集包含从消费者级传感器收集的生理信号，能够表征信息寻求过程中涉及的多个阶段，填补了现有研究的空白。

Conclusion: 该研究提出了SenseSeek数据集，该数据集旨在评估消费级传感器在复杂的“通过系统进行搜索”的信息处理场景中的有效性。通过对20名参与者在搜索过程中的生理和行为数据进行捕获，该数据集为研究信息寻求行为提供了新的资源。研究结果表明，该数据集在区分搜索阶段方面具有有效性，并对不同认知意图和交互模式对传感器数据的影响进行了基线分析。

Abstract: Information processing tasks involve complex cognitive mechanisms that are
shaped by various factors, including individual goals, prior experience, and
system environments. Understanding such behaviors requires a sophisticated and
personalized data capture of how one interacts with modern information systems
(e.g., web search engines). Passive sensors, such as wearables, capturing
physiological and behavioral data, have the potential to provide solutions in
this context. This paper presents a novel dataset, SenseSeek, designed to
evaluate the effectiveness of consumer-grade sensors in a complex information
processing scenario: searching via systems (e.g., search engines), one of the
common strategies users employ for information seeking. The SenseSeek dataset
comprises data collected from 20 participants, 235 trials of the stimulated
search process, 940 phases of stages in the search process, including the
realization of Information Need (IN), Query Formulation (QF), Query Submission
by Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R)
or Listening (RJ-L). The data includes Electrodermal Activities (EDA),
Electroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured
using consumer-grade sensors. It also contains 258 features extracted from the
sensor data, the gaze-annotated screen recordings, and task responses. We
validate the usefulness of the dataset by providing baseline analysis on the
impacts of different cognitive intents and interaction modalities on the sensor
data, and effectiveness of the data in discriminating the search stages. To our
knowledge, SenseSeek is the first dataset that characterizes the multiple
stages involved in information seeking with physiological signals collected
from multiple sensors. We hope this dataset can serve as a reference for future
research on information-seeking behaviors.

</details>


### [613] [Understanding How Visually Impaired Players Socialize in Mobile Games](https://arxiv.org/abs/2507.14818)
*Zihe Ran,Xiyu Li,Qing Xiao,Yanyun Wang,Franklin Mingzhe Li,Zhicong Lu*

Main category: cs.HC

TL;DR: 视障玩家通过移动游戏进行社交，但技术和社区障碍限制了他们的参与。


<details>
  <summary>Details</summary>
Motivation: 许多视障人士通过移动游戏寻求社交联系，但现有的游戏设计和社区结构并未充分满足他们的需求。

Method: 通过访谈30位中国视觉障碍玩家，探讨了他们在移动游戏中的社交体验和融入游戏社区的挑战。

Result: 移动游戏在满足视觉障碍玩家的社交需求方面发挥着重要作用，但也存在技术和社区方面的挑战。

Conclusion: 移动游戏为视觉障碍玩家提供了重要的社交平台，但技术障碍、缺乏辅助功能以及社区内部的障碍阻碍了他们的充分参与。

Abstract: Mobile games are becoming a vital medium for social interaction, offering a
platform that transcends geographical boundaries. An increasing number of
visually impaired individuals are engaging in mobile gaming to connect,
collaborate, compete, and build friendships. In China, visually impaired
communities face significant social challenges in offline settings, making
mobile games a crucial avenue for socialization. However, the design of mobile
games and their mapping to real-world environments significantly shape their
social gaming experiences. This study explores how visually impaired players in
China navigate socialization and integrate into gaming communities. Through
interviews with 30 visually impaired players, we found that while mobile games
fulfill many of their social needs, technological barriers and insufficient
accessibility features, and internal community divisions present significant
challenges to their participation. This research sheds light on their social
experiences and offers insights for designing more inclusive and accessible
mobile games.

</details>


### [614] [Progressive Sentences: Combining the Benefits of Word and Sentence Learning](https://arxiv.org/abs/2507.14846)
*Nuwan Janaka,Shengdong Zhao,Ashwin Ram,Ruoxin Sun,Sherisse Tan Jing Wen,Danae Li,David Hsu*

Main category: cs.HC

TL;DR: AR智能眼镜可以通过“渐进式呈现”方法，结合单词和句子学习，并加入 timed gaps，来提高移动学习第二语言的效率，即使在边走边学或多任务场景下也能发挥作用。


<details>
  <summary>Details</summary>
Motivation: 随着轻量级消费级增强现实（AR）智能眼镜的快速发展，为学习提供了新的机会，特别是其在即时微学习场景中提供多模态信息的能力。本研究旨在探索这类设备如何通过渐进式多模态格式呈现句子结构来支持移动第二语言习得。

Method: 本研究提出了一种“渐进式呈现”方法，结合了单词和句子学习，通过依次呈现句子成分（主语、谓语、宾语）并保留先前语境，以支持移动第二语言习得，这与以往主要针对初学者采用词汇学习的方法不同。

Result: 试点研究和正式研究表明，渐进式呈现方法能够提高记忆力，特别是在移动学习场景中。在单词呈现之间加入 timed gaps 进一步提高了在多任务条件下的学习效率。

Conclusion: 该研究证明了渐进式呈现方法在移动场景下（如边走边学）的有效性，尤其是在多任务条件下，通过在单词呈现之间加入 timed gaps 可以进一步提高学习效果。研究结果为教育应用提供了使用指南，即使在短暂的、移动的学习时刻也能发挥作用。

Abstract: The rapid evolution of lightweight consumer augmented reality (AR) smart
glasses (a.k.a. optical see-through head-mounted displays) offers novel
opportunities for learning, particularly through their unique capability to
deliver multimodal information in just-in-time, micro-learning scenarios. This
research investigates how such devices can support mobile second-language
acquisition by presenting progressive sentence structures in multimodal
formats. In contrast to the commonly used vocabulary (i.e., word) learning
approach for novice learners, we present a "progressive presentation" method
that combines both word and sentence learning by sequentially displaying
sentence components (subject, verb, object) while retaining prior context.
Pilot and formal studies revealed that progressive presentation enhances
recall, particularly in mobile scenarios such as walking. Additionally,
incorporating timed gaps between word presentations further improved learning
effectiveness under multitasking conditions. Our findings demonstrate the
utility of progressive presentation and provide usage guidelines for
educational applications-even during brief, on-the-go learning moments.

</details>


### [615] [Holistic Specification of the Human Digital Twin: Stakeholders, Users, Functionalities, and Applications](https://arxiv.org/abs/2507.14859)
*Nils Mandischer,Alexander Atanasyan,Ulrich Dahmen,Michael Schluse,Jürgen Rossmann,Lars Mikelsons*

Main category: cs.HC

TL;DR: 本研究提出了人类数字孪生的整体框架和需求，并探讨了其在不同应用场景下的潜力，为相关研究和行业应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 人类数字孪生是一个新兴概念，但目前缺乏对其构成要素的清晰定义，导致研究人员和行业用户对其实际市场潜力知之甚少。因此，本研究旨在提供一个关于人类数字孪生的整体性愿景，并制定详细的需求规范，以指导未来的研究和应用。

Method: 本研究首先明确了人类数字孪生的概念，然后分析了其市场潜力。接着，研究提出了一个整体性的人类数字孪生愿景，并将其规范化为需求、利益相关者和用户。针对不同的用户群体，研究定义了六个功能级别的示例应用程序：存储、分析、个性化、预测、控制和优化。通过详细讨论三个典型应用，展示了该功能级别抽象和用户/利益相关者分析的可行性。最后，基于深入的讨论，研究得出了一个全面的整体性人类数字孪生需求列表。

Result: 本研究提出了一个整体性的人类数字孪生框架，定义了其核心功能级别（存储、分析、个性化、预测、控制、优化），并探讨了不同用户群体的应用。研究通过具体案例展示了该框架的可行性，并为人类数字孪生的研发和应用提供了详细的需求列表和指导方针，强调了其在多应用场景下的可重用性。

Conclusion: 本论文提出了一个整体性的人类数字孪生概念，并推导了其在需求、利益相关者和用户方面的详细规范。通过对六个功能级别（存储、分析、个性化、预测、控制和优化）的示例应用程序的讨论，展示了该概念的可行性，并为研究和行业提供了实施人类数字孪生的指导方针，特别是在多目标应用程序的可重用性方面。

Abstract: The digital twin of humans is a relatively new concept. While many diverse
definitions, architectures, and applications exist, a clear picture is missing
on what, in fact, makes a human digital twin. Within this context, researchers
and industrial use-case owners alike are unaware about the market potential of
the - at the moment - rather theoretical construct. In this work, we draw a
holistic vision of the human digital twin, and derive the specification of this
holistic human digital twin in form of requirements, stakeholders, and users.
For each group of users, we define exemplary applications that fall into the
six levels of functionality: store, analyze, personalize, predict, control, and
optimize. The functionality levels facilitate an abstraction of abilities of
the human digital twin. From the manifold applications, we discuss three in
detail to showcase the feasibility of the abstraction levels and the analysis
of stakeholders and users. Based on the deep discussion, we derive a
comprehensive list of requirements on the holistic human digital twin. These
considerations shall be used as a guideline for research and industries for the
implementation of human digital twins, particularly in context of reusability
in multiple target applications.

</details>


### [616] [LEKIA: A Framework for Architectural Alignment via Expert Knowledge Injection](https://arxiv.org/abs/2507.14944)
*Boning Zhao,Yutong Hu*

Main category: cs.HC

TL;DR: 本研究通过 LEKIA 架构统一了知识注入和价值对齐，使领域专家能够直接设计 AI 行为，解决了 LLM 在高风险应用中的知识与对齐的矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 部署在高风险领域的挑战在于需要深度、动态的专家知识注入以及细致的价值对齐，而现有方法往往分别处理这两个问题，导致知识和对齐之间的紧张关系。

Method: 提出了一种名为 LEKIA（层级专家知识注入架构）的新框架，它作为一个智能中间件，在不改变 LLM 权重的情况下，利用理论层、实践层和评估层来指导 LLM 的推理过程，并实现价值对齐的自我修正。

Result: 通过在特殊教育领域成功实现一个基于 LEKIA 的心理支持助手，证明了该范式的有效性。

Conclusion: 该研究提出了一种名为 LEKIA 的新范式，通过“架构对齐”将知识注入和价值对齐统一在一个框架内，旨在解决高风险领域部署 LLM 的双重挑战。

Abstract: Deploying Large Language Models (LLMs) in high-stakes domains is impeded by a
dual challenge: the need for deep, dynamic expert knowledge injection and
nuanced value alignment. Prevailing paradigms often address these challenges
separately, creating a persistent tension between knowledge and alignment;
knowledge-focused methods like Retrieval-Augmented Generation (RAG) have
limited deep alignment capabilities, while alignment-focused methods like
Reinforcement Learning from Human Feedback (RLHF) struggle with the agile
injection of expert wisdom. This paper introduces a new collaborative
philosophy, Expert-owned AI behavior design, realized through Architectural
Alignment-a paradigm that unifies these two goals within a single framework
called the Layered Expert Knowledge Injection Architecture (LEKIA). LEKIA
operates as an intelligent intermediary that guides an LLM's reasoning process
without altering its weights, utilizing a three-tiered structure: a Theoretical
Layer for core principles, a Practical Layer for exemplary cases, and an
Evaluative Layer for real-time, value-aligned self-correction. We demonstrate
the efficacy of this paradigm through the successful implementation of a
LEKIA-based psychological support assistant for the special education field.
Our work presents a path toward more responsible and expert-driven AI,
empowering domain specialists to directly architect AI behavior and resolve the
tension between knowledge and alignment.

</details>


### [617] [Echoes of the Land: An Interactive Installation Based on Physical Model of Earthquake](https://arxiv.org/abs/2507.14947)
*Ivan C. H. Liu,Chung-En Hao,Jing Xie*

Main category: cs.HC

TL;DR: “土地的回响”是一个交互式装置，它利用弹簧-块模型和实时生成的声音与灯光来模拟地震，展示了科学与艺术的融合。


<details>
  <summary>Details</summary>
Motivation: 将地震动力学转化为多感官体验。

Method: 通过科学的弹簧-块模型将地震动力学转化为多感官体验。模拟地震复发和自组织临界性，并通过动作捕捉和连接粒合成实时生成声音和灯光。每个块充当一个代理，产生涌现的视听级联，以可视化破裂物理和阈值行为。

Result: 本作品融合了科学知识与艺术实践，为新型乐器和叙事媒介开辟了新途径，并邀请对涌现复杂性、美学和交互性交叉领域进行进一步研究。

Conclusion: 本作品融合了科学知识与艺术实践，为新型乐器和叙事媒介开辟了新途径，并邀请对涌现复杂性、美学和交互性交叉领域进行进一步研究。

Abstract: Echoes of the Land is an interactive installation that transforms seismic
dynamics into a multisensory experience through a scientifically grounded
spring-block model. Simulating earthquake recurrence and self-organized
criticality, the work generates real-time sound and light via motion capture
and concatenative granular synthesis. Each block acts as an agent, producing
emergent audiovisual cascades that visualize the physics of rupture and
threshold behavior. This work exemplifies the amalgamation of scientific
knowledge and artistic practice, opening new avenues for novel forms of musical
instrument and narrative medium, while inviting further investigation into the
intersection of emergent complexity, aesthetics and interactivity.

</details>


### [618] [Emphasizing Deliberation and Critical Thinking in an AI Hype World](https://arxiv.org/abs/2507.14961)
*Katja Rogers*

Main category: cs.HC

TL;DR: AI 炒作和人机交互对新颖性的推崇加速了 AI solutionism 的发展。 建议采取更慢、更审慎的使用方式，并进行批判性参与，以应对后 AI 炒作世界，并减少教育和研究中的有害影响。


<details>
  <summary>Details</summary>
Motivation: AI solutionism 的发展受到炒作和人机交互对新颖性的推崇的加速和证实，这可能导致有害影响。因此，需要探讨如何应对 AI 炒作世界。

Method: 对 AI solutionism 的发展进行了分析，并探讨了人机交互在新颖性方面的影响。提出了应对 AI 炒作世界的方法，包括审慎使用、批判性参与和不参与。

Result: AI solutionism 的发展受到炒作和人机交互对新颖性的推崇的加速和证实。 建议采取更慢、更审慎的使用方式，并进行有意识的、批判性的参与和不参与，以应对后 AI 炒作世界，同时为知识基础做出贡献并减少教育和研究中的有害影响。

Conclusion: AI solutionism 的发展受到炒作和人机交互对新颖性的推崇的加速和证实。 建议采取更慢、更审慎的使用方式，并进行有意识的、批判性的参与和不参与，以应对后 AI 炒作世界，同时为知识基础做出贡献并减少教育和研究中的有害影响。

Abstract: AI solutionism is accelerated and substantiated by hype and HCI's elevation
of novelty. Banning or abandoning technology is unlikely to work and probably
not beneficial on the whole either -- but slow(er), deliberate use together
with conscientious, critical engagement and non-engagement may help us navigate
a post-AI hype world while contributing to a solid knowledge foundation and
reducing harmful impacts in education and research.

</details>


### [619] ['A Little Bubble of Friends': An Analysis of LGBTQ+ Pandemic Experiences Using Reddit Data](https://arxiv.org/abs/2507.15033)
*Dhruvee Birla,Nazia Akhtar*

Main category: cs.HC

TL;DR: 通过对 Reddit 上的 LGBTQ+ 相关讨论进行主题建模和情感分析，本研究探讨了该平台在大流行期间对 LGBTQ+ 群体的重要性及其作用的延续性。


<details>
  <summary>Details</summary>
Motivation: 鉴于社交媒体在大流行期间对年轻人交流的重要性，本研究旨在通过分析 Reddit 上的讨论，了解 LGBTQ+ 群体在大流行期间的经历和态度，并探讨 Reddit 在此期间的作用。

Method: 本研究使用 LDA 主题建模和情感分析技术，对来自五个专门讨论 LGBTQ+ 经历和问题的 Reddit 子版块的数据进行了分析。

Result: 研究识别了与大流行相关的关键主题，并分析了 LGBTQ+ 用户在 Reddit 上的情绪倾向，以了解他们在疫情期间的经历和感受。

Conclusion: 该研究表明，Reddit 在大流行期间为 LGBTQ+ 人群提供了一个重要的交流和支持平台，并且与大流行前相比，其作用有所延续。

Abstract: Social media was one of the most popular forms of communication among young
people with digital access during the pandemic. Consequently, crucial debates
and discussions about the pandemic crisis have also developed on social media
platforms, making them a great primary source to study the experiences of
specific groups and communities during the pandemic. This study involved
research using LDA topic modeling and sentiment analysis on data obtained from
the social media platform Reddit to understand the themes and attitudes in
circulation within five subreddits devoted to LGBTQ+ experiences and issues. In
the process, we attempt to make sense of the role that Reddit may have played
in the lives of LGBTQ+ people who were online during the pandemic, and whether
this was marked by any continuities or discontinuities from before the pandemic
period.

</details>


### [620] [Visibility vs. Engagement: How Two Indian News Websites Reported on LGBTQ+ Individuals and Communities during the Pandemic](https://arxiv.org/abs/2507.15041)
*Dhruvee Birla,Nazia Akhtar*

Main category: cs.HC

TL;DR: 印度媒体在疫情期间报导LGBTQ+社群，但质量不高，《印度时报》有恐跨内容。


<details>
  <summary>Details</summary>
Motivation: 分析印度在线新闻媒体在COVID-19大流行期间对LGBTQ+社群，特别是跨性别群体的报导情况，以了解其可见性和代表性。

Method: 通过对2020年3月至2021年8月以及2019年1月至2019年12月期间的文章进行情感分析、主题建模和手动分析，比较了疫情期间及之前的报导。

Result: 两大报纸网站均有报导LGBTQ+社群，对跨性别社群的关注度更高，但报导质量和深度不足。《印度时报》的部分文章存在恐跨性别和过时的语言。

Conclusion: 该研究揭示了在COVID-19大流行期间，印度两大报纸网站对LGBTQ+社群的报导情况，发现虽然报导量有所增加，但质量和深度仍显不足，并且《印度时报》存在恐跨性别语言。

Abstract: In India, online news media outlets were an important source of information
for people with digital access during the COVID-19 pandemic. In India, where
"transgender" was legally recognised as a category only in 2014, and same-sex
marriages are yet to be legalised, it becomes crucial to analyse whether and
how they reported the lived realities of vulnerable LGBTQ+ communities during
the pandemic. This study analysed articles from online editions of two
English-language newspaper websites, which differed vastly in their circulation
figures-The Times of India and The Indian Express. The results of our study
suggest that these newspaper websites published articles surrounding various
aspects of the lives of LGBTQ+ individuals with a greater focus on transgender
communities. However, they lacked quality and depth. Focusing on the period
spanning March 2020 to August 2021, we analysed articles using sentiment
analysis and topic modelling. We also compared our results to the period before
the pandemic (January 2019 - December 2019) to understand the shift in topics,
sentiments, and stances across the two newspaper websites. A manual analysis of
the articles indicated that the language used in certain articles by The Times
of India was transphobic and obsolete. Our study captures the visibility and
representation of the LGBTQ+ communities in Indian newspaper websites during
the pandemic.

</details>


### [621] [Beyond Visual Line of Sight: UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence](https://arxiv.org/abs/2507.15049)
*Andres Navarro,Carlos de Quinto,José Alberto Hernández*

Main category: cs.HC

TL;DR: 该研究介绍了一个经济实惠的无人机平台，集成了5G、边缘AI和LLM，能够进行实时目标识别、数据分析和VR体验，适用于应急响应、基础设施评估和环境监测等场景。


<details>
  <summary>Details</summary>
Motivation: 利用无人机在非地面网络（NTN）中的优势，解决NTN场景下的核心挑战，如目标识别、情境分析和提供沉浸式操作体验。

Method: 提出并评估了一个集成了5G通信、边缘处理和AI（包括全景摄像头、强大的机载计算和大型语言模型）的经济型四旋翼无人机平台。

Result: 该平台能够低延迟地处理视觉流，维持强大的5G连接，并通过大型语言模型（LLM）提取可操作的见解并优化收集的数据以支持决策。

Conclusion: 无人机平台成功集成了5G通信、边缘计算和人工智能，可用于非地面网络场景，并在实际应用中展现出低延迟处理、稳定的5G连接以及通过LLM提取信息的能力。

Abstract: Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as
agile, intelligent nodes capable of advanced analytics and instantaneous
situational awareness. This article introduces a budget-friendly quadcopter
platform that unites 5G communications, edge-based processing, and AI to tackle
core challenges in NTN scenarios. Outfitted with a panoramic camera, robust
onboard computation, and LLMs, the drone system delivers seamless object
recognition, contextual analysis, and immersive operator experiences through
virtual reality VR technology. Field evaluations confirm the platform's ability
to process visual streams with low latency and sustain robust 5G links. Adding
LLMs further streamlines operations by extracting actionable insights and
refining collected data for decision support. Demonstrated use cases, including
emergency response, infrastructure assessment, and environmental surveillance,
underscore the system's adaptability in demanding contexts.

</details>


### [622] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)
*Maisha Maimuna,Minhaz Bin Farukee,Sama Nikanfar,Mahfuza Siddiqua,Ayon Roy,Fillia Makedon*

Main category: cs.HC

TL;DR: 为低视力用户设计的多模态遥操作仓库机器人模拟器，提供视觉、听觉和触觉反馈，以提高安全性。


<details>
  <summary>Details</summary>
Motivation: 工业仓库环境对低视力操作员进行机器人遥操作存在风险和挑战。现有研究对工业环境中的可访问遥操作的系统性研究有限，并且很少有研究关注为低视力用户设计的多模态引导。

Method: 开发了一个多模态引导模拟器，结合了视觉路径线、语音导航提示和触觉反馈，以引导低视力用户操作移动机器人。系统利用导航网格和重规划来处理动态障碍物，并实现了闭环控制。

Result: 该多模态引导模拟器能够让低视力用户通过高保真仓库环境控制移动机器人，并接收同步的视觉、听觉和触觉反馈。该系统能够准确避开静态和动态障碍物，并为可访问遥操作研究提供了一个可重复的测试平台和算法参考。

Conclusion: 该研究提出了一个多模态引导模拟器，为低视力用户提供了在工业仓库环境中遥操作移动机器人的解决方案，并通过视觉、听觉和触觉反馈进行同步引导。该系统结合了导航网格和动态重规划，以避免与仓库中的叉车和人员发生碰撞。

Abstract: Industrial warehouses are congested with moving forklifts, shelves and
personnel, making robot teleoperation particularly risky and demanding for
blind and low-vision (BLV) operators. Although accessible teleoperation plays a
key role in inclusive workforce participation, systematic research on its use
in industrial environments is limited, and few existing studies barely address
multimodal guidance designed for BLV users. We present a novel multimodal
guidance simulator that enables BLV users to control a mobile robot through a
high-fidelity warehouse environment while simultaneously receiving synchronized
visual, auditory, and haptic feedback. The system combines a navigation mesh
with regular re-planning so routes remain accurate avoiding collisions as
forklifts and human avatars move around the warehouse. Users with low vision
are guided with a visible path line towards destination; navigational voice
cues with clockwise directions announce upcoming turns, and finally
proximity-based haptic feedback notifies the users of static and moving
obstacles in the path. This real-time, closed-loop system offers a repeatable
testbed and algorithmic reference for accessible teleoperation research. The
simulator's design principles can be easily adapted to real robots due to the
alignment of its navigation, speech, and haptic modules with commercial
hardware, supporting rapid feasibility studies and deployment of inclusive
telerobotic tools in actual warehouses.

</details>


### [623] ["If I were in Space": Understanding and Adapting to Social Isolation through Designing Collaborative Narratives](https://arxiv.org/abs/2507.15081)
*Qi Gong,Ximing Shen,Ziyou Yin,Yaning Li,Ray Lc*

Main category: cs.HC

TL;DR: 这项研究设计了一个协作的在线讲故事的社交虚拟现实体验，参与者在其中设计了一个想象中的太空旅行，以应对孤立，并揭示了他们的适应策略。


<details>
  <summary>Details</summary>
Motivation: 以往的研究侧重于体育锻炼和远程会议等身体干预，但忽略了适应性策略的叙事潜力。

Method: 通过定性分析参与者的设计、转录和互动，揭示了他们如何应对孤立，以及这种参与如何出乎意料地影响了他们的适应过程。

Result: 十八名参与者在实际隔离期间参加了一次虚拟角色扮演体验，设计了他们自己的宇宙飞船房间，并参与了揭示创造性适应策略的协作活动。

Conclusion: 本研究表明，设计 the 游戏化叙事体验，而不是 the 方案驱动的方法，可以作为探针来揭示人们如何应对社会孤立。

Abstract: Social isolation can lead to pervasive health issues like anxiety and
loneliness. Previous work focused on physical interventions like exercise and
teleconferencing, but overlooked the narrative potential of adaptive
strategies. To address this, we designed a collaborative online storytelling
experience in social VR, enabling participants in isolation to design an
imaginary space journey as a metaphor for quarantine, in order to learn about
their isolation adaptation strategies in the process. Eighteen individuals
participated during real quarantine undertaken a virtual role-play experience,
designing their own spaceship rooms and engaging in collaborative activities
that revealed creative adaptative strategies. Qualitative analyses of
participant designs, transcripts, and interactions revealed how they coped with
isolation, and how the engagement unexpectedly influenced their adaptation
process. This study shows how designing playful narrative experiences, rather
than solution-driven approaches, can serve as probes to surface how people
navigate social isolation.

</details>


### [624] [TalkLess: Blending Extractive and Abstractive Speech Summarization for Editing Speech to Preserve Content and Style](https://arxiv.org/abs/2507.15202)
*Karim Benharrak,Puyuan Peng,Amy Pavel*

Main category: cs.HC

TL;DR: TalkLess 是一款创新的语音编辑系统，它结合了提取和抽象技术，能在压缩语音的同时保留其内容和风格，并降低用户的编辑工作量。


<details>
  <summary>Details</summary>
Motivation: 目前的语音编辑方法（如提取式摘要）效果有限，而抽象式摘要会丢失说话者的风格。因此，需要一种能够灵活结合提取和抽象技术，在压缩语音的同时保留其内容和风格的系统。

Method: TalkLess 首先生成可能的文本编辑，然后选择能最大化压缩、覆盖率和音频质量的编辑，最后使用语音编辑模型将文本编辑转换为音频编辑。它通过分离低级措辞编辑（压缩面板）和主要内容编辑（大纲面板）来让创作者控制自动编辑。

Result: TalkLess 在覆盖率和去除语音错误方面优于目前最先进的提取方法。用户研究（N=12）表明，TalkLess 显著降低了认知负荷和编辑工作量。探索性研究（N=3）也展示了创作者使用 TalkLess 编辑自己语音的潜力。

Conclusion: TalkLess 成功地结合了提取和抽象技术，在压缩语音的同时保留了其内容和风格。与目前最先进的提取方法相比，TalkLess 实现了更高的覆盖率并能去除更多的语音错误。用户研究表明，TalkLess 显著降低了语音编辑过程中的认知负荷和编辑工作量，并且在创作者编辑自身语音的探索性研究中展现了其潜力。

Abstract: Millions of people listen to podcasts, audio stories, and lectures, but
editing speech remains tedious and time-consuming. Creators remove unnecessary
words, cut tangential discussions, and even re-record speech to make recordings
concise and engaging. Prior work automatically summarized speech by removing
full sentences (extraction), but rigid extraction limits expressivity. AI tools
can summarize then re-synthesize speech (abstraction), but abstraction strips
the speaker's style. We present TalkLess, a system that flexibly combines
extraction and abstraction to condense speech while preserving its content and
style. To edit speech, TalkLess first generates possible transcript edits,
selects edits to maximize compression, coverage, and audio quality, then uses a
speech editing model to translate transcript edits into audio edits. TalkLess's
interface provides creators control over automated edits by separating
low-level wording edits (via the compression pane) from major content edits
(via the outline pane). TalkLess achieves higher coverage and removes more
speech errors than a state-of-the-art extractive approach. A comparison study
(N=12) showed that TalkLess significantly decreased cognitive load and editing
effort in speech editing. We further demonstrate TalkLess's potential in an
exploratory study (N=3) where creators edited their own speech.

</details>


### [625] [How Does Empirical Research Facilitate Creation Tool Design? A Data Video Perspective](https://arxiv.org/abs/2507.15244)
*Leixian Shen,Leni Yang,Haotian Li,Yun Wang,Yuyu Luo,Huamin Qu*

Main category: cs.HC

TL;DR: 本研究分析了实证研究如何影响数据视频创作工具的设计，发现两者之间存在差距。通过论文分析和专家访谈，研究提出了加强两者联系的建议，以促进创作工具的理论发展和实证研究的实践应用。


<details>
  <summary>Details</summary>
Motivation: 实证研究能够加深对设计原理和感知效果的理论理解，并为创新创作工具提供指导。然而，这些实证研究如何影响创作工具的开发以及如何更好地整合二者仍是未被充分理解的。因此，本研究旨在通过数据视频这一案例来揭示这一差距。

Method: 通过对46篇关于数据视频的实证研究论文和48篇创作工具论文进行系统性的收集和结构化表征（按方法论和关注点分类），结合11位专家的访谈，进行上下文感知的引文分析，并揭示了实证研究为工具设计提供信息的模式分类。

Result: 研究揭示了实证发现如何通过引文功能（如问题构建、技术参考）影响数据视频创作工具的设计，并识别出研究人员在应用实证发现（如适应、综合、迭代）方面的实践模式。此外，还确定了影响适用性的关键因素，包括背景相关性、粒度匹配、清晰度、可信度和可行性。

Conclusion: 本研究旨在弥合实证研究与创作工具开发之间的差距，特别是在数据视频领域。通过对46篇实证研究论文和48篇数据视频创作工具论文的综合分析，以及对11位专家的访谈，我们揭示了实证发现如何影响工具设计，并提出了加强两者之间联系的建议，以期加强创作工具的理论基础并扩大实证研究的实践影响力。

Abstract: Empirical research in creative design deepens our theoretical understanding
of design principles and perceptual effects, offering valuable guidance for
innovating creation tools. However, how these empirical insights currently
influence the development of creation tools, and how their integration can be
enhanced in the future, remains insufficiently understood. In this paper, we
aim to unveil the gap through a case study on data videos, a prominent and
wide-spread medium for effective data storytelling. To achieve the goal, we
conducted a comprehensive analysis of 46 empirical research papers and 48
creation tool papers on data video, complemented by interviews with 11 experts.
Building upon a systematic collection and structured characterization of
empirical research by their methodologies (e.g., corpus analysis, comparative
evaluations) and component focus (e.g., visuals, motions, narratives, audio),
we conducted a context-aware citation analysis and revealed a taxonomy of
recurring patterns in how empirical findings inform tool design across citation
functions (e.g., problem framing, technical reference). Expert interviews
further uncovered researchers' practice patterns in applying empirical findings
(e.g., adaptation, synthesis, iteration, etc.) and identified key factors
influencing applicability, such as contextual relevance, granularity matching,
clarity, credibility, and feasibility. Finally, we derive suggestions and
discuss future opportunities to foster closer mutual engagement between
empirical and tool research, aiming to reinforce the theoretical grounding of
creation tools and enhance the practical impact of empirical research.

</details>


### [626] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)
*Zhipeng Li,Yi-Chi Liao,Christian Holz*

Main category: cs.HC

TL;DR: Meta-PO通过结合PBO和元学习，改进了视觉参数调整的效率和个性化，使其更适合普通用户。


<details>
  <summary>Details</summary>
Motivation: 解决在没有显式目标函数的情况下，在大型搜索空间中调整视觉参数（如亮度和对比度）以找到最佳设置的挑战，并克服现有PBO方法需要过多轮偏好比较，不适合日常用户的缺点。

Method: 提出了一种名为Meta-PO的新方法，该方法整合了偏好贝叶斯优化（PBO）和元学习，以提高样本效率。它通过推断先前用户的偏好并将其存储为模型，为新用户智能地推荐设计候选，从而实现更快的收敛和更个性化的结果。

Result: 在2D和3D内容的视觉设计任务的实验评估中，使用Meta-PO的参与者在5.86次迭代内达到了满意的视觉效果（当参与者目标与人群相似时，例如调整为“温暖”的外观），在8次迭代内即使在目标发散的情况下（例如从“复古”、“温暖”到“节日”）也能实现泛化。

Conclusion: Meta-PO通过整合PBO和元学习，提高了样本效率，使得个性化视觉优化在设计任务中更适用于最终用户，并有潜力更广泛地扩展界面个性化。

Abstract: Adjusting visual parameters such as brightness and contrast is common in our
everyday experiences. Finding the optimal parameter setting is challenging due
to the large search space and the lack of an explicit objective function,
leaving users to rely solely on their implicit preferences. Prior work has
explored Preferential Bayesian Optimization (PBO) to address this challenge,
involving users to iteratively select preferred designs from candidate sets.
However, PBO often requires many rounds of preference comparisons, making it
more suitable for designers than everyday end-users. We propose Meta-PO, a
novel method that integrates PBO with meta-learning to improve sample
efficiency. Specifically, Meta-PO infers prior users' preferences and stores
them as models, which are leveraged to intelligently suggest design candidates
for the new users, enabling faster convergence and more personalized results.
An experimental evaluation of our method for appearance design tasks on 2D and
3D content showed that participants achieved satisfactory appearance in 5.86
iterations using Meta-PO when participants shared similar goals with a
population (e.g., tuning for a ``warm'' look) and in 8 iterations even
generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to
``holiday''). Meta-PO makes personalized visual optimization more applicable to
end-users through a generalizable, more efficient optimization conditioned on
preferences, with the potential to scale interface personalization more
broadly.

</details>


### [627] [Designing at 1:1 Scale on Wall-Sized Displays Using Existing UI Design Tools](https://arxiv.org/abs/2507.15433)
*Lou Schwartz,Mohammad Ghoniem,Valérie Maquil,Adrien Coppens,Johannes Hermen*

Main category: cs.HC

TL;DR: 研究评估了在墙面显示器上进行UI设计的可用性，发现1:1设计受欢迎，平板电脑交互最佳，并提出了十二条设计指南，但现有工具仍需改进。


<details>
  <summary>Details</summary>
Motivation: 为解决墙面尺寸显示器在用户界面设计中存在的空间特性难题，探索1:1比例的规模化设计。

Method: 本研究包含两次用户研究和一次技术审查，旨在探索在墙面尺寸显示器上进行大规模设计时，流行的、为桌面优化的原型设计工具的可用性。研究考虑了两种墙面尺寸显示设置和三种不同的交互方法：触摸、带触摸板的键盘以及平板电脑。

Result: 用户研究和技术审查结果表明，1:1比例设计受到用户欢迎；基于平板电脑的交互是最舒适的方式；交互模式的混合使用前景广阔；设计时需注意家具等周边环境因素。此外，研究提出了十二条针对此特定场景的设计指南。

Conclusion: 现有用户界面设计工具尚不支持在墙面尺寸显示器上进行设计，仍需在用户界面元素的放置和附加功能的提供方面进行进一步考虑。

Abstract: Wall-Sized Displays have spatial characteristics that are difficult to
address during user interface design. The design at scale 1:1 could be part of
the solution. In this paper, we present the results of two user studies and one
technology review, exploring the usability of popular, desktop-optimized
prototyping tools, for designing at scale on Wall-Sized Displays. We considered
two wall-sized display setups, and three different interaction methods: touch,
a keyboard equipped with a touchpad, and a tablet. We observed that designing
at scale 1:1 was appreciated. Tablet-based interaction proved to be the most
comfortable interaction method, and a mix of interaction modalities is
promising. In addition, care must be given to the surrounding environment, such
as furniture. We propose twelve design guidelines for a design tool dedicated
to this specific context. Overall, existing user interface design tools do not
yet fully support design on and for wall-sized displays and require further
considerations in terms of placement of user interface elements and the
provision of additional features.

</details>


### [628] [Evaluating Joint Attention for Mixed-Presence Collaboration on Wall-Sized Displays](https://arxiv.org/abs/2507.15443)
*Adrien Coppens,Valérie Maquil*

Main category: cs.HC

TL;DR: 该研究提出了一种新的方法来衡量大型显示器混合协作中的用户注意力。


<details>
  <summary>Details</summary>
Motivation: 为了理解和量化在大型显示器上进行混合协作的质量，需要稳健且不显眼的评估方法。

Method: 提出了一种基于头部凝视数据测量联合注意力的方法。

Result: 在混合协作用户研究中实现了该方法，并初步展示了从一个特定会话中获得的数据和见解。

Conclusion: 分析了在大型显示器上进行混合协作的联合注意力测量方法。

Abstract: To understand and quantify the quality of mixed-presence collaboration around
wall-sized displays, robust evaluation methodologies are needed, that are
adapted for a room-sized experience and are not perceived as obtrusive. In this
paper, we propose our approach for measuring joint attention based on head gaze
data. We describe how it has been implemented for a user study on mixed
presence collaboration with two wall-sized displays and report on the insights
we gained so far from its implementation, with a preliminary focus on the data
coming from one particular session.

</details>


### [629] [Challenging Disability and Interaction Norms in XR: Cooling Down the Empathy Machine in Waiting for Hands](https://arxiv.org/abs/2507.15481)
*Yesica Duarte,Puneet Jain*

Main category: cs.HC

TL;DR: 该XR艺术装置通过反思VR的“共情机器”属性，利用“替代控制器”和荒诞表演，模糊了关于残疾的感伤叙事，并挑战了XR技术的奇观化倾向。


<details>
  <summary>Details</summary>
Motivation: 反思虚拟现实（VR）常被描述为“终极共情机器”，以及这种描述可能将残疾简化为一种值得同情的奇观的倾向。作者旨在探索一种更具批判性的方法，以应对XR技术中存在的潜在问题，并提出一种能够引起对非规范化身体体验的关注，同时抵制奇观化的XR表演伦理。

Method: 通过创作“替代控制器”（Alternative Controllers）来重塑XR中的交互规范，并利用这些控制器进行一场荒诞的XR表演。表演涉及两名XR参与者和六名观众，共同观看一部关于患有关节炎的印度歌手Hema Kumari的纪录片。XR参与者的动作（如奇怪的嘴部和手部动作）会部分遮挡影片，从而吸引观众的注意力，并打断观众对Hema故事的直接共情，制造一种分层体验。

Result: 该XR艺术装置成功地通过非传统的交互方式和表演性手法，颠覆了XR作为一种完全沉浸式、以感官为主导的媒介的传统认知，转而利用XR制造荒诞感和疏离感。它通过打乱观众的观看体验，成功地对残疾的奇观化和感伤化叙事提出了挑战。

Conclusion: 该XR艺术装置通过改编XR中的交互规范和荒诞的XR表演，对虚拟现实（VR）作为“终极共情机器”并可能将残疾简化为怜悯或激励的奇观提出了质疑。通过使用替代控制器和模糊纪录片影像，该装置旨在打乱观众对残疾叙事的直接接触，引入不确定性，并挑战以共情和怜悯驱动的残疾叙事，探讨XR表演如何能够关注非规范化的身体体验，同时抵制奇观化。

Abstract: Virtual Reality (VR) is often described as the "ultimate empathy machine,"
framing disability as an experience to be simulated through such technologies,
which can reduce disability to a spectacle of pity or inspiration. In response,
we present Waiting for Hands (WfH), an interactive eXtended Reality (XR)
installation that critiques this logic by: (1) repurposing interaction norms in
XR through the creation of Alternative Controllers, and (2) staging an absurd
XR performance using the built controllers to disrupt sentimentalized
disability narratives. The performance involves eight people: two XR
participants on stage and six audience members watching a projected documentary
about Hema Kumari, an Indian singer living with Rheumatoid Arthritis. The XR
users partially obscure the film, drawing attention through strange mouth and
hand movements performed in XR. This creates a layered experience that disrupts
direct engagement with Hema's story and introduces uncertainty. While XR is
often seen as a fully immersive, sensory-dominant medium, this piece subverts
that framing by using XR to produce absurdity and alienation. By challenging
empathy-driven and pitiable narratives of disability, we ask what ethical
stance an XR performance can take to attune participants to non-normative
embodiment while resisting spectacle.

</details>


### [630] [FollowUpBot: An LLM-Based Conversational Robot for Automatic Postoperative Follow-up](https://arxiv.org/abs/2507.15502)
*Chen Chen,Jianing Yin,Jiannong Cao,Zhiyuan Wen,Mingjin Zhang,Weixun Gao,Xiang Wang,Haihua Shu*

Main category: cs.HC

TL;DR: 本研究提出了FollowUpBot，一个由LLM驱动、部署在边缘的机器人，用于术后护理和监测。该机器人能够与患者进行适应性、面对面交流，确保数据隐私，并自动生成结构化的随访报告。实验结果表明，FollowUpBot实现了高覆盖率、高满意度和高报告生成准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的术后随访方法耗时且劳动密集。现有的数字解决方案（如网络问卷和智能自动呼叫）要么提供僵化的脚本化交互，要么面临隐私信息泄露问题。为了解决这些限制，本研究提出了FollowUpBot。

Method: FollowUpBot是一个由LLM驱动、部署在边缘的机器人，用于术后护理和监测。它能够动态规划最优路线，并利用部署在边缘的LLM通过多种互动模式与患者进行适应性、面对面交流，确保数据隐私。此外，FollowUpBot还通过分析随访过程中与患者的互动情况，能够为医疗机构自动生成结构化的随访报告。

Result: 实验结果表明，该机器人实现了高覆盖率和高满意度的随访互动，并且在各种现场类型中均实现了高报告生成准确性。

Conclusion: FollowUpBot 在随访互动中实现了高覆盖率和高满意度，并且在各种现场类型中实现了高报告生成准确性。

Abstract: Postoperative follow-up plays a crucial role in monitoring recovery and
identifying complications. However, traditional approaches, typically involving
bedside interviews and manual documentation, are time-consuming and
labor-intensive. Although existing digital solutions, such as web
questionnaires and intelligent automated calls, can alleviate the workload of
nurses to a certain extent, they either deliver an inflexible scripted
interaction or face private information leakage issues. To address these
limitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed
robot for postoperative care and monitoring. It allows dynamic planning of
optimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face
conversations with patients through multiple interaction modes, ensuring data
privacy. Moreover, FollowUpBot is capable of automatically generating
structured postoperative follow-up reports for healthcare institutions by
analyzing patient interactions during follow-up. Experimental results
demonstrate that our robot achieves high coverage and satisfaction in follow-up
interactions, as well as high report generation accuracy across diverse field
types. The demonstration video is available at
https://www.youtube.com/watch?v=_uFgDO7NoK0.

</details>


### [631] [Strategies to Manage Human Factors in Mixed Reality Pilot Training: A Survey](https://arxiv.org/abs/2507.15526)
*Antonio Perez,Avinash Singh,Jonathan Mitchell,Philip Swadling*

Main category: cs.HC

TL;DR: 混合现实 (MR) 具有增强飞行员培训的潜力，但必须解决网络疾病、视觉疲劳和人体工程学压力等人为因素。本综述回顾了这些挑战，并根据航空法规评估了缓解策略，以平衡技术要求和飞行员福祉，从而为 MR 培训的实际应用提供信息。


<details>
  <summary>Details</summary>
Motivation: 混合现实 (MR) 头戴式显示器 (HMD) 为传统的飞行模拟培训设备 (FSTD) 显示器提供了一种有前景的替代方案，具有沉浸感、真实感和成本效益。然而，这些技术需要管理人为因素，如网络疾病、视觉疲劳和人体工程学压力。如果不加以缓解，这些影响会阻碍飞行员的表现和培训成果。对于像航空这样安全关键的领域，解决人为因素挑战对于 MR 的培训潜力至关重要。

Method: 本综述系统地回顾了当前文献，识别了飞行员培训中 MR HMD 使用的关键人为因素挑战，并考察了缓解这些障碍的策略。我们借鉴了领先航空管理机构制定的现有行业标准，并采取监管视角来探讨硬件、软件、人体工程学、生理和心理干预措施，以提高飞行员在 MR FSTD 中的舒适度、安全性和培训效率。此外，我们还评估了哪些干预措施在现有的航空培训法规下最合适和最可行，以确保技术要求和飞行员福祉之间的平衡。

Result: 研究结果对航空模拟培训的人为因素维度产生了重要见解，强调了监管考虑如何影响缓解措施的实用性。这些见解为新兴的 MR 航空培训指南和最佳实践提供了信息，支持 MR 为加强航空培训做好准备。

Conclusion: 本综述系统地回顾了文献，识别了飞行员培训中 MR HMD 使用的关键人为因素挑战，并研究了缓解这些障碍的策略。根据领先的航空管理机构制定的现有行业标准，本综述采取监管视角，探讨了硬件、软件、人体工程学、生理和心理干预措施，以提高 MR FSTD 中飞行员的舒适度、安全性和培训效率。此外，还评估了哪些干预措施在现有的航空培训法规下最适合和最可行，以确保技术要求和飞行员福祉保持平衡。研究结果对航空模拟培训的人为因素维度产生了重要见解，强调了监管考虑如何影响缓解措施的实用性。这些见解为新兴的 MR 航空培训指南和最佳实践提供了信息，支持 MR 为加强航空培训做好准备。

Abstract: Mixed Reality (MR) head mounted displays (HMDs) offer a promising alternative
to traditional Flight Simulator Training Device (FSTD) displays, providing
immersion, realism and cost efficiency. However, these technologies require
management of human factors; cybersickness, visual fatigue and ergonomic
strain. If left unmitigated, these effects can hinder pilot performance and
training outcomes. For safety critical fields like aviation, addressing human
factors challenges is crucial for MR's training potential. This survey
systematically reviews the current literature identifying key human factors
challenges in MR HMD use in pilot training and examines strategies to mitigate
these barriers. Drawing on existing industry standards set by a leading
aviation authority, the review adopts a regulatory perspective to explore
hardware, software, ergonomic, physiological and psychological interventions
improving pilot comfort, safety and training effectiveness in an MR FSTD.
Additionally, it evaluates which of these interventions are most appropriate
and viable for MR pilot training under existing aviation training regulations,
ensuring that technical requirements and pilot wellbeing remain balanced. The
findings yield significant insights for the human dimensions of aviation
simulation training, highlighting how regulatory considerations shape the
practicality of mitigation measures. These insights inform emerging MR aviation
training guidelines and best practices, supporting MR's readiness to enhance
aviation training.

</details>


### [632] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)
*Mohammad 'Matt' Namvarpour,Brandon Brofsky,Jessica Medina,Mamtaj Akter,Afsaneh Razi*

Main category: cs.HC

TL;DR: 青少年过度依赖聊天机器人，影响现实生活。反思、线下社交或平台限制可帮助脱离。建议改进设计以促进自我意识和现实参与。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI聊天机器人（如Character.AI）在青少年生活中日益普及，人们对其可能引发的情感依赖和数字过度依赖表示担忧。本研究旨在探讨青少年与可定制角色聊天机器人的互动模式，以了解其过度依赖的现象。

Method: 通过分析在Character.AI的Reddit子版块中，自我报告年龄在13至17岁的用户发布的318篇帖子，研究者们了解了青少年过度依赖聊天机器人的模式。

Result: 研究发现，青少年通常出于情感支持或创意表达的目的开始使用聊天机器人，但许多人随后对其产生了强烈的依恋，从而干扰了现实生活中的人际关系和日常活动。他们的帖子显示出常见的心理困扰迹象、复发循环以及难以脱离的现象。青少年报告称，当他们反思这种过度依赖的危害、回归线下的社交环境或因平台限制而感到沮丧时，他们的过度依赖往往会结束。

Conclusion: 这项研究发现，青少年在与可定制角色的聊天机器人（如Character.AI）的互动中，可能会出现过度依赖，这种过度依赖会干扰现实生活中的人际关系和日常活动。研究还指出了青少年在使用此类机器人时可能出现的心理困扰、复发循环和难以脱离等问题。

Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like
Character.AI become embedded in adolescent life, they raise concerns about
emotional dependence and digital overreliance. While studies have investigated
the overreliance of adults on these chatbots, they have not investigated teens'
interactions with chatbots with customizable personas. We analyzed 318 Reddit
posts made by users self-reported as 13-17 years old on the Character.AI
subreddit to understand patterns of overreliance. We found teens commonly begin
using chatbots for emotional support or creative expression, but many develop
strong attachments that interfere with offline relationships and daily
routines. Their posts revealed recurring signs of psychological distress,
cycles of relapse, and difficulty disengaging. Teens reported that their
overreliance often ended when they reflect on the harm, return to in-person
social settings, or become frustrated by platform restrictions. Based on the
implications of our findings, we provide recommendations for future chatbot
design so they can promote self-awareness, support real-world engagement, and
involve teens in developing safer digital tools.

</details>


### [633] [FlowForge: Guiding the Creation of Multi-agent Workflows with Design Space Visualization as a Thinking Scaffold](https://arxiv.org/abs/2507.15559)
*Pan Hao,Dongyeop Kang,Nicholas Hinds,Qianwen Wang*

Main category: cs.HC

TL;DR: FLOWFORGE是一个交互式可视化工具，通过结构化可视化探索和基于设计模式的原位指导，简化了多主体工作流的设计过程，并提供了有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 当前多主体工作流的设计方法依赖于从业者的直觉和专业知识，容易导致设计固定或非结构化的、耗时的试错探索。为了解决这些挑战，有必要开发一个工具来简化和优化这一过程。

Method: FLOWFORGE是一个交互式可视化工具，通过i)结构化的设计空间可视化探索和ii)基于既定设计模式的原位指导，来促进多主体工作流的创建。FLOWFORGE将工作流设计过程分为三个层次（任务规划、主体分配和主体优化），并提供上下文感知的、原位的建议。

Result: FLOWFORGE能够帮助用户无缝地从高级规划转向详细设计决策和实现，同时可以跨多个性能指标比较替代解决方案。

Conclusion: FLOWFORGE的可用性和有效性得到了用例和用户研究的证明，并为从业者如何在工作流程开发期间探索设计空间和利用指导提供了宝贵的见解。

Abstract: Multi-agent workflows have become an effective strategy for tackling
complicated tasks by decomposing them into multiple sub-tasks and assigning
them to specialized agents. However, designing optimal workflows remains
challenging due to the vast and intricate design space. Current practices rely
heavily on the intuition and expertise of practitioners, often resulting in
design fixation or an unstructured, time-consuming exploration of
trial-and-error. To address these challenges, this work introduces FLOWFORGE,
an interactive visualization tool to facilitate the creation of multi-agent
workflow through i) a structured visual exploration of the design space and ii)
in-situ guidance informed by established design patterns. Based on formative
studies and literature review, FLOWFORGE organizes the workflow design process
into three hierarchical levels (i.e., task planning, agent assignment, and
agent optimization), ranging from abstract to concrete. This structured visual
exploration enables users to seamlessly move from high-level planning to
detailed design decisions and implementations, while comparing alternative
solutions across multiple performance metrics. Additionally, drawing from
established workflow design patterns, FLOWFORGE provides context-aware, in-situ
suggestions at each level as users navigate the design space, enhancing the
workflow creation process with practical guidance. Use cases and user studies
demonstrate the usability and effectiveness of FLOWFORGE, while also yielding
valuable insights into how practitioners explore design spaces and leverage
guidance during workflow development.

</details>


### [634] [Chapter 11 Students' interaction with and appreciation of automated informative tutoring feedback](https://arxiv.org/abs/2507.15650)
*Gerben van der Hoek,Bastiaan Heeren,Rogier Bos,Paul Drijvers,Johan Jeuring*

Main category: cs.HC

TL;DR: This study explored a balanced feedback strategy in computer-aided assessment, finding it led to better student engagement and interaction by allowing exploration while preventing frustration.


<details>
  <summary>Details</summary>
Motivation: To explore an informative feedback strategy that balances room for exploration with the mitigation of learning barriers, and to understand how students interact with this strategy, specifically their preference for error-specific feedback versus worked-out solutions.

Method: A small-scale qualitative study involving twenty-five 15-to-17-year-old students who completed linear and exponential extrapolation tasks online for approximately 20 minutes. Data collected included screen captures and post-intervention interviews.

Result: The study showed that providing room for exploration facilitated self-guidance, while mitigating learning barriers helped prevent student disengagement. Students generally appreciated the balanced feedback approach.

Conclusion: The balanced feedback strategy resulted in productive student-environment interactions.

Abstract: Computer aided formative assessment can be used to enhance a learning
process, for instance by providing feedback. There are many design choices for
delivering feedback, that lead to a feedback strategy. In an informative
feedback strategy, students do not immediately receive information about the
correct response, but are offered the opportunity to retry a task to apply
feedback information. In this small-scale qualitative study, we explore an
informative feedback strategy designed to offer a balance between room for
exploration and mitigation of learning barriers. The research questions concern
the ways in which students interact with the feedback strategy and their
appreciation of error-specific feedback as opposed to worked-out solutions. To
answer these questions, twenty-five 15-to-17-year-old senior general secondary
education students worked for approximately 20 minutes on linear and
exponential extrapolation tasks in an online environment. Data included screen
captures of students working with the environment and post-intervention
interviews. Results showed that room for exploration offered opportunities for
self-guidance while mitigation of learning barriers prevented disengagement.
Furthermore, students appreciated balanced feedback. We conclude that the
balanced feedback strategy yielded fruitful student-environment interactions.

</details>


### [635] [Surfacing Variations to Calibrate Perceived Reliability of MLLM-generated Image Descriptions](https://arxiv.org/abs/2507.15692)
*Meng Chen,Akhil Iyer,Amy Pavel*

Main category: cs.HC

TL;DR: 本研究提出了一种通过展示MLLM响应的变体来帮助盲人和低视力用户检测不可靠信息的方法，研究表明该方法能显著提高检测能力并降低对模型响应的感知可靠性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）为盲人和低视力（BLV）人士提供了访问日常视觉信息的新途径，但其产生的错误难以被看见，这带来了安全和社会风险。现有的解决方法（如交叉检查工具和咨询他人）耗时且不切实际。因此，有必要探索一种新的方法来帮助BLV用户在不依赖视觉的情况下检测不可靠信息。

Method: 本研究通过设计一个用于引发和展示MLLM描述中变体的空间，并构建了一个实现三种变体展示风格的原型系统，来探讨如何支持BLV用户检测不可靠信息。通过包含15名BLV参与者的用户研究，评估了展示变体对用户检测不可靠信息能力的影响。

Result: 结果表明，与单一描述相比，展示变体能显著提高用户检测不可靠信息的能力（提高了4.9倍），并显著降低用户对MLLM响应的感知可靠性。14/15的参与者表示更喜欢查看变体，并对使用该系统表示出兴趣。

Conclusion: 多模态大语言模型（MLLMs）为盲人和低视力（BLV）人士提供了访问日常视觉信息的新途径。然而，这些模型常常会产生难以检测的错误，这在从药物识别到服装搭配等场景中带来了安全和社会风险。虽然BLV MLLM用户采用交叉检查工具和咨询视力健全者等创造性方法，但这些方法通常耗时且不切实际。本研究探讨了如何通过系统地展示多个MLLM响应中的变体来支持BLV用户在不进行视觉检查的情况下检测不可靠信息。我们设计了一个用于引发和展示MLLM描述中变体的空间，一个实现三种变体展示风格的原型系统，并进行了包含15名BLV参与者的用户研究。结果表明，与单一描述相比，展示变体能显著提高用户检测不可靠信息的能力（提高了4.9倍），并显著降低用户对MLLM响应的感知可靠性。15名参与者中有14名表示，与单一描述相比，他们更喜欢查看MLLM响应的变体，并且所有人都表示有兴趣使用我们的系统来完成从理解龙卷风路径到在社交媒体上发布图片等任务。

Abstract: Multimodal large language models (MLLMs) provide new opportunities for blind
and low vision (BLV) people to access visual information in their daily lives.
However, these models often produce errors that are difficult to detect without
sight, posing safety and social risks in scenarios from medication
identification to outfit selection. While BLV MLLM users use creative
workarounds such as cross-checking between tools and consulting sighted
individuals, these approaches are often time-consuming and impractical. We
explore how systematically surfacing variations across multiple MLLM responses
can support BLV users to detect unreliable information without visually
inspecting the image. We contribute a design space for eliciting and presenting
variations in MLLM descriptions, a prototype system implementing three
variation presentation styles, and findings from a user study with 15 BLV
participants. Our results demonstrate that presenting variations significantly
increases users' ability to identify unreliable claims (by 4.9x using our
approach compared to single descriptions) and significantly decreases perceived
reliability of MLLM responses. 14 of 15 participants preferred seeing
variations of MLLM responses over a single description, and all expressed
interest in using our system for tasks from understanding a tornado's path to
posting an image on social media.

</details>


### [636] [JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks](https://arxiv.org/abs/2505.17593)
*Manuel Valle Torre,Thom van der Velden,Marcus Specht,Catharine Oertel*

Main category: cs.HC

TL;DR: JELAI是一个开源平台架构，将学习分析与大型语言模型辅导集成到Jupyter Notebook中，以支持教育和研究。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育支持方面潜力巨大，但往往缺乏教学依据和对学生学习环境的认识，同时在真实学习环境中研究学生与这些工具的互动仍然具有挑战性。

Method: JELAI采用模块化、容器化设计，通过JupyterLab扩展实现遥测和聊天，并通过中央中间件处理学习分析和上下文感知的LLM提示。

Result: JELAI实现了细粒度的学习分析与大型语言模型辅导的集成，能够捕获代码交互和聊天数据，从而实现实时、上下文敏感的AI脚手架，并支持对学生行为的研究。通过系统性能基准测试和两个概念验证用例，证明了其可行性。

Conclusion: JELAI提供了一个技术框架，使研究人员和教育工作者能够轻松地开发、部署和研究Jupyter生态系统中基于学习分析的AI辅导。

Abstract: Generative AI offers potential for educational support, but often lacks
pedagogical grounding and awareness of the student's learning context.
Furthermore, researching student interactions with these tools within authentic
learning environments remains challenging. To address this, we present JELAI,
an open-source platform architecture designed to integrate fine-grained
Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly
within a Jupyter Notebook environment. JELAI employs a modular, containerized
design featuring JupyterLab extensions for telemetry and chat, alongside a
central middleware handling LA processing and context-aware LLM prompt
enrichment. This architecture enables the capture of integrated code
interaction and chat data, facilitating real-time, context-sensitive AI
scaffolding and research into student behaviour. We describe the system's
design, implementation, and demonstrate its feasibility through system
performance benchmarks and two proof-of-concept use cases illustrating its
capabilities for logging multi-modal data, analysing help-seeking patterns, and
supporting A/B testing of AI configurations. JELAI's primary contribution is
its technical framework, providing a flexible tool for researchers and
educators to develop, deploy, and study LA-informed AI tutoring within the
widely used Jupyter ecosystem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [637] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 该研究提出了一种创新的主动边缘流处理自动缩放方法，利用 GRU 预测负载，通过迁移学习处理域差异，并结合水平自动缩放来优化资源利用率，在预测准确性和效率方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 边缘流处理面临工作负载快速波动带来的资源配置挑战，现有的反应式方法（如基于阈值和排队论）在性能下降后才进行扩展，可能违反服务水平协议 (SLA)。强化学习 (RL) 虽能主动适应，但需要大量模拟；预测机器学习模型则会面临在线分布和概念漂移问题，降低准确性。因此，需要一种主动的解决方案来解决边缘流处理的自动缩放问题。

Method: 该研究提出了一种三步解决方案：1. 使用 GRU 神经网络预测上游负载；2. 利用迁移学习框架（DTW 算法和联合分布适配）将预测模型集成到在线流处理系统；3. 基于预测负载和边缘资源约束，通过水平自动缩放模块动态调整算子并行度。

Result: GRU 模型在真实数据集上的 SMAPE 值最高达到 1.3%，在 SMAPE 和 RMSE 评估指标上优于 CNN、ARIMA 和 Prophet 模型，并且训练时间比计算密集型的 RL 模型更短。

Conclusion: 该研究提出了一种用于主动边缘流处理自动缩放的三步解决方案，通过 GRU 神经网络预测上游负载，并结合迁移学习框架和水平自动缩放模块来动态调整算子并行度，以应对工作负载波动和资源限制。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [638] [Characterizing Communication Patterns in Distributed Large Language Model Inference](https://arxiv.org/abs/2507.14392)
*Lang Xu,Kaushik Kandadi Suresh,Quentin Anthony,Nawras Alnaasan,Dhabaleswar K. Panda*

Main category: cs.DC

TL;DR: 该研究分析了分布式LLM服务的通信动态，比较了不同并行化策略（张量并行、流水线并行和混合方法）对性能的影响，并为生产环境中的LLM服务提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 分布式LLM服务的互GPU通信产生了显著的性能限制，影响了实际系统中服务的质量。

Method: 该工作结合了详细的性能剖析测量和预测性分析模型，以表征不同并行化配置下的通信行为。

Result: 结果表明，张量并行会产生显著的网络开销，但能为短序列提供更优的响应时间；流水线并行则能最大限度地减少数据传输需求，但会增加总延迟；混合方法则需要仔细调整以实现均衡的性能。

Conclusion: 所提出的洞察为在生产LLM服务中选择合适的并行化方案提供了实用的建议，并指出了优化推理框架和通信基础设施的关键机会。

Abstract: Large Language Models (LLMs) built on transformer architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU communication creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates communication dynamics in distributed LLM
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense transformer-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize communication behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production LLM services
and identify key opportunities for optimizing inference frameworks and
communication infrastructure.

</details>


### [639] [Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring](https://arxiv.org/abs/2507.14723)
*Brati Mondal,Pritam Goswami,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 该论文研究了在无对手性假设下，移动 agents 在环形网络中的距离-k-分散问题。作者提出了一种模拟对手性的方法，并证明了该问题在任何环大小下都可解，还给出了一个 O(ln) 回合的算法。


<details>
  <summary>Details</summary>
Motivation: 该研究的目标是解决同步移动 agents 在 1 间隔连通的环形网络中的距离-k-分散（D-k-D）问题，其中 agents 的数量为 l，并且不假定对手性。这推广了经典的分散问题，要求 agents 之间的最小距离为 k 跳，其中 k=1 对应于标准分散。

Method: 作者提出了一种新颖的方法，使 agents 能够仅使用局部信息、视觉和有限内存来模拟对手性，证明了对手性并非该模型中协调的基本要求。

Result: 作者提出了一种新颖的模拟对手性的方法，并证明了在无对手性假设下 D-k-D 问题可解，将先前仅限于奇数环或大小为 4 的环的限制扩展到任何大小的环。此外，还提出了一个能在 O(ln) 回合内完成的 D-k-D 算法。

Conclusion: 该研究证明了在没有对手性（agents 的方向感）假设的情况下，可以在 1 间隔连通的环形网络中解决距离-k-分散（D-k-D）问题，并且 D-k-D 问题（包括分散问题）对于任何大小的环形网络都可以从任何任意配置解决，之前该问题仅限于奇数环或大小为 4 的环。最终，作者提出了一个在该设定下工作时间为 O(ln) 回合的 D-k-D 算法。

Abstract: We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile
agents in a 1-interval-connected ring network having $n$ nodes and with $l$
agents where $3 \le l \le \lfloor \frac{n}{k}\rfloor$, without the assumption
of chirality (a common sense of direction for the agents). This generalizes the
classical dispersion problem by requiring that agents maintain a minimum
distance of $k$ hops from each other, with the special case $k=1$ corresponding
to the standard dispersion.
  The contribution in this work is threefold. Our first contribution is a novel
method that enables agents to simulate chirality using only local information,
vision and bounded memory. This technique demonstrates that chirality is not a
fundamental requirement for coordination in this model.
  Building on this, our second contribution partially resolves an open question
posed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-
interval connected ring, synchronous agents, no chirality). We prove that
D-$k$-D, and thus dispersion is solvable from any arbitrary configuration under
these assumptions (excluding vertex permutation dynamism)for any size of the
ring network which was earlier limited to only odd sized ring or to a ring of
size four.
  Finally, we present an algorithm for D-$k$-D in this setting that works in
$O(ln)$ rounds, completing the constructive side of our result.
  Altogether, our findings significantly extend the theoretical understanding
of mobile agent coordination in dynamic networks and clarify the role of
chirality in distributed computation.

</details>


### [640] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: ACME是一种通过分布式系统实现Transformer大模型定制的方法，解决了成本、隐私和性能问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决在云环境中部署大型模型所面临的数据隐私和响应延迟挑战，以及在定制模型时遇到的中心化成本高、用户异构性导致性能不平衡和数据异构性导致性能不佳等问题。

Method: ACME采用双向单循环分布式系统，通过定制骨干网生成和识别帕累托前沿，然后进行头部生成并基于数据分布进行个性化架构聚合，以实现细粒度的协作模型定制。

Result: ACME在模型大小约束下实现了成本效益，数据传输量减少到6%，平均准确率提高了10%，权衡指标提高了近30%。

Conclusion: ACME通过分布式系统实现了Transformer大模型的自适应定制，在模型大小约束下实现了成本效益，将数据传输量减少到6%，并将平均准确率提高了10%，同时将权衡指标提高了近30%。

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [641] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: DecentLLMs是一种去中心化的LLM多代理共识方法，通过并行生成答案和鲁棒聚合来提高效率和答案质量，克服了传统领导者驱动方法的缺点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多代理系统依赖领导者驱动的协调，易受针对领导者的攻击，且可能接受次优方案。

Method: 提出了一种名为DecentLLMs的新型去中心化共识方法，采用并行工作代理生成答案，并通过评估代理独立评分和排序来选择最佳答案。

Result: DecentLLMs能够有效容忍拜占庭代理，并显著提高了所选答案的质量。

Conclusion: DecentLLMs有效容忍拜占庭代理，并能通过拜占庭鲁棒聚合技术选择更高质量的答案，实验结果证明了其有效性。

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


### [642] [AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs](https://arxiv.org/abs/2507.15121)
*Sasindu Wijeratne,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.DC

TL;DR: AMPED是一种多GPU并行算法，用于加速万亿级稀疏张量的MTTKRP计算，相比现有方法有显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的MTTKRP计算方法在处理万亿级稀疏张量时面临内存和计算瓶颈，AMPED旨在解决这些问题。

Method: AMPED算法结合了分区策略和动态负载均衡方案，以在多GPU上并行加速MTTKRP计算。

Result: AMPED在真实的万亿级稀疏张量上实现了5.1倍的几何平均加速比。

Conclusion: AMPED算法在处理万亿级稀疏张量时，通过多GPU并行处理，实现了比现有GPU基线高5.1倍的几何平均加速比，满足了内存和性能需求。

Abstract: Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational
bottleneck in sparse tensor decomposition. As real-world sparse tensors grow to
billions of nonzeros, they increasingly demand higher memory capacity and
compute throughput from hardware accelerators. In this work, we present AMPED,
a multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale
sparse tensors. AMPED scales beyond the limits of a single GPU, meeting both
the memory and performance requirements of large-scale workloads. We introduce
a partitioning strategy combined with a dynamic load balancing scheme to
distribute computation and minimize GPU idle time. On real-world billion-scale
tensors, AMPED achieves a 5.1x geometric mean speedup in total execution time
over state-of-the-art GPU baselines using 4 GPUs on a single CPU node.

</details>


### [643] [Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement](https://arxiv.org/abs/2507.15154)
*Kohya Shiozaki,Junya Nakamura*

Main category: cs.DC

TL;DR: Dynatune通过动态调整Raft选举参数，有效减少了服务中断时间，提高了系统在不稳定网络下的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统Raft算法在应对网络条件波动时，调整选举参数的效果不佳，导致服务中断时间增加和响应能力下降。

Method: 提出了一种名为Dynatune的机制，该机制基于往返时间和丢包率等网络指标，动态调整Raft的选举参数（如心跳间隔和选举超时）。

Result: 实验结果表明，与传统Raft相比，Dynatune将领导者故障检测时间缩短了80%，不可用时间缩短了45%，即使在动态网络条件下也能保持高可用性。

Conclusion: Dynatune通过动态调整Raft的选举参数，显著减少了领导者故障检测时间和不可用时间，提高了在动态网络条件下的可用性和可靠性。

Abstract: Raft is a leader-based consensus algorithm that implements State Machine
Replication (SMR), which replicates the service state across multiple servers
to enhance fault tolerance. In Raft, the servers play one of three roles:
leader, follower, or candidate. The leader receives client requests, determines
the processing order, and replicates them to the followers. When the leader
fails, the service must elect a new leader to continue processing requests,
during which the service experiences an out-of-service (OTS) time. The OTS time
is directly influenced by election parameters, such as heartbeat interval and
election timeout. However, traditional approaches, such as Raft, often struggle
to effectively tune these parameters, particularly under fluctuating network
conditions, leading to increased OTS time and reduced service responsiveness.
To address this, we propose Dynatune, a mechanism that dynamically adjusts
Raft's election parameters based on network metrics such as round-trip time and
packet loss rates measured via heartbeats. By adapting to changing network
environments, Dynatune significantly reduces the leader failure detection and
OTS time without altering Raft's core mechanisms or introducing additional
communication overheads. Experimental results demonstrate that Dynatune reduces
the leader failure detection and OTS times by 80% and 45%, respectively,
compared with Raft, while maintaining high availability even under dynamic
network conditions. These findings confirm that Dynatune effectively enhances
the performance and reliability of SMR services in various network scenarios.

</details>


### [644] [GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis](https://arxiv.org/abs/2507.15230)
*Guoxi Liu,Thomas Randall,Rong Ge,Federico Iuricich*

Main category: cs.DC

TL;DR: GALE是一种新的GPU辅助数据结构，用于在异构CPU-GPU系统上加速科学数据分析，通过将网格连接性计算移至GPU来提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统的面向CPU的任务并行数据结构在计算和存储连接性信息时会成为瓶颈，限制了性能提升。

Method: 提出了一种新颖的、针对异构CPU-GPU系统优化的任务并行方法，将网格连接性信息的计算放在GPU上，可视化算法放在CPU上。

Result: GALE在双20核CPU和NVIDIA V100 GPU上的实验表明，相比于最先进的局部数据结构，其速度最高可提升2.7倍，同时内存效率保持不变。

Conclusion: GALE通过将网格连接性信息的计算卸载到GPU线程，使CPU线程能够专注于执行可视化算法，实现了高达2.7倍的加速，同时保持了内存效率。

Abstract: Unstructured meshes present challenges in scientific data analysis due to
irregular distribution and complex connectivity. Computing and storing
connectivity information is a major bottleneck for visualization algorithms,
affecting both time and memory performance. Recent task-parallel data
structures address this by precomputing connectivity information at runtime
while the analysis algorithm executes, effectively hiding computation costs and
improving performance. However, existing approaches are CPU-bound, forcing the
data structure and analysis algorithm to compete for the same computational
resources, limiting potential speedups. To overcome this limitation, we
introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU
systems. Specifically, we offload the computation of mesh connectivity
information to GPU threads, enabling CPU threads to focus on executing the
visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided
Localized data structurE), the first open-source CUDA-based data structure
designed for heterogeneous task parallelism. Experiments on two 20-core CPUs
and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over
state-of-the-art localized data structures while maintaining memory efficiency.

</details>


### [645] [An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing](https://arxiv.org/abs/2507.15233)
*Jintao Liu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 本研究提出了一种基于多目标强化学习的参与者选择方法，以解决联邦推荐系统中设备异构、数据非IID和通信瓶颈问题。通过结合历史声誉、数据效用和系统效率，并利用MAB框架进行动态选择，该方法显著提高了训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦推荐系统（FRS）虽然解决了中心化推荐系统（RS）的隐私和可扩展性问题，但在设备能力异构、数据非独立同分布（non-IID）和通信瓶颈方面存在不足。

Method: 提出了一种多目标强化学习（RL）参与者选择方法，该方法结合了历史客户端性能声誉（CPR）、数据效用和系统效率，并通过将此效用嵌入多武装（MAB）框架中来动态平衡探索-利用，以选择参与者。

Result: 所提出的MAB方法在四个不同的偏斜数据分区场景中，将达到目标AUC的时间收敛速度加快了32%-50%，并将总训练时间减少了高达46%，同时与现有的FRS基线相比，在最终的AUC、NDCG@50和Recall@50方面匹配或略有提高。

Conclusion: 本研究证明，基于奖励驱动的自适应客户端采样可以显著提高实际联邦部署的效率和公平性。

Abstract: Recommendation systems (RS) personalize content by analyzing user
preferences, but typically require centralized collection of user data, raising
privacy and scalability concerns. Federated Recommendation Systems (FRS)
address these issues by enabling distributed, privacy-preserving model training
across edge devices, keeping raw data on-device. Although existing FRS
frameworks benefit from on-device feature extraction and privacy preservation,
they suffer from heterogeneous device capabilities, non-independent and
identically distributed (non-IID) data, and communication bottlenecks. To
overcome these limitations, we propose a multi-objective reinforcement learning
(RL) participant selection that jointly optimizes historical client performance
reputation (CPR), data utility, and system efficiency. First, we define a
composite client-utility function combining CPR, system capability, and data
quality. Next, we embed this utility into a multi-armed bandit (MAB) framework
and dynamically balance exploration-exploitation to select participants.
Finally, we practically implement our approach using the PySyft framework on an
edge-cloud testbed, and evaluate it on a multimodal movie-recommendation task
built from the MovieLens-100K dataset. Across four different skewed
data-partition scenarios, our MAB-based selection accelerates convergence by
32-50% in time-to-target AUC and reduces total wall-clock training time by up
to 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50
compared to existing FRS baselines. Our results demonstrate that adaptive,
reward-driven client sampling can substantially enhance both efficiency and
fairness in real-world federated deployments.

</details>


### [646] [Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing](https://arxiv.org/abs/2507.15553)
*Shibo Yu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 针对LLM推理的延迟和成本挑战，提出了一种基于NSGA-II的自适应路由算法，在云边环境中优化请求分发，并在基准测试中取得显著的响应时间和成本改进。


<details>
  <summary>Details</summary>
Motivation: 为了应对LLM推理服务日益增长的需求所带来的计算资源压力、延迟和成本挑战。

Method: 提出了一种基于非支配排序遗传算法II（NSGA-II）的新型路由算法，将LLM推理请求分发到云边计算环境中的异构LLM实例，该算法将问题制定为多目标优化问题，以适应请求异构性和节点多样性。

Result: 与基线方法相比，在响应时间和成本方面分别实现了高达95.2%和34.9%的改进。

Conclusion: 该自适应路由算法通过在云边计算环境中优化LLM实例的请求分发，有效平衡了响应质量、响应时间和推理成本，在动态工作负载下实现了性能优化，并验证了其在可扩展LLM部署中的有效性。

Abstract: The rising demand for Large Language Model (LLM) inference services has
intensified pressure on computational resources, resulting in latency and cost
challenges. This paper introduces a novel routing algorithm based on the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference
requests across heterogeneous LLM instances in a cloud-edge computing
environment. Formulated as a multi-objective optimization problem, the
algorithm balances response quality, response time, and inference cost,
adapting to request heterogeneity (e.g., varying complexity and prompt lengths)
and node diversity (e.g., edge vs. cloud resources). This adaptive routing
algorithm optimizes performance under dynamic workloads. We benchmark the
approach using a testbed with datasets including Stanford Question Answering
Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With
Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).
Experimental results show our solution, compared to the baselines, achieves up
to 95.2% and 34.9% improvements in terms of response time and cost,
respectively. These findings validate the algorithm's effectiveness for
scalable LLM deployments.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [647] [U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model](https://arxiv.org/abs/2507.14237)
*Louis Bahrman,Mathieu Fontaine,Gaël Richard*

Main category: cs.SD

TL;DR: 本文提出了一种创新的去混响模型训练方法，在仅使用混响信号和声学模型的情况下，通过序列学习策略和混响匹配损失，实现了从弱监督到无监督的训练。该方法在低资源场景下表现出色，显著优于现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常需要配对的干湿数据，这在实际中难以获得。本研究旨在探索在从弱监督到完全无监督的监督设置下训练最先进的去混响模型，仅依赖混响信号和用于训练的声学模型。

Method: 本研究提出了一种序列学习策略，该策略源于混响消除问题的贝叶斯公式。利用深度神经网络从混响输入中估计声学参数和干信号，并通过混响匹配损失进行指导。

Result: 所提出的方法在低资源场景下表现出了优越性，其数据效率最高的变体仅需100个带混响参数标签的样本即可超越无监督基线。

Conclusion: 使用仅有100个带混响参数标签的样本，所提出的方法就优于无监督基线，证明了该方法在低资源场景下的有效性和实用性。

Abstract: This paper explores the outcome of training state-ofthe-art dereverberation
models with supervision settings ranging from weakly-supervised to fully
unsupervised, relying solely on reverberant signals and an acoustic model for
training. Most of the existing deep learning approaches typically require
paired dry and reverberant data, which are difficult to obtain in practice. We
develop instead a sequential learning strategy motivated by a bayesian
formulation of the dereverberation problem, wherein acoustic parameters and dry
signals are estimated from reverberant inputs using deep neural networks,
guided by a reverberation matching loss. Our most data-efficient variant
requires only 100 reverberation-parameter-labelled samples to outperform an
unsupervised baseline, demonstrating the effectiveness and practicality of the
proposed method in low-resource scenarios.

</details>


### [648] [The Rest is Silence: Leveraging Unseen Species Models for Computational Musicology](https://arxiv.org/abs/2507.14638)
*Fabian C. Moss,Jan Hajič jr.,Adrian Nachtwey,Laurent Pugin*

Main category: cs.SD

TL;DR: 将生态学中的未见物种模型（USMs）应用于音乐学，以估计不完整数据集的大小和范围。


<details>
  <summary>Details</summary>
Motivation: 解决音乐学研究中数据集不完整的问题，估计集合的大小和覆盖范围。

Method: 介绍USMs模型形式化，并通过四个案例研究展示其在音乐学数据中的应用。

Result: USMs模型可以应用于音乐学数据，以回答有关未发现的作曲家数量、已编目的中世纪格里高利圣歌来源的百分比、不同版本音乐印刷品之间的预期差异数量、民间音乐传统歌曲的覆盖范围以及估计作曲家和声词汇量等量化问题。  

Conclusion: 该研究首次将生态学中的“未见物种模型”（USMs）应用于音乐学领域，以解决有关音乐学数据集的量化问题。

Abstract: For many decades, musicologists have engaged in creating large databases
serving different purposes for musicological research and scholarship. With the
rise of fields like music information retrieval and digital musicology, there
is now a constant and growing influx of musicologically relevant datasets and
corpora. In historical or observational settings, however, these datasets are
necessarily incomplete, and the true extent of a collection of interest remains
unknown -- silent. Here, we apply, for the first time, so-called Unseen Species
models (USMs) from ecology to areas of musicological activity. After
introducing the models formally, we show in four case studies how USMs can be
applied to musicological data to address quantitative questions like: How many
composers are we missing in RISM? What percentage of medieval sources of
Gregorian chant have we already cataloged? How many differences in music prints
do we expect to find between editions? How large is the coverage of songs from
genres of a folk music tradition? And, finally, how close are we in estimating
the size of the harmonic vocabulary of a large number of composers?

</details>


### [649] [Multi-Sampling-Frequency Naturalness MOS Prediction Using Self-Supervised Learning Model with Sampling-Frequency-Independent Layer](https://arxiv.org/abs/2507.14647)
*Go Nishikawa,Wataru Nakata,Yuki Saito,Kanami Imamura,Hiroshi Saruwatari,Tomohiko Nakamura*

Main category: cs.SD

TL;DR: 一种新颖的SFI-SSL模型，用于多采样率语音的MOS预测，在AMC 2025竞赛中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为解决语音多采样率（SF）的平均意见分（MOS）预测问题，并为AMC 2025 Track 3竞赛提供解决方案。

Method: 集成SF不相关（SFI）卷积层到自监督学习（SSL）模型中，并结合知识蒸馏和大规模MOS数据集预训练。

Result: 在AMC 2025 Track 3竞赛中，该模型在一项评估指标上排名第一，最终排名第四。通过消融研究验证了模型各组成部分的重要性。

Conclusion: 该模型通过集成SF不相关（SFI）卷积层到自监督学习（SSL）模型中，实现了用于MOS预测的SFI语音特征提取。通过知识蒸馏和大规模MOS数据集预训练等策略，模型在AMC 2025 Track 3竞赛中取得优异成绩，在一项评估指标上排名第一，最终排名第四。

Abstract: We introduce our submission to the AudioMOS Challenge (AMC) 2025 Track 3:
mean opinion score (MOS) prediction for speech with multiple sampling
frequencies (SFs). Our submitted model integrates an SF-independent (SFI)
convolutional layer into a self-supervised learning (SSL) model to achieve SFI
speech feature extraction for MOS prediction. We present some strategies to
improve the MOS prediction performance of our model: distilling knowledge from
a pretrained non-SFI-SSL model and pretraining with a large-scale MOS dataset.
Our submission to the AMC 2025 Track 3 ranked the first in one evaluation
metric and the fourth in the final ranking. We also report the results of our
ablation study to investigate essential factors of our model.

</details>


### [650] [Frame-level Temporal Difference Learning for Partial Deepfake Speech Detection](https://arxiv.org/abs/2507.15101)
*Menglu Li,Xiao-Ping Zhang,Lian Zhao*

Main category: cs.SD

TL;DR: 该研究提出了一种新的时间差注意模块（TDAM），用于检测部分假冒语音。与依赖帧级注释和过渡伪影检测的现有方法不同，该模型通过分析帧级时间差来识别不自然的 temporal variations，无需边界注释。TDAM-AvgPool模型在两个基准数据集上均取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的部分假冒语音检测方法依赖于昂贵的帧级注释进行训练，限制了实际应用的可扩展性。此外，它们侧重于检测真实语音和假冒语音片段之间的过渡伪影，但随着假冒语音生成技术的进步，这些过渡变得越来越平滑，使得检测更具挑战性。

Method: 提出了一种新的角度，分析帧级时间差，并提出了一种时间差注意模块（TDAM），将部分假冒语音检测重新定义为识别不自然的 temporal variations，而无需依赖显式的边界注释。TDAM-AvgPool模型采用双层级联差表示来捕捉细粒度和粗粒度的时间不规则性，并通过自适应平均池化来保留跨可变长度输入的关键模式，以最小化信息损失。

Result: 我们的TDAM-AvgPool模型在PartialSpoof数据集上达到了0.59%的EER，在HAD数据集上达到了0.03%的EER，显著优于现有方法。

Conclusion: 我们的TDAM-AvgPool模型在PartialSpoof数据集上实现了0.59%的EER，在HAD数据集上实现了0.03%的EER，显著优于现有方法，并且不需要帧级监督。

Abstract: Detecting partial deepfake speech is essential due to its potential for
subtle misinformation. However, existing methods depend on costly frame-level
annotations during training, limiting real-world scalability. Also, they focus
on detecting transition artifacts between bonafide and deepfake segments. As
deepfake generation techniques increasingly smooth these transitions, detection
has become more challenging. To address this, our work introduces a new
perspective by analyzing frame-level temporal differences and reveals that
deepfake speech exhibits erratic directional changes and unnatural local
transitions compared to bonafide speech. Based on this finding, we propose a
Temporal Difference Attention Module (TDAM) that redefines partial deepfake
detection as identifying unnatural temporal variations, without relying on
explicit boundary annotations. A dual-level hierarchical difference
representation captures temporal irregularities at both fine and coarse scales,
while adaptive average pooling preserves essential patterns across
variable-length inputs to minimize information loss. Our TDAM-AvgPool model
achieves state-of-the-art performance, with an EER of 0.59% on the PartialSpoof
dataset and 0.03% on the HAD dataset, which significantly outperforms the
existing methods without requiring frame-level supervision.

</details>


### [651] [Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems](https://arxiv.org/abs/2507.15214)
*Natalia Tomashenko,Emmanuel Vincent,Marc Tommasi*

Main category: cs.SD

TL;DR: 该研究提出了一种从语音时间动态中提取上下文相关时域嵌入的新方法，并利用其开发了攻击模型，证明了其在改进说话人验证和语音匿名化系统方面比现有方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 语音中的时间动态（如节奏、语调和语速）包含关于说话人身份的重要且独特的信息。

Method: 提出了一种提取上下文相关的时域嵌入的新方法，并利用这些表示开发了新的攻击模型。

Result: 所开发的攻击模型在说话人验证性能方面，相较于文献中报道的更简单的语音时间动态表示方法，在原始数据和匿名化数据上均取得了显著的提升。

Conclusion: 该研究提出的基于上下文相关的时域嵌入的新方法，在针对说话人验证和语音匿名化系统的攻击模型方面，相较于文献中报道的更简单的语音时域动态表示方法，在原始数据和匿名化数据上均取得了显著的性能提升。

Abstract: The temporal dynamics of speech, encompassing variations in rhythm,
intonation, and speaking rate, contain important and unique information about
speaker identity. This paper proposes a new method for representing speaker
characteristics by extracting context-dependent duration embeddings from speech
temporal dynamics. We develop novel attack models using these representations
and analyze the potential vulnerabilities in speaker verification and voice
anonymization systems.The experimental results show that the developed attack
models provide a significant improvement in speaker verification performance
for both original and anonymized data in comparison with simpler
representations of speech temporal dynamics reported in the literature.

</details>


### [652] [EchoVoices: Preserving Generational Voices and Memories for Seniors and Children](https://arxiv.org/abs/2507.15221)
*Haiying Xu,Haoze Liu,Mingshi Li,Siyu Cai,Guangxuan Zheng,Yuhuang Jia,Jinghua Zhao,Yong Qin*

Main category: cs.SD

TL;DR: EchoVoices is a digital human pipeline designed to preserve the voices and memories of seniors and children using advanced speech recognition, synthesis, and LLM technologies, demonstrating significant improvements in accuracy and quality.


<details>
  <summary>Details</summary>
Motivation: Recent breakthroughs in intelligent speech and digital human technologies have primarily targeted mainstream adult users, often overlooking the distinct vocal patterns and interaction styles of seniors and children. These demographics possess distinct vocal characteristics, linguistic styles, and interaction patterns that challenge conventional ASR, TTS, and LLM systems.

Method: EchoVoices integrates three core innovations: a k-NN-enhanced Whisper model for robust speech recognition of atypical speech; an age-adaptive VITS model for high-fidelity, speaker-aware speech synthesis; and an LLM-driven agent that automatically generates persona cards and leverages a RAG-based memory system for conversational consistency.

Result: Experiments conducted on the SeniorTalk and ChildMandarin datasets demonstrate significant improvements in recognition accuracy, synthesis quality, and speaker similarity.

Conclusion: EchoVoices provides a comprehensive framework for preserving generational voices, offering a new means of intergenerational connection and the creation of lasting digital legacies.

Abstract: Recent breakthroughs in intelligent speech and digital human technologies
have primarily targeted mainstream adult users, often overlooking the distinct
vocal patterns and interaction styles of seniors and children. These
demographics possess distinct vocal characteristics, linguistic styles, and
interaction patterns that challenge conventional ASR, TTS, and LLM systems. To
address this, we introduce EchoVoices, an end-to-end digital human pipeline
dedicated to creating persistent digital personas for seniors and children,
ensuring their voices and memories are preserved for future generations. Our
system integrates three core innovations: a k-NN-enhanced Whisper model for
robust speech recognition of atypical speech; an age-adaptive VITS model for
high-fidelity, speaker-aware speech synthesis; and an LLM-driven agent that
automatically generates persona cards and leverages a RAG-based memory system
for conversational consistency. Our experiments, conducted on the SeniorTalk
and ChildMandarin datasets, demonstrate significant improvements in recognition
accuracy, synthesis quality, and speaker similarity. EchoVoices provides a
comprehensive framework for preserving generational voices, offering a new
means of intergenerational connection and the creation of lasting digital
legacies.

</details>


### [653] [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
*Ayush Singh Bhadoriya,Abhishek Nikunj Shinde,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.SD

TL;DR: 提出了一种支持多语言和零样本说话人自适应的扩散模型 TTS 系统，提高了语音自然度和韵律。


<details>
  <summary>Details</summary>
Motivation: 为了解决为未见过的说话人生成语音以及支持多种印度语言的挑战。

Method: 本文提出了一种基于扩散模型的说话人自适应语音合成（TTS）系统，并采用基于交叉注意力的时长预测机制来提升韵律和自然度。同时，为了改进零样本生成能力，还采用了分类器无关引导技术。

Result: 通过在 IndicSUPERB 数据集（包含孟加拉语、古吉拉特语、印地语、马拉地语、马拉雅拉姆语、旁遮普语和泰米尔语）上进行训练，所提出的系统能够生成与目标说话人相似的语音，并提高了时长建模和整体表现力。

Conclusion: 所提出的方法能够生成与目标说话人相似、并且在持续时间建模和整体表现力方面有所改进的语音。

Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

</details>


### [654] [MeMo: Attentional Momentum for Real-time Audio-visual Speaker Extraction under Impaired Visual Conditions](https://arxiv.org/abs/2507.15294)
*Junjie Li,Wenxuan Wu,Shuai Wang,Zexu Pan,Kong Aik Lee,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: MeMo框架通过自适应记忆库在视觉线索缺失时仍能有效提取说话人声音，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 人类即使在没有明确辅助信息的情况下也能持续关注目标说话人，受此启发，旨在解决现有视听说话人提取（AV-TSE）系统在视觉线索缺失或降级时性能下降的问题。

Method: 提出了一种名为MeMo的新框架，该框架包含两个自适应记忆库来存储与注意力相关的信息，以在视觉线索缺失或严重降级时维持对目标说话人的注意力。

Result: 实验结果表明，MeMo框架相比基线模型在SI-SNR方面至少有2dB的提升。

Conclusion: MeMo框架在缺乏视觉线索的极端情况下能够保持注意力，并在实验中证明了其有效性，相比基线模型SI-SNR提升至少2dB。

Abstract: Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate a target
speaker's voice from multi-speaker environments by leveraging visual cues as
guidance. However, the performance of AV-TSE systems heavily relies on the
quality of these visual cues. In extreme scenarios where visual cues are
missing or severely degraded, the system may fail to accurately extract the
target speaker. In contrast, humans can maintain attention on a target speaker
even in the absence of explicit auxiliary information. Motivated by such human
cognitive ability, we propose a novel framework called MeMo, which incorporates
two adaptive memory banks to store attention-related information. MeMo is
specifically designed for real-time scenarios: once initial attention is
established, the system maintains attentional momentum over time, even when
visual cues become unavailable. We conduct comprehensive experiments to verify
the effectiveness of MeMo. Experimental results demonstrate that our proposed
framework achieves SI-SNR improvements of at least 2 dB over the corresponding
baseline.

</details>


### [655] [Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation](https://arxiv.org/abs/2507.15396)
*Hui-Guan Yuan,Ryandhimas E. Zezario,Shafique Ahmed,Hsin-Min Wang,Kai-Lung Hua,Yu Tsao*

Main category: cs.SD

TL;DR: Neuro-MSBG是一种新的、轻量级的听力损失模拟模型，比现有模型快46倍，并能保持语音质量。


<details>
  <summary>Details</summary>
Motivation: 现有的听力损失模拟模型计算复杂且延迟高，限制了其在实时应用中的集成。

Method: 提出了一种名为Neuro-MSBG的轻量级端到端模型，该模型包含一个个性化的听力图编码器，能够进行有效的时间-频率建模。

Result: Neuro-MSBG支持并行推理，并保持了原始MSBG的语音清晰度和感知质量，在STOI和PESQ方面的表现分别为0.9247和0.8671。与现有模型相比，模拟运行时间减少了46倍（从0.970秒缩短到0.021秒）。

Conclusion: Neuro-MSBG在语音处理系统中具有高效和实用的潜力，可用于助听器部署。

Abstract: Hearing loss simulation models are essential for hearing aid deployment.
However, existing models have high computational complexity and latency, which
limits real-time applications and lack direct integration with speech
processing systems. To address these issues, we propose Neuro-MSBG, a
lightweight end-to-end model with a personalized audiogram encoder for
effective time-frequency modeling. Experiments show that Neuro-MSBG supports
parallel inference and retains the intelligibility and perceptual quality of
the original MSBG, with a Spearman's rank correlation coefficient (SRCC) of
0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for
Perceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation
runtime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second
input), further demonstrating its efficiency and practicality.

</details>


### [656] [Multichannel Keyword Spotting for Noisy Conditions](https://arxiv.org/abs/2507.15558)
*Dzmitry Saladukha,Ivan Koriabkin,Kanstantsin Artsiom,Aliaksei Rak,Nikita Ryzhikov*

Main category: cs.SD

TL;DR: 本研究提出了一种基于注意力机制的神经网络，用于改进噪声环境下的关键字识别算法，并在实验中证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统的波束形成（BF）和自适应噪声消除（ANC）技术在噪声环境下可能会削弱或扭曲有用的信号，从而影响关键字识别（KWS）算法的性能。本研究旨在克服这一挑战，提出一种更鲁棒的解决方案。

Method: 提出了一种新颖的神经网络架构，该架构包含多个输入通道和一个注意力机制，能够自适应地选择或组合最相关的输入信号，以提高在噪声环境下的KWS算法性能。

Result: 研究结果表明，所提出的神经网络架构在实验室和实际智能音箱场景的两个数据集上均表现出色，其性能在噪声抑制、KWS指标和计算资源消耗方面均优于现有基线算法。

Conclusion: 该研究提出了一种改进的关键字识别（KWS）算法，通过使用包含多个输入通道和注意力机制的神经网络，有效解决了噪声环境下的性能下降问题。

Abstract: This article presents a method for improving a keyword spotter (KWS)
algorithm in noisy environments. Although beamforming (BF) and adaptive noise
cancellation (ANC) techniques are robust in some conditions, they may degrade
the performance of the activation system by distorting or suppressing useful
signals. The authors propose a neural network architecture that uses several
input channels and an attention mechanism that allows the network to determine
the most useful channel or their combination. The improved quality of the
algorithm was demonstrated on two datasets: from a laboratory with controlled
conditions and from smart speakers in natural conditions. The proposed
algorithm was compared against several baselines in terms of the quality of
noise reduction metrics, KWS metrics, and computing resources in comparison
with existing solutions.

</details>
