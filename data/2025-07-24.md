<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 95]
- [cs.CL](#cs.CL) [Total: 31]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.LG](#cs.LG) [Total: 67]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 16]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.RO](#cs.RO) [Total: 38]
- [cs.SI](#cs.SI) [Total: 8]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery](https://arxiv.org/abs/2507.16849)
*Yi-Shan Chu,Hsuan-Cheng Wei*

Main category: cs.CV

TL;DR: 提出一个基于ViT的框架，使用PCA和CI进行弱监督学习，以改进灾区影像分割，支持TASA的EVAP产品。


<details>
  <summary>Details</summary>
Motivation: 旨在改进台湾太空机构（TASA）开发的应急价值增值产品（EVAP），以支持和增强其在遥感影像中对灾区进行分割的能力。

Method: 提出了一种基于Vision Transformer（ViT）的深度学习框架，结合主成分分析（PCA）和置信指数（CI）进行弱监督学习，并使用多波段输入（Sentinel-2和Formosat-5影像）训练ViT编码器-解码器模型，支持多种解码器变体和多阶段损失策略。

Result: 在2022年鄱阳湖干旱和2023年罗德岛野火的案例研究中，证明了该框架能够改进分割结果的光滑度和可靠性。

Conclusion: 该框架在准确的地面实况数据不可用时，通过改进分割结果的光滑度和可靠性，为灾难测绘提供了一种可扩展的方法。

Abstract: We propose a vision transformer (ViT)-based deep learning framework to refine
disaster-affected area segmentation from remote sensing imagery, aiming to
support and enhance the Emergent Value Added Product (EVAP) developed by the
Taiwan Space Agency (TASA). The process starts with a small set of manually
annotated regions. We then apply principal component analysis (PCA)-based
feature space analysis and construct a confidence index (CI) to expand these
labels, producing a weakly supervised training set. These expanded labels are
then used to train ViT-based encoder-decoder models with multi-band inputs from
Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder
variants and multi-stage loss strategies to improve performance under limited
supervision. During the evaluation, model predictions are compared with
higher-resolution EVAP output to assess spatial coherence and segmentation
consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes
wildfire demonstrate that our framework improves the smoothness and reliability
of segmentation results, offering a scalable approach for disaster mapping when
accurate ground truth is unavailable.

</details>


### [2] [Coarse-to-fine crack cue for robust crack detection](https://arxiv.org/abs/2507.16851)
*Zelong Liu,Yuliang Gu,Zhichao Sun,Huachao Zhu,Xin Xiao,Bo Du,Laurent Najman,Yongchao Xu*

Main category: cs.CV

TL;DR: CrackCue 是一种新的裂缝检测方法，通过利用裂缝的细长结构生成“裂缝线索”，提高了模型在不同域下的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法在裂缝检测任务中虽然在数据集中表现优异，但在泛化到未见域时仍存在困难。先前的研究忽略了裂缝的细长结构特性。

Method: CrackCue 是一种基于粗到细裂缝线索生成的新型鲁棒裂缝检测方法。它利用裂缝的细长结构特性生成鲁棒的裂缝线索来指导裂缝检测。具体来说，首先在裂缝图像上进行最大池化和上采样操作，得到粗略的无裂缝背景。然后，通过一个重建网络获得精细的无裂缝背景。原始图像与精细无裂缝背景的差值提供了精细的裂缝线索，该线索嵌入了不受复杂背景、阴影和光照变化影响的鲁棒裂缝先验信息。该方法可以作为即插即用模块集成到现有的裂缝检测网络中。

Result: CrackCue 被集成到三个先进的裂缝检测网络中，并在广泛的实验中证明，能够显著提高基线方法的泛化能力和鲁棒性。

Conclusion: CrackCue 显著提高了基线方法的泛化能力和鲁棒性。

Abstract: Crack detection is an important task in computer vision. Despite impressive
in-dataset performance, deep learning-based methods still struggle in
generalizing to unseen domains. The thin structure property of cracks is
usually overlooked by previous methods. In this work, we introduce CrackCue, a
novel method for robust crack detection based on coarse-to-fine crack cue
generation. The core concept lies on leveraging the thin structure property to
generate a robust crack cue, guiding the crack detection. Specifically, we
first employ a simple max-pooling and upsampling operation on the crack image.
This results in a coarse crack-free background, based on which a fine
crack-free background can be obtained via a reconstruction network. The
difference between the original image and fine crack-free background provides a
fine crack cue. This fine cue embeds robust crack prior information which is
unaffected by complex backgrounds, shadow, and varied lighting. As a
plug-and-play method, we incorporate the proposed CrackCue into three advanced
crack detection networks. Extensive experimental results demonstrate that the
proposed CrackCue significantly improves the generalization ability and
robustness of the baseline methods. The source code will be publicly available.

</details>


### [3] [Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors](https://arxiv.org/abs/2507.16850)
*Mohamed Adjel*

Main category: cs.CV

TL;DR: 提出了一种新的方法，通过结合2D关键点检测和几何感知2D到3D提升，并利用相机内参和人体解剖学先验知识，实现了从单目图像进行快速、个性化、准确的3D人体姿态估计，无需专用硬件。


<details>
  <summary>Details</summary>
Motivation: 单目3D人体姿态估计在实时和非约束环境下仍然是一个挑战性问题。直接的图像到3D方法需要大量的标注数据和复杂的模型，而2D到3D提升方法则更轻量、更灵活，尤其是在结合了先验知识的情况下。

Method: 结合了实时的2D关键点检测和几何感知的2D到3D提升方法，并利用已知的相机内参和特定对象的解剖学先验知识。该方法基于自校准和生物力学约束逆运动学的最新进展，以生成大规模、合理的2D-3D训练对。

Result: 提出了一种结合实时2D关键点检测和几何感知2D到3D提升的框架，用于从单目图像中进行3D人体姿态估计。

Conclusion: 该框架旨在为边缘设备提供快速、个性化、准确的单目3D姿态估计，并在真实世界场景中实现无需专用硬件。

Abstract: Monocular 3D human pose estimation remains a challenging and ill-posed
problem, particularly in real-time settings and unconstrained environments.
While direct imageto-3D approaches require large annotated datasets and heavy
models, 2D-to-3D lifting offers a more lightweight and flexible
alternative-especially when enhanced with prior knowledge. In this work, we
propose a framework that combines real-time 2D keypoint detection with
geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics
and subject-specific anatomical priors. Our approach builds on recent advances
in self-calibration and biomechanically-constrained inverse kinematics to
generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic
datasets. We discuss how these ingredients can enable fast, personalized, and
accurate 3D pose estimation from monocular images without requiring specialized
hardware. This proposal aims to foster discussion on bridging data-driven
learning and model-based priors to improve accuracy, interpretability, and
deployability of 3D human motion capture on edge devices in the wild.

</details>


### [4] [CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.16854)
*Xiaoqiang He*

Main category: cs.CV

TL;DR: 本论文提出了一种名为CLAMP的端到端对比学习框架，用于解决多模态方面基础情感分析（MABSA）中的跨模态对齐噪声和细粒度表示不一致性问题。该框架通过渐进式注意力融合、多任务对比学习和自适应多损失聚合等模块，提高了跨模态表示的一致性和准确性，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方面基础情感分析（MABSA）方法面临跨模态对齐噪声和细粒度表示不一致性等挑战。全局模态对齐方法常常忽略方面词与其对应视觉区域的联系，而弥合文本和图像之间的表示差距仍然是一个挑战。

Method: CLAMP框架，包括渐进式注意力融合网络、多任务对比学习和自适应多损失聚合。渐进式注意力融合网络通过分层、多阶段的跨模态交互来增强文本特征和图像区域之间的细粒度对齐；多任务对比学习结合了全局模态对比和局部粒度对齐以增强跨模态表示一致性；自适应多损失聚合采用基于动态不确定性的加权机制，根据每个任务的不确定性来校准损失贡献，从而缓解梯度干扰。

Result: CLAMP框架在标准公开基准的评估中持续优于大多数现有的最先进方法。

Conclusion: CLAMP框架在标准公开基准的评估中持续优于大多数现有的最先进方法。

Abstract: Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect
terms within paired image-text data and determine their fine grained sentiment
polarities, representing a fundamental task for improving the effectiveness of
applications such as product review systems and public opinion monitoring.
Existing methods face challenges such as cross modal alignment noise and
insufficient consistency in fine-grained representations. While global modality
alignment methods often overlook the connection between aspect terms and their
corresponding local visual regions, bridging the representation gap between
text and images remains a challenge. To address these limitations, this paper
introduces an end to end Contrastive Learning framework with Adaptive
Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed
of three novel modules: Progressive Attention Fusion network, Multi-task
Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive
Attention Fusion network enhances fine-grained alignment between textual
features and image regions via hierarchical, multi-stage cross modal
interactions, effectively suppressing irrelevant visual noise. Secondly,
multi-task contrastive learning combines global modal contrast and local
granularity alignment to enhance cross modal representation consistency.
Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting
mechanism to calibrate loss contributions according to each task's uncertainty,
thereby mitigating gradient interference. Evaluation on standard public
benchmarks demonstrates that CLAMP consistently outperforms the vast majority
of existing state of the art methods.

</details>


### [5] [SIA: Enhancing Safety via Intent Awareness for Vision-Language Models](https://arxiv.org/abs/2507.16856)
*Youngjin Na,Sangheon Jeong,Youngwan Lee*

Main category: cs.CV

TL;DR: SIA通过意图感知推理来提高VLM的安全性，通过三阶段过程检测和缓解有害意图，并在关键基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着VLM在现实世界中的应用越来越多，图像和文本之间的微妙相互作用带来了新的安全风险，特别是看似无害的输入组合可能暴露有害意图，导致不安全的模型响应。现有的方法难以检测这种潜在风险。

Method: SIA是一个训练免费的提示工程框架，通过三阶段推理过程（视觉抽象、意图推理和意图条件响应改进）来主动检测和缓解多模式输入中的有害意图。

Result: SIA在SIUO、MM-SafetyBench和HoliSafe等安全关键基准上实现了显著的安全改进，优于先前的方法，同时在MMStar上通用推理准确性略有下降。

Conclusion: SIA通过在安全关键基准上实现显著的安全改进，并且在MMStar上仅有轻微的通用推理准确性下降，展示了意图感知推理在使VLM与以人为中心的价值观保持一致方面的价值。

Abstract: As vision-language models (VLMs) are increasingly deployed in real-world
applications, new safety risks arise from the subtle interplay between images
and text. In particular, seemingly innocuous inputs can combine to reveal
harmful intent, leading to unsafe model responses. Despite increasing attention
to multimodal safety, previous approaches based on post hoc filtering or static
refusal prompts struggle to detect such latent risks, especially when
harmfulness emerges only from the combination of inputs. We propose SIA (Safety
via Intent Awareness), a training-free prompt engineering framework that
proactively detects and mitigates harmful intent in multimodal inputs. SIA
employs a three-stage reasoning process: (1) visual abstraction via captioning,
(2) intent inference through few-shot chain-of-thought prompting, and (3)
intent-conditioned response refinement. Rather than relying on predefined rules
or classifiers, SIA dynamically adapts to the implicit intent inferred from the
image-text pair. Through extensive experiments on safety-critical benchmarks
including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves
substantial safety improvements, outperforming prior methods. Although SIA
shows a minor reduction in general reasoning accuracy on MMStar, the
corresponding safety gains highlight the value of intent-aware reasoning in
aligning VLMs with human-centric values.

</details>


### [6] [Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection](https://arxiv.org/abs/2507.16861)
*Xiang Li*

Main category: cs.CV

TL;DR: 本研究提出了一种新的方法，通过利用2D物体先验来解决LiDAR和摄像头特征在BEV表示中的不匹配问题，以提高3D感知能力。该方法包括PGDC、DAGF和SGDM三个部分，并在nuScenes数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前将LiDAR和摄像头输入集成到统一的鸟瞰图（BEV）表示中的方法，常常受到摄像头和LiDAR特征之间不匹配问题的影响。这种不匹配源于外部校准不精确和LiDAR在车辆运动中的滚动快门效应。本研究的关键在于，这些投影误差主要集中在物体-背景边界，而这些边界可以被2D检测器轻易识别。因此，本研究的主要动机是利用2D物体先验来在融合前预先对齐跨模态特征。

Method: 1. 提出先验引导深度校准（PGDC），利用2D先验校正局部不匹配，保留正确的跨模态特征对。 2. 提出不连续感知几何融合（DAGF）来处理PGDC校正后的结果，以抑制噪声并增强物体-背景边界处的清晰过渡。 3. 引入结构引导深度调制器（SGDM），使用门控注意力机制来有效融合已对齐的深度和图像特征。

Result: 该方法在nuScenes验证数据集上取得了最先进的性能，mAP达到71.5%，NDS达到73.6%。

Conclusion: 所提出的方法在nuScenes验证数据集上实现了最先进的性能，其mAP和NDS分别达到了71.5%和73.6%。

Abstract: Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)
representation is crucial for enhancing 3D perception capabilities of
autonomous vehicles. However, current methods are often affected by
misalignment between camera and LiDAR features. This misalignment leads to
inaccurate depth supervision in camera branch and erroneous fusion during
cross-modal feature aggregation. The root cause of this misalignment lies in
projection errors, stemming from minor extrinsic calibration inaccuracies and
rolling shutter effect of LiDAR during vehicle motion. In this work, our key
insight is that these projection errors are predominantly concentrated at
object-background boundaries, which are readily identified by 2D detectors.
Based on this, our main motivation is to utilize 2D object priors to pre-align
cross-modal features before fusion. To address local misalignment, we propose
Prior Guided Depth Calibration (PGDC), which leverages 2D priors to correct
local misalignment and preserve correct cross-modal feature pairs. To resolve
global misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)
to process calibrated results from PGDC, suppressing noise and explicitly
enhancing sharp transitions at object-background boundaries. To effectively
utilize these transition-aware depth representations, we incorporate Structural
Guidance Depth Modulator (SGDM), using a gated attention mechanism to
efficiently fuse aligned depth and image features. Our proposed method achieves
state-of-the-art performance on nuScenes validation dataset, with its mAP and
NDS reaching 71.5% and 73.6% respectively.

</details>


### [7] [Pixels, Patterns, but No Poetry: To See The World like Humans](https://arxiv.org/abs/2507.16863)
*Hongcheng Gao,Zihao Huang,Lin Xu,Jingyi Tang,Xinhao Li,Yue Liu,Haoyang Li,Taihang Hu,Minhua Lin,Xinlong Yang,Ge Wu,Balong Bi,Hongyu Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: MLLMs在感知方面远不如人类，即使是先进的模型也会在图灵眼测试（TET）中失败。TET基准专注于感知而非推理。微调视觉塔比上下文学习更能提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 探究多模态大语言模型（MLLMs）是否能像人类一样感知世界，弥补当前研究中对推理能力关注过多而对感知能力忽视的不足。

Method: 提出了一种名为图灵眼测试（TET）的感知导向基准，包含四个诊断任务，使用人类能够直观处理的合成图像来评估多模态大语言模型（MLLMs）的感知能力。

Result: 在TET基准上，最先进的MLLMs表现出严重的感知失败，而这些任务对人类来说却很容易。在语言骨干上进行上下文学习和训练无法提升模型在TET任务上的表现，但微调视觉塔可以快速适应，表明问题根源在于视觉塔的泛化能力，而非语言骨干的知识和推理能力。

Conclusion: 目前的MLLMs在感知能力方面存在显著的缺陷，与人类相比有很大差距。仅仅依靠语言模型或上下文学习无法弥补这一差距，而对视觉塔进行微调是提高模型感知能力的有效途径，这表明当前模型在视觉泛化能力方面存在不足。

Abstract: Achieving human-like perception and reasoning in Multimodal Large Language
Models (MLLMs) remains a central challenge in artificial intelligence. While
recent research has primarily focused on enhancing reasoning capabilities in
MLLMs, a fundamental question persists: Can Multimodal Large Language Models
truly perceive the world as humans do? This paper shifts focus from reasoning
to perception. Rather than constructing benchmarks specifically for reasoning,
we introduce the Turing Eye Test (TET), a challenging perception-oriented
benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on
synthetic images that humans process intuitively. Our findings reveal that
state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks
trivial for humans. Both in-context learning and training on language
backbone-effective for previous benchmarks-fail to improve performance on our
tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting
that our benchmark poses challenges for vision tower generalization rather than
for the knowledge and reasoning capabilities of the language backbone-a key gap
between current MLLMs and human perception. We release a representative subset
of TET tasks in this version, and will introduce more diverse tasks and methods
to enhance visual generalization in future work.

</details>


### [8] [HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting](https://arxiv.org/abs/2507.16873)
*Jeongeun Lee,Youngjae Yu,Dongha Lee*

Main category: cs.CV

TL;DR: HIPPO-Video 是一个新数据集，旨在解决个性化视频精彩集锦问题，并附带了一种名为 HiPHer 的新方法，该方法可实现高度以用户为中心的视频精彩集锦。


<details>
  <summary>Details</summary>
Motivation: 用户偏好是可变且复杂的，现有的视频数据集缺乏个性化，无法捕捉用户行为的复杂性，因此需要个性化的视频精彩集锦。

Method: 提出了一种名为 HiPHer 的新方法，该方法利用个性化观看历史来预测条件化偏好的、分段的显着性得分。

Result: HIPPO-Video 数据集包含 2,040 个（观看历史、显着性得分）对，涵盖 170 个语义类别中的 20,400 个视频，并已通过 HiPHer 方法得到验证。

Conclusion: HiPHer 方法通过利用个性化观看历史，在视频精彩集锦任务上超越了现有的通用方法和基于查询的方法。

Abstract: The exponential growth of video content has made personalized video
highlighting an essential task, as user preferences are highly variable and
complex. Existing video datasets, however, often lack personalization, relying
on isolated videos or simple text queries that fail to capture the intricacies
of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for
personalized video highlighting, created using an LLM-based user simulator to
generate realistic watch histories reflecting diverse user preferences. The
dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400
videos across 170 semantic categories. To validate our dataset, we propose
HiPHer, a method that leverages these personalized watch histories to predict
preference-conditioned segment-wise saliency scores. Through extensive
experiments, we demonstrate that our method outperforms existing generic and
query-based approaches, showcasing its potential for highly user-centric video
highlighting in real-world scenarios.

</details>


### [9] [ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension](https://arxiv.org/abs/2507.16877)
*Yizhi Hu,Zezhao Tian,Xingqun Qi,Chen Su,Bingkun Yang,Junhui Yin,Muyi Sun,Man Zhang,Zhenan Sun*

Main category: cs.CV

TL;DR: 由于现有方法在处理多实体场景中的复杂实体关系方面存在不足，并且缺乏高质量的、带有细粒度图像-文本-关系注释的数据集，因此提出了一种名为ReMeREC的新型框架。该框架能够联合利用视觉和文本线索来定位多个实体并对其进行关系建模。实验结果表明，ReMeREC在多实体定位和关系预测方面取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多实体场景中的复杂实体关系时存在不足，并且缺乏高质量的、带有细粒度图像-文本-关系注释的数据集，阻碍了该领域的进一步发展。

Method: 提出了一种名为ReMeREC的新型框架，该框架能够联合利用视觉和文本线索来定位多个实体并对其进行关系建模。该框架还包括文本自适应多实体感知器（TMP），用于从文本线索中推断实体数量和范围，以及实体间关系推理器（EIR），用于增强关系推理和全局场景理解。此外，还构建了一个名为EntityText的小型辅助数据集来提高对细粒度提示的语言理解能力。

Result: ReMeREC在多实体定位和关系预测方面取得了最先进的性能，在四个基准数据集上均优于现有方法。

Conclusion: ReMeREC在多实体定位和关系预测方面取得了最先进的性能，在四个基准数据集上均优于现有方法。

Abstract: Referring Expression Comprehension (REC) aims to localize specified entities
or regions in an image based on natural language descriptions. While existing
methods handle single-entity localization, they often ignore complex
inter-entity relationships in multi-entity scenes, limiting their accuracy and
reliability. Additionally, the lack of high-quality datasets with fine-grained,
paired image-text-relation annotations hinders further progress. To address
this challenge, we first construct a relation-aware, multi-entity REC dataset
called ReMeX, which includes detailed relationship and textual annotations. We
then propose ReMeREC, a novel framework that jointly leverages visual and
textual cues to localize multiple entities while modeling their
inter-relations. To address the semantic ambiguity caused by implicit entity
boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron
(TMP), which dynamically infers both the quantity and span of entities from
fine-grained textual cues, producing distinctive representations. Additionally,
our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and
global scene understanding. To further improve language comprehension for
fine-grained prompts, we also construct a small-scale auxiliary dataset,
EntityText, generated using large language models. Experiments on four
benchmark datasets show that ReMeREC achieves state-of-the-art performance in
multi-entity grounding and relation prediction, outperforming existing
approaches by a large margin.

</details>


### [10] [CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos](https://arxiv.org/abs/2507.16878)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang,Wentao Zhang*

Main category: cs.CV

TL;DR: CausalStep是一个用于视频的逐步因果推理基准，可以更严格地评估模型，弥合当前模型和人类水平之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准未能严格评估真正的因果和逐步推理能力，模型可以利用全局上下文来获得捷径解决方案。然而，在视频领域实现鲁棒的视频推理仍然是一个重大挑战。

Method: CausalStep通过将视频分割成因果关联的单元，并强制执行严格的顺序问答协议来评估逐步因果推理能力，每个问题都包含基于错误类型分类法精心设计的干扰项。

Result: CausalStep包含100个视频和1,852个多项选择问答对，并引入了七个诊断指标，可以精确诊断因果推理能力。与领先的专有和开源模型以及人类基线的实验表明，当前模型和人类水平的逐步推理之间存在显著差距。

Conclusion: CausalStep为视频推理树立了一个严格的基准，以推动在稳健和可解释的视频推理方面取得进展。

Abstract: Recent advances in large language models (LLMs) have improved reasoning in
text and image domains, yet achieving robust video reasoning remains a
significant challenge. Existing video benchmarks mainly assess shallow
understanding and reasoning and allow models to exploit global context, failing
to rigorously evaluate true causal and stepwise reasoning. We present
CausalStep, a benchmark designed for explicit stepwise causal reasoning in
videos. CausalStep segments videos into causally linked units and enforces a
strict stepwise question-answer (QA) protocol, requiring sequential answers and
preventing shortcut solutions. Each question includes carefully constructed
distractors based on error type taxonomy to ensure diagnostic value. The
benchmark features 100 videos across six categories and 1,852 multiple-choice
QA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,
enabling precise diagnosis of causal reasoning capabilities. Experiments with
leading proprietary and open-source models, as well as human baselines, reveal
a significant gap between current models and human-level stepwise reasoning.
CausalStep provides a rigorous benchmark to drive progress in robust and
interpretable video reasoning.

</details>


### [11] [Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed](https://arxiv.org/abs/2507.16880)
*Antoni Kowalczuk,Dominik Hintersdorf,Lukas Struppek,Kristian Kersting,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 现有的基于修剪的方法在防止文本到图像扩散模型复制训练数据方面效果不佳，因为即使进行微剪枝，微小的输入扰动也会重新触发复制。该研究表明，记忆的触发可能并不局限于特定位置，并提出了一种新的对抗性微调方法来提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决文本到图像扩散模型中因潜在记忆和复制训练数据而引发的数据隐私和知识产权问题，并评估现有缓解方法的有效性。

Method: 研究评估了基于修剪的方法的鲁棒性，并通过微调模型来提高其抵御数据复制的能力。

Result: 证明了即使在修剪后，通过微调文本嵌入也能重新触发数据复制，表明现有防御措施的脆弱性。此外，研究挑战了记忆局部性的假设，发现复制可以从文本嵌入空间的多个位置触发。

Conclusion: 现有基于修剪的方法不足以解决文本到图像扩散模型中的数据记忆和复制问题，需要新的方法来真正移除记忆内容。我们提出了一种新颖的对抗性微调方法，以提高模型的鲁棒性。

Abstract: Text-to-image diffusion models (DMs) have achieved remarkable success in
image generation. However, concerns about data privacy and intellectual
property remain due to their potential to inadvertently memorize and replicate
training data. Recent mitigation efforts have focused on identifying and
pruning weights responsible for triggering replication, based on the assumption
that memorization can be localized. Our research assesses the robustness of
these pruning-based approaches. We demonstrate that even after pruning, minor
adjustments to text embeddings of input prompts are sufficient to re-trigger
data replication, highlighting the fragility of these defenses. Furthermore, we
challenge the fundamental assumption of memorization locality, by showing that
replication can be triggered from diverse locations within the text embedding
space, and follows different paths in the model. Our findings indicate that
existing mitigation strategies are insufficient and underscore the need for
methods that truly remove memorized content, rather than attempting to suppress
its retrieval. As a first step in this direction, we introduce a novel
adversarial fine-tuning method that iteratively searches for replication
triggers and updates the model to increase robustness. Through our research, we
provide fresh insights into the nature of memorization in text-to-image DMs and
a foundation for building more trustworthy and compliant generative AI.

</details>


### [12] [Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning](https://arxiv.org/abs/2507.16886)
*Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: S2S-ST是一种新的空间转录组学填补框架，使用稀疏数据和自然图像进行训练，准确性高，成本低。


<details>
  <summary>Details</summary>
Motivation: 为了解决高分辨率空间转录组学（ST）数据成本高昂且获取困难的挑战。

Method: 提出了一种名为S2S-ST的新型框架，采用三项关键创新：1. 稀疏到稀疏的自监督学习策略，利用ST数据的内在空间模式；2. 与自然图像进行跨域共学习，增强特征表示；3. 级联数据一致性填补网络（CDCIN），在保持采样基因数据保真度的同时迭代优化预测。

Result: 在乳腺癌、肝脏和淋巴组织等多种组织类型的广泛实验表明，S2S-ST方法在填补准确性方面优于现有最先进的方法。

Conclusion: S2S-ST框架通过利用稀疏采样的ST数据和自然图像进行共训练，实现了准确的空间转录组学数据填补，提高了基因表达谱的精度，降低了对高分辨率数据的依赖，有望促进其在生物医学研究和临床应用中的普及。

Abstract: Spatial transcriptomics (ST) has revolutionized biomedical research by
enabling high resolution gene expression profiling within tissues. However, the
high cost and scarcity of high resolution ST data remain significant
challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel
framework for accurate ST imputation that requires only a single and low-cost
sparsely sampled ST dataset alongside widely available natural images for
co-training. Our approach integrates three key innovations: (1) a
sparser-to-sparse self-supervised learning strategy that leverages intrinsic
spatial patterns in ST data, (2) cross-domain co-learning with natural images
to enhance feature representation, and (3) a Cascaded Data Consistent
Imputation Network (CDCIN) that iteratively refines predictions while
preserving sampled gene data fidelity. Extensive experiments on diverse tissue
types, including breast cancer, liver, and lymphoid tissue, demonstrate that
our method outperforms state-of-the-art approaches in imputation accuracy. By
enabling robust ST reconstruction from sparse inputs, our framework
significantly reduces reliance on costly high resolution data, facilitating
potential broader adoption in biomedical research and clinical applications.

</details>


### [13] [AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation](https://arxiv.org/abs/2507.16940)
*Nima Fathi,Amar Kumar,Tal Arbel*

Main category: cs.CV

TL;DR: AURA是首个用于医学影像分析的视觉语言可解释性AI代理，它利用LLM和一套工具来提供交互式解释和评估，旨在提高透明度和临床相关性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的代理系统在许多领域都显示出了潜力，但它们在医学成像领域的应用仍处于初级阶段。这项工作旨在解决这一差距，并探索代理AI在医学影像分析中的潜力。

Method: AURA是一个视觉语言可解释性代理，利用Qwen-32B LLM架构，并集成了一个包含分割套件（具有相位接地、病理分割和解剖分割）、反事实图像生成模块和一套评估工具（包括像素级差异图分析、分类和先进的现成组件）的模块化工具箱。

Result: AURA能够进行动态交互、提供上下文解释和进行假设检验，实现了对医学影像的全面分析、解释和评估。

Conclusion: AURA代表了迈向更透明、更适应和临床对齐的AI系统的重要进步，并有望将医学影像分析从静态预测转变为交互式决策支持。

Abstract: Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm
shift from static prediction systems to agentic AI agents capable of reasoning,
interacting with tools, and adapting to complex tasks. While LLM-based agentic
systems have shown promise across many domains, their application to medical
imaging remains in its infancy. In this work, we introduce AURA, the first
visual linguistic explainability agent designed specifically for comprehensive
analysis, explanation, and evaluation of medical images. By enabling dynamic
interactions, contextual explanations, and hypothesis testing, AURA represents
a significant advancement toward more transparent, adaptable, and clinically
aligned AI systems. We highlight the promise of agentic AI in transforming
medical image analysis from static predictions to interactive decision support.
Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular
toolbox comprising: (i) a segmentation suite with phase grounding, pathology
segmentation, and anatomy segmentation to localize clinically meaningful
regions; (ii) a counterfactual image-generation module that supports reasoning
through image-level explanations; and (iii) a set of evaluation tools including
pixel-wise difference-map analysis, classification, and advanced
state-of-the-art components to assess diagnostic relevance and visual
interpretability.

</details>


### [14] [HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning](https://arxiv.org/abs/2507.17402)
*Li Jun,Wang Jinpeng,Tan Chaolei,Lian Niu,Chen Long,Zhang Min,Wang Yaowei,Xia Shu-Tao,Chen Bin*

Main category: cs.CV

TL;DR: HLFormer是一种新的双曲空间框架，用于部分相关视频检索，解决了现有方法在处理视频层次结构和语义方面的不足，并通过混合空间编码和偏序保持损失提高了匹配精度。


<details>
  <summary>Details</summary>
Motivation: 现有的部分相关视频检索方法在欧氏空间中存在几何畸变问题，未能充分捕捉视频固有的层次结构和语义，导致时间建模不佳。

Method: 提出了一种名为HLFormer的框架，利用双曲空间学习来弥补欧氏空间在层次建模能力上的不足。具体来说，HLFormer集成了洛伦兹注意力和欧氏注意力块，在混合空间中对视频嵌入进行编码，并使用均值引导自适应交互模块来动态融合特征。此外，还引入了偏序保持损失，通过洛伦兹锥约束来强制执行“文本<视频”的层次结构，以增强跨模态匹配。

Result: HLFormer在大量实验中表现优于最先进的方法。

Conclusion: HLFormer在部分相关视频检索任务上表现优于现有技术。

Abstract: Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of
matching untrimmed videos with text queries describing only partial content.
Existing methods suffer from geometric distortion in Euclidean space that
sometimes misrepresents the intrinsic hierarchical structure of videos and
overlooks certain hierarchical semantics, ultimately leading to suboptimal
temporal modeling. To address this issue, we propose the first hyperbolic
modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space
learning to compensate for the suboptimal hierarchical modeling capabilities of
Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block
and Euclidean Attention Block to encode video embeddings in hybrid spaces,
using the Mean-Guided Adaptive Interaction Module to dynamically fuse features.
Additionally, we introduce a Partial Order Preservation Loss to enforce "text <
video" hierarchy through Lorentzian cone constraints. This approach further
enhances cross-modal matching by reinforcing partial relevance between video
content and text queries. Extensive experiments show that HLFormer outperforms
state-of-the-art methods. Code is released at
https://github.com/lijun2005/ICCV25-HLFormer.

</details>


### [15] [Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts](https://arxiv.org/abs/2507.16946)
*Chiao-An Yang,Kuan-Chuan Peng,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 本研究提出了一个用于长尾在线异常检测（LTOAD）的无类别框架，解决了现有方法无法直接应用于在线设置的挑战，并在多个领域取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 针对长尾在线异常检测（LTOAD）这一新颖的任务，扩展了现有异常检测（AD）的研究方向，解决了离线最先进的LTAD方法无法直接应用于在线设置的问题。

Method: 提出了一种无类别框架来解决在线长尾异常检测（LTOAD）的挑战，并将其适应于在线学习设置。

Result: 该方法在大多数离线长尾异常检测设置（包括工业制造和医学领域）中均优于最先进的基线方法，在MVTec上的图像AUROC比访问类别标签和类别数量的方法高出+4.63%。在最具挑战性的长尾在线设置中，与基线方法相比，图像AUROC高出+0.53%。

Conclusion: 提出的无类别框架在大多数离线长尾异常检测设置（包括工业制造和医学领域）中均优于最先进的基线方法，在MVTec上的图像AUROC比访问类别标签和类别数量的方法高出+4.63%。在最具挑战性的长尾在线设置中，与基线方法相比，图像AUROC高出+0.53%。

Abstract: Anomaly detection (AD) identifies the defect regions of a given image. Recent
works have studied AD, focusing on learning AD without abnormal images, with
long-tailed distributed training data, and using a unified model for all
classes. In addition, online AD learning has also been explored. In this work,
we expand in both directions to a realistic setting by considering the novel
task of long-tailed online AD (LTOAD). We first identified that the offline
state-of-the-art LTAD methods cannot be directly applied to the online setting.
Specifically, LTAD is class-aware, requiring class labels that are not
available in the online setting. To address this challenge, we propose a
class-agnostic framework for LTAD and then adapt it to our online learning
setting. Our method outperforms the SOTA baselines in most offline LTAD
settings, including both the industrial manufacturing and the medical domain.
In particular, we observe +4.63% image-AUROC on MVTec even compared to methods
that have access to class labels and the number of classes. In the most
challenging long-tailed online setting, we achieve +0.53% image-AUROC compared
to baselines. Our LTOAD benchmark is released here:
https://doi.org/10.5281/zenodo.16283852 .

</details>


### [16] [Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging](https://arxiv.org/abs/2507.17412)
*Farnaz Khun Jush,Steffen Vogler,Matthias Lenga*

Main category: cs.CV

TL;DR: 本研究提出了一种名为C-MIR的新型医学影像检索方法，无需预先分割数据，能有效重排3D医学影像，提高肿瘤检索的准确性，尤其在结肠癌和肺癌方面效果显著，并有潜力用于肿瘤分期。


<details>
  <summary>Details</summary>
Motivation: 医学影像数量的不断增加给放射科医生在检索相关病例带来了挑战。本研究旨在通过开发先进的CBIR系统来解决这一问题，以实现更高效的病例检索，并为该领域的研究做出贡献。

Method: 本研究提出了一种新的框架，无需预先分割的数据和特定器官的数据集，即可用于体积医学影像的内容基础图像检索（CBIR）。该框架的核心是C-MIR，一种适应ColBERT的上下文晚期交互机制的三维医学影像重排方法。

Result: C-MIR在四个肿瘤部位进行了全面评估，使用了三种特征提取器和三种数据库配置。结果表明，C-MIR在肿瘤标记方面表现出显著优势，尤其是在结肠癌和肺癌方面（p<0.05）。此外，C-MIR能够有效定位感兴趣区域，无需预先分割数据集，为依赖昂贵数据丰富步骤的系统提供了一种计算效率更高的方法。研究还发现C-MIR在改善肿瘤分期方面具有潜力。

Conclusion: 该研究通过引入C-MIR，一种新颖的体积重排方法，成功地将晚期交互原则应用于3D医学影像，实现了有效的上下文感知重排，为医学影像检索提供了新的解决方案。

Abstract: The increasing volume of medical images poses challenges for radiologists in
retrieving relevant cases. Content-based image retrieval (CBIR) systems offer
potential for efficient access to similar cases, yet lack standardized
evaluation and comprehensive studies. Building on prior studies for tumor
characterization via CBIR, this study advances CBIR research for volumetric
medical images through three key contributions: (1) a framework eliminating
reliance on pre-segmented data and organ-specific datasets, aligning with large
and unstructured image archiving systems, i.e. PACS in clinical practice; (2)
introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's
contextualized late interaction mechanism for 3D medical imaging; (3)
comprehensive evaluation across four tumor sites using three feature extractors
and three database configurations. Our evaluations highlight the significant
advantages of C-MIR. We demonstrate the successful adaptation of the late
interaction principle to volumetric medical images, enabling effective
context-aware re-ranking. A key finding is C-MIR's ability to effectively
localize the region of interest, eliminating the need for pre-segmentation of
datasets and offering a computationally efficient alternative to systems
relying on expensive data enrichment steps. C-MIR demonstrates promising
improvements in tumor flagging, achieving improved performance, particularly
for colon and lung tumors (p<0.05). C-MIR also shows potential for improving
tumor staging, warranting further exploration of its capabilities. Ultimately,
our work seeks to bridge the gap between advanced retrieval techniques and
their practical applications in healthcare, paving the way for improved
diagnostic processes.

</details>


### [17] [Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks](https://arxiv.org/abs/2507.17000)
*Jacob Piland,Chris Sweet,Adam Czajka*

Main category: cs.CV

TL;DR: 现有方法仅关注真类CAM，而我们提出的方法结合了真类和假类CAM，并在多个二元分类任务中提高了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在二元分类任务中，假设真类和假类CAM应在人类识别的重要特征上发散。

Method: 提出了三种新的显著性引导训练方法，结合了真类和假类CAM，并开发了一个用于识别重要特征的后验工具。

Result: 在合成人脸检测、生物特征呈现攻击检测和胸部X射线异常分类等多种二元分类任务上，提出的方法提高了深度学习模型的泛化能力。

Conclusion: 提出的方法在多种二元分类任务中提高了深度学习模型的泛化能力，优于仅使用真类CAM的传统方法。

Abstract: Existing saliency-guided training approaches improve model generalization by
incorporating a loss term that compares the model's class activation map (CAM)
for a sample's true-class ({\it i.e.}, correct-label class) against a human
reference saliency map. However, prior work has ignored the false-class CAM(s),
that is the model's saliency obtained for incorrect-label class. We hypothesize
that in binary tasks the true and false CAMs should diverge on the important
classification features identified by humans (and reflected in human saliency
maps). We use this hypothesis to motivate three new saliency-guided training
methods incorporating both true- and false-class model's CAM into the training
strategy and a novel post-hoc tool for identifying important features. We
evaluate all introduced methods on several diverse binary close-set and
open-set classification tasks, including synthetic face detection, biometric
presentation attack detection, and classification of anomalies in chest X-ray
scans, and find that the proposed methods improve generalization capabilities
of deep learning models over traditional (true-class CAM only) saliency-guided
training approaches. We offer source codes and model weights\footnote{GitHub
repository link removed to preserve anonymity} to support reproducible
research.

</details>


### [18] [Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models](https://arxiv.org/abs/2507.17008)
*Gaston Gustavo Rios,Pedro Dal Bianco,Franco Ronchetti,Facundo Quiroga,Oscar Stanchi,Santiago Ponte Ahón,Waldo Hasperué*

Main category: cs.CV

TL;DR: 通过使用ReACGAN和SPADE两种GAN架构生成合成数据，并结合真实图像，成功解决了手语手势数据集小且不平衡的问题，提高了分类准确率，并实现了跨数据集的泛化。


<details>
  <summary>Details</summary>
Motivation: 大多数手语手势数据集存在严重的数据量不足和类别不平衡问题，对模型训练构成了重大挑战。

Method: 本文探索了通过生成合成数据来增强手语手势分类器训练数据的有效性。研究比较了两种用于数据生成的生成对抗网络（GAN）架构：ReACGAN（利用标签信息通过辅助分类器条件化数据生成过程）和SPADE（利用空间自适应归一化基于姿态信息条件化生成）。

Result: 研究提出的技术将RWTH数据集的准确率提高了5%，并证明了其泛化能力，在不重新训练生成器的情况下，实现了与单一来源训练分类器相当的性能。

Conclusion: 该研究通过生成合成数据来增强手语手势分类器的训练数据，并成功将RWTH数据集的准确率提高了5%，解决了小样本和不平衡数据集的局限性。此外，该方法在不重新训练生成器的情况下，利用在HaGRID数据集上训练的基于姿态的生成技术，实现了跨不同手语数据集的泛化能力。

Abstract: Most sign language handshape datasets are severely limited and unbalanced,
posing significant challenges to effective model training. In this paper, we
explore the effectiveness of augmenting the training data of a handshape
classifier by generating synthetic data. We use an EfficientNet classifier
trained on the RWTH German sign language handshape dataset, which is small and
heavily unbalanced, applying different strategies to combine generated and real
images. We compare two Generative Adversarial Networks (GAN) architectures for
data generation: ReACGAN, which uses label information to condition the data
generation process through an auxiliary classifier, and SPADE, which utilizes
spatially-adaptive normalization to condition the generation on pose
information. ReACGAN allows for the generation of realistic images that align
with specific handshape labels, while SPADE focuses on generating images with
accurate spatial handshape configurations. Our proposed techniques improve the
current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the
limitations of small and unbalanced datasets. Additionally, our method
demonstrates the capability to generalize across different sign language
datasets by leveraging pose-based generation trained on the extensive HaGRID
dataset. We achieve comparable performance to single-source trained classifiers
without the need for retraining the generator.

</details>


### [19] [Transformer Based Building Boundary Reconstruction using Attraction Field Maps](https://arxiv.org/abs/2507.17038)
*Muhammad Kamran,Mohammad Moein Sheikholeslami,Andreas Wichmann,Gunho Sohn*

Main category: cs.CV

TL;DR: 提出了一种名为Decoupled-PolyGCN的新型GCN模型，用于从卫星图像自动提取建筑足迹，相比现有方法在精度和召回率上均有提升，旨在解决传统方法耗时耗力的问题。


<details>
  <summary>Details</summary>
Motivation: 随着遥感卫星数量的增加和其提供的高分辨率视觉数据的爆炸式增长，从卫星图像生成和更新空间地图变得至关重要。然而，从卫星图像重建空间地图，特别是提取建筑足迹，是一个复杂的计算机视觉任务，需要高层次的对象表示，而这仍然是一个持续的挑战，通常需要耗费人力和手动过程。

Method: 该研究提出的方法利用图卷积网络（GCNs），并结合了几何规则性、多尺度/多分辨率特征以及吸引力场图来增强建筑足迹重建的准确性和规则性。

Result: 提出的Decoupled-PolyGCN模型在建筑足迹重建任务上，相比现有方法在平均精度（AP）上提高了6%，在平均召回率（AR）上提高了10%，能够生成精确且规则化的建筑足迹，适用于各种复杂场景。

Conclusion: 该研究提出了一种名为Decoupled-PolyGCN的新型深度学习方法，利用图卷积网络（GCNs）来解决从卫星图像重建建筑足迹的挑战。该方法通过引入几何规则性、多尺度/多分辨率特征以及吸引力场图，显著提高了性能。

Abstract: In recent years, the number of remote satellites orbiting the Earth has grown
significantly, streaming vast amounts of high-resolution visual data to support
diverse applications across civil, public, and military domains. Among these
applications, the generation and updating of spatial maps of the built
environment have become critical due to the extensive coverage and detailed
imagery provided by satellites. However, reconstructing spatial maps from
satellite imagery is a complex computer vision task, requiring the creation of
high-level object representations, such as primitives, to accurately capture
the built environment. While the past decade has witnessed remarkable
advancements in object detection and representation using visual data,
primitives-based object representation remains a persistent challenge in
computer vision. Consequently, high-quality spatial maps often rely on
labor-intensive and manual processes. This paper introduces a novel deep
learning methodology leveraging Graph Convolutional Networks (GCNs) to address
these challenges in building footprint reconstruction. The proposed approach
enhances performance by incorporating geometric regularity into building
boundaries, integrating multi-scale and multi-resolution features, and
embedding Attraction Field Maps into the network. These innovations provide a
scalable and precise solution for automated building footprint extraction from
a single satellite image, paving the way for impactful applications in urban
planning, disaster management, and large-scale spatial analysis. Our model,
Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,
demonstrating its ability to deliver accurate and regularized building
footprints across diverse and challenging scenarios.

</details>


### [20] [Controllable Hybrid Captioner for Improved Long-form Video Understanding](https://arxiv.org/abs/2507.17047)
*Kuleen Sasse,Efsun Sarioglu Kayi,Arun Reddy*

Main category: cs.CV

TL;DR: 该研究提出了一种改进视频理解的方法，通过结合视频字幕和视觉语言模型来丰富文本摘要，使大型语言模型能更好地回答关于视频内容的问题。实验证明，该方法能生成更详细的字幕，提高回答的准确性，并优化了字幕生成流程。


<details>
  <summary>Details</summary>
Motivation: 为了解决长视频数据密集且高维的问题，并使大型语言模型（LLM）能够理解和推理视频内容以回答自然语言查询，研究者们提出了一种构建文本记忆的方法。现有的方法主要依赖于视频字幕，但这些字幕往往侧重于人类动作，忽略了场景中的其他重要信息。因此，需要一种方法来丰富文本记忆，包含静态场景描述，以扩展可回答问题的范围。

Method: 该研究提出了一种利用视频字幕和视觉语言模型（VLM）来增强视频理解的方法。首先，通过将视频分割成更小的片段，利用视频字幕生成器（如LaViLa）创建文本摘要。然后，为了弥补纯粹的动作描述的不足，研究者引入了视觉语言模型（如LLaVA）来生成静态场景描述，并将这些描述整合到字幕日志中。最终目标是构建一个更全面、更丰富的文本记忆，以便大型语言模型（LLM）能够更准确地回答关于视频内容的复杂问题。

Result: 研究者们通过实验探索了优化视频分割方法、整合静态场景描述以及微调视频字幕生成器等多种策略。结果表明，整合LLaVA VLM生成的静态场景描述可以生成更详细、更完整的字幕日志，从而扩展了可回答问题的范围。此外，通过微调LaViLa视频字幕生成器，使其能够同时生成动作和场景字幕，并能根据特殊输入信号词切换字幕类型，显著提高了字幕生成效率，优于使用单独的字幕模型。

Conclusion: 文章提出了一种名为“可控混合字幕生成器”的模型，该模型能够生成动作和场景字幕，并且可以通过特殊的输入信号词来切换字幕类型，从而提高了字幕生成效率和视频内容的完整性。

Abstract: Video data, especially long-form video, is extremely dense and
high-dimensional. Text-based summaries of video content offer a way to
represent query-relevant content in a much more compact manner than raw video.
In addition, textual representations are easily ingested by state-of-the-art
large language models (LLMs), which enable reasoning over video content to
answer complex natural language queries. To solve this issue, we rely on the
progressive construction of a text-based memory by a video captioner operating
on shorter chunks of the video, where spatio-temporal modeling is
computationally feasible. We explore ways to improve the quality of the
activity log comprised solely of short video captions. Because the video
captions tend to be focused on human actions, and questions may pertain to
other information in the scene, we seek to enrich the memory with static scene
descriptions using Vision Language Models (VLMs). Our video understanding
system relies on the LaViLa video captioner in combination with a LLM to answer
questions about videos. We first explored different ways of partitioning the
video into meaningful segments such that the textual descriptions more
accurately reflect the structure of the video content. Furthermore, we
incorporated static scene descriptions into the captioning pipeline using LLaVA
VLM, resulting in a more detailed and complete caption log and expanding the
space of questions that are answerable from the textual memory. Finally, we
have successfully fine-tuned the LaViLa video captioner to produce both action
and scene captions, significantly improving the efficiency of the captioning
pipeline compared to using separate captioning models for the two tasks. Our
model, controllable hybrid captioner, can alternate between different types of
captions according to special input tokens that signals scene changes detected
in the video.

</details>


### [21] [Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models](https://arxiv.org/abs/2507.17050)
*Tz-Ying Wu,Tahani Trigui,Sharath Nittur Sridhar,Anand Bodas,Subarna Tripathi*

Main category: cs.CV

TL;DR: VideoNarrator通过结合现有模型，生成详细、时间精确的视频描述，解决了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有针对视频理解的多模态大语言模型（MLLMs）在生成时间对齐的叙述和减少幻觉方面存在挑战，特别是在不熟悉的情况下。

Method: VideoNarrator是一个无需训练的流水线，利用现成的多模态大语言模型（MLLMs）和视觉语言模型（VLMs）作为字幕生成器、上下文提供者或字幕验证者，以生成结构化的视频内容快照。

Result: 实验结果表明，VideoNarrator的组件协同作用显著提高了视频叙述的质量和准确性，有效减少了幻觉并改善了时间对齐。

Conclusion: VideoNarrator通过结合多种视觉语言模型，有效提升了视频描述的质量和准确性，减少了幻觉，并改善了时间对齐。

Abstract: In this paper, we introduce VideoNarrator, a novel training-free pipeline
designed to generate dense video captions that offer a structured snapshot of
video content. These captions offer detailed narrations with precise
timestamps, capturing the nuances present in each segment of the video. Despite
advancements in multimodal large language models (MLLMs) for video
comprehension, these models often struggle with temporally aligned narrations
and tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator
addresses these challenges by leveraging a flexible pipeline where
off-the-shelf MLLMs and visual-language models (VLMs) can function as caption
generators, context providers, or caption verifiers. Our experimental results
demonstrate that the synergistic interaction of these components significantly
enhances the quality and accuracy of video narrations, effectively reducing
hallucinations and improving temporal alignment. This structured approach not
only enhances video understanding but also facilitates downstream tasks such as
video summarization and video question answering, and can be potentially
extended for advertising and marketing applications.

</details>


### [22] [Few-Shot Learning in Video and 3D Object Detection: A Survey](https://arxiv.org/abs/2507.17079)
*Md Meftahul Ferdaus,Kendall N. Niles,Joe Tom,Mahdi Abdelguerfi,Elias Ioup*

Main category: cs.CV

TL;DR: This paper surveys Few-shot learning (FSL) for video and 3D object detection, showing its potential to reduce annotation costs and enable real-world applications by leveraging spatiotemporal information and specialized networks, despite challenges like data sparsity and overfitting.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning (FSL) is valuable in object detection as it reduces the need for expensive manual data labeling. It is particularly beneficial for video object detection, where annotating objects across frames is more laborious than for static images. FSL for 3D object detection is crucial for practical applications like autonomous driving, as it minimizes costly 3D annotation needs.

Method: This paper surveys recent advancements in Few-shot learning (FSL) for video and 3D object detection. It examines techniques like tube proposals and temporal matching networks for video, and specialized point cloud networks and losses for 3D detection. It also discusses core issues such as balancing generalization and overfitting, integrating prototype matching, and handling data modality properties.

Result: The survey illuminates FSL's potential to minimize supervision needs and enable deployment across video, 3D, and other real-world applications by efficiently leveraging information across feature, temporal, and data modalities. It addresses challenges like sparsity, lack of texture, class imbalance, generalization vs. overfitting, and prototype matching.

Conclusion: Few-shot learning (FSL) has significant potential to reduce annotation requirements and enable real-world applications in video, 3D detection, and other domains by efficiently leveraging information across feature, temporal, and data modalities. This paper surveys recent advancements in FSL for video and 3D object detection, highlighting its promise for minimizing supervision needs.

Abstract: Few-shot learning (FSL) enables object detection models to recognize novel
classes given only a few annotated examples, thereby reducing expensive manual
data labeling. This survey examines recent FSL advances for video and 3D object
detection. For video, FSL is especially valuable since annotating objects
across frames is more laborious than for static images. By propagating
information across frames, techniques like tube proposals and temporal matching
networks can detect new classes from a couple examples, efficiently leveraging
spatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces
challenges like sparsity and lack of texture. Solutions integrate FSL with
specialized point cloud networks and losses tailored for class imbalance.
Few-shot 3D detection enables practical autonomous driving deployment by
minimizing costly 3D annotation needs. Core issues in both domains include
balancing generalization and overfitting, integrating prototype matching, and
handling data modality properties. In summary, FSL shows promise for reducing
annotation requirements and enabling real-world video, 3D, and other
applications by efficiently leveraging information across feature, temporal,
and data modalities. By comprehensively surveying recent advancements, this
paper illuminates FSL's potential to minimize supervision needs and enable
deployment across video, 3D, and other real-world applications.

</details>


### [23] [SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction](https://arxiv.org/abs/2507.17083)
*Zaipeng Duan,Chenxu Dang,Xuzhong Hu,Pei An,Junfeng Ding,Jie Zhan,Yunbiao Xu,Jie Ma*

Main category: cs.CV

TL;DR: SDG-OCC 是一种新的多模态 3D 占用预测网络，通过结合语义和深度引导的视图转换以及融合和蒸馏，解决了单模态方法的局限性，并在 nuScenes 数据集上取得了 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为单模态，基于相机的方案缺乏深度信息，而基于 LiDAR 的方案在遮挡问题上表现不佳。现有的轻量级方法（如 Lift-Splat-Shoot）存在深度估计不准确和未能充分利用 3D LiDAR 点的几何及语义信息的问题。

Method: 提出了一种名为 SDG-OCC 的新型多模态占用预测网络，该网络结合了联合语义和深度引导的视图转换以及面向占用驱动的主动蒸馏。视图转换通过扩散和双线性离散化整合像素语义和共点深度来构建准确的深度分布。主动蒸馏从多模态数据中提取丰富的语义信息，并根据 LiDAR 识别的区域选择性地将知识转移到图像特征。此外，还提出了 SDG-Fusion（仅使用融合）和 SDG-KL（融合蒸馏）以实现最佳性能和更快推理。

Result: SDG-OCC 在 Occ3D-nuScenes 数据集上实现了 SOTA 性能和实时处理，并在 SurroundOcc-nuScenes 数据集上表现出可比性能。

Conclusion: SDG-OCC 及其变体 SDG-Fusion 和 SDG-KL 在 Occ3D-nuScenes 数据集上实现了 SOTA 性能和实时处理，并在更具挑战性的 SurroundOcc-nuScenes 数据集上表现出可比性能，证明了其有效性和鲁棒性。

Abstract: Multimodal 3D occupancy prediction has garnered significant attention for its
potential in autonomous driving. However, most existing approaches are
single-modality: camera-based methods lack depth information, while LiDAR-based
methods struggle with occlusions. Current lightweight methods primarily rely on
the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth
estimation and fails to fully exploit the geometric and semantic information of
3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction
network called SDG-OCC, which incorporates a joint semantic and depth-guided
view transformation coupled with a fusion-to-occupancy-driven active
distillation. The enhanced view transformation constructs accurate depth
distributions by integrating pixel semantics and co-point depth through
diffusion and bilinear discretization. The fusion-to-occupancy-driven active
distillation extracts rich semantic information from multimodal data and
selectively transfers knowledge to image features based on LiDAR-identified
regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses
fusion alone, and SDG-KL, which integrates both fusion and distillation for
faster inference. Our method achieves state-of-the-art (SOTA) performance with
real-time processing on the Occ3D-nuScenes dataset and shows comparable
performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating
its effectiveness and robustness. The code will be released at
https://github.com/DzpLab/SDGOCC.

</details>


### [24] [Yume: An Interactive World Generation Model](https://arxiv.org/abs/2507.17744)
*Xiaofeng Mao,Shaoheng Lin,Zhen Li,Chuanhao Li,Wenshuo Peng,Tong He,Jiangmiao Pang,Mingmin Chi,Yu Qiao,Kaipeng Zhang*

Main category: cs.CV

TL;DR: Yume creates interactive worlds from images using MVDT and advanced sampling techniques. The preview version allows keyboard exploration and achieves high-quality results.


<details>
  <summary>Details</summary>
Motivation: To create an interactive, realistic, and dynamic world from images, text, or videos, controllable via peripheral devices or neural signals.

Method: The framework consists of four components: 1. Camera motion quantization for stable training and keyboard interaction. 2. Masked Video Diffusion Transformer (MVDT) with a memory module for autoregressive, infinite video generation. 3. A sampler incorporating training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) for improved visual quality and control. 4. Model acceleration through adversarial distillation and caching.

Result: The preview version \method achieves remarkable results in diverse scenes and applications, demonstrating high-fidelity and interactive video world generation.

Conclusion: Yume (specifically the preview version \method) successfully creates dynamic worlds from input images, allowing exploration via keyboard. The framework utilizes camera motion quantization, MVDT with a memory module, TTS-SDE, and model acceleration techniques for high-fidelity and interactive generation. Training on the \sekai dataset yielded remarkable results across diverse scenes.

Abstract: Yume aims to use images, text, or videos to create an interactive, realistic,
and dynamic world, which allows exploration and control using peripheral
devices or neural signals. In this report, we present a preview version of
\method, which creates a dynamic world from an input image and allows
exploration of the world using keyboard actions. To achieve this high-fidelity
and interactive video world generation, we introduce a well-designed framework,
which consists of four main components, including camera motion quantization,
video generation architecture, advanced sampler, and model acceleration. First,
we quantize camera motions for stable training and user-friendly interaction
using keyboard inputs. Then, we introduce the Masked Video Diffusion
Transformer~(MVDT) with a memory module for infinite video generation in an
autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)
and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)
are introduced to the sampler for better visual quality and more precise
control. Moreover, we investigate model acceleration by synergistic
optimization of adversarial distillation and caching mechanisms. We use the
high-quality world exploration dataset \sekai to train \method, and it achieves
remarkable results in diverse scenes and applications. All data, codebase, and
model weights are available on https://github.com/stdstu12/YUME. Yume will
update monthly to achieve its original goal. Project page:
https://stdstu12.github.io/YUME-Project/.

</details>


### [25] [FedVLM: Scalable Personalized Vision-Language Models through Federated Learning](https://arxiv.org/abs/2507.17088)
*Arkajyoti Mitra,Afia Anjum,Paul Agbaje,Mert Pesé,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: FedVLM是一个联邦LoRA微调框架，通过个性化LoRA（pLoRA）解决联邦环境中VLM的非独立同分布（non-iid）数据问题，显著提升了客户端的适应性。


<details>
  <summary>Details</summary>
Motivation: 解决在数据分散且非独立同分布（non-iid）的联邦环境中大规模微调视觉语言模型（VLM）的挑战，并解决现有参数高效微调方法（如LoRA）在处理异构客户端数据时泛化能力不足的问题。

Method: 提出FedVLM框架，采用联邦LoRA微调，并引入个性化LoRA（pLoRA）以适应各客户端独特的数据分布，从而提高局部适应性并维持全局模型聚合。

Result: 在RLAIF-V数据集上的实验表明，pLoRA相比标准的LoRA在客户端特定性能上提高了24.5%，在非独立同分布（non-iid）设置下展现出更优越的适应能力。

Conclusion: FedVLM提供了一个可扩展且高效的解决方案，用于在联邦环境中微调视觉语言模型（VLM），并在分布式学习场景中推进了个性化适应。

Abstract: Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot
learning capabilities, making them essential for several downstream tasks.
However, fine-tuning these models at scale remains challenging, particularly in
federated environments where data is decentralized and non-iid across clients.
Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation)
reduce computational overhead but struggle with heterogeneous client data,
leading to suboptimal generalization. To address these challenges, we propose
FedVLM, a federated LoRA fine-tuning framework that enables decentralized
adaptation of VLMs while preserving model privacy and reducing reliance on
centralized training. To further tackle data heterogeneity, we introduce
personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each
client's unique data distribution, significantly improving local adaptation
while maintaining global model aggregation. Experiments on the RLAIF-V dataset
show that pLoRA improves client-specific performance by 24.5% over standard
LoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a
scalable and efficient solution for fine-tuning VLMs in federated settings,
advancing personalized adaptation in distributed learning scenarios.

</details>


### [26] [IONext: Unlocking the Next Era of Inertial Odometry](https://arxiv.org/abs/2507.17089)
*Shanshan Zhang,Siyue Wang,Tianshui Wen,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 IONext 的新型 CNN 骨干网络，用于视觉里程计。IONext 结合了 DADM 和 STGU 模块，能够同时捕捉全局运动模式和局部精细运动特征，并优化了时间建模。实验证明 IONext 在多个数据集上均优于现有的 Transformer 和 CNN 方法。


<details>
  <summary>Details</summary>
Motivation: Transformer 模型虽然擅长捕捉长程依赖关系，但在感知局部精细运动变化方面存在局限性，并且缺乏固有的归纳偏置，这会影响定位的准确性和泛化能力。现有研究表明，将大卷积核和 Transformer 风格的架构设计融入 CNN 可以有效扩大感受野，从而增强全局运动感知。受此启发，本文旨在开发一种能够同时捕获全局运动模式和局部精细运动特征的 CNN 模型。

Method: 本文提出了一种名为双翼自适应动态混合器 (DADM) 的新型基于 CNN 的模块，该模块能够自适应地从动态输入中捕获全局运动模式和局部细粒度运动特征。此外，还引入了时空门控单元 (STGU) 来选择性地提取时间域中的代表性且与任务相关的运动特征。基于 DADM 和 STGU，提出了名为 IONext 的新一代基于 CNN 的视觉里程计骨干网络。

Result: IONext 在六个公开数据集上进行了广泛的实验，结果表明其性能始终优于最先进的基于 Transformer 和 CNN 的方法。例如，在 RNIN 数据集上，与 iMOT 模型相比，IONext 将平均 ATE 降低了 10%，平均 RTE 降低了 12%。

Conclusion: IONext 在六个公开数据集上进行了广泛的实验，结果表明其性能始终优于最先进的基于 Transformer 和 CNN 的方法。例如，在 RNIN 数据集上，与 iMOT 模型相比，IONext 将平均 ATE 降低了 10%，平均 RTE 降低了 12%。

Abstract: Researchers have increasingly adopted Transformer-based models for inertial
odometry. While Transformers excel at modeling long-range dependencies, their
limited sensitivity to local, fine-grained motion variations and lack of
inherent inductive biases often hinder localization accuracy and
generalization. Recent studies have shown that incorporating large-kernel
convolutions and Transformer-inspired architectural designs into CNN can
effectively expand the receptive field, thereby improving global motion
perception. Motivated by these insights, we propose a novel CNN-based module
called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures
both global motion patterns and local, fine-grained motion features from
dynamic inputs. This module dynamically generates selective weights based on
the input, enabling efficient multi-scale feature aggregation. To further
improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),
which selectively extracts representative and task-relevant motion features in
the temporal domain. This unit addresses the limitations of temporal modeling
observed in existing CNN approaches. Built upon DADM and STGU, we present a new
CNN-based inertial odometry backbone, named Next Era of Inertial Odometry
(IONext). Extensive experiments on six public datasets demonstrate that IONext
consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based
methods. For instance, on the RNIN dataset, IONext reduces the average ATE by
10% and the average RTE by 12% compared to the representative model iMOT.

</details>


### [27] [Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation](https://arxiv.org/abs/2507.17121)
*Faisal Ahmed,Mohammad Alfrad Nobel Bhuiyan*

Main category: cs.CV

TL;DR: 提出了一种结合迁移学习和数据增强的深度学习框架，用于糖尿病视网膜病变（DR）的早期诊断，在二分类和五分类任务中均取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 早期通过自动眼底图像分析诊断糖尿病视网膜病变，可显著降低失明风险。

Method: 利用迁移学习和数据增强技术，评估了包括ResNet和EfficientNet在内的多种预训练卷积神经网络架构，并针对APTOS 2019数据集进行了二元和五分类的糖尿病视网膜病变分类。

Result: 在二分类任务中，准确率达到98.9%；在五分类任务中，准确率达到84.6%，优于现有方法。EfficientNet-B0和ResNet34在准确性和计算效率之间取得了最佳平衡。

Conclusion: 该框架结合了类别平衡增强和迁移学习，在糖尿病视网膜病变诊断方面表现出色，可扩展且准确，有潜力应用于实际临床环境。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and
early diagnosis through automated retinal image analysis can significantly
reduce the risk of blindness. This paper presents a robust deep learning
framework for both binary and five-class DR classification, leveraging transfer
learning and extensive data augmentation to address the challenges of class
imbalance and limited training data. We evaluate a range of pretrained
convolutional neural network architectures, including variants of ResNet and
EfficientNet, on the APTOS 2019 dataset.
  For binary classification, our proposed model achieves a state-of-the-art
accuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of
98.9%, and an AUC of 99.4%. In the more challenging five-class severity
classification task, our model obtains a competitive accuracy of 84.6% and an
AUC of 94.1%, outperforming several existing approaches. Our findings also
demonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between
accuracy and computational efficiency across both tasks.
  These results underscore the effectiveness of combining class-balanced
augmentation with transfer learning for high-performance DR diagnosis. The
proposed framework provides a scalable and accurate solution for DR screening,
with potential for deployment in real-world clinical environments.

</details>


### [28] [Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors](https://arxiv.org/abs/2507.17577)
*Chen Ma,Xinjie Xu,Shuyu Cheng,Qi Xuan*

Main category: cs.CV

TL;DR: 提出了一种先验引导方法，通过利用代理模型的迁移先验来提高硬标签黑盒对抗攻击的查询效率，并在理论和实践上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒对抗攻击中的硬标签攻击问题，该问题只能获得 top-1 预测标签，并改进现有方法中用于减少查询数量的“符号技巧”的梯度估计。

Method: 利用来自代理模型的基于迁移的先验，并通过在由先验和随机方向构成的子空间上投影真实梯度来近似梯度估计，从而在查询高效的方式下整合先验。

Result: 在ImageNet和CIFAR-10数据集上进行了广泛的实验，证明了该方法在查询效率方面显著优于11种最先进的方法。

Conclusion: 本文提出的先验引导方法在查询效率方面显著优于11种最先进的方法，并且在理论和实践上都提高了射线搜索的效率。

Abstract: One of the most practical and challenging types of black-box adversarial
attacks is the hard-label attack, where only the top-1 predicted label is
available. One effective approach is to search for the optimal ray direction
from the benign image that minimizes the $\ell_p$-norm distance to the
adversarial region. The unique advantage of this approach is that it transforms
the hard-label attack into a continuous optimization problem. The objective
function value is the ray's radius, which can be obtained via binary search at
a high query cost. Existing methods use a "sign trick" in gradient estimation
to reduce the number of queries. In this paper, we theoretically analyze the
quality of this gradient estimation and propose a novel prior-guided approach
to improve ray search efficiency both theoretically and empirically.
Specifically, we utilize the transfer-based priors from surrogate models, and
our gradient estimators appropriately integrate them by approximating the
projection of the true gradient onto the subspace spanned by these priors and
random directions, in a query-efficient manner. We theoretically derive the
expected cosine similarities between the obtained gradient estimators and the
true gradient, and demonstrate the improvement achieved by incorporating
priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that
our approach significantly outperforms 11 state-of-the-art methods in terms of
query efficiency.

</details>


### [29] [ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation](https://arxiv.org/abs/2507.17149)
*Bo Fang,Jianan Fan,Dongnan Liu,Hang Chang,Gerald J. Shami,Filip Braet,Weidong Cai*

Main category: cs.CV

TL;DR: ScSAM通过融合SAM和MAE先验知识，解决了亚细胞分割中的特征偏差和细节捕捉问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决亚细胞分割模型中由于亚细胞结构形态和分布的变异性而导致的特征学习偏差问题。现有方法忽视了特征多样性，导致训练偏差。SAM虽然提供了丰富的特征表示，但在亚细胞分割中面临标签空间差异和忽略精细空间细节的挑战。

Method: 提出了一种名为ScSAM的方法，该方法融合了预训练的Segment Anything Model (SAM) 和 Masked Autoencoder (MAE) 引导的细胞先验知识。具体包括设计了一个特征对齐和融合模块来对齐预训练的嵌入，并利用基于余弦相似度矩阵的类提示编码器来激活特定类别的特征。

Result: 在多种亚细胞图像数据集上的广泛实验表明，ScSAM的性能优于现有最先进的方法。

Conclusion: ScSAM通过融合预训练SAM和MAE引导的细胞先验知识，增强了特征鲁棒性，缓解了数据不平衡导致的训练偏差。实验证明ScSAM在多种亚细胞图像数据集上优于现有最先进方法。

Abstract: The significant morphological and distributional variability among
subcellular components poses a long-standing challenge for learning-based
organelle segmentation models, significantly increasing the risk of biased
feature learning. Existing methods often rely on single mapping relationships,
overlooking feature diversity and thereby inducing biased training. Although
the Segment Anything Model (SAM) provides rich feature representations, its
application to subcellular scenarios is hindered by two key challenges: (1) The
variability in subcellular morphology and distribution creates gaps in the
label space, leading the model to learn spurious or biased features. (2) SAM
focuses on global contextual understanding and often ignores fine-grained
spatial details, making it challenging to capture subtle structural alterations
and cope with skewed data distributions. To address these challenges, we
introduce ScSAM, a method that enhances feature robustness by fusing
pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge
to alleviate training bias from data imbalance. Specifically, we design a
feature alignment and fusion module to align pre-trained embeddings to the same
feature space and efficiently combine different representations. Moreover, we
present a cosine similarity matrix-based class prompt encoder to activate
class-specific features to recognize subcellular categories. Extensive
experiments on diverse subcellular image datasets demonstrate that ScSAM
outperforms state-of-the-art methods.

</details>


### [30] [UNICE: Training A Universal Image Contrast Enhancer](https://arxiv.org/abs/2507.17157)
*Ruodai Cui,Lei Zhang*

Main category: cs.CV

TL;DR: UNICE通过生成和融合多重曝光序列，实现了通用的图像对比度增强，泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 现有图像对比度增强方法泛化能力差，即使在特定任务的不同数据集上表现也不佳。因此，探索一种可用于各种对比度增强任务的通用模型是非常重要的。

Method: 收集了46,928张HDR原始图像，并渲染了328,496张sRGB图像，通过多重曝光融合构建了多重曝光序列（MES）及其对应的伪sRGB真实值。训练了一个网络从单张sRGB图像生成MES，然后训练了另一个网络将生成的MES融合得到增强后的图像。

Result: UNICE在不同任务和不同数据集上表现出比现有方法更强的泛化能力，甚至在多个无参考图像质量指标上优于手动创建的真实值。

Conclusion: 该研究提出了UNICE，一个通用的图像对比度增强方法，通过生成多重曝光序列（MES）并进行融合，实现了跨任务和跨数据集的强大泛化能力，并且优于现有方法和手动标注的真实值。

Abstract: Existing image contrast enhancement methods are typically designed for
specific tasks such as under-/over-exposure correction, low-light and backlit
image enhancement, etc. The learned models, however, exhibit poor
generalization performance across different tasks, even across different
datasets of a specific task. It is important to explore whether we can learn a
universal and generalized model for various contrast enhancement tasks. In this
work, we observe that the common key factor of these tasks lies in the need of
exposure and contrast adjustment, which can be well-addressed if high-dynamic
range (HDR) inputs are available. We hence collect 46,928 HDR raw images from
public sources, and render 328,496 sRGB images to build multi-exposure
sequences (MES) and the corresponding pseudo sRGB ground-truths via
multi-exposure fusion. Consequently, we train a network to generate an MES from
a single sRGB image, followed by training another network to fuse the generated
MES into an enhanced image. Our proposed method, namely UNiversal Image
Contrast Enhancer (UNICE), is free of costly human labeling. However, it
demonstrates significantly stronger generalization performance than existing
image contrast enhancement methods across and within different tasks, even
outperforming manually created ground-truths in multiple no-reference image
quality metrics. The dataset, code and model are available at
https://github.com/BeyondHeaven/UNICE.

</details>


### [31] [DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing](https://arxiv.org/abs/2507.17158)
*Bharath Krishnamurthy,Ajita Rattani*

Main category: cs.CV

TL;DR: DOOMGAN 是一种用于可见光谱眼部生物特征的新型生成模型，可有效模拟融合攻击，提高攻击成功率并保持眼部特征的真实性。


<details>
  <summary>Details</summary>
Motivation: 可见光谱眼部生物特征在准确性、抗欺骗性和非侵入性方面表现出色，但容易受到融合攻击的威胁。目前，可见光谱眼部数据中的融合攻击研究不足，需要先进的生成模型来模拟这些攻击，同时保持详细的眼部特征。

Method: DOOMGAN 模型，包含基于地标的可见眼部解剖结构编码、用于逼真融合合成的注意力引导生成以及用于优化收敛的多方面损失的动态加权。

Result: DOOMGAN 在严格的阈值下实现了比基线方法高出 20% 的攻击成功率，在椭圆虹膜结构生成方面提高了 20%，在凝视一致性方面提高了 30%。此外，还发布了首个全面的眼部融合数据集。

Conclusion: 鉴于可见光谱眼部生物特征在准确性、抗欺骗性和非侵入性方面表现出色，因此它们已成为一种重要的生物识别模态。然而，融合攻击（通过混合多个个体的特征创建的合成生物特征）对生物识别系统的完整性构成了威胁。虽然融合攻击在近红外虹膜和面部生物特征方面得到了广泛研究，但在可见光谱眼部数据方面的融合攻击仍未得到充分探索。为了应对这一挑战，我们提出了 DOOMGAN，一个包含基于地标的可见眼部解剖结构编码、用于逼真融合合成的注意力引导生成以及用于优化收敛的多方面损失的动态加权的模型。DOOMGAN 在严格的阈值下实现了比基线方法高出 20% 的攻击成功率，在椭圆虹膜结构生成方面提高了 20%，在凝视一致性方面提高了 30%。此外，我们还发布了首个全面的眼部融合数据集，以支持该领域的进一步研究。

Abstract: Ocular biometrics in the visible spectrum have emerged as a prominent
modality due to their high accuracy, resistance to spoofing, and non-invasive
nature. However, morphing attacks, synthetic biometric traits created by
blending features from multiple individuals, threaten biometric system
integrity. While extensively studied for near-infrared iris and face
biometrics, morphing in visible-spectrum ocular data remains underexplored.
Simulating such attacks demands advanced generation models that handle
uncontrolled conditions while preserving detailed ocular features like iris
boundaries and periocular textures. To address this gap, we introduce DOOMGAN,
that encompasses landmark-driven encoding of visible ocular anatomy,
attention-guided generation for realistic morph synthesis, and dynamic
weighting of multi-faceted losses for optimized convergence. DOOMGAN achieves
over 20% higher attack success rates than baseline methods under stringent
thresholds, along with 20% better elliptical iris structure generation and 30%
improved gaze consistency. We also release the first comprehensive ocular
morphing dataset to support further research in this domain.

</details>


### [32] [Multi-Scale PCB Defect Detection with YOLOv8 Network Improved via Pruning and Lightweight Network](https://arxiv.org/abs/2507.17176)
*Li Pingzhen,Xu Sheng,Chen Jing,Su Chengyue*

Main category: cs.CV

TL;DR: 为解决传统 PCB 缺陷检测精度和速度不足的问题，本研究提出了一种基于 YOLOv8 的多尺度检测方法。通过优化骨干网络（Ghost-HGNetv2）、颈部网络（C2f-Faster）、检测头（GCDetect）以及引入新的损失函数（Inner-MPDIoU）和自适应剪枝，显著提升了对微小缺陷的检测能力。实验证明，该模型在精度和速度上均优于 YOLOv8n，mAP 指标有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统 PCB 缺陷检测模型难以同时兼顾精度和计算成本，无法满足高精度、实时检测微小缺陷的需求，因此需要一种更优的检测方法。

Method: 本研究提出了一种改进的 YOLOv8 多尺度 PCB 缺陷检测方法。具体包括：1. 骨干网络采用参数量更少的 Ghost-HGNetv2 结构，并利用多级特征提取图像语义特征。2. 颈部网络集成参数量少的 C2f-Faster，增强多级特征融合能力。3. 检测头部分设计了新的 GCDetect 检测头，实现了边界框和类别的预测共享 GroupConv 权重，并使用少量分组卷积完成回归和分类任务。4. 设计了 Inner-MPDIoU 边界损失函数，以提高小目标的检测和定位精度。5. 通过优化的自适应剪枝率对模型进行剪枝，进一步降低模型复杂度。

Result: 该模型在公开 PCB 缺陷数据集上的实验结果显示，mAP0.5 达到了 99.32%，mAP0.5:0.9 达到了 75.18%，相比 YOLOv8n 提升了 10.13%。这表明该模型在检测速度和精度方面均具有优势。

Conclusion: 该研究提出了一种改进的 YOLOv8 多尺度 PCB 缺陷检测方法，通过采用 Ghost-HGNetv2 作为骨干网络，结合 C2f-Faster 增强多级特征融合，并设计了 GCDetect 检测头以减少参数量并保持检测精度。此外，还引入了 Inner-MPDIoU 边界损失函数来提升小目标检测能力，并通过自适应剪枝优化进一步降低模型复杂度。实验结果表明，该模型在公开 PCB 缺陷数据集上取得了显著的性能提升，mAP0.5 达到 99.32%，mAP0.5:0.9 达到 75.18%，相比 YOLOv8n 提升了 10.13%。

Abstract: With the high density of printed circuit board (PCB) design and the high
speed of production, the traditional PCB defect detection model is difficult to
take into account the accuracy and computational cost, and cannot meet the
requirements of high accuracy and real-time detection of tiny defects.
Therefore, in this paper, a multi-scale PCB defect detection method is improved
with YOLOv8 using a comprehensive strategy of tiny target sensitivity strategy,
network lightweighting and adaptive pruning, which is able to improve the
detection speed and accuracy by optimizing the backbone network, the neck
network and the detection head, the loss function and the adaptive pruning
rate. Firstly, a Ghost-HGNetv2 structure with fewer parameters is used in the
backbone network, and multilevel features are used to extract image semantic
features to discover accurate defects. Secondly, we integrate C2f-Faster with
small number of parameters in the neck section to enhance the ability of
multi-level feature fusion. Next, in the Head part, we design a new GCDetect
detection head, which allows the prediction of bounding boxes and categories to
share the weights of GroupConv, and uses a small number of grouping
convolutions to accomplish the regression and classification tasks, which
significantly reduces the number of parameters while maintaining the accuracy
of detection. We also design the Inner-MPDIoU boundary loss function to improve
the detection and localization of tiny targets. Finally, the model was pruned
by an optimized adaptive pruning rate to further reduce the complexity of the
model. Experimental results show that the model exhibits advantages in terms of
accuracy and speed. On the publicly available PCB defect dataset, mAP0.5
reaches 99.32% and mAP0.5:0.9 reaches 75.18%, which is 10.13% higher compared
to YOLOv8n.

</details>


### [33] [Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment](https://arxiv.org/abs/2507.17182)
*Linghe Meng,Jiarun Song*

Main category: cs.CV

TL;DR: 针对AIGC质量评估的挑战，提出了一种多层次视觉表示范式，通过提取、融合和聚合多层次特征来提升评估的准确性，并开发了MGLF-Net和MPEF-Net两种网络以分别应对感知质量评估和文本到图像对应任务。


<details>
  <summary>Details</summary>
Motivation: 现有的AI生成内容（AIGC）质量评估方法主要依赖单层次视觉特征，难以捕捉AIGC图像中的复杂失真。为了解决这一局限性，需要一种能够处理多维度挑战（从低级视觉感知到高级语义理解）的方法。

Method: 提出了一种多层次视觉表示范式，包括多层次特征提取、分层融合和联合聚合。基于此范式，开发了两种网络：多层次全局-局部融合网络（MGLF-Net），用于感知质量评估，通过双CNN和Transformer视觉骨干提取互补的局部和全局特征；多层次提示嵌入融合网络（MPEF-Net），通过在每个特征级别将提示语义嵌入到视觉特征融合过程中，实现文本到图像对应。

Result: MGLF-Net和MPEF-Net在基准测试中取得了优异的性能。

Conclusion: 实验结果表明，所提出的多层次视觉评估范式在感知质量评估和文本到图像对应任务上均表现出色，验证了其有效性。

Abstract: The quality assessment of AI-generated content (AIGC) faces multi-dimensional
challenges, that span from low-level visual perception to high-level semantic
understanding. Existing methods generally rely on single-level visual features,
limiting their ability to capture complex distortions in AIGC images. To
address this limitation, a multi-level visual representation paradigm is
proposed with three stages, namely multi-level feature extraction, hierarchical
fusion, and joint aggregation. Based on this paradigm, two networks are
developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net)
is designed for the perceptual quality assessment, extracting complementary
local and global features via dual CNN and Transformer visual backbones. The
Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image
correspondence by embedding prompt semantics into the visual feature fusion
process at each feature level. The fused multi-level features are then
aggregated for final evaluation. Experiments on benchmarks demonstrate
outstanding performance on both tasks, validating the effectiveness of the
proposed multi-level visual assessment paradigm.

</details>


### [34] [Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification](https://arxiv.org/abs/2507.17185)
*M. A. Rasel,Sameem Abdul Kareem,Zhenli Kwan,Nik Aimee Azizah Faheem,Winn Hui Han,Rebecca Kai Jan Choong,Shin Shen Yong,Unaizah Obaidellah*

Main category: cs.CV

TL;DR: 该研究提出了一种结合图像处理和CNN的方法，用于分析皮肤镜图像中的病变形状以辅助皮肤病诊断，特别是识别不对称病变。该方法在不对称病变检测和病变形状分类方面均取得了优于现有研究的成果。


<details>
  <summary>Details</summary>
Motivation: 为了在皮肤镜图像中更有效地识别皮肤病变（尤其是黑色素瘤），该研究旨在提供一种辅助诊断技术。病变形状，特别是其不对称性，是诊断黑色素瘤的关键标准之一。该方法旨在帮助非专业人士理解不对称病变的诊断标准，并提高诊断的准确性。

Method: 该研究首先对非标注数据集进行了标注，基于临床评估引入对称性信息。随后，提出了一种监督学习图像处理算法，用于分析病变形状的几何模式。此外，利用预训练的卷积神经网络（CNN）提取病变形状、颜色和纹理特征，并训练多类支持向量机（SVM）分类器。

Result: 在基于几何学的实验中，不对称皮肤病变的检测率为99.00%。在基于CNN的实验中，在分类病变形状（不对称、半对称、对称）方面，最佳性能表现为Kappa得分为94%，宏观F1得分为95%，加权F1得分为97%。

Conclusion: 该研究提出了一种基于图像处理和卷积神经网络（CNN）的算法，用于分析皮肤镜图像中病变形状的几何模式，以辅助诊断皮肤病，特别是识别不对称病变。研究结果表明，该方法在检测不对称病变方面达到了99.00%的检测率，并且在分类病变形状（不对称、半对称、对称）方面取得了94%的Kappa分数、95%的宏观F1分数和97%的加权F1分数，优于现有文献中的最先进方法。

Abstract: In dermoscopic images, which allow visualization of surface skin structures
not visible to the naked eye, lesion shape offers vital insights into skin
diseases. In clinically practiced methods, asymmetric lesion shape is one of
the criteria for diagnosing melanoma. Initially, we labeled data for a
non-annotated dataset with symmetrical information based on clinical
assessments. Subsequently, we propose a supporting technique, a supervised
learning image processing algorithm, to analyze the geometrical pattern of
lesion shape, aiding non-experts in understanding the criteria of an asymmetric
lesion. We then utilize a pre-trained convolutional neural network (CNN) to
extract shape, color, and texture features from dermoscopic images for training
a multiclass support vector machine (SVM) classifier, outperforming
state-of-the-art methods from the literature. In the geometry-based experiment,
we achieved a 99.00% detection rate for dermatological asymmetric lesions. In
the CNN-based experiment, the best performance is found with 94% Kappa Score,
95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes
(Asymmetric, Half-Symmetric, and Symmetric).

</details>


### [35] [Vec2Face+ for Face Dataset Generation](https://arxiv.org/abs/2507.17192)
*Haiyu Wu,Jaskirat Singh,Sicong Tian,Liang Zheng,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: Vec2Face+通过改进的生成策略合成了高质量的人脸识别训练数据集（VFace系列），在准确性上超越了现有合成数据集和真实世界数据集CASIA-WebFace，但合成数据训练的模型存在身份一致性和偏差问题，需要未来研究关注。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别训练数据合成方法在增加类内属性变化时，忽视了保持类内身份一致性的必要性。

Method: 提出Vec2Face+模型，直接从图像特征生成人脸图像，并可控地调整身份和属性。通过三种策略实现高质量训练数据：1. 采样区分度高的向量以保证身份分离性；2. 提出AttrOP算法增加属性变化；3. 提出基于LoRA的姿态控制，以更高效且保持身份一致性的方式生成侧面人脸。

Result: 生成了包含10K身份的VFace10K合成数据集，使人脸识别模型在七个真实世界测试集上达到最先进的准确性。将数据集规模扩大到4M和12M（VFace100K和VFace300K），在五个真实世界测试集上的准确性超过了真实数据集CASIA-WebFace。

Conclusion: Vec2Face+生成的合成数据在多个真实世界测试集上实现了最先进的准确性，并且在某些情况下优于真实世界的数据集，这是首次合成数据集在准确性上超越CASIA-WebFace。然而，研究也指出，在双胞胎验证方面，只有少数合成数据集能超越随机猜测，并且使用合成数据训练的模型比使用真实数据训练的模型存在更大的偏差。

Abstract: When synthesizing identities as face recognition training data, it is
generally believed that large inter-class separability and intra-class
attribute variation are essential for synthesizing a quality dataset. % This
belief is generally correct, and this is what we aim for. However, when
increasing intra-class variation, existing methods overlook the necessity of
maintaining intra-class identity consistency. % To address this and generate
high-quality face training data, we propose Vec2Face+, a generative model that
creates images directly from image features and allows for continuous and easy
control of face identities and attributes. Using Vec2Face+, we obtain datasets
with proper inter-class separability and intra-class variation and identity
consistency using three strategies: 1) we sample vectors sufficiently different
from others to generate well-separated identities; 2) we propose an AttrOP
algorithm for increasing general attribute variations; 3) we propose LoRA-based
pose control for generating images with profile head poses, which is more
efficient and identity-preserving than AttrOP. % Our system generates VFace10K,
a synthetic face dataset with 10K identities, which allows an FR model to
achieve state-of-the-art accuracy on seven real-world test sets. Scaling the
size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets
yield higher accuracy than the real-world training dataset, CASIA-WebFace, on
five real-world test sets. This is the first time a synthetic dataset beats the
CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11
synthetic datasets outperforms random guessing (\emph{i.e., 50\%}) in twin
verification and that models trained with synthetic identities are more biased
than those trained with real identities. Both are important aspects for future
investigation.

</details>


### [36] [DesignLab: Designing Slides Through Iterative Detection and Correction](https://arxiv.org/abs/2507.17202)
*Jooyeol Yun,Heng Wang,Yotaro Shimose,Jaegul Choo,Shingo Takamatsu*

Main category: cs.CV

TL;DR: DesignLab通过引入设计评审者和设计贡献者两个角色，并利用微调后的大型语言模型进行迭代式设计优化，能够生成比现有方法更高质量的演示幻灯片。


<details>
  <summary>Details</summary>
Motivation: 现有自动化设计工具虽然能提供布局和配色方案，但在实际工作流程的关键环节——输出内容的精炼方面能力不足。本研究旨在解决非专家在制作高质量演示幻灯片时遇到的挑战，即设计过程的复杂性以及现有工具无法进行有效迭代优化的问题。

Method: 设计一个名为DesignLab的系统，将设计过程分解为两个角色：设计评审者（识别设计问题）和设计贡献者（修正设计问题）。利用微调后的大型语言模型分别扮演这两个角色，并通过模拟中间草稿（引入受控扰动）来训练模型，使评审者学习识别设计错误，贡献者学习修正设计错误，从而实现迭代式设计优化。

Result: 实验结果表明，DesignLab生成的幻灯片质量优于现有的设计生成方法，包括一款商业工具。该系统通过拥抱设计的迭代本质，能够生成专业、精良的幻灯片。

Conclusion: DesignLab通过将设计过程分解为设计评审者和设计贡献者两个角色，并利用微调后的大型语言模型进行迭代优化，能够生成比现有设计生成方法（包括商业工具）更高质量的演示幻灯片。

Abstract: Designing high-quality presentation slides can be challenging for non-experts
due to the complexity involved in navigating various design choices. Numerous
automated tools can suggest layouts and color schemes, yet often lack the
ability to refine their own output, which is a key aspect in real-world
workflows. We propose DesignLab, which separates the design process into two
roles, the design reviewer, who identifies design-related issues, and the
design contributor who corrects them. This decomposition enables an iterative
loop where the reviewer continuously detects issues and the contributor
corrects them, allowing a draft to be further polished with each iteration,
reaching qualities that were unattainable. We fine-tune large language models
for these roles and simulate intermediate drafts by introducing controlled
perturbations, enabling the design reviewer learn design errors and the
contributor learn how to fix them. Our experiments show that DesignLab
outperforms existing design-generation methods, including a commercial tool, by
embracing the iterative nature of designing which can result in polished,
professional slides.

</details>


### [37] [VBCD: A Voxel-Based Framework for Personalized Dental Crown Design](https://arxiv.org/abs/2507.17205)
*Linda Wei,Chang Liu,Wenran Zhang,Zengji Zhang,Shaoting Zhang,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出了一种基于体素的自动化牙团设计（VBCD）框架，通过使用曲率和边缘线惩罚损失（CMPL）以及基于FDI牙齿编号系统的位置提示，提高了牙冠设计的精度和质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决牙科技师手动设计修复性牙冠耗时费力的问题。

Method: 提出了一种新颖的基于体素的自动化牙冠设计（VBCD）框架。该框架首先从体素化的口内扫描生成初始粗糙牙冠，然后通过结合了感知距离的细化器来提高精度和质量。在训练阶段，采用了曲率和边缘线惩罚损失（CMPL）来增强生成牙冠与边缘线的对齐。此外，还引入了基于FDI牙齿编号系统的位置提示，以进一步提高生成牙冠的精度。

Result: 在大量口内扫描数据集上的评估表明，所提出的方法在准确性和质量方面优于现有方法。

Conclusion: 本文提出的VBCD框架在大量口内扫描数据集上进行了评估，结果表明该方法优于现有方法，为个性化牙冠设计提供了稳健的解决方案。

Abstract: The design of restorative dental crowns from intraoral scans is
labor-intensive for dental technicians. To address this challenge, we propose a
novel voxel-based framework for automated dental crown design (VBCD). The VBCD
framework generates an initial coarse dental crown from voxelized intraoral
scans, followed by a fine-grained refiner incorporating distance-aware
supervision to improve accuracy and quality. During the training stage, we
employ the Curvature and Margin line Penalty Loss (CMPL) to enhance the
alignment of the generated crown with the margin line. Additionally, a
positional prompt based on the FDI tooth numbering system is introduced to
further improve the accuracy of the generated dental crowns. Evaluation on a
large-scale dataset of intraoral scans demonstrated that our approach
outperforms existing methods, providing a robust solution for personalized
dental crown design.

</details>


### [38] [A Low-Cost Machine Learning Approach for Timber Diameter Estimation](https://arxiv.org/abs/2507.17219)
*Fatemeh Hasanzadeh Fard,Sanaz Hasanzadeh Fard,Mehdi Jonoobi*

Main category: cs.CV

TL;DR: 使用 YOLOv5 自动估算木材直径，准确率达 0.64，适用于实际工业环境。


<details>
  <summary>Details</summary>
Motivation: 木材加工行业（例如锯木厂和 MDF 生产线）需要准确高效地识别木材的种类和厚度。传统方法依赖人工，速度慢、不一致且易出错。

Method: 本研究采用 YOLOv5 目标检测算法，在公共数据集 (TimberSeg 1.0) 上进行微调，通过边界框尺寸检测单独的原木并估计厚度。

Result: 该模型实现了 0.64 的平均精度均值 (mAP@0.5)，即使在计算资源有限的情况下也能可靠地检测原木。

Conclusion: 该模型有望集成到现有工作流程中，用于现场库存管理和初步分类，特别是在中小型企业中。

Abstract: The wood processing industry, particularly in facilities such as sawmills and
MDF production lines, requires accurate and efficient identification of species
and thickness of the wood. Although traditional methods rely heavily on expert
human labor, they are slow, inconsistent, and prone to error, especially when
processing large volumes. This study focuses on practical and cost-effective
machine learning frameworks that automate the estimation of timber log diameter
using standard RGB images captured under real-world working conditions. We
employ the YOLOv5 object detection algorithm, fine-tuned on a public dataset
(TimberSeg 1.0), to detect individual timber logs and estimate thickness
through bounding-box dimensions. Unlike previous methods that require expensive
sensors or controlled environments, this model is trained on images taken in
typical industrial sheds during timber delivery. Experimental results show that
the model achieves a mean Average Precision (mAP@0.5) of 0.64, demonstrating
reliable log detection even with modest computing resources. This lightweight,
scalable solution holds promise for practical integration into existing
workflows, including on-site inventory management and preliminary sorting,
particularly in small and medium-sized operations.

</details>


### [39] [PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models](https://arxiv.org/abs/2507.17220)
*Jiansong Wan,Chengming Zhou,Jinkua Liu,Xiangge Huang,Xiaoyu Chen,Xiaohan Yi,Qisen Yang,Baiting Zhu,Xin-Qiang Cai,Lixing Liu,Rushuai Yang,Chuheng Zhang,Sherif Abdelfattah,Hayong Shin,Pushi Zhang,Li Zhao,Jiang Bian*

Main category: cs.CV

TL;DR: PIG-Nav 通过改进预训练策略（早期融合、辅助任务）和数据处理（游戏视频）来增强视觉导航模型，显著提高了零样本和微调性能，并减少了对标注数据的需求。


<details>
  <summary>Details</summary>
Motivation: 为了在多样化的环境中实现可泛化的导航，并提高在未见过的设置下的零样本性能，研究了用于视觉导航的预训练（基础）模型。

Method: PIG-Nav (Pretrained Image-Goal Navigation) 的方法包括：1. 模型方面：集成早期融合网络结构，通过适当预训练的 Vision Transformer (ViT) 图像编码器结合视觉观察和目标图像；引入辅助任务来增强全局导航表示学习。2. 数据集方面：提出一种新颖的数据预处理流程，用于大规模游戏视频数据集的导航模型训练，并通过增加具有不同游戏玩法视频的数据集来提高模型性能。

Result: PIG-Nav 在两种复杂模拟环境和一种真实环境中，在零样本设置下平均提高了 22.6%，在微调设置下平均提高了 37.5%，超越了现有的视觉导航基础模型。该模型在需要更少微调数据的情况下仍保持了有竞争力的性能，显示了其在真实世界中部署的潜力。

Conclusion: PIG-Nav 在两种复杂模拟环境和一种真实环境中，在零样本设置下平均提高了 22.6%，在微调设置下平均提高了 37.5%，超越了现有的视觉导航基础模型。该模型在需要更少微调数据的情况下仍保持了有竞争力的性能，显示了其在真实世界中部署的潜力。

Abstract: Recent studies have explored pretrained (foundation) models for vision-based
robotic navigation, aiming to achieve generalizable navigation and positive
transfer across diverse environments while enhancing zero-shot performance in
unseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal
Navigation), a new approach that further investigates pretraining strategies
for vision-based navigation models and contributes in two key areas.
Model-wise, we identify two critical design choices that consistently improve
the performance of pretrained navigation models: (1) integrating an
early-fusion network structure to combine visual observations and goal images
via appropriately pretrained Vision Transformer (ViT) image encoder, and (2)
introducing suitable auxiliary tasks to enhance global navigation
representation learning, thus further improving navigation performance.
Dataset-wise, we propose a novel data preprocessing pipeline for efficiently
labeling large-scale game video datasets for navigation model training. We
demonstrate that augmenting existing open navigation datasets with diverse
gameplay videos improves model performance. Our model achieves an average
improvement of 22.6% in zero-shot settings and a 37.5% improvement in
fine-tuning settings over existing visual navigation foundation models in two
complex simulated environments and one real-world environment. These results
advance the state-of-the-art in pretrained image-goal navigation models.
Notably, our model maintains competitive performance while requiring
significantly less fine-tuning data, highlighting its potential for real-world
deployment with minimal labeled supervision.

</details>


### [40] [MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training](https://arxiv.org/abs/2507.17239)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: MaskedCLIP通过半监督预训练，结合配对和非配对数据，提升医学影像分析基础模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学影像分析基础模型主要依赖配对或非配对图像数据进行预训练，这限制了模型学习更丰富、更全面的图像特征的能力。本研究旨在探索半监督预训练方法，以充分利用配对和非配对图像数据，从而学习更具通用性的图像特征。

Method: 提出了一种名为MaskedCLIP的框架，该框架结合了掩码图像建模（Masked Image Modeling）和对比语言-图像预训练（Contrastive Language-Image Pre-training），并引入了一个桥接Transformer来连接两种不同类型数据的特征空间，以及一个掩码知识蒸馏损失函数，以实现半监督预训练。

Result: 通过在视网膜图像分析任务上的广泛实验证明，该方法比现有方法更有效，并且数据效率更高，能够学习到更具通用性的图像特征。

Conclusion: 该研究提出了一种名为MaskedCLIP的半监督预训练框架，通过结合配对和非配对图像数据，利用掩码图像建模和对比语言-图像预训练，有效提升了医学影像分析中基础模型学习的性能和数据效率。

Abstract: Foundation models have recently gained tremendous popularity in medical image
analysis. State-of-the-art methods leverage either paired image-text data via
vision-language pre-training or unpaired image data via self-supervised
pre-training to learn foundation models with generalizable image features to
boost downstream task performance. However, learning foundation models
exclusively on either paired or unpaired image data limits their ability to
learn richer and more comprehensive image features. In this paper, we
investigate a novel task termed semi-supervised vision-language pre-training,
aiming to fully harness the potential of both paired and unpaired image data
for foundation model learning. To this end, we propose MaskedCLIP, a
synergistic masked image modeling and contrastive language-image pre-training
framework for semi-supervised vision-language pre-training. The key challenge
in combining paired and unpaired image data for learning a foundation model
lies in the incompatible feature spaces derived from these two types of data.
To address this issue, we propose to connect the masked feature space with the
CLIP feature space with a bridge transformer. In this way, the more semantic
specific CLIP features can benefit from the more general masked features for
semantic feature extraction. We further propose a masked knowledge distillation
loss to distill semantic knowledge of original image features in CLIP feature
space back to the predicted masked image features in masked feature space. With
this mutually interactive design, our framework effectively leverages both
paired and unpaired image data to learn more generalizable image features for
downstream tasks. Extensive experiments on retinal image analysis demonstrate
the effectiveness and data efficiency of our method.

</details>


### [41] [TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition](https://arxiv.org/abs/2507.17335)
*Guangzhu Xu,Zhi Ke,Pengcheng Zuo,Bangjun Lei*

Main category: cs.CV

TL;DR: 本研究提出了一种用于单双线中国车牌识别的统一方法，结合了轻量级视觉编码器、文本解码器和透视变换网络（PTN），有效解决了数据集稀缺和识别精度问题，并在实验中取得了优异的准确率和处理速度。


<details>
  <summary>Details</summary>
Motivation: 开放环境下的车牌识别应用广泛，但车牌种类的多样性和成像条件的复杂性带来了巨大挑战。现有的基于CNN和CRNN的方法在识别性能上存在局限性。因此，本研究旨在提出一种更优的解决方案，以克服这些挑战，提高车牌识别的准确性和鲁棒性，并解决双线车牌数据集不足的问题。

Method: 本研究提出了一种统一的解决方案，集成了一个轻量级的视觉编码器和一个文本解码器，并置于一个针对单双线中国车牌的预训练框架中。为了解决双线车牌数据集稀缺的问题，研究人员通过合成图像、在真实场景中应用纹理映射以及与真实车牌图像融合等方式，构建了一个单/双线车牌数据集。此外，为了提升识别精度，引入了一个透视变换网络（PTN），该网络通过车牌角点坐标回归作为隐变量，并以车牌视角分类信息进行监督。

Result: 该算法在经过透视校正的CCPD测试集上，在粗略定位干扰下达到了99.34%的平均识别准确率，在精细定位干扰下准确率提升至99.58%。在双线车牌测试集上，平均识别准确率为98.70%，处理速度达到167帧/秒，显示出良好的实用性。

Conclusion: 该研究提出了一种结合轻量级视觉编码器和文本解码器的统一解决方案，用于应对开放环境中单双线中国车牌识别的挑战。通过引入透视变换网络（PTN）来提高识别精度和鲁棒性，并利用合成数据增强了数据集。实验结果表明，该方法在CCPD测试集上达到了99.34%的平均识别准确率，在双线车牌测试集上达到了98.70%的准确率，同时处理速度高达167帧/秒，具有很强的实用性。

Abstract: License plate recognition in open environments is widely applicable across
various domains; however, the diversity of license plate types and imaging
conditions presents significant challenges. To address the limitations
encountered by CNN and CRNN-based approaches in license plate recognition, this
paper proposes a unified solution that integrates a lightweight visual encoder
with a text decoder, within a pre-training framework tailored for single and
double-line Chinese license plates. To mitigate the scarcity of double-line
license plate datasets, we constructed a single/double-line license plate
dataset by synthesizing images, applying texture mapping onto real scenes, and
blending them with authentic license plate images. Furthermore, to enhance the
system's recognition accuracy, we introduce a perspective correction network
(PTN) that employs license plate corner coordinate regression as an implicit
variable, supervised by license plate view classification information. This
network offers improved stability, interpretability, and low annotation costs.
The proposed algorithm achieves an average recognition accuracy of 99.34% on
the corrected CCPD test set under coarse localization disturbance. When
evaluated under fine localization disturbance, the accuracy further improves to
99.58%. On the double-line license plate test set, it achieves an average
recognition accuracy of 98.70%, with processing speeds reaching up to 167
frames per second, indicating strong practical applicability.

</details>


### [42] [Perceptual Classifiers: Detecting Generative Images using Perceptual Features](https://arxiv.org/abs/2507.17240)
*Krishna Srikar Durbha,Asvin Kumar Venkataramanan,Rajesh Sureddi,Alan C. Bovik*

Main category: cs.CV

TL;DR: 利用IQA模型区分真实和AI生成图像，并在检测假图像方面取得最新进展。


<details>
  <summary>Details</summary>
Motivation: 解决互联网上GenAI内容激增的问题，并改进现有GenAI内容检测方法的泛化能力。

Method: 利用现有IQA模型捕获真实图像的统计特征，并训练一个两层网络进行GenAI图像检测。

Result: 所提出的方法在检测各种生成模型中的假图像方面取得了最先进的性能，并且在图像退化方面表现出良好的鲁棒性。

Conclusion: 通过利用现有图像质量评估（IQA）模型来区分真实图像和AI生成图像，并在GenAI图像检测任务中展示了最先进的性能，同时保持了对图像失真的显著鲁棒性。

Abstract: Image Quality Assessment (IQA) models are employed in many practical image
and video processing pipelines to reduce storage, minimize transmission costs,
and improve the Quality of Experience (QoE) of millions of viewers. These
models are sensitive to a diverse range of image distortions and can accurately
predict image quality as judged by human viewers. Recent advancements in
generative models have resulted in a significant influx of "GenAI" content on
the internet. Existing methods for detecting GenAI content have progressed
significantly with improved generalization performance on images from unseen
generative models. Here, we leverage the capabilities of existing IQA models,
which effectively capture the manifold of real images within a bandpass
statistical space, to distinguish between real and AI-generated images. We
investigate the generalization ability of these perceptual classifiers to the
task of GenAI image detection and evaluate their robustness against various
image degradations. Our results show that a two-layer network trained on the
feature space of IQA models demonstrates state-of-the-art performance in
detecting fake images across generative models, while maintaining significant
robustness against image degradations.

</details>


### [43] [Unsupervised Exposure Correction](https://arxiv.org/abs/2507.17252)
*Ruodai Cui,Li Niu,Guosheng Hu*

Main category: cs.CV

TL;DR: 这项研究提出了一种名为UEC的无监督曝光校正方法，它无需手动标注，泛化能力更强，性能更好。该方法使用仿真ISP流水线数据和RCD数据集进行训练，并提出了一种参数效率高且性能优越的变换函数。同时，研究证明了曝光校正对下游任务（如边缘检测）的积极影响。


<details>
  <summary>Details</summary>
Motivation: 当前曝光校正方法面临三个主要挑战：劳动密集型配对数据标注、有限的泛化能力以及在低级计算机视觉任务中性能下降。本研究旨在通过提出一种无监督方法来解决这些问题。

Method: 研究引入了一种名为UEC（Unsupervised Exposure Correction）的创新性无监督曝光校正方法，利用仿真ISP流水线中的自由配对数据进行训练，避免了对昂贵手动标注数据的需求。该方法通过使用大规模辐射校正数据集（RCD）来强调曝光变化，并开发了一种能够保留图像细节的变换函数。

Result: UEC方法无需手动标注，泛化能力更强，并且在低级下游任务中表现更好。其提出的变换函数在保留图像细节的同时，性能优于最先进的有监督方法，且参数量仅占后者的0.01%。研究还证明了曝光校正能有效改善边缘检测等下游任务的性能。

Conclusion: 该研究提出了一种名为UEC的创新性无监督曝光校正方法，无需手动标注，具有更强的泛化能力，并能提升低级视觉任务的性能。该方法使用仿真ISP流水线中的自由配对数据进行训练，避免了昂贵的手动标注，从而减少了标注中的个体风格偏差，提高了泛化能力。此外，研究提出了一个大规模辐射校正数据集（RCD）以促进无监督学习，并开发了一种能保留图像细节的变换函数，其性能优于最先进的有监督方法，而参数量仅占后者的0.01%。该研究还探讨了曝光校正对边缘检测等下游任务的广泛影响，证明了其在减轻不良曝光对低级特征不利影响方面的有效性。

Abstract: Current exposure correction methods have three challenges, labor-intensive
paired data annotation, limited generalizability, and performance degradation
in low-level computer vision tasks. In this work, we introduce an innovative
Unsupervised Exposure Correction (UEC) method that eliminates the need for
manual annotations, offers improved generalizability, and enhances performance
in low-level downstream tasks. Our model is trained using freely available
paired data from an emulated Image Signal Processing (ISP) pipeline. This
approach does not need expensive manual annotations, thereby minimizing
individual style biases from the annotation and consequently improving its
generalizability. Furthermore, we present a large-scale Radiometry Correction
Dataset, specifically designed to emphasize exposure variations, to facilitate
unsupervised learning. In addition, we develop a transformation function that
preserves image details and outperforms state-of-the-art supervised methods
[12], while utilizing only 0.01% of their parameters. Our work further
investigates the broader impact of exposure correction on downstream tasks,
including edge detection, demonstrating its effectiveness in mitigating the
adverse effects of poor exposure on low-level features. The source code and
dataset are publicly available at https://github.com/BeyondHeaven/uec_code.

</details>


### [44] [URPO: A Unified Reward & Policy Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.17515)
*Songshuo Lu,Hua Wang,Zhi Chen,Yaohua Tang*

Main category: cs.CV

TL;DR: URPO框架将指令遵循和奖励建模统一在单一模型和训练阶段，使用GRPO优化统一格式化数据，提高了指令遵循和推理能力，并产生了更优的内部评估器，比分离式模型更简单、高效、有效。


<details>
  <summary>Details</summary>
Motivation: 大型模型对齐流水线通常采用策略模型与单独训练的奖励模型分离的方式，这种方式复杂、资源密集，并且由于奖励信号静态而存在性能上限。URPO旨在解决这些问题。

Method: URPO框架通过将指令遵循（“玩家”）和奖励建模（“裁判”）统一在单个模型和单个训练阶段中，并使用组相对策略优化（GRPO）循环来优化统一的生成格式化数据，从而实现目标。

Result: URPO在Qwen2.5-7B模型上进行了实验，结果显示：1. 指令遵循得分从AlpacaEval的42.24提升到44.84。2. 综合推理平均分从32.66提升到35.66。3. 作为训练的副产品，URPO培养了一个更优的内部评估器，RewardBench得分达到85.15，优于其替代的专用奖励模型（83.55）。

Conclusion: URPO框架通过将指令遵循（“玩家”）和奖励建模（“裁判”）统一在单个模型和单个训练阶段中，克服了传统大型模型对齐流水线中策略模型与单独训练的奖励模型分离带来的复杂性、资源密集性以及静态奖励信号导致的性能上限问题。该方法将所有对齐数据（包括偏好对、可验证推理和开放式指令）重塑为统一的生成格式，并通过单一的组相对策略优化（GRPO）循环进行优化。这使得模型能够从地面真实偏好和可验证逻辑中学习，同时为开放式任务生成自己的奖励。

Abstract: Large-scale alignment pipelines typically pair a policy model with a
separately trained reward model whose parameters remain frozen during
reinforcement learning (RL). This separation creates a complex,
resource-intensive pipeline and suffers from a performance ceiling due to a
static reward signal. We propose a novel framework, Unified Reward & Policy
Optimization (URPO), that unifies instruction-following ("player") and reward
modeling ("referee") within a single model and a single training phase. Our
method recasts all alignment data-including preference pairs, verifiable
reasoning, and open-ended instructions-into a unified generative format
optimized by a single Group-Relative Policy Optimization (GRPO) loop. This
enables the model to learn from ground-truth preferences and verifiable logic
while simultaneously generating its own rewards for open-ended tasks.
Experiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified
model significantly outperforms a strong baseline using a separate generative
reward model, boosting the instruction-following score on AlpacaEval from 42.24
to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,
URPO cultivates a superior internal evaluator as a byproduct of training,
achieving a RewardBench score of 85.15 and surpassing the dedicated reward
model it replaces (83.55). By eliminating the need for a separate reward model
and fostering a co-evolutionary dynamic between generation and evaluation, URPO
presents a simpler, more efficient, and more effective path towards robustly
aligned language models.

</details>


### [45] [VisionTrap: Unanswerable Questions On Visual Data](https://arxiv.org/abs/2507.17262)
*Asir Saadat,Syem Aziz,Shahriar Mahmud,Abdullah Ibne Masud Mahi,Sabbir Ahmed*

Main category: cs.CV

TL;DR: 该研究评估了视觉问答（VQA）模型处理无法回答问题的能力，并提出了一个名为 VisionTrap 的新数据集，其中包含各种无法回答的问题，以测试模型的局限性识别能力。


<details>
  <summary>Details</summary>
Motivation: 目前的研究主要集中在视觉问答（VQA）模型如何根据现实世界的图像回答可回答的问题，而对于模型如何处理无法回答的问题，特别是它们应该弃权的情况，探索有限。

Method: 提出了一种名为 VisionTrap 的数据集，其中包含三个类别的无法回答的问题，涵盖了多种图像类型：(1) 融合了物体和动物的混合实体，(2) 描绘了非传统或不可能场景的物体，(3) 虚构或不存在的人物。所提出的问题在逻辑上结构化但本质上无法回答，旨在测试模型是否能正确识别其局限性。

Result: 研究结果强调了在 VQA 基准测试中包含此类问题的重要性，以评估模型是否倾向于回答，即使它们应该弃权。

Conclusion: 该研究强调了将无法回答的问题纳入 VQA 基准测试的重要性，以评估模型是否倾向于回答，即使它们应该弃权。

Abstract: Visual Question Answering (VQA) has been a widely studied topic, with
extensive research focusing on how VLMs respond to answerable questions based
on real-world images. However, there has been limited exploration of how these
models handle unanswerable questions, particularly in cases where they should
abstain from providing a response. This research investigates VQA performance
on unrealistically generated images or asking unanswerable questions, assessing
whether models recognize the limitations of their knowledge or attempt to
generate incorrect answers. We introduced a dataset, VisionTrap, comprising
three categories of unanswerable questions across diverse image types: (1)
hybrid entities that fuse objects and animals, (2) objects depicted in
unconventional or impossible scenarios, and (3) fictional or non-existent
figures. The questions posed are logically structured yet inherently
unanswerable, testing whether models can correctly recognize their limitations.
Our findings highlight the importance of incorporating such questions into VQA
benchmarks to evaluate whether models tend to answer, even when they should
abstain.

</details>


### [46] [PolarAnything: Diffusion-based Polarimetric Image Synthesis](https://arxiv.org/abs/2507.17268)
*Kailong Zhang,Youwei Lyu,Heng Guo,Si Li,Zhanyu Ma,Boxin Shi*

Main category: cs.CV

TL;DR: 提出了一种名为PolarAnything的新方法，可以从单个RGB图像生成逼真的偏振图像，无需3D模型，并能在下游任务中取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的偏振相机不易获得，限制了其在图像增强和3D重建等任务中的广泛应用，这驱动了对合成真实感偏振图像的需求。现有的模拟器依赖于参数化偏振图像形成模型，并需要大量的3D资产，无法生成大规模真实感图像。

Method: 提出了一种基于扩散模型的生成框架，并采用有效的表示策略来保留偏振属性的保真度，以解决现有偏振模拟器（如Mitsuba）在生成大规模真实感图像方面的局限性。

Result: 生成的偏振图像具有高质量和真实感，并且能够支持偏振形状等下游任务。

Conclusion: PolarAnything能够从单个RGB输入合成具有真实感和物理准确性的偏振图像，消除了对3D资产集合的依赖，并通过实验证明了其生成高质量偏振图像的能力，并支持偏振形状等下游任务。

Abstract: Polarization images facilitate image enhancement and 3D reconstruction tasks,
but the limited accessibility of polarization cameras hinders their broader
application. This gap drives the need for synthesizing photorealistic
polarization images.The existing polarization simulator Mitsuba relies on a
parametric polarization image formation model and requires extensive 3D assets
covering shape and PBR materials, preventing it from generating large-scale
photorealistic images. To address this problem, we propose PolarAnything,
capable of synthesizing polarization images from a single RGB input with both
photorealism and physical accuracy, eliminating the dependency on 3D asset
collections. Drawing inspiration from the zero-shot performance of pretrained
diffusion models, we introduce a diffusion-based generative framework with an
effective representation strategy that preserves the fidelity of polarization
properties. Experiments show that our model generates high-quality polarization
images and supports downstream tasks like shape from polarization.

</details>


### [47] [Dual-branch Prompting for Multimodal Machine Translation](https://arxiv.org/abs/2507.17588)
*Jie Wang,Zhendong Yang,Liansong Zong,Xiaobo Zhang,Dexian Wang,Ji Zhang*

Main category: cs.CV

TL;DR: D2P-MMT是一个鲁棒的视觉引导翻译框架，它使用扩散模型生成的重建图像来避免噪声，并通过双分支提示和分布对齐来提高翻译性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有MMT方法在推理时依赖配对的图像-文本输入以及对不相关的视觉噪声敏感的问题，以提高鲁棒性和实际应用性。

Method: 提出了一种基于扩散的双分支提示框架D2P-MMT，该框架仅需要源文本和预训练扩散模型生成的重建图像，并在训练期间采用双分支提示策略，同时从真实和重建图像中学习，并通过分布对齐损失来强制对齐两个分支的输出分布。

Result: D2P-MMT能够过滤掉干扰性的视觉细节，同时保留语义线索，并在Multi30K数据集上实现了优于现有最先进方法的翻译性能。

Conclusion: D2P-MMT在Multi30K数据集上取得了优于现有最先进方法的翻译性能。

Abstract: Multimodal Machine Translation (MMT) typically enhances text-only translation
by incorporating aligned visual features. Despite the remarkable progress,
state-of-the-art MMT approaches often rely on paired image-text inputs at
inference and are sensitive to irrelevant visual noise, which limits their
robustness and practical applicability. To address these issues, we propose
D2P-MMT, a diffusion-based dual-branch prompting framework for robust
vision-guided translation. Specifically, D2P-MMT requires only the source text
and a reconstructed image generated by a pre-trained diffusion model, which
naturally filters out distracting visual details while preserving semantic
cues. During training, the model jointly learns from both authentic and
reconstructed images using a dual-branch prompting strategy, encouraging rich
cross-modal interactions. To bridge the modality gap and mitigate
training-inference discrepancies, we introduce a distributional alignment loss
that enforces consistency between the output distributions of the two branches.
Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves
superior translation performance compared to existing state-of-the-art
approaches.

</details>


### [48] [Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation](https://arxiv.org/abs/2507.17281)
*Huanli Zhuo,Leilei Ma,Haifeng Zhao,Shiwei Zhou,Dengdi Sun,Yanping Fu*

Main category: cs.CV

TL;DR: FA-SAM通过AGM和IPEF模块实现了SAM的全自动医学图像分割，并提高了对不良提示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决SAM-based单源域泛化模型在医学图像分割中面临的两个主要挑战：1. SAM分割高度依赖特定领域的专家标注提示，限制了其全自动分割能力和临床应用。2. 提示编码器接收到不良提示（如过大或过小的边界框）时，可能导致SAM生成错误的分割掩码。

Method: FA-SAM框架引入了自动提示生成模型（AGM）和图像-提示嵌入融合（IPEF）模块。AGM通过浅层特征不确定性建模（SUFM）为目标域生成边界框提示，实现了SAM的全自动分割。IPEF模块将SAM的图像嵌入和提示嵌入的多尺度信息进行融合，以捕捉目标对象的全局和局部细节，从而减轻了不良提示的影响。

Result: 实验结果表明，FA-SAM在公开的前列腺和眼底血管数据集上验证了其有效性，证明了其在解决上述挑战方面的潜力。

Conclusion: FA-SAM框架能够有效地实现全自动医学图像分割，并减轻了不良提示对SAM分割结果的影响。

Abstract: Although SAM-based single-source domain generalization models for medical
image segmentation can mitigate the impact of domain shift on the model in
cross-domain scenarios, these models still face two major challenges. First,
the segmentation of SAM is highly dependent on domain-specific expert-annotated
prompts, which prevents SAM from achieving fully automated medical image
segmentation and therefore limits its application in clinical settings. Second,
providing poor prompts (such as bounding boxes that are too small or too large)
to the SAM prompt encoder can mislead SAM into generating incorrect mask
results. Therefore, we propose the FA-SAM, a single-source domain
generalization framework for medical image segmentation that achieves fully
automated SAM. FA-SAM introduces two key innovations: an Auto-prompted
Generation Model (AGM) branch equipped with a Shallow Feature Uncertainty
Modeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module
integrated into the SAM mask decoder. Specifically, AGM models the uncertainty
distribution of shallow features through the SUFM module to generate bounding
box prompts for the target domain, enabling fully automated segmentation with
SAM. The IPEF module integrates multiscale information from SAM image
embeddings and prompt embeddings to capture global and local details of the
target object, enabling SAM to mitigate the impact of poor prompts. Extensive
experiments on publicly available prostate and fundus vessel datasets validate
the effectiveness of FA-SAM and highlight its potential to address the above
challenges.

</details>


### [49] [PointLAMA: Latent Attention meets Mamba for Efficient Point Cloud Pretraining](https://arxiv.org/abs/2507.17296)
*Xuanyu Lin,Xiaona Zeng,Xianwei Zheng,Xutao Li*

Main category: cs.CV

TL;DR: PointLAMA是一个点云预训练框架，通过结合任务感知序列化、混合编码器（潜在注意力和Mamba）和条件扩散，解决了Mamba在点云建模中缺乏局部归纳偏置的问题，并在效率和性能上取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: Mamba作为一种骨干模型在点云建模中受到广泛关注，但其缺乏局部归纳偏置限制了捕捉3D数据细粒度几何结构的能力。为了解决这个问题，我们提出了PointLAMA。

Method: 提出了一种名为PointLAMA的点云预训练框架，结合了任务感知点云序列化、包含潜在注意力和Mamba块的混合编码器以及基于Mamba骨干的条件扩散机制。具体来说，任务感知点云序列化分别采用希尔伯特/转换-希尔伯特空间填充曲线和轴对齐排序来处理分类和分割任务中的点标记。潜在注意力模块包含点对多头潜在注意力（PMLA）模块，该模块利用PMLA和Mamba共享的潜在空间特性，以增强局部上下文建模并保持效率。在预训练中引入条件扩散机制，通过去噪扰动特征序列而非显式点重建来提升表示学习能力。

Result: 实验结果表明，PointLAMA在多个基准数据集上取得了有竞争力的性能，同时参数量和计算量最小。

Conclusion: PointLAMA框架在点云预训练任务中表现出竞争力，具有较少的参数量和计算量，验证了其在高效点云预训练方面的有效性。

Abstract: Mamba has recently gained widespread attention as a backbone model for point
cloud modeling, leveraging a state-space architecture that enables efficient
global sequence modeling with linear complexity. However, its lack of local
inductive bias limits its capacity to capture fine-grained geometric structures
in 3D data. To address this limitation, we propose \textbf{PointLAMA}, a point
cloud pretraining framework that combines task-aware point cloud serialization,
a hybrid encoder with integrated Latent Attention and Mamba blocks, and a
conditional diffusion mechanism built upon the Mamba backbone. Specifically,
the task-aware point cloud serialization employs Hilbert/Trans-Hilbert
space-filling curves and axis-wise sorting to structurally align point tokens
for classification and segmentation tasks, respectively. Our lightweight Latent
Attention block features a Point-wise Multi-head Latent Attention (PMLA)
module, which is specifically designed to align with the Mamba architecture by
leveraging the shared latent space characteristics of PMLA and Mamba. This
enables enhanced local context modeling while preserving overall efficiency. To
further enhance representation learning, we incorporate a conditional diffusion
mechanism during pretraining, which denoises perturbed feature sequences
without relying on explicit point-wise reconstruction. Experimental results
demonstrate that PointLAMA achieves competitive performance on multiple
benchmark datasets with minimal parameter count and FLOPs, validating its
effectiveness for efficient point cloud pretraining.

</details>


### [50] [VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization](https://arxiv.org/abs/2507.17455)
*Sania Waheed,Na Min An,Michael Milford,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: A new method combines vision-language models with visual place recognition for more accurate and reliable map localization from images, outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: Geo-localization from a single image at planet scale is a challenging task due to the vast diversity of locations, environmental conditions, and scene variations. Traditional methods struggle with scalability and perceptual aliasing, while classification-based approaches lack generalization and require extensive training data. Although recent vision-language models (VLMs) offer a promising alternative, they are prone to hallucinations and lack interpretability, making them unreliable as standalone solutions. Therefore, there is a need for a more robust and accurate geo-localization system.

Method: This paper proposes a novel hybrid geo-localization framework that combines vision-language models (VLMs) with retrieval-based visual place recognition (VPR) methods. The framework first uses a VLM to generate a prior, which constrains the retrieval search space. Then, a retrieval step is performed, followed by a re-ranking mechanism that selects the most geographically plausible matches based on feature similarity and proximity to the initially estimated coordinates.

Result: The proposed hybrid geo-localization framework consistently outperforms prior state-of-the-art methods on multiple geo-localization benchmarks. Specifically, it achieves improvements of up to 4.51% at the street level and up to 13.52% at the city level. These results demonstrate the effectiveness of combining VLM-generated geographic priors with VPR for scalable, robust, and accurate geo-localization.

Conclusion: Geo-localization from a single image at planet scale is a fundamental and challenging task. Traditional methods struggle with scalability and generalization. Recent advances in vision-language models (VLMs) offer a promising alternative but are prone to hallucinations and lack interpretability. This work proposes a novel hybrid geo-localization framework that combines the strengths of VLMs with retrieval-based visual place recognition (VPR) methods. The approach leverages a VLM to generate a prior, guiding the retrieval search space, followed by a retrieval step and a re-ranking mechanism. The proposed framework consistently outperforms prior state-of-the-art methods on multiple geo-localization benchmarks, demonstrating that VLM-generated geographic priors in combination with VPR lead to scalable, robust, and accurate geo-localization systems.

Abstract: Geo-localization from a single image at planet scale (essentially an advanced
or extreme version of the kidnapped robot problem) is a fundamental and
challenging task in applications such as navigation, autonomous driving and
disaster response due to the vast diversity of locations, environmental
conditions, and scene variations. Traditional retrieval-based methods for
geo-localization struggle with scalability and perceptual aliasing, while
classification-based approaches lack generalization and require extensive
training data. Recent advances in vision-language models (VLMs) offer a
promising alternative by leveraging contextual understanding and reasoning.
However, while VLMs achieve high accuracy, they are often prone to
hallucinations and lack interpretability, making them unreliable as standalone
solutions. In this work, we propose a novel hybrid geo-localization framework
that combines the strengths of VLMs with retrieval-based visual place
recognition (VPR) methods. Our approach first leverages a VLM to generate a
prior, effectively guiding and constraining the retrieval search space. We then
employ a retrieval step, followed by a re-ranking mechanism that selects the
most geographically plausible matches based on feature similarity and proximity
to the initially estimated coordinates. We evaluate our approach on multiple
geo-localization benchmarks and show that it consistently outperforms prior
state-of-the-art methods, particularly at street (up to 4.51%) and city level
(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in
combination with VPR lead to scalable, robust, and accurate geo-localization
systems.

</details>


### [51] [Learning-based Stage Verification System in Manual Assembly Scenarios](https://arxiv.org/abs/2507.17304)
*Xingjian Zhang,Yutong Duan,Zaishu Chen*

Main category: cs.CV

TL;DR: "该研究提出了一种新颖的方法，利用多个机器学习模型，在仅使用最少数量的视觉传感器的情况下，实现了对装配过程的精确监控，准确率超过92%，并优于传统方法。"


<details>
  <summary>Details</summary>
Motivation: "在工业4.0的背景下，在仅限于使用视觉传感器的情况下，在装配过程中有效监控多个目标和状态至关重要。传统方法通常依赖多种传感器类型或复杂的硬件设置来实现高精度的监控，这在动态的工业环境中成本高昂且难以实施。"

Method: "通过集成来自相同时间戳的状态信息，利用多个机器学习模型来精确监控，并在仅使用最少数量的视觉传感器的情况下实现这一点。"

Result: "该方法实现了超过92%的平均准确率，并超越了传统方法，提供了增强的错误检测和可视化能力，为操作员提供实时的、可操作的指导。"

Conclusion: "该方法通过集成来自相同时间戳的状态信息，以超过92%的平均准确率检测和确认装配过程的当前阶段。此外，该方法通过提供实时的、可操作的操作员指导，在提高装配监控的准确性和效率的同时，减少了对昂贵硬件解决方案的依赖，使其成为现代工业应用的可行选择。"

Abstract: In the context of Industry 4.0, effective monitoring of multiple targets and
states during assembly processes is crucial, particularly when constrained to
using only visual sensors. Traditional methods often rely on either multiple
sensor types or complex hardware setups to achieve high accuracy in monitoring,
which can be cost-prohibitive and difficult to implement in dynamic industrial
environments. This study presents a novel approach that leverages multiple
machine learning models to achieve precise monitoring under the limitation of
using a minimal number of visual sensors. By integrating state information from
identical timestamps, our method detects and confirms the current stage of the
assembly process with an average accuracy exceeding 92%. Furthermore, our
approach surpasses conventional methods by offering enhanced error detection
and visuali-zation capabilities, providing real-time, actionable guidance to
operators. This not only improves the accuracy and efficiency of assembly
monitoring but also re-duces dependency on expensive hardware solutions, making
it a more practical choice for modern industrial applications.

</details>


### [52] [CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance](https://arxiv.org/abs/2507.17312)
*Peiqi Chen,Lei Yu,Yi Wan,Yingying Pei,Xinyi Liu,Yongxiang Yao,Yingying Zhang,Lixiang Ru,Liheng Zhong,Jingdong Chen,Ming Yang,Yongjun Zhang*

Main category: cs.CV

TL;DR: CasP improves semi-dense feature matching by using cascaded correspondence priors to guide a two-phase matching process, enhancing accuracy and efficiency, and showing strong performance in geometric estimation and cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Existing semi-dense feature matching pipelines rely on a global search across the entire feature map for coarse matches, which limits accuracy and efficiency. CasP aims to overcome this limitation.

Method: CasP pipeline leverages cascaded correspondence priors, decomposing matching into two progressive phases with a region-based selective cross-attention mechanism to enhance feature discriminability. The search range for one-to-one matches is restricted to one-to-many prior areas from the first phase. It also incorporates high-level features to reduce low-level feature extraction costs.

Result: CasP achieves acceleration gains with higher resolution, with its lite model offering a ~2.2x speedup at 1152 resolution compared to ELoFTR. Experiments demonstrate its superiority in geometric estimation and cross-domain generalization.

Conclusion: Semi-dense feature matching methods using CasP show superiority in geometric estimation and cross-domain generalization, making them suitable for latency-sensitive and high-robustness applications like SLAM and UAV systems.

Abstract: Semi-dense feature matching methods have shown strong performance in
challenging scenarios. However, the existing pipeline relies on a global search
across the entire feature map to establish coarse matches, limiting further
improvements in accuracy and efficiency. Motivated by this limitation, we
propose a novel pipeline, CasP, which leverages cascaded correspondence priors
for guidance. Specifically, the matching stage is decomposed into two
progressive phases, bridged by a region-based selective cross-attention
mechanism designed to enhance feature discriminability. In the second phase,
one-to-one matches are determined by restricting the search range to the
one-to-many prior areas identified in the first phase. Additionally, this
pipeline benefits from incorporating high-level features, which helps reduce
the computational costs of low-level feature extraction. The acceleration gains
of CasP increase with higher resolution, and our lite model achieves a speedup
of $\sim2.2\times$ at a resolution of 1152 compared to the most efficient
method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority
in geometric estimation, particularly with impressive cross-domain
generalization. These advantages highlight its potential for latency-sensitive
and high-robustness applications, such as SLAM and UAV systems. Code is
available at https://github.com/pq-chen/CasP.

</details>


### [53] [From Scan to Action: Leveraging Realistic Scans for Embodied Scene Understanding](https://arxiv.org/abs/2507.17585)
*Anna-Maria Halacheva,Jan-Nico Zaech,Sombit Dey,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 该研究提出了一种基于USD的统一注释集成方法，解决了真实3D场景扫描数据的应用挑战，并在LLM场景编辑和机器人模拟任务中取得了高成功率。


<details>
  <summary>Details</summary>
Motivation: 真实世界的3D场景扫描数据具有高度的真实性和泛化能力，但其数据量大、注释格式多样、工具兼容性差等问题限制了其在下游应用中的广泛使用。

Method: 提出了一种统一的注释集成方法，使用USD（通用场景描述）作为核心，并为特定应用创建了不同的USD风味，以解决真实世界3D场景扫描数据中存在的注释格式多样性和工具兼容性等问题。

Result: 该方法在两个下游应用中得到了验证：1）基于LLM的场景编辑，LLM对数据的理解和适应能力得到有效提升，成功率达到80%；2）机器人模拟，策略学习的成功率达到87%。

Conclusion: 该方法通过统一的USD注释集成和应用特定的USD风味，有效解决了真实世界3D场景扫描数据的挑战，并在基于LLM的场景编辑和机器人模拟等下游应用中取得了显著成效（LLM应用成功率为80%，机器人策略学习成功率为87%）。

Abstract: Real-world 3D scene-level scans offer realism and can enable better
real-world generalizability for downstream applications. However, challenges
such as data volume, diverse annotation formats, and tool compatibility limit
their use. This paper demonstrates a methodology to effectively leverage these
scans and their annotations. We propose a unified annotation integration using
USD, with application-specific USD flavors. We identify challenges in utilizing
holistic real-world scan datasets and present mitigation strategies. The
efficacy of our approach is demonstrated through two downstream applications:
LLM-based scene editing, enabling effective LLM understanding and adaptation of
the data (80% success), and robotic simulation, achieving an 87% success rate
in policy learning.

</details>


### [54] [CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits](https://arxiv.org/abs/2507.17327)
*Chao He,Jianqiang Ren,Jianjing Xiang,Xiejie Shen*

Main category: cs.CV

TL;DR: CartoonAlive generates interactive 2D cartoon digital humans from a single photo using Live2D technology, offering a fast and expressive alternative to traditional methods.


<details>
  <summary>Details</summary>
Motivation: While digital humans advance, interactive 2D cartoon-style digital humans, specifically Live2D models, have received less attention despite being more efficient and expressive than 3D models or 2D video-based solutions.

Method: CartoonAlive leverages the shape basis concept from 3D face modeling to construct facial blendshapes for Live2D and infers blendshape weights from detected facial keypoints, achieving 3D-like motion through layered segmentation.

Result: The CartoonAlive method can generate a high-quality Live2D digital human resembling the input portrait in less than 30 seconds.

Conclusion: CartoonAlive method enables rapid generation of expressive and visually accurate Live2D models from a single portrait image, offering a scalable solution for interactive 2D cartoon character creation.

Abstract: With the rapid advancement of large foundation models, AIGC, cloud rendering,
and real-time motion capture technologies, digital humans are now capable of
achieving synchronized facial expressions and body movements, engaging in
intelligent dialogues driven by natural language, and enabling the fast
creation of personalized avatars. While current mainstream approaches to
digital humans primarily focus on 3D models and 2D video-based representations,
interactive 2D cartoon-style digital humans have received relatively less
attention. Compared to 3D digital humans that require complex modeling and high
rendering costs, and 2D video-based solutions that lack flexibility and
real-time interactivity, 2D cartoon-style Live2D models offer a more efficient
and expressive alternative. By simulating 3D-like motion through layered
segmentation without the need for traditional 3D modeling, Live2D enables
dynamic and real-time manipulation. In this technical report, we present
CartoonAlive, an innovative method for generating high-quality Live2D digital
humans from a single input portrait image. CartoonAlive leverages the shape
basis concept commonly used in 3D face modeling to construct facial blendshapes
suitable for Live2D. It then infers the corresponding blendshape weights based
on facial keypoints detected from the input image. This approach allows for the
rapid generation of a highly expressive and visually accurate Live2D model that
closely resembles the input portrait, within less than half a minute. Our work
provides a practical and scalable solution for creating interactive 2D cartoon
characters, opening new possibilities in digital content creation and virtual
character animation. The project homepage is
https://human3daigc.github.io/CartoonAlive_webpage/.

</details>


### [55] [PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.17596)
*Maciej K. Wozniak,Lianhang Liu,Yixi Cai,Patric Jensfelt*

Main category: cs.CV

TL;DR: PRIX is an efficient end-to-end driving architecture using only camera data, outperforming larger models while being significantly smaller and faster, making it practical for mass-market vehicles.


<details>
  <summary>Details</summary>
Motivation: While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras.

Method: PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning.

Result: PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size.

Conclusion: PRIX is a novel and efficient end-to-end driving architecture that operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. It achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment.

Abstract: While end-to-end autonomous driving models show promising results, their
practical deployment is often hindered by large model sizes, a reliance on
expensive LiDAR sensors and computationally intensive BEV feature
representations. This limits their scalability, especially for mass-market
vehicles equipped only with cameras. To address these challenges, we propose
PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving
architecture operates using only camera data, without explicit BEV
representation and forgoing the need for LiDAR. PRIX leverages a visual feature
extractor coupled with a generative planning head to predict safe trajectories
from raw pixel inputs directly. A core component of our architecture is the
Context-aware Recalibration Transformer (CaRT), a novel module designed to
effectively enhance multi-level visual features for more robust planning. We
demonstrate through comprehensive experiments that PRIX achieves
state-of-the-art performance on the NavSim and nuScenes benchmarks, matching
the capabilities of larger, multimodal diffusion planners while being
significantly more efficient in terms of inference speed and model size, making
it a practical solution for real-world deployment. Our work is open-source and
the code will be at https://maxiuw.github.io/prix.

</details>


### [56] [PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image](https://arxiv.org/abs/2507.17332)
*Hyeongjin Nam,Donghwan Kim,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: PARTE通过使用3D人体部件信息来解决3D人体纹理错位问题，在单张图像中重建纹理化的人体表面。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体重建方法的主要限制之一是不同人体部件之间的人体纹理错位。为了解决这个问题，PARTE框架利用3D人体部件信息来指导纹理重建，确保每个部件的纹理保持独立且不与其他部件混合，从而利用结构一致性作为推断单张图像中不可见区域纹理的关键线索。

Method: PARTE框架利用3D人体部件信息作为重建3D人体纹理的关键指导。该框架包括一个3D部件分割模块（PartSegmenter），用于从单个图像推断3D人体部件信息，以及一个部件引导纹理化模块（PartTexturer），用于将部件信息整合到纹理重建中，该模块从预先训练的、在人体部件纹理对齐方面具有图像生成网络中获取先验知识。

Result: 实验证明，PARTE框架在3D人体重建方面达到了最先进的质量。

Conclusion: PARTE框架在3D人体重建方面取得了最先进的质量。

Abstract: The misaligned human texture across different human parts is one of the main
limitations of existing 3D human reconstruction methods. Each human part, such
as a jacket or pants, should maintain a distinct texture without blending into
others. The structural coherence of human parts serves as a crucial cue to
infer human textures in the invisible regions of a single image. However, most
existing 3D human reconstruction methods do not explicitly exploit such part
segmentation priors, leading to misaligned textures in their reconstructions.
In this regard, we present PARTE, which utilizes 3D human part information as a
key guide to reconstruct 3D human textures. Our framework comprises two core
components. First, to infer 3D human part information from a single image, we
propose a 3D part segmentation module (PartSegmenter) that initially
reconstructs a textureless human surface and predicts human part labels based
on the textureless surface. Second, to incorporate part information into
texture reconstruction, we introduce a part-guided texturing module
(PartTexturer), which acquires prior knowledge from a pre-trained image
generation network on texture alignment of human parts. Extensive experiments
demonstrate that our framework achieves state-of-the-art quality in 3D human
reconstruction. The project page is available at
https://hygenie1228.github.io/PARTE/.

</details>


### [57] [Monocular Semantic Scene Completion via Masked Recurrent Networks](https://arxiv.org/abs/2507.17661)
*Xuzhi Wang,Xinran Wu,Song Wang,Lingdong Kong,Ziping Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为MonoMRN的新型两阶段框架，用于单目语义场景补全，通过引入MS-GRU和距离注意力投影来克服现有方法的局限性，并在基准数据集上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单阶段框架在处理复杂场景时存在不足，尤其是在深度估计不准确的情况下，并且难以同时进行可见区域分割和遮挡区域推理。

Method: 提出了一种新颖的两阶段框架，称为MonoMRN，将MSSC分解为粗粒度MSSC和掩码递归网络。该框架引入了掩码稀疏门控循环单元（MS-GRU）和距离注意力投影，以提高性能和效率。

Result: MonoMRN在NYUv2和SemanticKITTI数据集上实现了最先进的性能，并证明了其在室内和室外场景的有效性以及对各种干扰的鲁棒性。

Conclusion: 该模型在NYUv2和SemanticKITTI数据集上取得了最先进的性能，并且在各种干扰下表现出鲁棒性。

Abstract: Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise
occupancy and semantic category from a single-view RGB image. Existing methods
adopt a single-stage framework that aims to simultaneously achieve visible
region segmentation and occluded region hallucination, while also being
affected by inaccurate depth estimation. Such methods often achieve suboptimal
performance, especially in complex scenes. We propose a novel two-stage
framework that decomposes MSSC into coarse MSSC followed by the Masked
Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent
Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask
updating mechanism, and a sparse GRU design is proposed to reduce the
computation cost. Additionally, we propose the distance attention projection to
reduce projection errors by assigning different attention scores according to
the distance to the observed surface. Experimental results demonstrate that our
proposed unified framework, MonoMRN, effectively supports both indoor and
outdoor scenes and achieves state-of-the-art performance on the NYUv2 and
SemanticKITTI datasets. Furthermore, we conduct robustness analysis under
various disturbances, highlighting the role of the Masked Recurrent Network in
enhancing the model's resilience to such challenges. The source code is
publicly available.

</details>


### [58] [Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free Framework for Weak Moving Target Detection](https://arxiv.org/abs/2507.17334)
*Weihua Gao,Chunxu Ren,Wenlong Niu,Xiaodong Peng*

Main category: cs.CV

TL;DR: 该研究提出了一种名为TPS的新框架，用于在没有人工标注的情况下检测低空监视中的微弱移动目标。该框架通过将检测任务重构为像素级时间信号建模，并引入TSRNet和DMSAttention模块来重建短暂信号，同时利用图遍历来提高准确性。实验证明该方法在性能和速度上均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从低信噪比、小空间范围和复杂背景杂波的弱移动目标中提取鲁棒特征方面存在困难，并且缺乏可靠的标注。为了解决这些限制，需要一种无需手动标注即可实现高性能检测的方法。

Method: 提出了一种新颖的时间点监督（TPS）框架，将弱小目标的检测重新表述为像素级时间信号建模问题，并开发了一个时间信号重建网络（TSRNet）。TSRNet采用编码器-解码器架构，并集成了一个动态多尺度注意力（DMSAttention）模块来增强对不同时间模式的敏感度。此外，还采用了一种基于图的轨迹挖掘策略来抑制误报并确保时间一致性。

Result: 在专门构建的低信噪比数据集上进行的广泛实验表明，该框架在无需人工标注的情况下，性能优于现有最先进方法，并且运行速度超过1000 FPS。

Conclusion: 该框架在无需人工标注的情况下，在低信噪比数据集上实现了超越最先进方法的检测性能，并且运行速度超过1000 FPS，证明了其在实际场景中实时部署的潜力。

Abstract: In low-altitude surveillance and early warning systems, detecting weak moving
targets remains a significant challenge due to low signal energy, small spatial
extent, and complex background clutter. Existing methods struggle with
extracting robust features and suffer from the lack of reliable annotations. To
address these limitations, we propose a novel Temporal Point-Supervised (TPS)
framework that enables high-performance detection of weak targets without any
manual annotations.Instead of conventional frame-based detection, our framework
reformulates the task as a pixel-wise temporal signal modeling problem, where
weak targets manifest as short-duration pulse-like responses. A Temporal Signal
Reconstruction Network (TSRNet) is developed under the TPS paradigm to
reconstruct these transient signals.TSRNet adopts an encoder-decoder
architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention)
module to enhance its sensitivity to diverse temporal patterns. Additionally, a
graph-based trajectory mining strategy is employed to suppress false alarms and
ensure temporal consistency.Extensive experiments on a purpose-built low-SNR
dataset demonstrate that our framework outperforms state-of-the-art methods
while requiring no human annotations. It achieves strong detection performance
and operates at over 1000 FPS, underscoring its potential for real-time
deployment in practical scenarios.

</details>


### [59] [Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras](https://arxiv.org/abs/2507.17664)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: Talk2Event 是一个用于事件相机和语言理解的大规模基准测试，EventRefer 是一个用于此基准测试的属性感知框架。


<details>
  <summary>Details</summary>
Motivation: 实现事件相机数据流与人类语言之间的连接，以实现对动态环境的理解。

Method: EventRefer 是一个框架，它使用事件和属性的混合来动态融合多属性表示，以实现由语言驱动的事件感知。

Result: Talk2Event 是一个包含超过 30,000 个带注释的引用表达式的大规模数据集，EventRefer 在事件专用、帧专用和事件-帧融合设置中均优于最先进的方法。

Conclusion: Talk2Event 和 EventRefer 为事件相机和语言之间的连接奠定了基础，并在各种设置下实现了最先进的性能。

Abstract: Event cameras offer microsecond-level latency and robustness to motion blur,
making them ideal for understanding dynamic environments. Yet, connecting these
asynchronous streams to human language remains an open challenge. We introduce
Talk2Event, the first large-scale benchmark for language-driven object
grounding in event-based perception. Built from real-world driving data, we
provide over 30,000 validated referring expressions, each enriched with four
grounding attributes -- appearance, status, relation to viewer, and relation to
other objects -- bridging spatial, temporal, and relational reasoning. To fully
exploit these cues, we propose EventRefer, an attribute-aware grounding
framework that dynamically fuses multi-attribute representations through a
Mixture of Event-Attribute Experts (MoEE). Our method adapts to different
modalities and scene dynamics, achieving consistent gains over state-of-the-art
baselines in event-only, frame-only, and event-frame fusion settings. We hope
our dataset and approach will establish a foundation for advancing multimodal,
temporally-aware, and language-driven perception in real-world robotics and
autonomy.

</details>


### [60] [Perspective-Invariant 3D Object Detection](https://arxiv.org/abs/2507.17665)
*Ao Liang,Lingdong Kong,Dongyue Lu,Youquan Liu,Jian Fang,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: Pi3DET：首个包含车辆、四足和无人机LiDAR数据的3D目标检测基准，并提出跨平台适应框架，实现通用3D感知。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-based 3D目标检测方法主要集中在车载平台，忽略了其他自主平台，导致研究存在空白。

Method: 提出了一种跨平台适应框架，通过几何和特征层面的鲁棒对齐，实现跨视角的3D检测，将知识从车辆平台迁移到其他平台。

Result: 提出的框架在跨平台任务上表现出色，相比现有方法有显著提升，并建立了评估当前3D检测器在跨平台场景下弹性和鲁棒性的基准。

Conclusion: 本研究提出了Pi3DET数据集和跨平台适应框架，为非车载平台和跨平台3D目标检测提供了新的基准和方法，并在实验中验证了其有效性，旨在推动通用、统一的3D感知系统发展。

Abstract: With the rise of robotics, LiDAR-based 3D object detection has garnered
significant attention in both academia and industry. However, existing datasets
and methods predominantly focus on vehicle-mounted platforms, leaving other
autonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,
the first benchmark featuring LiDAR data and 3D bounding box annotations
collected from multiple platforms: vehicle, quadruped, and drone, thereby
facilitating research in 3D object detection for non-vehicle platforms as well
as cross-platform 3D detection. Based on Pi3DET, we propose a novel
cross-platform adaptation framework that transfers knowledge from the
well-studied vehicle platform to other platforms. This framework achieves
perspective-invariant 3D detection through robust alignment at both geometric
and feature levels. Additionally, we establish a benchmark to evaluate the
resilience and robustness of current 3D detectors in cross-platform scenarios,
providing valuable insights for developing adaptive 3D perception systems.
Extensive experiments validate the effectiveness of our approach on challenging
cross-platform tasks, demonstrating substantial gains over existing adaptation
methods. We hope this work paves the way for generalizable and unified 3D
perception systems across diverse and complex environments. Our Pi3DET dataset,
cross-platform benchmark suite, and annotation toolkit have been made publicly
available.

</details>


### [61] [DeMo++: Motion Decoupling for Autonomous Driving](https://arxiv.org/abs/2507.17342)
*Bozhou Zhang,Nan Song,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: DeMo++ 框架通过将运动估计解耦为整体运动意图和精细时空状态，并引入跨场景轨迹交互和结合 Attention 与 Mamba 的混合模型，解决了现有方法在模拟轨迹时空演变方面的不足，并在多项自动驾驶任务中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的“一个查询-一个轨迹”范式在模拟轨迹复杂的时空演变方面存在不足，可能导致碰撞或次优结果。需要一种能同时建模运动意图多样性和轨迹时空演变的方法。

Method: DeMo++ 框架将运动估计解耦为两个组件：整体运动意图（捕捉多样的潜在移动方向）和精细时空状态（跟踪场景内代理的动态进展并实现自我完善能力）。引入了跨场景轨迹交互机制来探索相邻场景中运动的关系。模型结合了 Attention 和 Mamba 机制。

Result: DeMo++ 能够全面地建模运动意图的多样性和每个轨迹的时空演变，在各项基准测试中取得了最先进的性能。

Conclusion: DeMo++ 在运动预测（Argoverse 2 和 nuScenes）、运动规划（nuPlan）和端到端规划（NAVSIM）等多个基准测试中均取得了最先进的性能。

Abstract: Motion forecasting and planning are tasked with estimating the trajectories
of traffic agents and the ego vehicle, respectively, to ensure the safety and
efficiency of autonomous driving systems in dynamically changing environments.
State-of-the-art methods typically adopt a one-query-one-trajectory paradigm,
where each query corresponds to a unique trajectory for predicting multi-mode
trajectories. While this paradigm can produce diverse motion intentions, it
often falls short in modeling the intricate spatiotemporal evolution of
trajectories, which can lead to collisions or suboptimal outcomes. To overcome
this limitation, we propose DeMo++, a framework that decouples motion
estimation into two distinct components: holistic motion intentions to capture
the diverse potential directions of movement, and fine spatiotemporal states to
track the agent's dynamic progress within the scene and enable a
self-refinement capability. Further, we introduce a cross-scene trajectory
interaction mechanism to explore the relationships between motions in adjacent
scenes. This allows DeMo++ to comprehensively model both the diversity of
motion intentions and the spatiotemporal evolution of each trajectory. To
effectively implement this framework, we developed a hybrid model combining
Attention and Mamba. This architecture leverages the strengths of both
mechanisms for efficient scene information aggregation and precise trajectory
state sequence modeling. Extensive experiments demonstrate that DeMo++ achieves
state-of-the-art performance across various benchmarks, including motion
forecasting (Argoverse 2 and nuScenes), motion planning (nuPlan), and
end-to-end planning (NAVSIM).

</details>


### [62] [Principled Multimodal Representation Learning](https://arxiv.org/abs/2507.17343)
*Xiaohao Liu,Xiaobo Xia,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: PMRL是一种新的多模态表示学习框架，无需锚点即可稳定地同步对齐多种模态，通过优化主奇异值实现，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统多模态表示学习方法依赖成对对比学习，受限于预定义的锚点模态，阻碍了所有模态间的对齐。近期方法虽然实现了多模态同步对齐，但仍存在固定锚点限制和奇异值积优化不稳定的问题。

Method: PMRL框架基于秩-1 Gram矩阵的理论洞见，通过优化表示矩阵的主奇异值来实现多模态的同步对齐，解决了固定锚点和奇异值积优化不稳定的问题。提出了一种基于softmax的损失函数，优先考虑最大的奇异值。此外，基于实例的对比正则化用于保持实例间可分离性并防止表示坍塌。

Result: 实验结果表明，PMRL在各种任务上的表现优于基线方法。

Conclusion: PMRL框架通过优化表示矩阵的主奇异值来实现多模态的同步对齐，无需锚点依赖，并且更稳定。实验证明了PMRL在各种任务上的优越性。

Abstract: Multimodal representation learning seeks to create a unified representation
space by integrating diverse data modalities to improve multimodal
understanding. Traditional methods often depend on pairwise contrastive
learning, which relies on a predefined anchor modality, restricting alignment
across all modalities. Recent advances have investigated the simultaneous
alignment of multiple modalities, yet several challenges remain, such as
limitations imposed by fixed anchor points and instability arising from
optimizing the product of singular values. To address the challenges, in this
paper, we propose Principled Multimodal Representation Learning (PMRL), a novel
framework that achieves simultaneous alignment of multiple modalities without
anchor dependency in a more stable manner. Specifically, grounded in the
theoretical insight that full alignment corresponds to a rank-1 Gram matrix,
PMRL optimizes the dominant singular value of the representation matrix to
align modalities along a shared leading direction. We propose a softmax-based
loss function that treats singular values as logits to prioritize the largest
singular value. Besides, instance-wise contrastive regularization on the
leading eigenvectors maintains inter-instance separability and prevents
representation collapse. Extensive experiments across diverse tasks demonstrate
PMRL's superiority compared to baseline methods. The source code will be
publicly available.

</details>


### [63] [Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation](https://arxiv.org/abs/2507.17347)
*Haotian Chen,Zhiyong Xiao*

Main category: cs.CV

TL;DR: Swin-TUNA是一种参数高效的食物图像分割方法，通过引入可训练适配器和分层特征自适应机制，在大幅减少参数量和计算资源的同时，实现了比现有模型更好的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对现有基于Transformer的大规模模型（如FoodSAM）在参数量和计算资源需求方面难以满足工业部署需求的问题，旨在提出一种高效的食物图像语义分割技术。

Method: 提出了一种名为Swin-TUNA的参数高效微调（PEFT）方法，该方法将多尺度可训练适配器集成到Swin Transformer架构中，并通过设计不同尺度的深度和维度映射中的可分离卷积，结合任务无关和任务特定的特征的动态平衡策略，实现了分层特征自适应。

Result: 在FoodSeg103数据集上达到50.56%的mIoU，在UECFoodPix Complete数据集上达到74.94%的mIoU，参数量减少98.7%（仅8.13M），性能优于FoodSAM，并且在低数据场景下具有更快的收敛速度和更强的泛化能力。

Conclusion: Swin-TUNA通过集成多尺度可训练适配器到Swin Transformer架构中，实现了高效且参数量减少98.7%（仅8.13M）的食物图像分割，同时在FoodSeg103和UECFoodPix Complete数据集上取得了优于FoodSAM的性能（mIoU分别为50.56%和74.94%）。该方法在低数据场景下展现出更快的收敛速度和更强的泛化能力，为轻量化食物图像处理提供了有效的解决方案。

Abstract: In the field of food image processing, efficient semantic segmentation
techniques are crucial for industrial applications. However, existing
large-scale Transformer-based models (such as FoodSAM) face challenges in
meeting practical deploymentrequirements due to their massive parameter counts
and high computational resource demands. This paper introduces TUNable Adapter
module (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that
integrates multiscale trainable adapters into the Swin Transformer
architecture, achieving high-performance food image segmentation by updating
only 4% of the parameters. The core innovation of Swin-TUNA lies in its
hierarchical feature adaptation mechanism: it designs separable convolutions in
depth and dimensional mappings of varying scales to address the differences in
features between shallow and deep networks, combined with a dynamic balancing
strategy for tasks-agnostic and task-specific features. Experiments demonstrate
that this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and
UECFoodPix Complete datasets, respectively, surpassing the fully parameterized
FoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).
Furthermore, Swin-TUNA exhibits faster convergence and stronger generalization
capabilities in low-data scenarios, providing an efficient solution for
assembling lightweight food image.

</details>


### [64] [Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field](https://arxiv.org/abs/2507.17351)
*Yuzhe Zhu,Lile Cai,Kangkang Lu,Fayao Liu,Xulei Yang*

Main category: cs.CV

TL;DR: 通过主动学习策略，可以显著降低训练语义感知 NeRF 所需的像素级标注成本。


<details>
  <summary>Details</summary>
Motivation: 收集像素级类别标签以训练语义感知 NeRF 的成本高昂，本文旨在通过主动学习来减轻标注负担。

Method: 本文探索了主动学习在减少语义感知 NeRF 标注成本方面的应用，研究了采样粒度和采样策略等设计选择，并提出了一种考虑 3D 几何约束的新型主动学习策略。

Result: 实验表明，主动学习能有效降低训练语义感知 NeRF 的标注成本，相比随机采样，标注成本降低了 2 倍以上。

Conclusion: 通过主动学习，在减少标注成本的同时，有效地训练了语义感知 NeRF，实现了比随机采样高 2 倍以上的标注成本降低。

Abstract: Neural Radiance Field (NeRF) models are implicit neural scene representation
methods that offer unprecedented capabilities in novel view synthesis.
Semantically-aware NeRFs not only capture the shape and radiance of a scene,
but also encode semantic information of the scene. The training of
semantically-aware NeRFs typically requires pixel-level class labels, which can
be prohibitively expensive to collect. In this work, we explore active learning
as a potential solution to alleviate the annotation burden. We investigate
various design choices for active learning of semantically-aware NeRF,
including selection granularity and selection strategies. We further propose a
novel active learning strategy that takes into account 3D geometric constraints
in sample selection. Our experiments demonstrate that active learning can
effectively reduce the annotation cost of training semantically-aware NeRF,
achieving more than 2X reduction in annotation cost compared to random
sampling.

</details>


### [65] [Exploring Active Learning for Semiconductor Defect Segmentation](https://arxiv.org/abs/2507.17359)
*Lile Cai,Ramanpreet Singh Pahwa,Xun Xu,Jie Wang,Richard Chang,Lining Zhang,Chuan-Sheng Foo*

Main category: cs.CV

TL;DR: Active learning with contrastive pretraining and a rareness-aware acquisition function reduces annotation needs for defect identification in semiconductor XRM scans, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for visual analysis, particularly for tasks like semantic segmentation in semiconductor defect identification using X-Ray microscopy (XRM), require substantial annotated data, which is costly and time-consuming to acquire. Active learning (AL) is explored as a solution to minimize this annotation burden.

Method: This work explores active learning (AL) to reduce annotation effort for deep learning models in semiconductor X-Ray microscopy (XRM) defect identification. Two main challenges are identified: large domain shift and severe class imbalance. To tackle these, the paper proposes contrastive pretraining on unlabeled data for better initialization in AL cycles and a rareness-aware acquisition function to prioritize samples with rare classes. The method is evaluated on XRM scans of high bandwidth memory structures.

Result: The proposed method, which includes contrastive pretraining and a rareness-aware acquisition function, demonstrates state-of-the-art performance on a semiconductor dataset derived from XRM scans of high bandwidth memory structures.

Conclusion: The proposed method, incorporating contrastive pretraining and a rareness-aware acquisition function, achieves state-of-the-art performance in semiconductor XRM scan analysis by effectively addressing the challenges of large domain shift and severe class imbalance in active learning.

Abstract: The development of X-Ray microscopy (XRM) technology has enabled
non-destructive inspection of semiconductor structures for defect
identification. Deep learning is widely used as the state-of-the-art approach
to perform visual analysis tasks. However, deep learning based models require
large amount of annotated data to train. This can be time-consuming and
expensive to obtain especially for dense prediction tasks like semantic
segmentation. In this work, we explore active learning (AL) as a potential
solution to alleviate the annotation burden. We identify two unique challenges
when applying AL on semiconductor XRM scans: large domain shift and severe
class-imbalance. To address these challenges, we propose to perform contrastive
pretraining on the unlabelled data to obtain the initialization weights for
each AL cycle, and a rareness-aware acquisition function that favors the
selection of samples containing rare classes. We evaluate our method on a
semiconductor dataset that is compiled from XRM scans of high bandwidth memory
structures composed of logic and memory dies, and demonstrate that our method
achieves state-of-the-art performance.

</details>


### [66] [Exploring Spatial Diversity for Region-based Active Learning](https://arxiv.org/abs/2507.17367)
*Lile Cai,Xun Xu,Lining Zhang,Chuan-Sheng Foo*

Main category: cs.CV

TL;DR: 通过在基于区域的主动学习中引入空间多样性，可以显著降低语义分割的标注成本，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 为了降低像素级密集预测任务（如语义分割）的数据标注成本，因为获取大规模标注数据集需要高昂的成本。

Method: 提出了一种基于区域的主动学习策略，通过强制局部空间多样性并将其与传统的不确定性等选择标准相结合，以减少标注成本并保持高性能。

Result: 在Cityscapes和PASCAL VOC数据集上进行了应用，证明了空间多样性的加入有效地提高了基于不确定性的和基于特征多样性的主动学习方法的性能，并在标注成本大大降低的情况下取得了优于现有方法的性能。

Conclusion: 所提出的框架通过在不确定性或特征多样性标准中加入空间多样性，有效提高了基于区域的主动学习方法的性能。该方法在仅使用5-9%的标注像素的情况下，达到了全监督方法的95%性能，超越了所有最先进的基于区域的主动学习方法。

Abstract: State-of-the-art methods for semantic segmentation are based on deep neural
networks trained on large-scale labeled datasets. Acquiring such datasets would
incur large annotation costs, especially for dense pixel-level prediction tasks
like semantic segmentation. We consider region-based active learning as a
strategy to reduce annotation costs while maintaining high performance. In this
setting, batches of informative image regions instead of entire images are
selected for labeling. Importantly, we propose that enforcing local spatial
diversity is beneficial for active learning in this case, and to incorporate
spatial diversity along with the traditional active selection criterion, e.g.,
data sample uncertainty, in a unified optimization framework for region-based
active learning. We apply this framework to the Cityscapes and PASCAL VOC
datasets and demonstrate that the inclusion of spatial diversity effectively
improves the performance of uncertainty-based and feature diversity-based
active learning methods. Our framework achieves $95\%$ performance of fully
supervised methods with only $5-9\%$ of the labeled pixels, outperforming all
state-of-the-art region-based active learning methods for semantic
segmentation.

</details>


### [67] [SFUOD: Source-Free Unknown Object Detection](https://arxiv.org/abs/2507.17373)
*Keon-Hee Park,Seun-An Choe,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出源无关未知物体检测（SFUOD）新场景，并提出 CollaPAUL 框架，通过协同调优和基于主轴的未知标签分配来检测未知物体。


<details>
  <summary>Details</summary>
Motivation: 现有源无关目标检测方法假设目标域只存在源域中定义的物体（闭集设置），这限制了检测器识别和检测未定义物体（未知物体）的能力。为了解决这个问题，本文提出了源无关未知物体检测（SFUOD）新场景，旨在使检测器能够识别已知物体并检测未知物体。

Method: CollaPAUL 框架通过协同调优和基于主轴的未知标签分配来解决 SFUOD 问题。协同调优整合了来自辅助编码器的目标域依赖知识和来自预训练检测器的源域依赖知识，并通过跨域注意力机制进行融合。基于主轴的未知标签分配通过主轴投影估计物体性和模型预测的置信分数来为未知物体分配伪标签。

Result: CollaPAUL 框架在 SFUOD 基准测试中取得了最先进的性能。

Conclusion: CollaPAUL 在 SFUOD 基准测试中取得了最先进的性能，并且大量的实验验证了其有效性。

Abstract: Source-free object detection adapts a detector pre-trained on a source domain
to an unlabeled target domain without requiring access to labeled source data.
While this setting is practical as it eliminates the need for the source
dataset during domain adaptation, it operates under the restrictive assumption
that only pre-defined objects from the source domain exist in the target
domain. This closed-set setting prevents the detector from detecting undefined
objects. To ease this assumption, we propose Source-Free Unknown Object
Detection (SFUOD), a novel scenario which enables the detector to not only
recognize known objects but also detect undefined objects as unknown objects.
To this end, we propose CollaPAUL (Collaborative tuning and Principal
Axis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning
enhances knowledge adaptation by integrating target-dependent knowledge from
the auxiliary encoder with source-dependent knowledge from the pre-trained
detector through a cross-domain attention mechanism. Additionally, principal
axes-based unknown labeling assigns pseudo-labels to unknown objects by
estimating objectness via principal axes projection and confidence scores from
model predictions. The proposed CollaPAUL achieves state-of-the-art
performances on SFUOD benchmarks, and extensive experiments validate its
effectiveness.

</details>


### [68] [A Conditional Probability Framework for Compositional Zero-shot Learning](https://arxiv.org/abs/2507.17377)
*Peng Wu,Qiuxia Lai,Hao Fang,Guo-Sen Xie,Yilong Yin,Xiankai Lu,Wenguan Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种新的条件概率框架（CPF）来解决成分零样本学习（CZSL）中的属性-对象依赖性问题，通过显式建模这种关系并利用文本描述增强特征学习，取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统CZSL方法假设属性和对象是独立的，忽略了它们之间的语义约束和上下文依赖性，而这对于准确识别未知组合至关重要。例如，“条纹”属性适用于“斑马”或“衬衫”，但不适用于“天空”或“水”；“年轻”属性在“年轻的树”和“年轻的狗”中的含义也不同。因此，捕获属性-对象相互依赖性是CZSL中的一个基本但被忽视的挑战。

Method: 提出了一种条件概率框架（CPF），将组合的概率分解为对象的可能性和属性的条件可能性。通过引入文本描述来增强对象特征学习，并利用跨注意机制引导属性学习，以实现更好的上下文对齐。通过联合优化对象和条件属性的可能性来捕获组合依赖性。

Result: 通过条件概率框架（CPF）和增强的对象特征学习，该方法能够有效捕获组合依赖性，并在多个CZSL基准测试中展现出优于现有方法的性能，成功推广到未知的组合。

Conclusion: 本研究提出了一种条件概率框架（CPF），通过将组合的概率分解为对象的可能性和属性的条件可能性来明确建模属性-对象依赖关系，以解决成分零样本学习（CZSL）中的关键挑战，即捕获属性-对象相互依赖性。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen combinations
of known objects and attributes by leveraging knowledge from previously seen
compositions. Traditional approaches primarily focus on disentangling
attributes and objects, treating them as independent entities during learning.
However, this assumption overlooks the semantic constraints and contextual
dependencies inside a composition. For example, certain attributes naturally
pair with specific objects (e.g., "striped" applies to "zebra" or "shirts" but
not "sky" or "water"), while the same attribute can manifest differently
depending on context (e.g., "young" in "young tree" vs. "young dog"). Thus,
capturing attribute-object interdependence remains a fundamental yet
long-ignored challenge in CZSL. In this paper, we adopt a Conditional
Probability Framework (CPF) to explicitly model attribute-object dependencies.
We decompose the probability of a composition into two components: the
likelihood of an object and the conditional likelihood of its attribute. To
enhance object feature learning, we incorporate textual descriptors to
highlight semantically relevant image regions. These enhanced object features
then guide attribute learning through a cross-attention mechanism, ensuring
better contextual alignment. By jointly optimizing object likelihood and
conditional attribute likelihood, our method effectively captures compositional
dependencies and generalizes well to unseen compositions. Extensive experiments
on multiple CZSL benchmarks demonstrate the superiority of our approach. Code
is available at here.

</details>


### [69] [EndoGen: Conditional Autoregressive Endoscopic Video Generation](https://arxiv.org/abs/2507.17388)
*Xinyu Liu,Hengyu Liu,Cheng Wang,Tianming Liu,Yixuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了EndoGen，一个条件内窥镜视频生成框架，通过SGP策略和SAT机制提高了生成视频的质量和多样性，并改善了息肉分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 以往的内窥镜视频生成方法要么侧重于静态图像，缺乏动态上下文，要么依赖于无条件生成，无法为临床医生提供有意义的参考。因此，本文旨在提出一种条件内窥镜视频生成框架。

Method: 本文提出了一种条件内窥镜视频生成框架EndoGen，构建了一个自回归模型，并采用了定制的时空网格-帧模式（SGP）策略，将生成多个帧的学习重新构建为基于网格的图像生成模式。此外，还提出了一种语义感知令牌掩蔽（SAT）机制，通过选择性地关注语义上有意义的区域来增强模型生成丰富多样内容的能力。

Result: 通过大量实验证明，该框架在生成高质量、条件引导的内窥镜内容方面是有效的，并且提高了息肉分割等下游任务的性能。

Conclusion: 所提出的EndoGen框架能够生成高质量、条件引导的内窥镜内容，并提升了息肉分割等下游任务的性能。

Abstract: Endoscopic video generation is crucial for advancing medical imaging and
enhancing diagnostic capabilities. However, prior efforts in this field have
either focused on static images, lacking the dynamic context required for
practical applications, or have relied on unconditional generation that fails
to provide meaningful references for clinicians. Therefore, in this paper, we
propose the first conditional endoscopic video generation framework, namely
EndoGen. Specifically, we build an autoregressive model with a tailored
Spatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the
learning of generating multiple frames as a grid-based image generation
pattern, which effectively capitalizes the inherent global dependency modeling
capabilities of autoregressive architectures. Furthermore, we propose a
Semantic-Aware Token Masking (SAT) mechanism, which enhances the model's
ability to produce rich and diverse content by selectively focusing on
semantically meaningful regions during the generation process. Through
extensive experiments, we demonstrate the effectiveness of our framework in
generating high-quality, conditionally guided endoscopic content, and improves
the performance of downstream task of polyp segmentation. Code released at
https://www.github.com/CUHK-AIM-Group/EndoGen.

</details>


### [70] [HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs](https://arxiv.org/abs/2507.17394)
*Zhaolin Cai,Fan Li,Ziwei Zheng,Yanjun Qin*

Main category: cs.CV

TL;DR: HiProbe-VAD利用预训练大语言模型的中间层信息，在无需微调的情况下，实现了高效、可泛化的视频异常检测，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测（VAD）方法存在计算需求大和依赖大量标注数据集的问题，限制了其实际应用。为了解决这些限制，研究旨在利用预训练的多模态大语言模型（MLLMs）的潜力，实现无需微调的VAD。

Method: 提出了一种名为HiProbe-VAD的新框架，该框架利用预训练的多模态大语言模型（MLLMs）进行视频异常检测（VAD），无需进行微调。通过动态层显著性探测（DLSP）机制，智能识别并提取MLLMs推理过程中最优中间层的隐藏状态。然后，利用提取的隐藏状态，通过一个轻量级的异常评分和时间定位模块来高效地检测异常并生成解释。

Result: HiProbe-VAD在UCF-Crime和XD-Violence数据集上的实验结果表明，该框架的性能优于现有的无训练方法以及大多数传统方法。此外，该框架在不同的MLLMs之间表现出卓越的跨模型泛化能力，无需任何调整。

Conclusion: HiProbe-VAD框架通过利用预训练的多模态大语言模型（MLLMs）的中间隐藏状态，并在无需微调的情况下，实现了高效的视频异常检测（VAD）。该方法通过动态层显著性探测（DLSP）机制提取信息丰富的隐藏状态，并结合轻量级异常评分和时间定位模块，有效检测异常并生成解释。实验证明，HiProbe-VAD在UCF-Crime和XD-Violence数据集上优于现有无训练和大多数传统方法，并展现出跨模型泛化能力。

Abstract: Video Anomaly Detection (VAD) aims to identify and locate deviations from
normal patterns in video sequences. Traditional methods often struggle with
substantial computational demands and a reliance on extensive labeled datasets,
thereby restricting their practical applicability. To address these
constraints, we propose HiProbe-VAD, a novel framework that leverages
pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring
fine-tuning. In this paper, we discover that the intermediate hidden states of
MLLMs contain information-rich representations, exhibiting higher sensitivity
and linear separability for anomalies compared to the output layer. To
capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)
mechanism that intelligently identifies and extracts the most informative
hidden states from the optimal intermediate layer during the MLLMs reasoning.
Then a lightweight anomaly scorer and temporal localization module efficiently
detects anomalies using these extracted hidden states and finally generate
explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate
that HiProbe-VAD outperforms existing training-free and most traditional
approaches. Furthermore, our framework exhibits remarkable cross-model
generalization capabilities in different MLLMs without any tuning, unlocking
the potential of pre-trained MLLMs for video anomaly detection and paving the
way for more practical and scalable solutions.

</details>


### [71] [Physics-based Human Pose Estimation from a Single Moving RGB Camera](https://arxiv.org/abs/2507.17406)
*Ayce Idil Aytekin,Chuqiao Li,Diogo Luvizon,Rishabh Dabral,Martin Oswald,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: MoviCam 是首个包含真实相机轨迹、场景几何和 3D 人体运动（带接触标签）的非合成数据集。PhysDynPose 是一个结合场景几何和物理约束的物理方法，用于在相机运动和非平面场景中进行更精确的人体运动跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有的基于单目和基于物理的人体姿态跟踪方法在处理非平面场景或移动相机时会出现伪影，并且在真实世界视频上的评估缺乏地面真实数据或在合成数据集上进行评估，这些都无法真实地模拟真实世界的光传输、相机运动以及姿态引起的外观和几何变化。

Method: 提出 MoviCam 数据集，包含动态移动的单目 RGB 相机、场景几何和具有人体-场景接触标签的 3D 人体运动的真实世界轨迹。提出 PhysDynPose 方法，结合场景几何和物理约束，通过运动估计器获得人体姿势，通过鲁棒的 SLAM 方法捕捉动态相机轨迹，从而在世界坐标系中恢复人体姿势，然后使用场景感知物理优化器来优化运动估计。

Result: MoviCam 数据集和 PhysDynPose 方法可以解决上述问题，即使是现有最先进的方法在具有挑战性的移动相机和非平面环境中也会遇到困难，而所提出的方法能够稳健地估计世界坐标系中的人体和相机位姿。

Conclusion: 提出的 PhysDynPose 方法在存在相机运动和非平面场景的情况下，能够稳健地估计世界坐标系中的人体和相机位姿，并且优于现有方法。

Abstract: Most monocular and physics-based human pose tracking methods, while achieving
state-of-the-art results, suffer from artifacts when the scene does not have a
strictly flat ground plane or when the camera is moving. Moreover, these
methods are often evaluated on in-the-wild real world videos without
ground-truth data or on synthetic datasets, which fail to model the real world
light transport, camera motion, and pose-induced appearance and geometry
changes. To tackle these two problems, we introduce MoviCam, the first
non-synthetic dataset containing ground-truth camera trajectories of a
dynamically moving monocular RGB camera, scene geometry, and 3D human motion
with human-scene contact labels. Additionally, we propose PhysDynPose, a
physics-based method that incorporates scene geometry and physical constraints
for more accurate human motion tracking in case of camera motion and non-flat
scenes. More precisely, we use a state-of-the-art kinematics estimator to
obtain the human pose and a robust SLAM method to capture the dynamic camera
trajectory, enabling the recovery of the human pose in the world frame. We then
refine the kinematic pose estimate using our scene-aware physics optimizer.
From our new benchmark, we found that even state-of-the-art methods struggle
with this inherently challenging setting, i.e. a moving camera and non-planar
environments, while our method robustly estimates both human and camera poses
in world coordinates.

</details>


### [72] [CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality Optimization in Computed Tomography](https://arxiv.org/abs/2507.17420)
*Sneha George Gnanakalavathy,Hairil Abdul Razak,Robert Meertens,Jonathan E. Fieldsend,Xujiong Ye,Mohammed M. Abdelsamea*

Main category: cs.CV

TL;DR: CAPRI-CT是一个因果感知的深度学习框架，用于CT成像中的因果分析和预测推理，以优化图像质量，它整合了图像数据和采集元数据，利用VAE提取特征，并通过预测SNR和支持反事实推理来实现“假设”模拟，旨在无需重复物理扫描即可优化CT方案。


<details>
  <summary>Details</summary>
Motivation: 在计算机断层扫描（CT）中，在尽量降低辐射暴露的同时实现高图像质量仍然是一个关键的临床挑战。

Method: 该研究提出的CAPRI-CT框架集成了图像数据和采集元数据（例如管电压、管电流和造影剂类型），利用变分自编码器（VAEs）提取特征并生成因果表示，然后融合这些特征来预测信噪比（SNR）并支持反事实推理，以进行“假设”模拟，例如改变造影剂（类型和浓度）或扫描参数。CAPRI-CT采用集成学习方法进行训练和验证。

Result: CAPRI-CT框架通过集成学习方法进行训练和验证，在预测性能方面表现出色，能够进行“假设”模拟，例如改变造影剂（类型和浓度）或扫描参数。

Conclusion: CAPRI-CT提供了一种新颖的因果感知深度学习框架，通过整合图像数据和采集元数据来优化CT成像的图像质量，并促进了预测和可解释性，为放射科医生和技师设计更有效的CT方案提供了可行的见解，而无需重复的物理扫描。

Abstract: In computed tomography (CT), achieving high image quality while minimizing
radiation exposure remains a key clinical challenge. This paper presents
CAPRI-CT, a novel causal-aware deep learning framework for Causal Analysis and
Predictive Reasoning for Image Quality Optimization in CT imaging. CAPRI-CT
integrates image data with acquisition metadata (such as tube voltage, tube
current, and contrast agent types) to model the underlying causal relationships
that influence image quality. An ensemble of Variational Autoencoders (VAEs) is
employed to extract meaningful features and generate causal representations
from observational data, including CT images and associated imaging parameters.
These input features are fused to predict the Signal-to-Noise Ratio (SNR) and
support counterfactual inference, enabling what-if simulations, such as changes
in contrast agents (types and concentrations) or scan parameters. CAPRI-CT is
trained and validated using an ensemble learning approach, achieving strong
predictive performance. By facilitating both prediction and interpretability,
CAPRI-CT provides actionable insights that could help radiologists and
technicians design more efficient CT protocols without repeated physical scans.
The source code and dataset are publicly available at
https://github.com/SnehaGeorge22/capri-ct.

</details>


### [73] [Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection](https://arxiv.org/abs/2507.17436)
*Yehao Lu,Minghe Weng,Zekang Xiao,Rui Jiang,Wei Su,Guangcong Zheng,Ping Lu,Xi Li*

Main category: cs.CV

TL;DR: Dynamic-DINO是一种新的MoE模型，在实时开放词汇目标检测中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 探索Mixture of Experts (MoE)架构在实时开放词汇目标检测器中的潜力，这类模型利用大规模视觉-语言数据集但模型较小。

Method: 提出了一种动态推理框架Dynamic-DINO，通过MoE调优策略扩展Grounding DINO 1.5 Edge，并设计了粒度分解机制将FFN分解为多个专家网络，同时提出预训练权重分配策略和路由器初始化来防止微调初期性能下降。

Result: Dynamic-DINO在仅使用1.56M开源数据预训练的情况下，性能优于在私有Grounding20M数据集上预训练的Grounding DINO 1.5 Edge。

Conclusion: Dynamic-DINO通过有效的MoE调优策略将Grounding DINO 1.5 Edge从密集模型扩展为动态推理框架，并在浅层和深层发现了专家协作的新模式。

Abstract: The Mixture of Experts (MoE) architecture has excelled in Large
Vision-Language Models (LVLMs), yet its potential in real-time open-vocabulary
object detectors, which also leverage large-scale vision-language datasets but
smaller models, remains unexplored. This work investigates this domain,
revealing intriguing insights. In the shallow layers, experts tend to cooperate
with diverse peers to expand the search space. While in the deeper layers,
fixed collaborative structures emerge, where each expert maintains 2-3 fixed
partners and distinct expert combinations are specialized in processing
specific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding
DINO 1.5 Edge from a dense model to a dynamic inference framework via an
efficient MoE-Tuning strategy. Additionally, we design a granularity
decomposition mechanism to decompose the Feed-Forward Network (FFN) of base
model into multiple smaller expert networks, expanding the subnet search space.
To prevent performance degradation at the start of fine-tuning, we further
propose a pre-trained weight allocation strategy for the experts, coupled with
a specific router initialization. During inference, only the input-relevant
experts are activated to form a compact subnet. Experiments show that,
pretrained with merely 1.56M open-source data, Dynamic-DINO outperforms
Grounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.

</details>


### [74] [Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection](https://arxiv.org/abs/2507.17456)
*Francesco Tonini,Lorenzo Vaquero,Alessandro Conti,Cigdem Beyan,Elisa Ricci*

Main category: cs.CV

TL;DR: A new framework called DYSCO uses text and visual information to improve human-object interaction detection, especially for rare interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing HOI methods rely on large datasets with manual annotations, which are labor-intensive, inconsistent, and limit scalability. Advances in Vision-Language Models (VLMs) offer untapped potential for enhancing interaction representation, but key gaps remain in prior work.

Method: DYSCO framework utilizes textual and visual interaction representations within a multimodal registry, incorporates a small set of visual cues, uses innovative interaction signatures to improve the semantic alignment of verbs, and employs a multi-head attention mechanism that adaptively weights the contributions of the visual and textual features.

Result: Experimental results demonstrate that DYSCO surpasses training-free state-of-the-art models and is competitive with training-based approaches, particularly excelling in rare interactions.

Conclusion: DYSCO surpasses training-free state-of-the-art models and is competitive with training-based approaches, particularly excelling in rare interactions.

Abstract: Human-Object Interaction (HOI) detection aims to identify humans and objects
within images and interpret their interactions. Existing HOI methods rely
heavily on large datasets with manual annotations to learn interactions from
visual cues. These annotations are labor-intensive to create, prone to
inconsistency, and limit scalability to new domains and rare interactions. We
argue that recent advances in Vision-Language Models (VLMs) offer untapped
potential, particularly in enhancing interaction representation. While prior
work has injected such potential and even proposed training-free methods, there
remain key gaps. Consequently, we propose a novel training-free HOI detection
framework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively
utilizes textual and visual interaction representations within a multimodal
registry, enabling robust and nuanced interaction understanding. This registry
incorporates a small set of visual cues and uses innovative interaction
signatures to improve the semantic alignment of verbs, facilitating effective
generalization to rare interactions. Additionally, we propose a unique
multi-head attention mechanism that adaptively weights the contributions of the
visual and textual features. Experimental results demonstrate that our DYSCO
surpasses training-free state-of-the-art models and is competitive with
training-based approaches, particularly excelling in rare interactions. Code is
available at https://github.com/francescotonini/dysco.

</details>


### [75] [ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents](https://arxiv.org/abs/2507.17462)
*Chang Nie,Guangming Wang,Zhe Lie,Hesheng Wang*

Main category: cs.CV

TL;DR: A new framework called ERMV efficiently edits 4D multi-view sequential images for robot imitation learning by using single-frame editing and robot state conditions. It addresses challenges like maintaining consistency across views, expanding the editing window, and ensuring semantic integrity using novel attention mechanisms, sparse sampling, and a feedback system with an MLLM.


<details>
  <summary>Details</summary>
Motivation: Data augmentation is needed for 4D multi-view sequential images in robot imitation learning, as high cost and scarcity of high-quality data limit the generalization and application of embodied intelligence policies like Vision-Language-Action (VLA) models. Existing methods for editing 4D multi-view sequential images for manipulation tasks are lacking.

Method: ERMV addresses challenges through Epipolar Motion-Aware Attention (EMA-Attn) for spatio-temporal consistency, a Sparse Spatio-Temporal (STT) module to expand the editing window with low computational cost, and a feedback intervention mechanism using a Multimodal Large Language Model (MLLM) to ensure semantic integrity.

Result: Extensive experiments demonstrate that ERMV-augmented data significantly boosts the robustness and generalization of VLA models in both simulated and real-world environments.

Conclusion: ERMV-augmented data significantly boosts the robustness and generalization of VLA models in both simulated and real-world environments.

Abstract: Robot imitation learning relies on 4D multi-view sequential images. However,
the high cost of data collection and the scarcity of high-quality data severely
constrain the generalization and application of embodied intelligence policies
like Vision-Language-Action (VLA) models. Data augmentation is a powerful
strategy to overcome data scarcity, but methods for editing 4D multi-view
sequential images for manipulation tasks are currently lacking. Thus, we
propose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation
framework that efficiently edits an entire multi-view sequence based on
single-frame editing and robot state conditions. This task presents three core
challenges: (1) maintaining geometric and appearance consistency across dynamic
views and long time horizons; (2) expanding the working window with low
computational costs; and (3) ensuring the semantic integrity of critical
objects like the robot arm. ERMV addresses these challenges through a series of
innovations. First, to ensure spatio-temporal consistency in motion blur, we
introduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that
learns pixel shift caused by movement before applying geometric constraints.
Second, to maximize the editing working window, ERMV pioneers a Sparse
Spatio-Temporal (STT) module, which decouples the temporal and spatial views
and remodels a single-frame multi-view problem through sparse sampling of the
views to reduce computational demands. Third, to alleviate error accumulation,
we incorporate a feedback intervention Mechanism, which uses a Multimodal Large
Language Model (MLLM) to check editing inconsistencies and request targeted
expert guidance only when necessary. Extensive experiments demonstrate that
ERMV-augmented data significantly boosts the robustness and generalization of
VLA models in both simulated and real-world environments.

</details>


### [76] [Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls](https://arxiv.org/abs/2507.17467)
*Elena Pitta,Tom Kouwenhoven,Tessa Verhoef*

Main category: cs.CV

TL;DR: LLaMA 3.2 11B Vision 模型在视觉蕴含任务上的表现表明，该任务可用于评估多模态理解能力，但需注意提示设计、样本数量和顺序等因素的影响。微调可提升模型性能并获得有意义的解释，但其视觉基础仍有待考量。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨视觉蕴含（VE）任务在多模态语言模型视觉-语言理解评估中的可靠性，并深入分析其优势和局限性。

Method: 本研究使用 LLaMA 3.2 11B Vision 模型作为测试用例，通过零样本、少样本和微调等多种实验设置，探索了提示设计、 in-context 示例的数量和顺序以及视觉信息的可访问性等因素对 VE 性能的影响。此外，还采用了基于解释的评估来深入探究模型的推理过程。

Result: 研究发现，三次样本推理优于零样本基线，但过多的样本弊大于利。标签的顺序对模型的预测有显著影响。在缺乏视觉信息的情况下，模型容易产生幻觉，这表明其可能过度依赖语言先验。微调模型在 e-SNLI-VE 数据集上达到了 83.3% 的准确率，优于 OFA-X 模型。解释评估显示，微调模型能提供与人类相似的、有语义的解释（BERTScore F1-score 为 89.2%）。然而，在有限视觉信息的情况下，相似的 BERTScore 结果引发了对该任务视觉基础的质疑。

Conclusion: 该研究表明，视觉蕴含（VE）任务在评估多模态语言模型的视觉-语言理解能力方面具有一定的效用，但也存在局限性。研究结果强调了改进多模态评估方法的方向。

Abstract: This study investigates the extent to which the Visual Entailment (VE) task
serves as a reliable probe of vision-language understanding in multimodal
language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond
reporting performance metrics, we aim to interpret what these results reveal
about the underlying possibilities and limitations of the VE task. We conduct a
series of experiments across zero-shot, few-shot, and fine-tuning settings,
exploring how factors such as prompt design, the number and order of in-context
examples and access to visual information might affect VE performance. To
further probe the reasoning processes of the model, we used explanation-based
evaluations. Results indicate that three-shot inference outperforms the
zero-shot baselines. However, additional examples introduce more noise than
they provide benefits. Additionally, the order of the labels in the prompt is a
critical factor that influences the predictions. In the absence of visual
information, the model has a strong tendency to hallucinate and imagine
content, raising questions about the model's over-reliance on linguistic
priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on
the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.
Additionally, the explanation evaluation demonstrates that the fine-tuned model
provides semantically meaningful explanations similar to those of humans, with
a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore
results in experiments with limited vision, questioning the visual grounding of
this task. Overall, our results highlight both the utility and limitations of
VE as a diagnostic task for vision-language understanding and point to
directions for refining multimodal evaluation methods.

</details>


### [77] [SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving](https://arxiv.org/abs/2507.17479)
*Chuang Chen,Xiaolin Qin,Jing Hu,Wenyi Ge*

Main category: cs.CV

TL;DR: SRMambaV2通过仿生2D选择性扫描自注意力机制、双分支网络和渐进自适应损失，有效解决了LiDAR点云上采样中稀疏区域的重建难题，提高了精度和细节保留。


<details>
  <summary>Details</summary>
Motivation: 现有的将3D点云数据转换为2D图像超分辨率任务的研究，由于感知图像的特征表示稀疏且模糊，导致在精确重建复杂的3D空间拓扑结构方面存在重大困难。为了解决这个问题，本研究旨在提高长距离稀疏区域的上采样精度，同时保持整体几何重建质量。

Method: 提出了一种新颖的稀疏点云上采样方法SRMambaV2，其特点包括：1. 受人类驾驶员视觉感知启发，设计了一种仿生2D选择性扫描自注意力（2DSSA）机制来建模遥远稀疏区域的特征分布。2. 引入了双分支网络架构来增强稀疏特征的表示。3. 引入了渐进自适应损失（PAL）函数，在整个上采样过程中进一步优化细粒度细节的重建。

Result: 实验结果表明，SRMambaV2在定性和定量评估中均取得了优越的性能，有效解决了LiDAR点云上采样中的稀疏性和复杂3D结构挑战。

Conclusion: SRMambaV2在汽车稀疏点云上采样任务中表现出优越的性能，在定性和定量评估中均优于现有方法，证明了其有效性和实用价值。

Abstract: Upsampling LiDAR point clouds in autonomous driving scenarios remains a
significant challenge due to the inherent sparsity and complex 3D structures of
the data. Recent studies have attempted to address this problem by converting
the complex 3D spatial scenes into 2D image super-resolution tasks. However,
due to the sparse and blurry feature representation of range images, accurately
reconstructing detailed and complex spatial topologies remains a major
difficulty. To tackle this, we propose a novel sparse point cloud upsampling
method named SRMambaV2, which enhances the upsampling accuracy in long-range
sparse regions while preserving the overall geometric reconstruction quality.
Specifically, inspired by human driver visual perception, we design a
biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the
feature distribution in distant sparse areas. Meanwhile, we introduce a
dual-branch network architecture to enhance the representation of sparse
features. In addition, we introduce a progressive adaptive loss (PAL) function
to further refine the reconstruction of fine-grained details during the
upsampling process. Experimental results demonstrate that SRMambaV2 achieves
superior performance in both qualitative and quantitative evaluations,
highlighting its effectiveness and practical value in automotive sparse point
cloud upsampling tasks.

</details>


### [78] [Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease](https://arxiv.org/abs/2507.17486)
*Hugues Roy,Reuben Dorent,Ninon Burgos*

Main category: cs.CV

TL;DR: 本文提出 AnoBFN，一种利用贝叶斯流网络进行无监督异常检测的新方法，在阿尔茨海默病检测任务中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测 (UAD) 在神经影像学中对于识别偏离健康受试者数据至关重要，有助于神经系统疾病的诊断。然而，贝叶斯流网络 (BFNs) 这一新颖的生成模型类别尚未应用于医学成像或异常检测。

Method: 本文提出 AnoBFN，一种基于贝叶斯流网络 (BFNs) 的无监督异常检测方法。BFNs 结合了扩散模型和贝叶斯推理的优点。AnoBFN 通过条件图像生成（处理高相关噪声）和在生成过程中递归地从输入图像获取信息来保留受试者特异性。

Result: AnoBFN 在 FDG PET 图像的阿尔茨海默病相关异常检测任务中，相比于 beta-VAE、f-AnoGAN 和 AnoDDPM 等现有方法，取得了更好的性能，有效检测出异常并降低了假阳性率。

Conclusion: AnoBFN 在 FDG PET 图像的阿尔茨海默病相关异常检测任务中表现优于基于 VAEs (beta-VAE)、GANs (f-AnoGAN) 和扩散模型 (AnoDDPM) 的最先进方法，能够有效检测异常并降低误报率。

Abstract: Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for
identifying deviations from healthy subject data and thus facilitating the
diagnosis of neurological disorders. In this work, we focus on Bayesian flow
networks (BFNs), a novel class of generative models, which have not yet been
applied to medical imaging or anomaly detection. BFNs combine the strength of
diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension
of BFNs for UAD, designed to: i) perform conditional image generation under
high levels of spatially correlated noise, and ii) preserve subject specificity
by incorporating a recursive feedback from the input image throughout the
generative process. We evaluate AnoBFN on the challenging task of Alzheimer's
disease-related anomaly detection in FDG PET images. Our approach outperforms
other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and
diffusion models (AnoDDPM), demonstrating its effectiveness at detecting
anomalies while reducing false positive rates.

</details>


### [79] [DFDNet: Dynamic Frequency-Guided De-Flare Network](https://arxiv.org/abs/2507.17489)
*Minglong Xue,Aoxiang Ning,Shivakumara Palaiahnakote,Mingliang Zhou*

Main category: cs.CV

TL;DR: 提出DFDNet，一种在频域中分离内容和光晕信息的去光晕网络，通过GDFG和LDGM模块有效去除大尺度光晕伪影并修复细节损伤，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在去除大尺度光晕伪影和修复近光源区域的结构损伤方面仍有不足。光晕伪影在频域中比在空间域中与参考图像的差异更显著。

Method: 提出了一种新颖的动态频域引导去光晕网络（DFDNet），该网络在频域中将内容信息与光晕伪影分离。DFDNet包含一个全局动态频域引导（GDFG）模块和一个局部细节引导模块（LDGM）。GDFG模块通过动态优化全局频域特征来指导网络感知光晕伪影的频域特征，从而有效分离光晕信息和内容信息。LDGM通过对比学习策略对齐光源的局部特征与参考图像，减少了光晕去除对局部细节的损害，并改善了细粒度的图像恢复。

Result: 实验结果表明，所提出的方法在性能上优于现有的最先进方法。

Conclusion: DFDNet在去除大尺度光晕伪影和修复近光源区域的结构损伤方面表现出色，并且在图像细节恢复方面有所提升。

Abstract: Strong light sources in nighttime photography frequently produce flares in
images, significantly degrading visual quality and impacting the performance of
downstream tasks. While some progress has been made, existing methods continue
to struggle with removing large-scale flare artifacts and repairing structural
damage in regions near the light source. We observe that these challenging
flare artifacts exhibit more significant discrepancies from the reference
images in the frequency domain compared to the spatial domain. Therefore, this
paper presents a novel dynamic frequency-guided deflare network (DFDNet) that
decouples content information from flare artifacts in the frequency domain,
effectively removing large-scale flare artifacts. Specifically, DFDNet consists
mainly of a global dynamic frequency-domain guidance (GDFG) module and a local
detail guidance module (LDGM). The GDFG module guides the network to perceive
the frequency characteristics of flare artifacts by dynamically optimizing
global frequency domain features, effectively separating flare information from
content information. Additionally, we design an LDGM via a contrastive learning
strategy that aligns the local features of the light source with the reference
image, reduces local detail damage from flare removal, and improves
fine-grained image restoration. The experimental results demonstrate that the
proposed method outperforms existing state-of-the-art methods in terms of
performance. The code is available at
\href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}.

</details>


### [80] [Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation](https://arxiv.org/abs/2507.17508)
*Jorgen Cani,Christos Diou,Spyridon Evangelatos,Vasileios Argyriou,Panagiotis Radoglou-Grammatikis,Panagiotis Sarigiannidis,Iraklis Varlamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本研究对自动X射线检测领域的十种最新深度学习方法进行了全面的比较评估，涵盖了六个数据集和多种性能指标，为该领域的研究提供了重要的见解和可复现的资源。


<details>
  <summary>Details</summary>
Motivation: 自动X射线检测在公共场所的安全筛查中至关重要，但面临物体遮挡、物品物理特性差异、扫描设备多样性以及训练数据有限等挑战。尽管已有大量研究，但实验评估往往不完整且结果相互矛盾。因此，有必要对该领域的最新深度学习方法进行系统、详细和全面的比较评估，以阐明研究现状并促进未来的研究。

Method: 该研究采用了一个全面的评估框架，该框架包括：a) 六个公开的X射线非法物品检测数据集（OPIXray、CLCXray、SIXray、EDS、HiXray和PIDray）；b) 十种最先进的物体检测方案，涵盖了卷积神经网络（CNN）、定制CNN、通用Transformer以及CNN-Transformer混合架构等主要类别；c) 多种检测指标（mAP50和mAP50:95）和时间/计算复杂度指标（推理时间（毫秒）、参数大小（兆字节）和计算负载（GFLOPS））。通过分析这些数据，研究人员对各种方法的性能进行了深入的比较和评估。

Result: 该研究对十种不同的最先进物体检测方案在六个大型公共X射线数据集上的性能进行了详细的比较评估。分析结果揭示了各种方法的整体表现、物体级别检测能力、在不同数据集上的特定表现以及它们在时间和计算复杂度方面的权衡。研究强调了这些关键方面的见解，旨在为该领域的研究提供指导。

Conclusion: 该研究通过在一个包含六个数据集和十种最先进的物体检测方案的全面评估框架下，对基于深度学习的X射线物体检测方法进行了系统、详细和彻底的比较评估。评估结果强调了物体检测方案的整体行为、物体级别的检测性能、特定数据集的观察结果以及时间效率和计算复杂性分析。该研究为X射线安全筛查领域的研究提供了宝贵的见解和可复现的实验结果，并公开了评估代码和模型权重以支持进一步的研究。

Abstract: Automated X-ray inspection is crucial for efficient and unobtrusive security
screening in various public settings. However, challenges such as object
occlusion, variations in the physical properties of items, diversity in X-ray
scanning devices, and limited training data hinder accurate and reliable
detection of illicit items. Despite the large body of research in the field,
reported experimental evaluations are often incomplete, with frequently
conflicting outcomes. To shed light on the research landscape and facilitate
further research, a systematic, detailed, and thorough comparative evaluation
of recent Deep Learning (DL)-based methods for X-ray object detection is
conducted. For this, a comprehensive evaluation framework is developed,
composed of: a) Six recent, large-scale, and widely used public datasets for
X-ray illicit item detection (OPIXray, CLCXray, SIXray, EDS, HiXray, and
PIDray), b) Ten different state-of-the-art object detection schemes covering
all main categories in the literature, including generic Convolutional Neural
Network (CNN), custom CNN, generic transformer, and hybrid CNN-transformer
architectures, and c) Various detection (mAP50 and mAP50:95) and
time/computational-complexity (inference time (ms), parameter size (M), and
computational load (GFLOPS)) metrics. A thorough analysis of the results leads
to critical observations and insights, emphasizing key aspects such as: a)
Overall behavior of the object detection schemes, b) Object-level detection
performance, c) Dataset-specific observations, and d) Time efficiency and
computational complexity analysis. To support reproducibility of the reported
experimental results, the evaluation code and model weights are made publicly
available at https://github.com/jgenc/xray-comparative-evaluation.

</details>


### [81] [Accelerating Parallel Diffusion Model Serving with Residual Compression](https://arxiv.org/abs/2507.17511)
*Jiajun Luo,Yicheng Xiao,Jianru Xu,Yangxiu You,Rongwei Lu,Chen Tang,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

TL;DR: CompactFusion通过残差压缩技术减少了扩散模型并行推理的通信开销，实现了更快的速度和更高的质量。


<details>
  <summary>Details</summary>
Motivation: 并行推理会产生大量的通信开销，限制了效率和可扩展性。

Method: CompactFusion框架通过残差压缩（传输步进激活差异）和轻量级错误反馈来减少通信开销，同时保持生成质量。

Result: CompactFusion显著降低了通信开销，同时保持了生成质量，在4xL20上实现了3.0倍的加速，在慢速网络上实现了6.7倍的加速，优于现有方法。

Conclusion: CompactFusion通过仅传输压缩残差（步进激活差异）并集成轻量级错误反馈，实现了显著的数据缩减和高保真度，从而解决了并行推理中的通信开销问题。它在4xL20上实现了3.0倍的加速和更高的保真度，在慢速网络上实现了6.7倍的加速，为并行扩散推理树立了新范式，并且易于集成到现有模型中。

Abstract: Diffusion models produce realistic images and videos but require substantial
computational resources, necessitating multi-accelerator parallelism for
real-time deployment. However, parallel inference introduces significant
communication overhead from exchanging large activations between devices,
limiting efficiency and scalability. We present CompactFusion, a compression
framework that significantly reduces communication while preserving generation
quality. Our key observation is that diffusion activations exhibit strong
temporal redundancy-adjacent steps produce highly similar activations,
saturating bandwidth with near-duplicate data carrying little new information.
To address this inefficiency, we seek a more compact representation that
encodes only the essential information. CompactFusion achieves this via
Residual Compression that transmits only compressed residuals (step-wise
activation differences). Based on empirical analysis and theoretical
justification, we show that it effectively removes redundant data, enabling
substantial data reduction while maintaining high fidelity. We also integrate
lightweight error feedback to prevent error accumulation. CompactFusion
establishes a new paradigm for parallel diffusion inference, delivering lower
latency and significantly higher generation quality than prior methods. On
4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also
uniquely supports communication-heavy strategies like sequence parallelism on
slow networks, achieving 6.7x speedup over prior overlap-based method.
CompactFusion applies broadly across diffusion models and parallel settings,
and integrates easily without requiring pipeline rework. Portable
implementation demonstrated on xDiT is publicly available at
https://github.com/Cobalt-27/CompactFusion

</details>


### [82] [STQE: Spatial-Temporal Quality Enhancement for G-PCC Compressed Dynamic Point Clouds](https://arxiv.org/abs/2507.17522)
*Tian Guo,Hui Yuan,Xiaolong Mao,Shiqi Jiang,Raouf Hamzaoui,Sam Kwong*

Main category: cs.CV

TL;DR: 提出了一种名为 STQE 的新网络，用于提高 G-PCC 压缩动态点云的视觉质量。通过利用空间和时间相关性，并采用创新的模块（如基于重渲染的运动补偿、通道感知的时间注意力、高斯引导的邻域特征聚合和基于 Pearson 相关系数的联合损失函数），STQE 在 PSNR 和 BD-rate 方面取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 解决了现有研究中针对压缩动态点云的质量增强不足的问题，特别是有效利用点云帧之间的空间-时间相关性的探索不足。

Method: 提出了一种空间-时间属性质量增强 (STQE) 网络，该网络利用空间和时间相关性来提高 G-PCC 压缩动态点云的视觉质量。具体包括：1. 基于重渲染的运动补偿模块，用于精确的帧间几何对齐。2. 通道感知的时间注意力模块，用于跨双向参考帧动态突出相关区域。3. 高斯引导的邻域特征聚合模块，用于捕获几何和颜色属性之间的空间依赖性。4. 基于 Pearson 相关系数的联合损失函数，用于缓解点对均方误差优化中的过度平滑效应。

Result: STQE 在 G-PCC 的最新测试模型上实现了显著的性能提升。

Conclusion: STQE 在 Luma、Cb 和 Cr 分量上分别实现了 0.855 dB、0.682 dB 和 0.828 dB 的 delta PSNR 提升，并分别实现了 -25.2%、-31.6% 和 -32.5% 的 Bj{\o}ntegaard Delta 率 (BD-rate) 降低。

Abstract: Very few studies have addressed quality enhancement for compressed dynamic
point clouds. In particular, the effective exploitation of spatial-temporal
correlations between point cloud frames remains largely unexplored. Addressing
this gap, we propose a spatial-temporal attribute quality enhancement (STQE)
network that exploits both spatial and temporal correlations to improve the
visual quality of G-PCC compressed dynamic point clouds. Our contributions
include a recoloring-based motion compensation module that remaps reference
attribute information to the current frame geometry to achieve precise
inter-frame geometric alignment, a channel-aware temporal attention module that
dynamically highlights relevant regions across bidirectional reference frames,
a Gaussian-guided neighborhood feature aggregation module that efficiently
captures spatial dependencies between geometry and color attributes, and a
joint loss function based on the Pearson correlation coefficient, designed to
alleviate over-smoothing effects typical of point-wise mean squared error
optimization. When applied to the latest G-PCC test model, STQE achieved
improvements of 0.855 dB, 0.682 dB, and 0.828 dB in delta PSNR, with
Bj{\o}ntegaard Delta rate (BD-rate) reductions of -25.2%, -31.6%, and -32.5%
for the Luma, Cb, and Cr components, respectively.

</details>


### [83] [Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding](https://arxiv.org/abs/2507.17533)
*Liwen Liu,Weidong Yang,Lipeng Ma,Ben Fei*

Main category: cs.CV

TL;DR: MMPT是一个多模态多任务预训练框架，通过三种预训练任务（TLR、PLR、MCL）增强点云理解，无需3D标注，提升了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态预训练方法主要依赖单一预训练任务，限制了模型获取多模态数据的能力，影响了在复杂和多样化领域下游任务的表现。

Method: 提出了一种名为MMPT的多模态多任务预训练框架，包含三种预训练任务：(i) Token-level reconstruction (TLR) 恢复掩码点，（ii）Point-level reconstruction (PLR) 预测掩码点位置，（iii）Multi-modal contrastive learning (MCL) 结合跨模态特征对应。

Result: 在各种判别性和生成性应用中，与最先进的方法相比，MMPT表现出了更优越的性能。

Conclusion: 该框架通过多项任务学习，增强了点云理解能力，并且无需3D标注，可扩展性强，有效提升了在各种下游任务中的表现。

Abstract: Recent advances in multi-modal pre-training methods have shown promising
effectiveness in learning 3D representations by aligning multi-modal features
between 3D shapes and their corresponding 2D counterparts. However, existing
multi-modal pre-training frameworks primarily rely on a single pre-training
task to gather multi-modal data in 3D applications. This limitation prevents
the models from obtaining the abundant information provided by other relevant
tasks, which can hinder their performance in downstream tasks, particularly in
complex and diverse domains. In order to tackle this issue, we propose MMPT, a
Multi-modal Multi-task Pre-training framework designed to enhance point cloud
understanding. Specifically, three pre-training tasks are devised: (i)
Token-level reconstruction (TLR) aims to recover masked point tokens, endowing
the model with representative learning abilities. (ii) Point-level
reconstruction (PLR) is integrated to predict the masked point positions
directly, and the reconstructed point cloud can be considered as a transformed
point cloud used in the subsequent task. (iii) Multi-modal contrastive learning
(MCL) combines feature correspondences within and across modalities, thus
assembling a rich learning signal from both 3D point cloud and 2D image
modalities in a self-supervised manner. Moreover, this framework operates
without requiring any 3D annotations, making it scalable for use with large
datasets. The trained encoder can be effectively transferred to various
downstream tasks. To demonstrate its effectiveness, we evaluated its
performance compared to state-of-the-art methods in various discriminant and
generative applications under widely-used benchmarks.

</details>


### [84] [An h-space Based Adversarial Attack for Protection Against Few-shot Personalization](https://arxiv.org/abs/2507.17554)
*Xide Xu,Sandesh Kamath,Muhammad Atif Butt,Bogdan Raducanu*

Main category: cs.CV

TL;DR: HAAD 和 HAAD-KV 是新的反定制化方法，可防御扩散模型中的隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成定制化图像方面的多功能性引发了对隐私的担忧，尤其是在未经授权修改私人内容方面。因此，需要开发基于对抗性攻击的保护机制，以破坏扩散模型。

Method: HAAD 和 HAAD-KV 利用扩散模型 h-space 的抽象特性来设计对抗性攻击，以阻止定制化。HAAD-KV 进一步通过仅基于 h-space 的 KV 参数来提高效率和防御能力。

Result: HAAD 和 HAAD-KV 在阻止扩散模型中的定制化方面被证明是有效的，并且在对抗性攻击方面优于最先进的技术，同时 HAAD-KV 提供了更高的计算效率。

Conclusion: HAAD and HAAD-KV 是一种新颖的、基于 h-space 的对抗性攻击方法，可用于防御扩散模型中的定制化。尽管方法简单，但它们在降级图像生成过程方面优于最先进的对抗性攻击。

Abstract: The versatility of diffusion models in generating customized images from few
samples raises significant privacy concerns, particularly regarding
unauthorized modifications of private content. This concerning issue has
renewed the efforts in developing protection mechanisms based on adversarial
attacks, which generate effective perturbations to poison diffusion models. Our
work is motivated by the observation that these models exhibit a high degree of
abstraction within their semantic latent space (`h-space'), which encodes
critical high-level features for generating coherent and meaningful content. In
this paper, we propose a novel anti-customization approach, called HAAD
(h-space based Adversarial Attack for Diffusion models), that leverages
adversarial attacks to craft perturbations based on the h-space that can
efficiently degrade the image generation process. Building upon HAAD, we
further introduce a more efficient variant, HAAD-KV, that constructs
perturbations solely based on the KV parameters of the h-space. This strategy
offers a stronger protection, that is computationally less expensive. Despite
their simplicity, our methods outperform state-of-the-art adversarial attacks,
highlighting their effectiveness.

</details>


### [85] [RemixFusion: Residual-based Mixed Representation for Large-scale Online RGB-D Reconstruction](https://arxiv.org/abs/2507.17594)
*Yuqing Lan,Chenyang Zhu,Shuaifeng Zhi,Jiazhao Zhang,Zhoufeng Wang,Renjiao Yi,Yijie Wang,Kai Xu*

Main category: cs.CV

TL;DR: RemixFusion是一种结合显式和隐式表示的新型场景重建方法，通过残差学习实现细节丰富、高效的大规模在线重建和相机跟踪。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有神经隐式表示在重建细节不足和学习耗时的问题，以及传统显式表示（如TSDF）的局限性，RemixFusion旨在实现高质量、大规模的在线RGB-D重建。

Method: 提出了一种新颖的残差混合表示方法，结合了显式的TSDF网格和隐式的神经模块来生成残差，以实现细节丰富的重建。该方法还通过优化位姿变化和采用自适应梯度放大技术来处理多帧联合位姿优化，并通过局部移动体积实现高效的在线学习。

Result: RemixFusion能够实现细节丰富且具有时间与内存预算限制的重建，克服了纯隐式表示的平滑化结果，并实现了高质量的相机跟踪。该方法在映射和跟踪精度上均优于现有最先进的方法。

Conclusion: RemixFusion在精度和跟踪方面超越了所有最先进的方法，包括基于显式或隐式表示的方法，尤其是在大规模场景中。

Abstract: The introduction of the neural implicit representation has notably propelled
the advancement of online dense reconstruction techniques. Compared to
traditional explicit representations, such as TSDF, it improves the mapping
completeness and memory efficiency. However, the lack of reconstruction details
and the time-consuming learning of neural representations hinder the widespread
application of neural-based methods to large-scale online reconstruction. We
introduce RemixFusion, a novel residual-based mixed representation for scene
reconstruction and camera pose estimation dedicated to high-quality and
large-scale online RGB-D reconstruction. In particular, we propose a
residual-based map representation comprised of an explicit coarse TSDF grid and
an implicit neural module that produces residuals representing fine-grained
details to be added to the coarse grid. Such mixed representation allows for
detail-rich reconstruction with bounded time and memory budget, contrasting
with the overly-smoothed results by the purely implicit representations, thus
paving the way for high-quality camera tracking. Furthermore, we extend the
residual-based representation to handle multi-frame joint pose optimization via
bundle adjustment (BA). In contrast to the existing methods, which optimize
poses directly, we opt to optimize pose changes. Combined with a novel
technique for adaptive gradient amplification, our method attains better
optimization convergence and global optimality. Furthermore, we adopt a local
moving volume to factorize the mixed scene representation with a
divide-and-conquer design to facilitate efficient online learning in our
residual-based framework. Extensive experiments demonstrate that our method
surpasses all state-of-the-art ones, including those based either on explicit
or implicit representations, in terms of the accuracy of both mapping and
tracking on large-scale scenes.

</details>


### [86] [InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and LiDAR Reflectance Modeling](https://arxiv.org/abs/2507.17613)
*Xiaoxue Chen,Bhargav Chandaka,Chih-Hao Lin,Ya-Qin Zhang,David Forsyth,Hao Zhao,Shenlong Wang*

Main category: cs.CV

TL;DR: InvRGB+L利用LiDAR强度来改进从RGB+LiDAR序列进行的逆向渲染，特别是在材料估计方面，并且在城市逆向渲染和LiDAR模拟方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统逆向图形方法主要依赖RGB观测，并将LiDAR主要用于几何信息，这常常导致在可见光干扰下材料估计不佳。研究人员发现，在不同光谱范围内主动照明捕获的LiDAR强度值可以为可变光照下的鲁棒材料估计提供互补线索。

Method: InvRGB+L是一个新颖的逆向渲染模型，它利用物理启发的LiDAR着色模型和RGB-LiDAR材料一致性损失，结合RGB和LiDAR强度信息，从单个RGB+LiDAR序列中重建大型、可重渲染和动态场景。

Result: 该模型能够生成城市和室内场景的新视角RGB和LiDAR渲染，并支持重渲染、夜间模拟和动态对象插入。

Conclusion: InvRGB+L在城市逆向渲染和LiDAR模拟方面取得了超越现有最先进方法的结果。

Abstract: We present InvRGB+L, a novel inverse rendering model that reconstructs large,
relightable, and dynamic scenes from a single RGB+LiDAR sequence. Conventional
inverse graphics methods rely primarily on RGB observations and use LiDAR
mainly for geometric information, often resulting in suboptimal material
estimates due to visible light interference. We find that LiDAR's intensity
values-captured with active illumination in a different spectral range-offer
complementary cues for robust material estimation under variable lighting.
Inspired by this, InvRGB+L leverages LiDAR intensity cues to overcome
challenges inherent in RGB-centric inverse graphics through two key
innovations: (1) a novel physics-based LiDAR shading model and (2) RGB-LiDAR
material consistency losses. The model produces novel-view RGB and LiDAR
renderings of urban and indoor scenes and supports relighting, night
simulations, and dynamic object insertions, achieving results that surpass
current state-of-the-art methods in both scene-level urban inverse rendering
and LiDAR simulation.

</details>


### [87] [Vision Transformer attention alignment with human visual perception in aesthetic object evaluation](https://arxiv.org/abs/2507.17616)
*Miguel Carrasco,César González-Martín,José Aranda,Luis Oliveros*

Main category: cs.CV

TL;DR: 本研究发现，ViT模型的某些注意力头（特别是#12）可以模拟人类的视觉注意力模式，但总体上ViT的注意力比人类更全局化。研究结果有助于将ViT应用于产品设计和审美评估。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索视觉注意力机制在人类感知和审美评估中的作用，以及视觉Transformer（ViT）的注意力机制与人类视觉注意力模式的对应关系，尤其是在审美评估的背景下，这在以往的研究中探索不足。

Method: 本研究通过眼动追踪实验，记录了30名参与者在观看20件手工制品（包括藤条包和姜罐）时的注视模式和生成人类视觉注意力的热图。同时，使用经过预训练的ViT模型和DINO（无标签自蒸馏）分析了相同的物体，并提取了12个注意力头的注意力图。研究使用KL散度在不同的高斯参数（sigma=0.1至3.0）下比较了人类和ViT的注意力分布。

Result: 研究发现在sigma=2.4±0.03时，注意力头#12与人类视觉模式的相关性最强。研究还发现不同注意力头之间存在显著差异，其中注意力头#7和#9与人类注意力的偏差最大（p<0.05，Tukey HSD检验）。结果表明，ViT表现出比人类更全局的注意力模式，但某些注意力头可以近似人类的视觉行为。

Conclusion: ViT的某些注意力头可以近似人类的视觉行为，特别是对于篮筐物品的带扣等特定物体特征。研究结果表明，ViT注意力机制在产品设计和美学评估方面具有潜在的应用价值，同时也凸显了人类感知与当前AI模型在注意力策略上的根本差异。

Abstract: Visual attention mechanisms play a crucial role in human perception and
aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have
demonstrated remarkable capabilities in computer vision tasks, yet their
alignment with human visual attention patterns remains underexplored,
particularly in aesthetic contexts. This study investigates the correlation
between human visual attention and ViT attention mechanisms when evaluating
handcrafted objects. We conducted an eye-tracking experiment with 30
participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal
objects comprising basketry bags and ginger jars. Using a Pupil Labs
eye-tracker, we recorded gaze patterns and generated heat maps representing
human visual attention. Simultaneously, we analyzed the same objects using a
pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting
attention maps from each of the 12 attention heads. We compared human and ViT
attention distributions using Kullback-Leibler divergence across varying
Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal
correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest
alignment with human visual patterns. Significant differences were found
between attention heads, with heads #7 and #9 demonstrating the greatest
divergence from human attention (p< 0.05, Tukey HSD test). Results indicate
that while ViTs exhibit more global attention patterns compared to human focal
attention, certain attention heads can approximate human visual behavior,
particularly for specific object features like buckles in basketry items. These
findings suggest potential applications of ViT attention mechanisms in product
design and aesthetic evaluation, while highlighting fundamental differences in
attention strategies between human perception and current AI models.

</details>


### [88] [Reusing Attention for One-stage Lane Topology Understanding](https://arxiv.org/abs/2507.17617)
*Yang Li,Zongzheng Zhang,Xuchong Qiu,Xinrun Li,Ziming Liu,Leichen Wang,Ruikai Li,Zhenxin Zhu,Huan-ang Gao,Xiaojian Lin,Zhiyong Cui,Hang Zhao,Hao Zhao*

Main category: cs.CV

TL;DR: 提出了一种单阶段架构，通过重用 Transformer 解码器中的中间注意力资源，同时预测交通元素、车道中心线和拓扑关系，提高了准确性和推理速度，并首次证明了从使用标准定义 (SD) 地图的模型到不使用 SD 地图的模型可以进行知识蒸馏，即使在没有 SD 地图的情况下也能获得卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的两阶段方法由于错误传播和增加的计算开销而效率低下，因此需要一种改进的方法来准确理解车道拓扑关系，以实现安全的自动驾驶。

Method: 提出了一种单阶段架构，可同时预测交通元素、车道中心线和拓扑关系，并通过在不同的 Transformer 解码器中重用中间注意力资源来改进车道拓扑理解的准确性和推理速度。

Result: 该方法在准确性和效率方面均优于基线方法，在车道线检测、交通元素识别和拓扑推理方面取得了卓越的成果。

Conclusion: 该方法在OpenLane-V2数据集上的大量实验表明，在准确性和效率方面均优于基线方法，在车道线检测、交通元素识别和拓扑推理方面取得了卓越的成果。

Abstract: Understanding lane toplogy relationships accurately is critical for safe
autonomous driving. However, existing two-stage methods suffer from
inefficiencies due to error propagations and increased computational overheads.
To address these challenges, we propose a one-stage architecture that
simultaneously predicts traffic elements, lane centerlines and topology
relationship, improving both the accuracy and inference speed of lane topology
understanding for autonomous driving. Our key innovation lies in reusing
intermediate attention resources within distinct transformer decoders. This
approach effectively leverages the inherent relational knowledge within the
element detection module to enable the modeling of topology relationships among
traffic elements and lanes without requiring additional computationally
expensive graph networks. Furthermore, we are the first to demonstrate that
knowledge can be distilled from models that utilize standard definition (SD)
maps to those operates without using SD maps, enabling superior performance
even in the absence of SD maps. Extensive experiments on the OpenLane-V2
dataset show that our approach outperforms baseline methods in both accuracy
and efficiency, achieving superior results in lane detection, traffic element
identification, and topology reasoning. Our code is available at
https://github.com/Yang-Li-2000/one-stage.git.

</details>


### [89] [The Early Bird Identifies the Worm: You Can't Beat a Head Start in Long-Term Body Re-ID (ECHO-BID)](https://arxiv.org/abs/2507.17640)
*Thomas M. Metz,Matthew Q. Hill,Alice J. O'Toole*

Main category: cs.CV

TL;DR: ECHO-BID模型通过使用EVA-02 Large骨干网络并在最具挑战性的换装数据上进行迁移学习，在长期重新识别任务上取得了最先进的成果，尤其在遮挡场景下表现突出。模型的成功归因于更大的模型尺寸和预训练期间的掩码图像建模。


<details>
  <summary>Details</summary>
Motivation: 在不受约束的视角环境中，由于距离、视角、成像条件和服装的变化，进行身份识别存在重大挑战。

Method: 提出了一种基于对象预训练的EVA-02 Large骨干网络的ECHO-BID模型，并在各种约束、非约束和遮挡场景的基准数据集上进行了评估。

Result: ECHO-BID在具有挑战性的换装数据上进行迁移学习后，在长期重新识别方面取得了最先进的成果，显著优于其他方法，并在遮挡视角场景中也表现出色。

Conclusion: 选择正确的预训练骨干网络架构和迁移学习协议可以显著提高长期重新识别性能。

Abstract: Person identification in unconstrained viewing environments presents
significant challenges due to variations in distance, viewpoint, imaging
conditions, and clothing. We introduce $\textbf{E}$va $\textbf{C}$lothes-Change
from $\textbf{H}$idden $\textbf{O}$bjects - $\textbf{B}$ody
$\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models built
on object-pretrained EVA-02 Large backbones. We compare ECHO-BID to 9 other
models that vary systematically in backbone architecture, model size, scale of
object classification pretraining, and transfer learning protocol. Models were
evaluated on benchmark datasets across constrained, unconstrained, and occluded
settings. ECHO-BID, with transfer learning on the most challenging
clothes-change data, achieved state-of-the-art results on long-term re-id --
substantially outperforming other methods. ECHO-BID also surpassed other
methods by a wide margin in occluded viewing scenarios. A combination of
increased model size and Masked Image Modeling during pretraining underlie
ECHO-BID's strong performance on long-term re-id. Notably, a smaller, but more
challenging transfer learning dataset, generalized better across datasets than
a larger, less challenging one. However, the larger dataset with an additional
fine-tuning step proved best on the most difficult data. Selecting the correct
pretrained backbone architecture and transfer learning protocols can drive
substantial gains in long-term re-id performance.

</details>


### [90] [CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts](https://arxiv.org/abs/2507.17651)
*Olaf Dünkel,Artur Jesslen,Jiahao Xie,Christian Theobalt,Christian Rupprecht,Adam Kortylewski*

Main category: cs.CV

TL;DR: CNS-Bench是一个连续扰动基准，通过使用LoRA适配器生成连续的、真实的扰动，并结合过滤机制，用于评估图像分类器的OOD鲁棒性。研究表明，模型在不同扰动和强度下的表现不同，连续评估有助于识别故障点。


<details>
  <summary>Details</summary>
Motivation: 在真实世界中使用计算机视觉模型时，评估其在潜在的分布外（OOD）场景下的性能是一个重要挑战。虽然常用的简单合成扰动通常无法捕捉现实世界中发生的滋扰偏移，而最近的扩散模型虽然可以生成用于基准测试的真实图像，但仅限于二元滋扰偏移。

Method: 提出了一种名为CNS-Bench的连续扰动基准，该基准通过将LoRA适配器应用于扩散模型来生成具有连续严重程度的各种单个扰动，并提出了一种能够超越先前方法的过滤机制，以实现使用生成模型的可靠基准测试。

Result: 对40多个分类器在各种滋扰偏移下的鲁棒性进行了大规模研究，发现在连续尺度上评估模型性能可以识别模型的故障点，提供对模型鲁棒性的更细致的理解。

Conclusion: 模型在不同扰动下和不同扰动强度下的排名会发生变化，这在仅使用常见的二元扰动时无法被捕捉。在连续尺度上评估模型性能有助于识别模型的故障点，从而更细致地理解模型的鲁棒性。

Abstract: An important challenge when using computer vision models in the real world is
to evaluate their performance in potential out-of-distribution (OOD) scenarios.
While simple synthetic corruptions are commonly applied to test OOD robustness,
they often fail to capture nuisance shifts that occur in the real world.
Recently, diffusion models have been applied to generate realistic images for
benchmarking, but they are restricted to binary nuisance shifts. In this work,
we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD
robustness of image classifiers for continuous and realistic generative
nuisance shifts. CNS-Bench allows generating a wide range of individual
nuisance shifts in continuous severities by applying LoRA adapters to diffusion
models. To address failure cases, we propose a filtering mechanism that
outperforms previous methods, thereby enabling reliable benchmarking with
generative models. With the proposed benchmark, we perform a large-scale study
to evaluate the robustness of more than 40 classifiers under various nuisance
shifts. Through carefully designed comparisons and analyses, we find that model
rankings can change for varying shifts and shift scales, which cannot be
captured when applying common binary shifts. Additionally, we show that
evaluating the model performance on a continuous scale allows the
identification of model failure points, providing a more nuanced understanding
of model robustness. Project page including code and data:
https://genintel.github.io/CNS.

</details>


### [91] [Attention (as Discrete-Time Markov) Chains](https://arxiv.org/abs/2507.17657)
*Yotam Erel,Olaf Dünkel,Rishabh Dabral,Vladislav Golyanik,Christian Theobalt,Amit H. Bermano*

Main category: cs.CV

TL;DR: 将注意力矩阵视为马尔可夫链，揭示了Token间的关系，提出了TokenRank衡量全局重要性，在图像生成和分割任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了提供对现代视觉Transformer中Token注意力机制的新理解，并统一和扩展现有的注意力分数操作。

Method: 将注意力矩阵解释为离散时间马尔可夫链，通过矩阵乘法和特征分析计算亚稳态及其分布，并定义了衡量全局Token重要性的TokenRank（马尔可夫链的稳态向量）。

Result: 提出了亚稳态的概念，表明语义相似的Token会聚集成簇，而噪声注意力分数则会分散。使用马尔可夫链的稳态向量TokenRank，在无条件图像生成任务上取得了改进，并实现了最先进的零样本分割效果。

Conclusion: 该框架为视觉Transformer中的Token注意力机制提供了新的视角，通过将注意力矩阵解释为离散时间马尔可夫链，统一了选择、求和、平均等操作，并引入了间接注意力。

Abstract: We introduce a new interpretation of the attention matrix as a discrete-time
Markov chain. Our interpretation sheds light on common operations involving
attention scores such as selection, summation, and averaging in a unified
framework. It further extends them by considering indirect attention,
propagated through the Markov chain, as opposed to previous studies that only
model immediate effects. Our main observation is that tokens corresponding to
semantically similar regions form a set of metastable states, where the
attention clusters, while noisy attention scores tend to disperse. Metastable
states and their prevalence can be easily computed through simple matrix
multiplication and eigenanalysis, respectively. Using these lightweight tools,
we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define
TokenRank -- the steady state vector of the Markov chain, which measures global
token importance. We demonstrate that using it brings improvements in
unconditional image generation. We believe our framework offers a fresh view of
how tokens are being attended in modern visual transformers.

</details>


### [92] [See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2507.17659)
*Junjie Wang,Yunhan Tang,Yijie Wang,Zhihao Yuan,Huan Wang,Yangfan He,Bin Li*

Main category: cs.CV

TL;DR: Synergos-VQA是一个创新的框架，通过融合三种证据流（整体、结构、因果）来克服MLLMs在KBVQA中的单维证据瓶颈，从而实现更全面、可靠的推理，并在多个基准测试中取得最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在知识基础视觉问答（KBVQA）方面存在依赖单维证据的推理瓶颈，无法实现全面、多层面的理解。

Method: 提出了一种名为Synergos-VQA的新型协同推理框架，该框架在推理时并行生成和融合三种互补的证据流：整体证据（感知整个场景）、结构证据（识别关键对象）和因果证据（确保推理的稳健性）。

Result: Synergos-VQA在三个具有挑战性的基准测试（包括OK-VQA和A-OKVQA）上确立了新的最先进水平，并表现出强大的即插即用能力，显著提升了各种开源MLLMs的性能。

Conclusion: Synergos-VQA通过协同融合整体、结构和因果证据，在OK-VQA和A-OKVQA等三个具有挑战性的基准测试中确立了新的最先进水平，并证明了方法论设计优于模型规模。

Abstract: Multimodal Large Language Models (MLLMs) have pushed the frontiers of
Knowledge-Based Visual Question Answering (KBVQA), yet their reasoning is
fundamentally bottlenecked by a reliance on uni-dimensional evidence. This
"seeing only the trees, but not the forest" approach prevents robust,
multi-faceted understanding. Inspired by the principle of seeing both the
forest and trees, we propose Synergos-VQA, a novel synergistic reasoning
framework. At its core, Synergos-VQA concurrently generates and fuses three
complementary evidence streams at inference time: (1) Holistic Evidence to
perceive the entire scene (the "forest"), (2) Structural Evidence from a
prototype-driven module to identify key objects (the "trees"), and (3) Causal
Evidence from a counterfactual probe to ensure the reasoning is robustly
grounded. By synergistically fusing this multi-faceted evidence, our framework
achieves a more comprehensive and reliable reasoning process. Extensive
experiments show that Synergos-VQA decisively establishes a new
state-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.
Furthermore, our approach demonstrates strong plug-and-play capabilities,
significantly boosting various open-source MLLMs and proving that superior
methodological design can outperform sheer model scale.

</details>


### [93] [BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems](https://arxiv.org/abs/2507.17722)
*Malsha Ashani Mahawatta Dona,Beatriz Cabrero-Daniel,Yinan Yu,Christian Berger*

Main category: cs.CV

TL;DR: LLMs和VLMs在理解交通状况方面很有前途，但它们容易产生幻觉，需要像BetterCheck这样的检测策略。


<details>
  <summary>Details</summary>
Motivation: LLMs和VLMs在理解复杂交通状况方面表现出色，可能适用于汽车感知系统，但它们容易产生幻觉，可能导致灾难性的决策。

Method: 系统评估了3个最先进的VLMs在Waymo开放数据集的交通状况子集上的性能，以支持捕获VLM支持的感知系统中的幻觉的安全护栏。

Result: LLMs和VLMs即使在处理细微细节时也表现出卓越的图像理解能力，但它们仍然容易产生幻觉，需要幻觉检测策略。

Conclusion: LLMs和VLMs在理解图像方面表现出色，但仍然容易产生幻觉，需要幻觉检测策略，例如我们提出的BetterCheck。

Abstract: Large language models (LLMs) are growingly extended to process multimodal
data such as text and video simultaneously. Their remarkable performance in
understanding what is shown in images is surpassing specialized neural networks
(NNs) such as Yolo that is supporting only a well-formed but very limited
vocabulary, ie., objects that they are able to detect. When being
non-restricted, LLMs and in particular state-of-the-art vision language models
(VLMs) show impressive performance to describe even complex traffic situations.
This is making them potentially suitable components for automotive perception
systems to support the understanding of complex traffic situations or edge case
situation. However, LLMs and VLMs are prone to hallucination, which mean to
either potentially not seeing traffic agents such as vulnerable road users who
are present in a situation, or to seeing traffic agents who are not there in
reality. While the latter is unwanted making an ADAS or autonomous driving
systems (ADS) to unnecessarily slow down, the former could lead to disastrous
decisions from an ADS. In our work, we are systematically assessing the
performance of 3 state-of-the-art VLMs on a diverse subset of traffic
situations sampled from the Waymo Open Dataset to support safety guardrails for
capturing such hallucinations in VLM-supported perception systems. We observe
that both, proprietary and open VLMs exhibit remarkable image understanding
capabilities even paying thorough attention to fine details sometimes difficult
to spot for us humans. However, they are also still prone to making up elements
in their descriptions to date requiring hallucination detection strategies such
as BetterCheck that we propose in our work.

</details>


### [94] [A Comprehensive Evaluation Framework for the Study of the Effects of Facial Filters on Face Recognition Accuracy](https://arxiv.org/abs/2507.17729)
*Kagan Ozturk,Louisa Conwill,Jacob Gutierrez,Kevin Bowyer,Walter J. Scheirer*

Main category: cs.CV

TL;DR: 面部滤镜会影响人脸识别，但可以通过检测和恢复滤镜效果来改善识别性能，并且该研究还发现了跨文化差异。


<details>
  <summary>Details</summary>
Motivation: 先前的研究虽然证明了面部滤镜可能对自动人脸识别性能产生负面影响，但这些研究仅限于少数精心挑选的特定风格的滤镜。为了更有效地包含各种社交媒体应用程序中存在的各种滤镜，需要一个能够进行更大规模研究的框架。

Method: 提出一个框架，用于对各种社交媒体应用程序中的面部滤镜对自动识别的影响进行更大规模的研究。该框架包括一个受控的人脸图像数据集，一个原则性的滤镜选择过程，以及一组评估滤镜对识别影响的实验。

Result: 通过对来自美国应用程序（Instagram和Snapchat）以及中国应用程序（Meitu和Pitu）的滤镜进行案例研究，揭示了跨文化差异。

Conclusion: 面部滤镜效果可以被检测到并恢复，从而提高面部识别性能。

Abstract: Facial filters are now commonplace for social media users around the world.
Previous work has demonstrated that facial filters can negatively impact
automated face recognition performance. However, these studies focus on small
numbers of hand-picked filters in particular styles. In order to more
effectively incorporate the wide ranges of filters present on various social
media applications, we introduce a framework that allows for larger-scale study
of the impact of facial filters on automated recognition. This framework
includes a controlled dataset of face images, a principled filter selection
process that selects a representative range of filters for experimentation, and
a set of experiments to evaluate the filters' impact on recognition. We
demonstrate our framework with a case study of filters from the American
applications Instagram and Snapchat and the Chinese applications Meitu and Pitu
to uncover cross-cultural differences. Finally, we show how the filtering
effect in a face embedding space can easily be detected and restored to improve
face recognition performance.

</details>


### [95] [Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention](https://arxiv.org/abs/2507.17745)
*Yiwen Chen,Zhihao Li,Yikai Wang,Hu Zhang,Qin Li,Chi Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: Ultra3D是一個高效的3D生成框架，通過VecSet表示和部件注意力機制顯著提高了稀疏體素建模的速度和質量。


<details>
  <summary>Details</summary>
Motivation: 現有框架的兩階段擴散管線中的注意力機制存在二次複雜性，導致計算效率低下，影響了稀疏體素表示在3D內容生成中的應用。本研究旨在提高稀疏體素建模的效率，同時不損害生成質量。

Method: Ultra3D框架利用緊湊的VecSet表示來高效生成粗略對象佈局，減少令牌計數並加速體素坐標預測。通過引入部件注意力（一種幾何感知的局部注意力機制）來細化第二階段的每體素潛在特徵，該機制將注意力計算限制在語義一致的部件區域內，從而保留結構連續性並避免不必要的全局注意力。為支持此機制，構建了一個可擴展的部件註釋管道，將原始網格轉換為帶部件標籤的稀疏體素。

Result: Ultra3D通過部件注意力機制實現了高達6.7倍的潛在生成速度提升，同時保持了高生成質量，並支持高分辨率3D生成。

Conclusion: Ultra3D在1024分辨率下支持高分辨率3D生成，并在视觉保真度和用户偏好方面均达到最先进性能。

Abstract: Recent advances in sparse voxel representations have significantly improved
the quality of 3D content generation, enabling high-resolution modeling with
fine-grained geometry. However, existing frameworks suffer from severe
computational inefficiencies due to the quadratic complexity of attention
mechanisms in their two-stage diffusion pipelines. In this work, we propose
Ultra3D, an efficient 3D generation framework that significantly accelerates
sparse voxel modeling without compromising quality. Our method leverages the
compact VecSet representation to efficiently generate a coarse object layout in
the first stage, reducing token count and accelerating voxel coordinate
prediction. To refine per-voxel latent features in the second stage, we
introduce Part Attention, a geometry-aware localized attention mechanism that
restricts attention computation within semantically consistent part regions.
This design preserves structural continuity while avoiding unnecessary global
attention, achieving up to 6.7x speed-up in latent generation. To support this
mechanism, we construct a scalable part annotation pipeline that converts raw
meshes into part-labeled sparse voxels. Extensive experiments demonstrate that
Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves
state-of-the-art performance in both visual fidelity and user preference.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [96] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

TL;DR: This paper unifies NLP content selection tasks using an instruction-guided framework (IGCS) and a new benchmark (igcsbench). They created a synthetic dataset that improves performance through transfer learning and addressed inference challenges, offering resources for future research.


<details>
  <summary>Details</summary>
Motivation: Traditional content selection tasks in NLP have been studied in isolation, with different modeling approaches, datasets, and evaluation metrics. This work aims to unify these tasks under a single framework to leverage shared objectives and potentially improve performance through transfer learning and standardized evaluation.

Method: The proposed method is instruction-guided content selection (IGCS), where task definitions and instance-specific requests are given as instructions to a language model. The paper introduces a unified benchmark (igcsbench) and a large synthetic dataset to support this framework. It also addresses generic inference time issues and proposes a generic evaluation metric.

Result: The paper introduces a unified framework (IGCS) and a benchmark (igcsbench) for diverse content selection tasks. It demonstrates that transfer learning with a large synthetic dataset can boost performance on these tasks. The paper also addresses inference time issues and proposes a generic evaluation metric, suggesting the utility of their resources for future models.

Conclusion: The paper proposes a unified framework called instruction-guided content selection (IGCS) for various NLP tasks that involve selecting relevant text spans. The framework uses instructions to guide language models. The paper also introduces a benchmark called igcsbench, a large synthetic dataset, and addresses inference time issues for LLM-based content selection models. The authors suggest that their resources and methods can benefit future content selection models.

Abstract: A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [97] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

TL;DR: AI Consult，一种基于LLM的临床决策支持工具，通过识别错误和辅助诊断，在肯尼亚的初级保健诊所中显著减少了临床错误，并获得了医生的积极评价，但需要良好的实施和医生采纳。


<details>
  <summary>Details</summary>
Motivation: 评估基于大语言模型（LLM）的临床决策支持工具（AI Consult）在实际护理中的影响，该工具旨在识别潜在的文档和临床决策错误，作为临床医生的安全网。

Method: 通过对肯尼亚一家初级卫生保健诊所网络Penda Health进行的质量改进研究，比较了39,849次有AI Consult支持与无AI Consult支持的就诊结果。由独立医生对就诊进行评分，以识别临床错误。

Result: 与未使用AI Consult的临床医生相比，使用AI Consult的临床医生诊断错误相对减少了16%，治疗错误相对减少了13%。AI Consult的引入每年可在Penda诊所避免22,000例诊断错误和29,000例治疗错误。75%的AI Consult用户认为该工具“大量”提高了护理质量。

Conclusion: 该研究表明，基于大语言模型（LLM）的临床决策支持工具（如AI Consult）可以有效减少临床实践中的诊断和治疗错误，并得到临床医生的积极评价，但其成功实施需要与临床工作流程相协调并鼓励医生使用。

Abstract: We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [98] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

TL;DR: SPADE是一种新颖的解码方法，通过对齐中间层和输出层表示来优化早停算法，从而在降低大规模语言模型推理成本的同时保持其准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有早停算法因中间层与输出层表示不匹配导致解码不准确，从而损害性能的问题。

Method: 提出了一种新颖的解码方法SPADE（SPace Alignment DEcoding），通过传播一个最小化的缩减序列（仅包含起始符和答案符）来使中间层表示与输出层对齐。此外，通过训练一个SPADE的线性近似模型来计算基于熵的置信度指标，以优化早停决策过程。将两者结合，创建了一种混合早停算法，该算法监控置信度水平并在中间层停止推理，同时使用SPADE生成高质量的输出。

Result: 该方法显著降低了推理成本，同时不损害准确性。

Conclusion: 该方法显著降低了推理成本，同时不损害准确性，为在大规模语言模型部署到实际应用中提供了可扩展且高效的解决方案。

Abstract: Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [99] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

TL;DR: SALU 是一种新的方法，通过在 LLM 的生成过程中整合无法回答性检测，并使用 RLHF 来奖励弃权和惩罚幻觉，从而提高了对话式信息检索的可靠性。


<details>
  <summary>Details</summary>
Motivation: 对话式信息检索（CIR）系统在可靠地处理无法回答的问题方面面临挑战，以防止产生误导性或幻觉性内容，而传统方法可能与核心生成大型语言模型（LLM）不一致。

Method: SALU 通过多任务学习框架进行训练，以实现标准问答和明确的弃权生成，并结合了置信度评分引导的强化学习（RLHF）来惩罚幻觉并奖励弃权。

Result: SALU 在 C-IR_Answerability 数据集上的实验表明，其在整体准确性方面持续优于包括混合 LLM-分类器系统在内的强有力基线，并在事实准确性、适当弃权和减少幻觉方面获得高分。

Conclusion: SALU 克服了传统方法在处理无法回答问题时的不一致性，并通过 RLHF 显着减少了幻觉，证明了其在“知道何时说‘我不知道’”方面的鲁棒性。

Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [100] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

TL;DR: mKGQAgent框架通过LLM代理协调，将多语言问答转化为SPARQL查询，在Text2SPARQL挑战赛2025中夺冠。


<details>
  <summary>Details</summary>
Motivation: 为了应对通过多语言自然语言接口访问知识图谱的挑战，需要将自然语言问题转化为用于查询结构化知识的SPARQL查询。

Method: 本文提出了一种受人类启发的框架mKGQAgent，将自然语言问题转化为SPARQL查询的任务分解为模块化、可解释的子任务。该框架利用了计划、实体链接和查询细化等协调LLM代理工作流，并通过经验池进行上下文学习。

Result: mKGQAgent在DBpedia和Corporate两个KGQA基准测试中取得了领先成果，在Text2SPARQL挑战赛2025中获得第一名。

Conclusion: mKGQAgent在多语言知识图谱问答（KGQA）方面表现出色，通过协调大型语言模型（LLM）代理的工作流，实现了高效且可解释的自然语言到SPARQL查询的转换，并在Text2SPARQL挑战赛2025中取得第一名。

Abstract: Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [101] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

TL;DR: 本研究通过生成多语言合成农业数据集并微调特定语言的大型语言模型，提高了模型在农业领域的表现，尤其是在多语言和资源匮乏的情况下。


<details>
  <summary>Details</summary>
Motivation: 为了使农民能够及时以本民族语言获取准确的农业相关信息，这对农业领域的成功至关重要。然而，目前公开的通用大型语言模型在农业领域的应用通常只提供通用咨询，缺乏本地和多语言的精确性。

Method: 本研究通过从特定农业文档中生成多语言（英语、印地语、旁遮普语）合成农业数据集，并对特定语言的大型语言模型进行微调，来解决现有模型在本地和多语言环境中缺乏精确性、培训不足以及缺乏高质量、特定区域数据集的问题。

Result: 在经过整理的多语言数据集上进行的评估表明，与基线模型相比，经过微调的模型在事实准确性、相关性和农业共识方面有了显著的改进。

Conclusion: 本研究强调了合成数据驱动的、面向语言的微调作为提高大型语言模型在农业领域性能的有效策略，特别是在多语言和资源匮乏的环境中。通过提供更准确、更本地化的农业咨询服务，本研究为弥合人工智能驱动的农业解决方案在不同语言社区中的知识差距迈出了有意义的一步。

Abstract: Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [102] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
*Shanbo Cheng,Yu Bao,Zhichao Huang,Yu Lu,Ningxin Peng,Lu Xu,Runsheng Yu,Rong Cao,Ting Han,Zeyang Li,Sitong Liu,Shengtao Ma,Shiguang Pan,Jiongchen Xiao,Nuo Xu,Meng Yang,Rong Ye,Yiming Yu,Ruofei Zhang,Wanyi Zhang,Wenhao Zhu,Liehao Zou,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-LiveInterpret 2.0 是一个创新的端到端同步口译模型，通过其双工语音到语音理解-生成框架，解决了现有SI系统的诸多挑战，并在翻译质量和延迟方面取得了显著改进，甚至优于商业解决方案。


<details>
  <summary>Details</summary>
Motivation: 同步口译（SI）是翻译行业最具挑战性的领域之一，现有的自动系统在转录和翻译质量、实时语音生成、多说话人混淆以及长篇演讲中的语音膨胀等方面存在诸多挑战。

Method: Seed-LiveInterpret 2.0 是一个端到端的同步口译模型，通过新颖的双工语音到语音理解-生成框架，解决了转录和翻译质量差、缺乏实时语音生成、多说话人混淆以及翻译语音膨胀等问题。通过大规模预训练和强化学习，该模型在翻译准确性和延迟之间取得了更好的平衡。

Result: Seed-LiveInterpret 2.0 在复杂场景下的正确率超过70%，并且在翻译质量方面显著优于商业SI解决方案，平均延迟从近10秒减少到近乎实时的3秒，提高了近70%的实用性。

Conclusion: Seed-LiveInterpret 2.0 在翻译质量和延迟方面显著优于商业解决方案，将克隆语音的平均延迟从近10秒降低到近乎实时的3秒，降低了近70%，大大提高了实用性。

Abstract: Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

</details>


### [103] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

TL;DR: 本研究通过一种新的基于姓名的基准测试方法，发现大型语言模型（LLM）即使在没有明确国籍标签的情况下也会表现出对特定国籍的偏见。研究表明，小型模型比大型模型更容易产生偏见且准确性更低，并且在模糊语境下更容易保留错误。这揭示了LLM偏见的顽固性，对AI在全球范围内的部署具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 在LLM中，即使没有明确的人口统计学标记，也可能存在针对特定国籍的潜在偏见。本研究旨在通过一种更贴近实际应用的、用文化指示性姓名替换显式国籍标签的方法来调查这种偏见。

Method: 提出了一种新颖的基于姓名的基准测试方法，该方法源自BBQ数据集，用于研究用文化指示性姓名替换显式国籍标签对LLM（包括OpenAI、Google和Anthropic的LLM）偏见和准确性的影响。

Result: 研究发现，与大型模型相比，小型模型在准确性较低的情况下表现出更多的偏见。例如，在模糊上下文中，Claude Haiku的刻板印象偏见得分为9%，而Claude Sonnet为3.5%，同时Claude Sonnet的准确性高出117.7%。此外，小型模型在模糊上下文中保留了更大比例的现有错误，例如GPT-4o-mini保留了76%的错误率，而GPT-4o为68%。

Conclusion: LLM中存在的针对特定国籍的潜在偏见，即使在没有明确人口统计学标记的情况下也会显现。通过使用基于姓名的基准测试方法，我们发现模型规模与偏见和准确性之间存在权衡。小型模型在准确性较低的情况下表现出更多的偏见，并且在模糊上下文中保留了更大比例的现有错误。这些发现强调了LLM中偏见的顽固性，并对其在多样化、全球化背景下的AI系统的开发和部署产生了深远影响。

Abstract: Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [104] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

TL;DR: 本研究利用GPT-3.5和GPT-4.5等大语言模型，通过多标签分类方法，对电子健康记录中的自杀相关因素进行了更复杂、更全面的识别，并提出了新的评估方法，证明了生成式AI在处理此类临床数据方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 自杀仍然是一个紧迫的全球卫生危机，每年有超过720,000人死亡，数百万人受到自杀意念（SI）、自杀行为（SA）等自杀相关因素（SrFs）的影响。因此，早期识别SrFs对于及时干预至关重要。虽然之前的研究大多将自杀意念视为二元分类任务，忽略了共现风险因素的复杂性，但本研究旨在利用LLMs解决这一挑战。

Method: 本研究探索了使用生成式大语言模型（LLMs），特别是GPT-3.5和GPT-4.5，对精神病学电子健康记录（EHRs）中的自杀相关因素（SrFs）进行多标签分类（MLC）。研究提出了新颖的端到端生成式MLC流程，并引入了先进的评估方法，包括标签集级别指标和用于错误分析的多标签混淆矩阵。

Result: 经过微调的GPT-3.5模型在部分匹配准确率（0.94）和F1分数（0.91）方面取得了最佳性能。而使用引导式提示的GPT-4.5模型在包括罕见或少数标签集在内的所有标签集上表现出更优越、更均衡、更稳健的性能。研究还揭示了系统性的错误模式，例如将SI和SA混淆，以及模型倾向于谨慎地过度标记。

Conclusion: 本研究证明了使用生成式人工智能进行复杂临床分类任务的可行性，并为构建非结构化电子健康记录数据以支持大规模临床研究和循证医学提供了蓝图。

Abstract: Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [105] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

TL;DR: 该研究提出了一种利用网络搜索和代码执行的工具使用代理系统，以提高LLM在事实、数学和代码任务上的评估质量。实验证明了工具的有效性，但也指出了对提示等参数的敏感性以及对新基准测试的需求。


<details>
  <summary>Details</summary>
Motivation: 为了提高LLM评估和反馈的质量，特别是在那些难以获得硬编码指标的领域（如聊天响应质量）以及事实陈述较多的响应中，解决标注者可能过度关注写作质量而非事实准确性的问题。

Method: 提出了一种使用工具的代理系统，通过网络搜索和代码执行来增强AI标注系统，以在长篇事实、数学和代码任务等领域提供更高质量的反馈。

Result: 在长篇事实、数学和代码任务以及一般标注任务上进行了广泛的实验评估。结果表明，外部工具在许多情况下可以提高性能，但并非在所有情况下都有效。

Conclusion: 该研究表明，在某些情况下，外部工具可以提高LLM评估的性能，但并非总是如此。实验突显了性能对简单参数（如提示）的敏感性，以及对改进（非饱和）标注基准的需要。

Abstract: Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [106] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

TL;DR: 通过为每个特征查找最优阈值，可以更有效地将文本嵌入转换为二进制表示，从而提高准确性。


<details>
  <summary>Details</summary>
Motivation: 在NLP应用中，高效的文本嵌入至关重要，需要存储和计算效率。该研究旨在探索使用二进制表示（条形码）替代实值特征，以提高NLP嵌入的效率和准确性。

Method: 提出了一种基于坐标搜索的优化框架，为每个特征（而不仅仅是所有特征的固定阈值）识别最优阈值，以将连续嵌入转换为二值表示（条形码）。

Result: 与传统的二值化方法相比，使用最优阈值生成的二进制嵌入在准确性方面表现更优，并在各种NLP任务和数据集上进行了广泛的实验和统计测试。

Conclusion: 通过为每个特征确定最优阈值，所提出的坐标搜索优化框架在二值化文本嵌入方面取得了优于传统方法的性能，为各种机器学习应用提供了有效且准确的二进制表示。

Abstract: Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [107] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: CogDual, inspired by cognitive psychology, is a new RPLA that uses a cognize-then-respond approach with self-awareness and situational awareness to improve character consistency and context alignment, outperforming existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for Role-Playing Language Agents (RPLAs) neglect the underlying cognitive mechanisms driving character behaviors, often relying solely on prompt engineering or supervised fine-tuning.

Method: CogDual adopts a cognize-then-respond reasoning paradigm by jointly modeling external situational awareness and internal self-awareness, optimized with reinforcement learning using two general-purpose reward schemes.

Result: CogDual generates responses with improved character consistency and contextual alignment, outperforming existing baselines on CoSER, Cross-MR, and LifeChoice benchmarks.

Conclusion: CogDual consistently outperforms existing baselines and generalizes effectively across diverse role-playing tasks.

Abstract: Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [108] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: SKA-Bench是一个新的基准，用于评估大语言模型对结构化知识（如表格和知识图谱）的理解能力。现有模型在该基准上表现不佳，尤其是在处理噪声、知识单元顺序和信息整合方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对结构化知识（SK）的评估不够严谨，且仅关注单一类型的SK，因此需要一个更全面、更严谨的基准来诊断LLMs的结构化知识理解的缺陷。

Method: 提出SKA-Bench，一个包含知识图谱（KG）、表格、KG+文本和表格+文本四种结构化知识形式的问答基准。通过三阶段流程构建问题、答案、正例知识单元和噪声知识单元，并扩展为噪声鲁棒性、顺序不敏感性、信息整合和负例拒绝四个能力测试平台，对8个代表性LLMs进行了实证评估。

Result: 通过在8个代表性LLMs上的实证评估，表明现有LLMs在理解结构化知识方面仍有很大提升空间，并且其性能受到多种因素的影响。具体来说，噪声量、知识单元的顺序以及模型是否产生幻觉都会影响模型的表现。

Conclusion: 现有的大语言模型（LLMs）在理解结构化知识（SK）方面仍然面临重大挑战，其性能受到噪声量、知识单元顺序和幻觉现象等因素的影响。SKA-Bench 提供了一个更全面、更严格的基准来诊断这些不足。

Abstract: Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [109] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

TL;DR: FinGAIA是一个针对金融领域的AI代理基准，旨在评估和改进AI代理在金融任务中的表现。


<details>
  <summary>Details</summary>
Motivation: AI代理在金融领域的协同能力仍有待探索，需要一个专门的基准来评估其在金融场景下的实际能力。

Method: 提出了FinGAIA基准，包含407个涵盖七个子领域（证券、基金、银行、期货、信托、保险、资产管理）和三个层级（基础业务分析、资产决策支持、战略风险管理）的任务。在零样本设置下评估了10种主流AI代理。

Result: 在零样本设置下，表现最好的ChatGPT达到了48.9%的准确率，但与金融专家相比仍有差距。识别出了五种常见的失败模式，包括跨模态对齐缺陷、金融术语偏差和操作流程意识障碍。

Conclusion: 该研究提出了FinGAIA，一个针对金融领域的AI代理基准，包含407个涵盖七个主要金融子领域和三个层级深度的任务，旨在评估和推动金融领域AI代理的发展。

Abstract: The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [110] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 该研究探索了基于图的检索增强生成（RAG）方法在更广泛数据集上的通用性，特别是评估了GeAR在SIGIR 2025 LiveRAG挑战赛上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的方法通常针对特定任务，如多跳问答和查询导向摘要，其在更广泛数据集上的通用性证据有限。

Method: 研究尝试调整一个先进的基于图的检索增强生成（RAG）解决方案，即GeAR。

Result: 该研究将GeAR应用于SIGIR 2025 LiveRAG挑战赛的数据集，以评估其性能。

Conclusion: 该研究旨在探索GeAR在SIGIR 2025 LiveRAG挑战赛上的性能和局限性。

Abstract: Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [111] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

TL;DR: LLM在道德判断上存在“多元化道德鸿沟”，与人类的判断分布和价值观多样性存在差距。DMP方法通过结合人类价值观，可提升LLM的道德判断对齐度和多样性。


<details>
  <summary>Details</summary>
Motivation: 随着人类越来越依赖LLM获取道德建议，了解LLM的道德判断与人类判断的贴合程度至关重要，但目前这方面的研究尚有不足。

Method: 本文将LLM与人类在道德困境问题上的判断视为一个“多元化分布对齐”任务。通过构建“道德困境数据集”（包含1618个真实道德困境及人类的二元判断和自由文本理由），并提取了包含3783个价值表达的60个价值观分类，对比了LLM与人类判断的分布差异。在此基础上，提出了一种基于Dirichlet分布的采样方法——动态道德剖析（DMP），用于根据人类衍生的价值剖析来条件化模型输出。

Result: 在人类共识度高的情况下，LLM的判断能与人类保持一致，但随着人类分歧的增加，模型与人类判断的一致性会急剧下降。与人类相比，LLM在判断时依赖的价值观范围更为狭窄。DMP方法将模型与人类判断的对齐度提高了64.3%，并增强了价值多样性。

Conclusion: LLMs在道德困境问题上与人类的道德判断存在显著的“多元化道德鸿沟”，这体现在模型判断的分布和所依赖的价值观多样性上。动态道德剖析（DMP）方法通过引入基于人类价值观的条件约束，能有效提升模型在道德判断上的对齐度和价值观多样性，是实现更符合人类多元道德观念的LLM指导的重要一步。

Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [112] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

TL;DR: CLARIFID是一个新的框架，通过模仿专家工作流程（先“发现”后“印象”）、多视图融合和优化的解码策略，来提高放射学报告的临床准确性和全面性，实验结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成放射学报告的方法在生成临床可靠的结论方面存在不足，并且通常依赖单视图图像，限制了诊断的全面性。本研究旨在通过模仿专家的两步工作流程来直接优化诊断的正确性。

Method: CLARIFID框架通过以下方式优化诊断的正确性：1. 学习从“发现”到“印象”的逻辑流程，通过面向章节的预训练实现。2. 使用Proximal Policy Optimization进行微调，以CheXbert F1分数作为“印象”部分的奖励。3. 强制执行“发现”部分完成后再生成“印象”部分的推理感知解码。4. 通过基于视觉变换器的多视图编码器融合多个胸部X射线视图。在推理过程中，采用推理感知下文强制策略，并进行报告级重排序，确保模型先生成全面的“发现”部分，再生成“印象”，从而保持临床推理的一致性。

Result: 实验结果表明，CLARIFID框架在MIMIC-CXR数据集上取得了优于现有基线方法的临床效果，并在标准自然语言生成指标和临床感知评分方面表现更佳。

Conclusion: CLARIFID框架在MIMIC-CXR数据集上实现了优于现有基线方法的临床效果，并在标准自然语言生成指标和临床感知评分方面表现更佳。

Abstract: Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [113] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文提出了一种名为Triple X的语音识别系统，通过结合编码器-适配器-LLM架构和多阶段训练策略，在多语言对话场景下实现了高识别准确率，并在MLC-SLM挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 本文旨在优化多语言对话场景下的语音识别准确性，并利用基于文本的大型语言模型的强大推理能力，同时结合领域特定的适应性。

Method: 本文提出了一种创新的编码器-适配器-LLM架构，并结合多阶段训练策略和大规模多语言音频数据集，以优化多语言对话场景下的语音识别准确性。

Result: 实验结果表明，本研究提出的方法在开发集和测试集上均取得了具有竞争力的词错误率（WER）表现。

Conclusion: 本研究提出的Triple X语音识别系统在MLC-SLM挑战赛任务1中取得了有竞争力的词错误率（WER）表现，并在挑战赛排名中获得第二名。

Abstract: This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [114] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

TL;DR: 该研究分析了情感、叙事和缓述等主观因素对论证强度的影响，发现它们对客观和主观论证质量的影响模式不同，情感的影响则与修辞运用相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对主观因素（如个人故事）与论证强度之间关系的广泛分析，旨在解决这一差距。

Method: 采用回归分析方法，对两个标准数据集进行分析，量化情感、叙事和缓述等主观因素对论证强度的影响。同时，对用于标注这些主观因素的自动化标注方法进行了比较和评估。

Result: 叙事和缓述对客观和主观论证质量有相反的影响，而情感的影响取决于其修辞运用而非领域。

Conclusion: 该研究量化了主观因素（情感、叙事和缓述）对客观论证质量和主观说服力的影响，并揭示了这些主观因素对论证强度的影响模式存在差异。

Abstract: In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [115] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

TL;DR: 本文提出置信 RAG，一种通过多次生成并选择置信度最高响应来改进检索增强生成的方法，显著提高了 LLM 的响应质量。


<details>
  <summary>Details</summary>
Motivation: LLM 在各个领域产生了巨大影响，因此，将最新信息整合到 LLM 或添加外部知识来构建特定领域模型的方法备受关注。检索增强生成（RAG）作为一种推理时扩展方法，因其低成本和最小的参数调整工作量而备受瞩目。然而，由于训练数据和模型架构的异质性，RAG 中使用的各种嵌入模型在不同领域表现出不同的优势，这通常会导致不同的相似性计算结果，进而导致 LLM 的响应质量不同。

Method: 本文提出了两种方法来增强 RAG：混合嵌入 RAG 和置信 RAG。混合嵌入 RAG 基于标准化相似性对来自多个嵌入模型的检索进行排序和选择；置信 RAG 使用不同的嵌入模型多次生成响应，然后选择置信度最高的响应。

Result: 混合嵌入 RAG 的表现不如标准的 RAG。置信 RAG 在不同 LLM 和嵌入模型上平均比 vanilla LLM 和 RAG 分别提高了约 10% 和 5%。

Conclusion: Confident RAG 是一种高效的即插即用方法，可用于不同领域，在不同 LLM 和嵌入模型上均取得了一致的结果，平均比 vanilla LLM 和 RAG 分别提高了约 10% 和 5%。

Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [116] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

TL;DR: 该研究提出了MultiNRC多语言推理基准测试，发现现有LLMs在处理非英语母语的推理任务时表现不佳，尤其是在涉及文化背景知识时。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）虽然在英语推理方面取得了显著进步，但其在不同语言和文化背景下的多语言推理能力评估仍然有限。现有的多语言推理基准测试多是通过翻译英语基准测试而来，这使得这些基准测试的语境偏向于英语语言/文化。因此，有必要开发一个能够更全面、更公平地评估LLMs多语言推理能力的基准测试。

Method: 作者引入了一个名为“多语言本地推理挑战”（MultiNRC）的新基准测试，其中包含超过1000个由法语、西班牙语和中文母语者编写的、符合当地语言和文化背景的推理问题。该基准测试涵盖了语言推理、文字游戏和谜语、文化/传统推理以及具有文化相关性的数学推理四类。作者还提供了这些多语言问题的英语翻译版本，以便直接比较模型在不同语言下的推理能力。随后，作者系统地评估了14个主流的大型语言模型在MultiNRC及其英语版本上的表现。

Result: 1. 现有LLMs在多语言本地推理方面表现不佳，在MultiNRC基准测试中的得分均低于50%。 2. LLMs在处理语言推理、文化推理和逻辑推理任务时，表现出不同的优势和劣势。 3. 大多数模型在英语数学推理任务上的表现（+10%）显著优于其在原始语言上的表现，这表明模型在处理具有文化背景的知识方面仍然存在挑战。

Conclusion: 现有的大型语言模型（LLMs）在多语言本地推理能力方面仍有不足，在MultiNRC基准测试中的得分均低于50%。不同模型在处理语言、文化和逻辑推理任务时表现出不同的优势和劣势。与在英语中相比，大多数模型在数学推理任务上表现更好，这表明它们在处理文化背景知识方面仍面临挑战。

Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


### [117] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

TL;DR: 本研究展示了如何利用先进的AI技术（LLM、ASR、语音合成）构建电话调查系统，以期在扩大研究规模的同时，兼顾人性化交互和方法论严谨性。研究发现，缩短问卷长度和提高AI的响应速度有助于改善调查的关键指标（完成率、退出率、满意度）。


<details>
  <summary>Details</summary>
Motivation: 随着语音助手（如智能音箱和手机助手）的普及，定量调查研究人员获得了一种新的数据收集模式：AI电话调查。与早期的IVR技术相比，AI电话调查能够提供更自然、更具适应性的访谈体验，因为它能更好地处理中断、修正和人类语音中的其他特殊情况。本研究旨在利用AI技术（特别是LLM、ASR和语音合成）来构建一个更优化的定量调查系统，以期在扩大研究规模的同时，兼顾人性化交互和方法论严谨性。

Method: 本研究构建并测试了一个基于大型语言模型（LLM）、自动语音识别（ASR）和语音合成技术的AI系统，用于进行定量调查。该系统在设计上遵循了研究的最佳实践，如问题顺序随机化、答案顺序随机化和精确措辞。通过对SSRS Opinion Panel进行两次试点调查，并与后续的人工访谈调查进行比较，评估了该系统的有效性，重点衡量了问卷完成率、中途退出率和受访者满意度这三个关键指标。

Result: 研究结果表明，问卷的长度和AI访谈员的响应速度是影响调查效果的关键因素。具体来说，较短的问卷和响应更快的AI访谈员能够全面提升问卷完成率、降低中途退出率，并提高受访者的满意度。

Conclusion: AI电话调查系统可以通过LLM、ASR和语音合成技术来拓展定量研究的规模，同时保持人性化的交互和严谨的方法论。研究结果表明，较短的问卷和更具响应性的AI访谈员有助于提高问卷完成率、降低中途退出率和提升受访者满意度。

Abstract: With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


### [118] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
*Brian DeRenzi,Anna Dixon,Mohamed Aymane Farhi,Christian Resch*

Main category: cs.CL

TL;DR: 本研究通过LLM创建非洲语言的合成文本和语音数据，成功降低了语音合成成本，并显著提升了ASR性能，特别是在豪萨语中。研究为低资源语言的ASR改进提供了有效途径，并公开了数据和模型以促进社区发展。


<details>
  <summary>Details</summary>
Motivation: 由于非洲大多数语言的语音技术仍未普及，本研究旨在通过创建大规模合成语音语料库来解决这一挑战，以降低成本并提高非洲语言的ASR性能。

Method: 研究采用了三步流程：大型语言模型（LLM）驱动的文本创建、文本到语音（TTS）语音合成以及自动语音识别（ASR）微调。具体而言，他们为八种语言创建了可读性得分高于5/7的合成文本，并为豪萨语、恩卢奥语和奇切瓦语这三种语言创建了超过2500小时的合成语音数据。在ASR模型方面，他们使用 Wav2Vec-BERT-2.0 模型，并结合真实数据和合成数据进行微调，以评估ASR性能的提升。研究还包含性别细分的ASR性能评估，并分析了评价者间信度、ASR错误和评估数据集，以改进评价流程和数据。

Result: 研究表明，合成数据在提高非洲语言的ASR性能方面是有效的。例如，在豪萨语中，结合250小时真实数据和250小时合成数据的模型，其表现与仅使用500小时真实数据的基线模型相当；而使用579小时真实数据和450至993小时合成数据则达到了最佳性能。对于资源匮乏的语言，例如奇切瓦语，使用1:2的真实数据与合成数据比例，词错误率（WER）相对提高了约6.5%；而恩卢奥语使用1:1的比例，在部分评估数据上显示出相似的改进，但在其他数据上则不然。研究还发现，需要更可靠的评审协议和更准确的评估数据集来改进评价。

Conclusion: 该研究首次系统性评估了非洲语言大规模语音合成语料库在自动语音识别（ASR）中的应用，并公开了所有数据和模型以鼓励进一步研究。

Abstract: Speech technology remains out of reach for most of the over 2300 languages in
Africa. We present the first systematic assessment of large-scale synthetic
voice corpora for African ASR. We apply a three-step process: LLM-driven text
creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages
for which we create synthetic text achieved readability scores above 5 out of
7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created
more than 2,500 hours of synthetic voice data at below 1% of the cost of real
data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h
synthetic Hausa matched a 500h real-data-only baseline, while 579h real and
450h to 993h synthetic data created the best performance. We also present
gender-disaggregated ASR performance evaluation. For very low-resource
languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2
real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on
some evaluation data, but not on others. Investigating intercoder reliability,
ASR errors and evaluation datasets revealed the need for more robust reviewer
protocols and more accurate evaluation data. All data and models are publicly
released to invite further work to improve synthetic data for African
languages.

</details>


### [119] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 提出了一种基于增长界矩阵（GBM）的正则化技术，用于提高LSTM、S4和CNN等NLP模型的鲁棒性，并在实验中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有NLP模型容易受到对抗性攻击（如同义词替换），尤其是循环网络和现代状态空间模型（如S4）的鲁棒性研究不足，而这些模型由于其顺序处理和复杂的参数动态性而面临独特的挑战。

Method: 提出了一种基于增长界矩阵（GBM）的正则化技术，并将其应用于长短期记忆（LSTM）、状态空间模型（S4）和卷积神经网络（CNN）三种架构，以提高模型对输入扰动的鲁棒性。

Result: 在多种架构和基准数据集上的广泛实验表明，该方法在对抗性鲁棒性方面比现有基线提高了8.8%，并在对抗防御方面优于几种最先进的方法。

Conclusion: 本研究引入了一种基于增长界矩阵（GBM）的新型正则化技术，以提高NLP模型对输入扰动的鲁棒性，并首次系统地分析了状态空间模型（SSM）的鲁棒性。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [120] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: WSM框架通过将学习率衰减视为模型平均，并在实践中确定合并持续时间是关键，在多个基准测试中超越了WSD方法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决学习率（LR）调度中的衰减阶段问题，探索无需衰减的方法，并提出一种能够统一多种衰减策略并与现有优化方法兼容的通用框架。

Method: 提出了一种名为Warmup-Stable and Merge (WSM)的通用框架，该框架建立了学习率衰减与模型合并之间的形式化联系，将各种衰减策略（如余弦衰减、线性衰减和平方根倒数衰减）统一为原则性的模型平均方案，并与多种优化方法兼容。通过实验确定合并持续时间（检查点聚合的训练窗口）是影响模型性能的最关键因素。

Result: WSM框架在MATH、HumanEval和MMLU-Pro基准测试中取得了显著的性能提升（分别为+3.5%、+2.9%和+5.5%），优于广泛采用的Warmup-Stable-Decay (WSD)方法。此外，WSM在监督微调场景中也显示出性能优势，并确定合并持续时间是影响模型性能的最关键因素。

Conclusion: WSM框架通过将学习率衰减视为模型平均方案，提供了一个统一的理论基础，能够模拟余弦衰减、线性衰减和平方根倒数衰减等多种衰减策略，并与多种优化方法兼容。实验表明，合并持续时间是影响模型性能的最关键因素。WSM在多个基准测试中持续优于广泛采用的WSD方法，在MATH上提高了+3.5%，在HumanEval上提高了+2.9%，在MMLU-Pro上提高了+5.5%，并且在监督微调场景中也表现出性能优势。

Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [121] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
*Victor Hartman,Petter Törnberg*

Main category: cs.CL

TL;DR: 本研究利用零样本大型语言模型（LLMs）实现了大规模跨语言负面竞选活动分析，发现执政党较少使用负面言论，而极端和民粹主义政党（特别是极右翼）则更常使用。LLMs 为政治传播研究提供了可扩展、透明和可复制的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的负面竞选活动分类方法成本高昂且扩展性有限，限制了实证研究。本研究旨在克服这些限制，并对负面竞选活动进行大规模跨国研究。

Method: 本研究引入零样本大型语言模型（LLMs）作为跨语言负面竞选活动分类的新方法，并在十种语言的基准数据集上进行了测试，证明其性能可与母语使用者媲美，并优于传统的监督机器学习方法。

Result: 在十种语言的基准数据集上，LLMs 的表现与母语使用者相当，并优于传统的监督机器学习方法。对 1800 万条推文的分析显示，执政党使用负面信息的可能性较低，而意识形态极端和民粹主义政党（尤其是极右翼政党）则更频繁地使用负面信息。

Conclusion: 该研究展示了零样本大型语言模型（LLMs）在跨语言负面竞选活动分类中的潜力，并利用此方法进行了迄今为止最大规模的跨国负面竞选活动研究。研究结果揭示了跨国模式：执政党较少使用负面信息，而意识形态极端和民粹主义政党（尤其是极右翼政党）则更倾向于使用负面信息。这有助于我们理解政党特征如何影响多党制下的战略沟通，并证明了 LLMs 在政治传播研究中的可扩展、透明和可复制性。

Abstract: Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

</details>


### [122] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
*Changxin Tian,Kunlong Chen,Jia Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出效率效益（EL）指标和统一扩展定律，用于预测和指导MoE模型的扩展，并通过Ling-mini-beta模型验证了该方法的有效性，该模型在计算资源消耗降低7倍以上的情况下，性能与一个较大的密集模型相当。


<details>
  <summary>Details</summary>
Motivation: 解决在MoE模型中，根据给定的MoE配置（如专家激活比例和粒度）预测模型容量仍然是一个未解决的问题。

Method: 通过大规模实证研究，训练了超过300个参数量高达280亿的模型，系统地研究了MoE架构配置与EL之间的关系。将研究结果整合到一个统一的扩展定律中。

Result: EL主要由专家激活比例和总计算预算驱动，两者都遵循可预测的幂律。专家粒度作为一种非线性调节因子，具有明确的最佳范围。研究结果被整合到一个统一的扩展定律中，能够根据MoE架构的配置准确预测其EL。

Conclusion: 该研究提出了效率效益（EL）指标，用于量化MoE模型相对于其密集对应模型的计算优势，并制定了统一的扩展定律，可以根据配置准确预测EL。通过Ling-mini-beta模型验证了该扩展定律的准确性，该模型以更少的计算资源实现了与更大密集模型的相当的性能。

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

</details>


### [123] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
*Parker Riley,Siamak Shakeri,Waleed Ammar,Jonathan H. Clark*

Main category: cs.CL

TL;DR: TyDi QA-WANA is a new 28K QA dataset in 10 languages from Western Asia and Northern Africa, focusing on genuine questions and long contexts, collected without translation for cultural relevance. Baseline performance is included.


<details>
  <summary>Details</summary>
Motivation: To create a question-answering dataset for western Asia and northern Africa that elicits genuine information-seeking questions and is culturally relevant, suitable for evaluating models' abilities to utilize large text contexts.

Method: The dataset was created by collecting 28K examples divided among 10 language varieties of western Asia and northern Africa. The data collection process was designed to elicit information-seeking questions, where the asker is genuinely curious to know the answer. Each question is paired with an entire article that may or may not contain the answer. The data was collected directly in each language variety, without the use of translation.

Result: Performance of two baseline models is presented, and the code and data are released to facilitate further improvement by the research community.

Conclusion: The dataset TyDi QA-WANA, comprising 28K examples across 10 language varieties of western Asia and northern Africa, is presented. The dataset is designed to elicit genuine information-seeking questions paired with entire articles, suitable for evaluating models on large text contexts. Data was collected directly in each language to ensure cultural relevance. Baseline model performance is provided, and the code and data are released.

Abstract: We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

</details>


### [124] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
*Karen Zhou,John Giorgi,Pranav Mani,Peng Xu,Davis Liang,Chenhao Tan*

Main category: cs.CL

TL;DR: 本研究提出了一种新的方法，通过提炼用户反馈来创建结构化清单，以更准确、可扩展地评估AI生成的临床笔记质量，并证明了其优于现有方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成的临床笔记质量具有挑战性，因为专家评审主观性强且难以规模化，而现有的自动化指标与医生偏好不符。

Method: 提出一个系统化地将真实用户反馈提炼为结构化清单的流程，并使用基于LLM的评估器来执行这些清单。

Result: 研究结果表明，所提出的反馈提炼清单在覆盖度、多样性和人类评分预测能力方面优于基线方法，并且该清单对质量下降的扰动具有鲁棒性，与临床医生偏好对齐，并具有作为评估方法的实际价值。

Conclusion: 该研究提出的基于用户反馈的结构化清单，在评估AI生成的临床笔记方面，优于基线方法，并且与临床医生的偏好高度一致，具有实际应用价值。

Abstract: AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

</details>


### [125] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
*Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Bo Zhao,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: Megrez2是一种新颖的、轻量级的、高性能的语言模型架构，优化了设备原生部署。它通过跨层专家共享和预门控路由来减少参数量和提高推理速度。Megrez2-Preview在多种任务上表现出色，证明了其在资源受限应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 提出Megrez2，一种优化的语言模型架构，用于设备原生部署。

Method: Megrez2采用新颖的跨层专家共享机制，通过跨相邻Transformer层重用专家模块来显著减少总参数量，同时保持大部分模型容量。它还采用了预门控路由，实现了内存高效的专家加载和更快的推理。

Result: Megrez2-Preview在语言理解、指令遵循、数学推理和代码生成等任务上表现出与更大模型相当或更优越的性能，尽管其仅激活3B参数和存储7.5B参数。

Conclusion: Megrez2架构在准确性、效率和可部署性之间取得了平衡，是资源受限应用的有力竞争者。

Abstract: We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.

</details>


### [126] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
*Linbo Cao,Jinman Zhao*

Main category: cs.CL

TL;DR: 为了解决数据污染和成本问题，我们提出了一种辩论评估方法，将问答任务转化为辩论，用裁判模型进行裁决。实验证明此方法能有效评估真实推理能力，即使是经过“污染”的模型在此方法下表现也会下降。此方法成本较低且可扩展性强。


<details>
  <summary>Details</summary>
Motivation: 为了应对日益增长的关于数据污染、记忆化以及数据集创建成本上升的担忧，同时评估大型语言模型在标准问答基准上的表现。

Method: 提出了一种由辩论驱动的评估范式，将现有的问答数据集转化为结构化的对抗性辩论。一个模型负责辩护官方答案，另一个模型构建并辩护替代性答案，由一个不了解正确答案的裁判模型进行裁决。

Result: 实验结果验证了该方法的稳健性及其对抗数据污染的有效性。例如，在一个在测试问题上进行了微调的 Llama 3.1 模型上，准确率从 50% 显著提高到 82%，但在辩论中的表现却有所下降。此外，即使是能力较弱的裁判也能可靠地区分出更强的辩手，表明这种评估方法可以有效地扩展到未来更强大的系统，且成本仅为创建新基准的一小部分。

Conclusion: 该框架通过将标准问答任务转化为结构化的对抗性辩论，为评估大型语言模型的真正推理能力提供了一条可持续的途径，这表明“在测试集上进行预训练不再是万能的”。

Abstract: As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [127] [Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots](https://arxiv.org/abs/2507.17049)
*Pablo Valle,Chengjie Lu,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: 提出了新的VLA模型评估指标，比单纯的成功率更能反映模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA模型评估方法（如任务成功率）未能充分捕捉任务执行质量和模型置信度，需要新的评估指标。

Method: 提出八个不确定性指标和五个质量指标，并通过涉及908个成功任务执行的大规模实证研究进行评估，分析了这些指标与人类领域专家标记的任务质量标签之间的相关性。

Result: 研究提出的部分指标与人类评估具有中等到强的相关性，并且能够区分不同质量的任务执行，甚至可以用于评估不成功的任务，为改进VLA模型评估提供了新的途径。

Conclusion: 该研究提出了用于评估视觉语言动作（VLA）模型在机器人操作任务中表现的不确定性和质量指标，并进行了大规模实证研究，证明了这些指标与人类专家判断具有中等到强的相关性，能够区分不同质量的任务执行，挑战了仅依赖成功率的传统评估方法，为VLA机器人系统的实时监控和自适应改进提供了方向。

Abstract: Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.

</details>


### [128] [Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing](https://arxiv.org/abs/2507.17093)
*Danushka Liyanage,Nelum Attanayake,Zijian Luo,Rahul Gopinath*

Main category: cs.SE

TL;DR: This work examines the reliability of reachability estimators by proposing an evaluation framework with synthetic programs and adapting a reliability check on real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Fuzzers are often guided by coverage, making the estimation of maximum achievable coverage a key concern in fuzzing. However, achieving 100% coverage is infeasible for most real-world software systems, regardless of effort. While static reachability analysis can provide an upper bound, it is often highly inaccurate. Recently, statistical estimation methods based on species richness estimators from biostatistics have been proposed as a potential solution. Yet, the lack of reliable benchmarks with labeled ground truth has limited rigorous evaluation of their accuracy.

Method: To address the challenge of labeled ground truth, we propose an evaluation framework that synthetically generates large programs with complex control flows, ensuring well-defined reachability and providing ground truth for evaluation. To address the criticism from use of synthetic benchmarks, we adapt a reliability check for reachability estimators on real-world benchmarks without labeled ground truth -- by varying the size of sampling units, which, in theory, should not affect the estimate.

Result: These two studies together will help answer the question of whether current reachability estimators are reliable, and defines a protocol to evaluate future improvements in reachability estimation.

Conclusion: Fuzzers

Abstract: Background: Fuzzers are often guided by coverage, making the estimation of
maximum achievable coverage a key concern in fuzzing. However, achieving 100%
coverage is infeasible for most real-world software systems, regardless of
effort. While static reachability analysis can provide an upper bound, it is
often highly inaccurate. Recently, statistical estimation methods based on
species richness estimators from biostatistics have been proposed as a
potential solution. Yet, the lack of reliable benchmarks with labeled ground
truth has limited rigorous evaluation of their accuracy.
  Objective: This work examines the reliability of reachability estimators from
two axes: addressing the lack of labeled ground truth and evaluating their
reliability on real-world programs.
  Methods: (1) To address the challenge of labeled ground truth, we propose an
evaluation framework that synthetically generates large programs with complex
control flows, ensuring well-defined reachability and providing ground truth
for evaluation. (2) To address the criticism from use of synthetic benchmarks,
we adapt a reliability check for reachability estimators on real-world
benchmarks without labeled ground truth -- by varying the size of sampling
units, which, in theory, should not affect the estimate.
  Results: These two studies together will help answer the question of whether
current reachability estimators are reliable, and defines a protocol to
evaluate future improvements in reachability estimation.

</details>


### [129] [Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations](https://arxiv.org/abs/2507.17165)
*Taher A. Ghaleb,Dulina Rathnayake*

Main category: cs.SE

TL;DR: 大型语言模型（LLMs）在根据自然语言描述生成 GitHub Actions CI 配置方面表现不佳，需要进一步改进以满足实际需求。


<details>
  <summary>Details</summary>
Motivation: 鉴于 CI 服务（如 GitHub Actions）需要开发者编写耗时且易出错的 YAML 配置，而大型语言模型（LLMs）在自动化软件工程任务中的应用日益广泛，本研究旨在探索 LLMs 生成 CI 配置的能力，以解决手动配置的痛点。

Method: 本研究评估了六种大型语言模型（LLMs），包括三种通用模型（GPT-4o、Llama 和 Gemma）和三种代码预训练模型（GPT-4.1、Code Llama 和 CodeGemma），用于根据自然语言描述生成 GitHub Actions 配置。研究人员还构建了一个包含描述和对应最佳实践 YAML 配置的数据集，并使用零样本提示进行评估。

Result: 零样本提示的准确率最高可达 69%，但只有 3% 的完美匹配。代码预训练模型在 CI 配置生成任务上略逊于通用模型，表明 LLMs 在此方面存在局限性。GPT-4o 的输出存在一些问题，例如步骤缺失、重命名、描述误解和不必要的添加，这些问题会影响配置的正确性。

Conclusion: 代码预训练模型在基于 YAML 的 CI 任务中表现略逊于通用模型，这表明了大型语言模型在生成 CI 配置方面存在局限性。对 GPT-4o 输出的分析揭示了诸如缺少或重命名步骤、误解描述以及不必要的添加等问题，这些问题可能会影响结构和上下文的正确性，表明生成质量与可执行 CI 配置所需的精度之间存在差距。我们的研究为改进大型语言模型与配置文件语言的对齐以及指导未来 CI 自动化和工具支持的努力提供了见解。

Abstract: Continuous Integration (CI) services, such as GitHub Actions, require
developers to write YAML-based configurations, which can be tedious and
error-prone. Despite the increasing use of Large Language Models (LLMs) to
automate software engineering tasks, their ability to generate CI
configurations remains underexplored. This paper presents a preliminary study
evaluating six LLMs for generating GitHub Actions configurations from natural
language descriptions. We assess three general-purpose foundation models
(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code
Llama, and CodeGemma). We also introduce the first labeled dataset of its kind,
constructed from GitHub Actions documentation, pairing descriptions with
corresponding best-practice YAML configurations. Zero-shot prompting achieves
up to 69% similarity with the ground truth, with only 3% perfect matches.
Code-pretrained models slightly underperform compared to general-purpose ones
in YAML-based CI tasks, revealing LLM limitations for CI configuration
generation. Analyzing GPT-4o outputs reveals issues like missing or renamed
steps, misinterpreted descriptions, and unnecessary additions that may affect
structural and contextual correctness, indicating a gap between generation
quality and the precision required for executable CI configurations. Our
research offers insights for improving LLM alignment with configuration
languages and guiding future efforts on CI automation and tooling support.

</details>


### [130] [On the Feasibility of Quantum Unit Testing](https://arxiv.org/abs/2507.17235)
*Andriy Miranskyy,José Campos,Anila Mjeda,Lei Zhang,Ignacio García Rodríguez de Guzmán*

Main category: cs.SE

TL;DR: 量子软件测试：量子中心测试（Statevector和Inverse测试）比统计测试更精确、更高效。


<details>
  <summary>Details</summary>
Motivation: 量子软件日益增长的复杂性给软件验证和确认带来了重大挑战，特别是在单元测试方面。

Method: 对1,796,880个变异量子电路进行实证研究和详细分析，比较了传统统计方法与专门为量子电路设计的测试（如Statevector测试、Swap测试和Inverse测试）的能力，以及实现高可靠性所需的测量次数。

Result: 量子中心测试（Statevector和Inverse测试）在检测量子电路预期状态和实际状态之间的细微差异方面，以及在所需测量次数以实现高可靠性方面，均优于统计测试。

Conclusion: 量子中心测试，特别是Statevector测试和Inverse测试，在精确性和效率方面具有明显优势，可减少误报和漏报。

Abstract: The increasing complexity of quantum software presents significant challenges
for software verification and validation, particularly in the context of unit
testing. This work presents a comprehensive study on quantum-centric unit
tests, comparing traditional statistical approaches with tests specifically
designed for quantum circuits. These include tests that run only on a classical
computer, such as the Statevector test, as well as those executable on quantum
hardware, such as the Swap test and the novel Inverse test. Through an
empirical study and detailed analysis on 1,796,880 mutated quantum circuits, we
investigate (a) each test's ability to detect subtle discrepancies between the
expected and actual states of a quantum circuit, and (b) the number of
measurements required to achieve high reliability. The results demonstrate that
quantum-centric tests, particularly the Statevector test and the Inverse test,
provide clear advantages in terms of precision and efficiency, reducing both
false positives and false negatives compared to statistical tests. This work
contributes to the development of more robust and scalable strategies for
testing quantum software, supporting the future adoption of fault-tolerant
quantum computers and promoting more reliable practices in quantum software
engineering.

</details>


### [131] [CASCADE: LLM-Powered JavaScript Deobfuscator at Google](https://arxiv.org/abs/2507.17691)
*Shan Jiang,Pranoy Kovuri,David Tao,Zhixun Tan*

Main category: cs.SE

TL;DR: CASCADE 是一种结合 Gemini 和 JSIR 的混合去混淆方法，能有效恢复 JavaScript 代码的语义和行为，已在谷歌生产环境中使用。


<details>
  <summary>Details</summary>
Motivation: 软件混淆，尤其是在 JavaScript 中，阻碍了代码的理解和分析，给软件测试、静态分析和恶意软件检测带来了重大挑战。

Method: CASCADE 采用 Gemini 来识别潜在的混淆技术的基础组成部分，即关键的 prelude 函数，并利用 JSIR 进行后续的代码转换，以恢复语义元素和程序行为。

Result: CASCADE 成功恢复了原始字符串和 API 名称等语义元素，揭示了原始程序行为，消除了数千条硬编码规则，提高了 JavaScript 去混淆的效率，并减少了逆向工程的努力。

Conclusion: CASCADE是一种新颖的混合方法，通过结合 Gemini 的先进编码能力和编译器中间表示（IR）的确定性转换能力（特别是 JavaScript IR (JSIR)），能够有效恢复原始字符串和 API 名称等语义元素，并揭示原始程序行为。该方法克服了现有的静态和动态去混淆技术的局限性，消除了数千条硬编码规则，同时实现了可靠性和灵活性。CASCADE 已在谷歌的生产环境中部署，在 JavaScript 去混淆效率方面取得了显著的改进，并减少了逆向工程的努力。

Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.

</details>


### [132] [Understanding Prompt Programming Tasks and Questions](https://arxiv.org/abs/2507.17264)
*Jenny T. Liang,Chenyang Yang,Agnia Sergeyuk,Travis D. Breaux,Brad A. Myers*

Main category: cs.SE

TL;DR: 提示编程需要手动完成，工具未能解决开发者在更新提示时提出的关键问题。


<details>
  <summary>Details</summary>
Motivation: 随着提示编程（将提示嵌入软件）的兴起，了解开发者的需求至关重要，因为他们需要对提示进行大量更改。然而，开发者在更新提示时提出的问题以及这些问题对他们计划更改的影响尚不清楚，这可能导致提示编程工具未能满足开发者的需求。

Method: 通过对16名提示程序员进行访谈、观察8名开发者的提示修改过程以及对50名开发者进行调查，开发了一个包含25个任务和51个问题的分类法，并评估了每个任务和问题的重要性。随后，将该分类法与48种研究和商业工具进行了比较。

Result: 研究发现，提示编程任务目前都是手动完成的，并且在51个问题中有16个（包括大部分最重要的问题）仍未得到解答。这表明当前的工具未能充分支持提示编程。

Conclusion: 现有的提示编程工具对提示编程任务的支持不足，大多数重要问题仍未得到解答，这表明在提示编程工具方面存在重要的改进机会。

Abstract: Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.

</details>


### [133] [Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning](https://arxiv.org/abs/2507.17270)
*Alessandro Aneggi,Andrea Janes*

Main category: cs.SE

TL;DR: 分布式实时分析系统项目因“大爆炸式”集成而失败，暴露出沟通、测试和规划方面的缺陷，项目仅运行了6分钟。建议采用早期模拟部署、加强沟通和自上而下规划来改进此类项目。


<details>
  <summary>Details</summary>
Motivation: 分析一个分布式实时分析系统项目，以识别在集成过程中遇到的技术和组织障碍，并提出改进建议，以应对复杂性和降低风险。

Method: 通过对一个为期一年的分布式实时分析系统项目的分析，采用边缘计算和机器学习技术，并进行根本原因分析，识别技术和组织障碍，包括沟通不畅、缺乏早期集成测试以及对自上而下规划的抵制，同时考虑了心理因素，如偏爱已完全开发组件而非模型。

Result: 集成尝试仅实现了六分钟的系统功能，远低于预期的40分钟，暴露了“大爆炸式”集成方法的缺陷。

Conclusion: 该项目经验表明，在分布式实时分析系统中，采用“大爆炸式”集成方法会导致严重的技术和组织障碍，并强调了早期基于模拟的部署、强大的沟通以及自上而下的规划的重要性。传统的敏捷方法在这种环境下可能不足够，需要采用模拟驱动工程和结构化集成周期来规避风险。

Abstract: This experience report analyses a one year project focused on building a
distributed real-time analytics system using edge computing and machine
learning. The project faced critical setbacks due to a big-bang integration
approach, where all components developed by multiple geographically dispersed
partners were merged at the final stage. The integration effort resulted in
only six minutes of system functionality, far below the expected 40 minutes.
Through root cause analysis, the study identifies technical and organisational
barriers, including poor communication, lack of early integration testing, and
resistance to topdown planning. It also considers psychological factors such as
a bias toward fully developed components over mockups. The paper advocates for
early mock based deployment, robust communication infrastructures, and the
adoption of topdown thinking to manage complexity and reduce risk in reactive,
distributed projects. These findings underscore the limitations of traditional
Agile methods in such contexts and propose simulation-driven engineering and
structured integration cycles as key enablers for future success.

</details>


### [134] [Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation](https://arxiv.org/abs/2507.17271)
*Shuaiyu Zhou,Zhengran Zeng,Xiaoling Zhou,Rui Xie,Shikun Zhang,Wei Ye*

Main category: cs.SE

TL;DR: 该研究提出了一种名为 Seed&Steer 的新方法，用于改进基于 LLM 的单元测试生成。该方法将前缀生成和断言生成分开，并使用初始化复杂性和循环复杂性来衡量它们的难度。Seed&Steer 结合了传统单元测试工具和 LLM 的优势，以提高编译成功率和测试覆盖率。实验结果表明，Seed&Steer 在编译通过率和代码覆盖率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 最近，基于大型语言模型（LLM）的方法在自动测试生成方面取得了显著的进步，引起了学术界和工业界的关注。我们通过分离前缀生成和断言生成，从一个新颖的角度重新审视了基于 LLM 的单元测试生成。为了表征各自的挑战，我们定义了初始化复杂性并采用循环复杂性来衡量前缀和断言生成的难度，揭示前者主要影响编译成功，而后者影响测试覆盖率。

Method: Seed&Steer 是一种两步方法，结合了传统的单元测试技术和大型语言模型的能力。Seed&Steer 利用传统的单元测试工具（例如 EvoSuite）生成具有高编译成功率的方法调用，这些方法调用作为种子来指导 LLM 构建有效的测试上下文。然后，它引入分支提示来帮助 LLM 探索不同的执行路径（例如，正常、边界和异常情况）并生成具有高覆盖率的断言。

Result: Seed&Steer 成功编译了先前失败的 792 和 887 个案例，将编译通过率提高了约 7%。它还在不同复杂度的焦点方法的平均分支和行覆盖率方面达到了约 73%，覆盖率提高了 1.09* 到 1.26*。

Conclusion: Seed&Steer 在五个真实的 Java 项目上进行了评估，与最先进的基线进行了比较。结果表明，Seed&Steer 将编译通过率提高了约 7%，在两个 LLM 上成功编译了先前失败的 792 和 887 个案例。它还在不同复杂度的焦点方法的分支和行覆盖率方面达到了约 73% 的覆盖率，覆盖率提高了 1.09* 到 1.26*。

Abstract: Unit tests play a vital role in the software development lifecycle. Recent
advances in Large Language Model (LLM)-based approaches have significantly
improved automated test generation, garnering attention from both academia and
industry. We revisit LLM-based unit test generation from a novel perspective by
decoupling prefix generation and assertion generation. To characterize their
respective challenges, we define Initialization Complexity and adopt Cyclomatic
Complexity to measure the difficulty of prefix and assertion generation,
revealing that the former primarily affects compilation success, while the
latter influences test coverage. To address these challenges, we propose
Seed&Steer, a two-step approach that combines traditional unit testing
techniques with the capabilities of large language models. Seed&Steer leverages
conventional unit testing tools (e.g., EvoSuite) to generate method invocations
with high compilation success rates, which serve as seeds to guide LLMs in
constructing effective test contexts. It then introduces branching cues to help
LLMs explore diverse execution paths (e.g., normal, boundary, and exception
cases) and generate assertions with high coverage. We evaluate Seed&Steer on
five real-world Java projects against state-of-the-art baselines. Results show
that Seed&Steer improves the compilation pass rate by approximately 7%,
successfully compiling 792 and 887 previously failing cases on two LLMs. It
also achieves up to ~73% branch and line coverage across focal methods of
varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our
code, dataset, and experimental scripts will be publicly released to support
future research and reproducibility.

</details>


### [135] [Data Virtualization for Machine Learning](https://arxiv.org/abs/2507.17293)
*Saiful Khan,Joyraj Chakraborty,Philip Beaucamp,Niraj Bhujel,Min Chen*

Main category: cs.SE

TL;DR: ML teams need data virtualization to manage many concurrent workflows and large amounts of data. This paper presents a data virtualization service designed for scalability.


<details>
  <summary>Details</summary>
Motivation: Machine learning teams have multiple concurrent ML workflows, involving significant data storage, processing, and maintenance. Data virtualization is critical for supporting these ML workflows.

Method: The paper presents the design and implementation of a data virtualization service, focusing on its service architecture and service operations.

Result: The infrastructure currently supports six ML applications, each with more than one ML workflow, and the data virtualization service allows for future growth.

Conclusion: The data virtualization service is designed to support a growing number of ML applications and workflows.

Abstract: Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.

</details>


### [136] [How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?](https://arxiv.org/abs/2507.17314)
*Ricardo Hidalgo Aragón,Jesús M. González-Barahona,Gregorio Robles*

Main category: cs.SE

TL;DR: 研究了Scratch项目中计算思维能力与代码设计坏味之间的关系，结果可用于改进编程教育和软件维护。


<details>
  <summary>Details</summary>
Motivation: 为了解代码坏味在初学者使用类Scratch的块状编程项目中的重要性，以及计算思维能力与代码设计问题之间的联系。

Method: 从约200万个公开的Scratch项目中提取9个计算思维分数和40个代码坏味指标。通过描述性统计、相关性检验、交叉验证和机器学习模型进行分析，并辅以定性抽查来解释定量模式。

Result: 识别出特定的计算思维维度与特定的代码坏味之间的关联，以及任务背景是否会调节这些关联。该研究为未来的教育干预提供了循证的课程设计、自动化反馈系统和效应量基准。

Conclusion: 该研究首次大规模、细粒度地将特定的计算思维能力与具体的代码设计缺陷和反模式联系起来，为理解编程习惯如何影响早期技能获取提供了见解，并有助于计算教育理论和可持续软件维护的实践工具。

Abstract: Context. Code smells, which are recurring anomalies in design or style, have
been extensively researched in professional code. However, their significance
in block-based projects created by novices is still largely unknown.
Block-based environments such as Scratch offer a unique, data-rich setting to
examine how emergent design problems intersect with the cultivation of
computational-thinking (CT) skills. Objective. This research explores the
connection between CT proficiency and design-level code smells--issues that may
hinder software maintenance and evolution--in programs created by Scratch
developers. We seek to identify which CT dimensions align most strongly with
which code smells and whether task context moderates those associations.
Method. A random sample of aprox. 2 million public Scratch projects is mined.
Using open-source linters, we extract nine CT scores and 40 code smell
indicators from these projects. After rigorous pre-processing, we apply
descriptive analytics, robust correlation tests, stratified cross-validation,
and exploratory machine-learning models; qualitative spot-checks contextualize
quantitative patterns. Impact. The study will deliver the first large-scale,
fine-grained map linking specific CT competencies to concrete design flaws and
antipatterns. Results are poised to (i) inform evidence-based curricula and
automated feedback systems, (ii) provide effect-size benchmarks for future
educational interventions, and (iii) supply an open, pseudonymized dataset and
reproducible analysis pipeline for the research community. By clarifying how
programming habits influence early skill acquisition, the work advances both
computing-education theory and practical tooling for sustainable software
maintenance and evolution.

</details>


### [137] [Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java](https://arxiv.org/abs/2507.17369)
*Corentin Latappy,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes,Lina Ochoa*

Main category: cs.SE

TL;DR: Roseau 是一种新的静态分析工具，用于检测 API 演化中的重大变更 (BC)，其准确性和效率均优于现有工具，并能处理大规模纵向分析。


<details>
  <summary>Details</summary>
Motivation: 理解 API 演化和重大变更 (BC) 的引入对于库维护者管理向后兼容性和研究人员进行软件库演化的实证研究至关重要。现有的工具（如 JApiCmp 和 Revapi）依赖于二进制 JAR 文件，这限制了它们在大规模纵向研究和提交级别 BC 检测等方面的应用。

Method: Roseau 是一种新的静态分析工具，它从库代码中构建技术无关的 API 模型，并进行丰富的语义分析。它可以从源代码或字节码构建 API 模型，并针对大规模的库历史纵向分析进行了优化。通过比较不同版本的 API 模型来识别 BC。

Result: Roseau 在准确性方面优于 JApiCmp 和 Revapi，F1 分数分别为 0.99、0.86 和 0.91。它能够在大规模库（包括具有数十万行代码的库）上以不到两秒的时间检测版本之间的 BC。Roseau 还能够显著缩短分析时间，例如在分析 Google Guava API 的 14 年演化和 6,839 次提交时，将分析时间从几天缩短到几分钟。

Conclusion: Roseau 是一项新颖的静态分析工具，它利用包含丰富语义分析的库代码构建技术无关的 API 模型。API 模型可以用于研究 API 演化，并进行比较以识别任何两个库版本之间的重大变更 (BC)。Roseau 的准确性（F1 = 0.99）高于 JApiCmp (F1 = 0.86) 和 Revapi (F1 = 0.91)，并且能够以更短的分析时间处理大型库，为 API 演化研究提供了更有效、更准确的解决方案。

Abstract: Understanding API evolution and the introduction of breaking changes (BCs) in
software libraries is essential for library maintainers to manage backward
compatibility and for researchers to conduct empirical studies on software
library evolution. In Java, tools such as JApiCmp and Revapi are commonly used
to detect BCs between library releases, but their reliance on binary JARs
limits their applicability. This restriction hinders large-scale longitudinal
studies of API evolution and fine-grained analyses such as commit-level BC
detection. In this paper, we introduce Roseau, a novel static analysis tool
that constructs technology-agnostic API models from library code equipped with
rich semantic analyses. API models can be analyzed to study API evolution and
compared to identify BCs between any two versions of a library (releases,
commits, branches, etc.). Unlike traditional approaches, Roseau can build API
models from source code or bytecode, and is optimized for large-scale
longitudinal analyses of library histories. We assess the accuracy,
performance, and suitability of Roseau for longitudinal studies of API
evolution, using JApiCmp and Revapi as baselines. We extend and refine an
established benchmark of BCs and show that Roseau achieves higher accuracy (F1
= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular
libraries from Maven Central and find that Roseau delivers excellent
performance, detecting BCs between versions in under two seconds, including in
libraries with hundreds of thousands of lines of code. We further illustrate
the limitations of JApiCmp and Revapi for longitudinal studies and the novel
analysis capabilities offered by Roseau by tracking the evolution of Google's
Guava API and the introduction of BCs over 14 years and 6,839 commits, reducing
analysis times from a few days to a few minutes.

</details>


### [138] [Investigating Training Data Detection in AI Coders](https://arxiv.org/abs/2507.17389)
*Tianlin Li,Yunxiang Wei,Zhiming Li,Aishan Liu,Qing Guo,Xianglong Liu,Dongning Sun,Yang Liu*

Main category: cs.SE

TL;DR: 鉴于CodeLLM可能泄露敏感信息，本研究评估了七种TDD方法在代码数据上的表现，并引入了CodeSnitch数据集及相关测试策略，旨在改进代码TDD的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管CodeLLM在软件工程中越来越重要，但它们有时会生成包含专有或敏感代码片段的输出，这引起了对训练数据使用不当以及隐私和知识产权风险的担忧。因此，训练数据检测（TDD）对于确保CodeLLM的负责任和合规部署至关重要。然而，现有的TDD方法在代码数据上的有效性仍未得到充分探索。

Method: 本研究通过对七种最先进的TDD方法进行实证研究，并在八个CodeLLM上评估其性能来解决代码数据的TDD问题。引入了CodeSnitch数据集，并设计了基于代码克隆检测分类的突变策略来测试TDD方法的鲁棒性。

Result: 该研究对当前代码TDD技术进行了系统的评估，并为未来开发更有效、更鲁棒的检测方法提供了见解。

Conclusion: 本研究对用于代码数据的七种最先进的TDD方法进行了全面的实证研究，并在八个CodeLLM上评估了它们的性能。研究还引入了一个名为CodeSnitch的基准数据集，并在模拟不同代码克隆类型的设置下测试了TDD方法的鲁棒性。

Abstract: Recent advances in code large language models (CodeLLMs) have made them
indispensable tools in modern software engineering. However, these models
occasionally produce outputs that contain proprietary or sensitive code
snippets, raising concerns about potential non-compliant use of training data,
and posing risks to privacy and intellectual property. To ensure responsible
and compliant deployment of CodeLLMs, training data detection (TDD) has become
a critical task. While recent TDD methods have shown promise in natural
language settings, their effectiveness on code data remains largely
underexplored. This gap is particularly important given code's structured
syntax and distinct similarity criteria compared to natural language. To
address this, we conduct a comprehensive empirical study of seven
state-of-the-art TDD methods on source code data, evaluating their performance
across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a
function-level benchmark dataset comprising 9,000 code samples in three
programming languages, each explicitly labeled as either included or excluded
from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design
targeted mutation strategies to test the robustness of TDD methods under three
distinct settings. These mutation strategies are grounded in the
well-established Type-1 to Type-4 code clone detection taxonomy. Our study
provides a systematic assessment of current TDD techniques for code and offers
insights to guide the development of more effective and robust detection
methods in the future.

</details>


### [139] [AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests](https://arxiv.org/abs/2507.17542)
*Lara Khatib,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: AssertFlip uses LLMs to generate passing tests, then inverts them to fail, successfully reproducing bugs where previous methods failed.


<details>
  <summary>Details</summary>
Motivation: Bug reproduction is critical but most bugs lack executable tests for reproduction. AssertFlip addresses this by automatically generating Bug Reproducible Tests (BRTs) using LLMs.

Method: AssertFlip first generates passing tests on the buggy behaviour and then inverts these tests to fail when the bug is present, using large language models (LLMs).

Result: AssertFlip achieves a fail-to-pass success rate of 43.6% on the SWT-Bench-Verified subset, outperforming all known techniques.

Conclusion: AssertFlip outperform all known techniques in the leaderboard of SWT-Bench, achieving a fail-to-pass success rate of 43.6% on the SWT-Bench-Verified subset.

Abstract: Bug reproduction is critical in the software debugging and repair process,
yet the majority of bugs in open-source and industrial settings lack executable
tests to reproduce them at the time they are reported, making diagnosis and
resolution more difficult and time-consuming. To address this challenge, we
introduce AssertFlip, a novel technique for automatically generating Bug
Reproducible Tests (BRTs) using large language models (LLMs). Unlike existing
methods that attempt direct generation of failing tests, AssertFlip first
generates passing tests on the buggy behaviour and then inverts these tests to
fail when the bug is present. We hypothesize that LLMs are better at writing
passing tests than ones that crash or fail on purpose. Our results show that
AssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a
benchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass
success rate of 43.6% on the SWT-Bench-Verified subset.

</details>


### [140] [CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning](https://arxiv.org/abs/2507.17548)
*Lingxiao Tang,He Ye,Zhongxin Liu,Xiaoxue Ren,Lingfeng Bao*

Main category: cs.SE

TL;DR: 提出 CodeReasoner 框架，通过改进数据集和两阶段训练（指令调优 + GRPO 强化学习）来提升 LLM 的代码推理能力，实验结果表明其性能显著优于现有方法，并能在 14B 模型上超越 GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据质量低和监督微调的局限性，先前代码推理方法在泛化能力方面表现不佳。为了解决这些挑战，提出了 CodeReasoner 框架。

Method: CodeReasoner 框架包含数据集构建和一个两阶段训练过程。首先，提出了一种数据集构建方法，专注于 Python 程序的 الأصلي 执行逻辑。然后，应用指令调优，从强大的教师模型中提取执行特定知识。最后，通过 GRPO 强化学习来增强推理和泛化能力。

Result: CodeReasoner 框架通过数据集构建和两阶段训练（指令调优和 GRPO 强化学习）来提高代码推理能力。实验结果表明，CodeReasoner 显著优于先前方法，并且在 14B 模型上优于 GPT-4o。

Conclusion: CodeReasoner 在三个广泛使用的代码推理基准上，使用 7B 模型将性能提高了 27.1% 至 40.2%，并且 7B 模型在输入/输出和覆盖率预测等关键任务上可与 GPT-4o 相媲美。扩展到 14B 后，CodeReasoner 在所有基准测试中的表现均优于 GPT-4o。

Abstract: Code reasoning is a fundamental capability for large language models (LLMs)
in the code domain. It involves understanding and predicting a program's
execution behavior, such as determining the output for a given input or whether
a specific statement will be executed. This capability is essential for
downstream tasks like debugging, code generation, and program repair. Prior
approaches mainly rely on supervised fine-tuning to improve performance in code
reasoning tasks. However, they often show limited gains and fail to generalize
across diverse scenarios. We argue this is due to two core issues: the low
quality of training data and the limitations of supervised fine-tuning, which
struggles to teach general reasoning skills. To address these challenges, we
propose CodeReasoner, a framework that spans both dataset construction and a
two-stage training process. First, we introduce a method to construct datasets
that focus on the core execution logic of Python programs. Next, we apply
instruction tuning to inject execution-specific knowledge distilled from a
powerful teacher model. We then enhance reasoning and generalization through
GRPO reinforcement learning on top of the fine-tuned model. Experiments on
three widely-used code reasoning benchmarks show that CodeReasoner improves
performance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the
7B model matches GPT-4o on key tasks like input/output and coverage prediction.
When scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.
Ablation studies confirm the effectiveness of each training stage and highlight
the importance of reasoning chains.

</details>


### [141] [Contextual Code Retrieval for Commit Message Generation: A Preliminary Study](https://arxiv.org/abs/2507.17690)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: C3Gen 通过整合仓库中的上下文代码来改进提交信息生成，优于仅依赖代码差异的传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的提交信息生成方法主要依赖代码差异，但这种方法不足以捕捉生成高质量、信息丰富的提交信息所需的全部上下文。

Method: 提出了一种名为 C3Gen 的基于上下文代码检索的方法，通过从仓库中检索与提交相关的代码片段并将其纳入模型输入来丰富上下文信息。

Result: 实验结果表明，C3Gen 能够有效利用附加信息生成更全面、更具信息量的提交信息，并具有更高的实际价值。进一步的分析强调了基于相似性的度量标准的可靠性问题，并为 CMG 提供了经验性见解。

Conclusion: 通过结合仓库范围内的附加信息，C3Gen 能够有效地生成更全面、更具信息量且在实际开发场景中具有更高实用价值的提交信息。

Abstract: A commit message describes the main code changes in a commit and plays a
crucial role in software maintenance. Existing commit message generation (CMG)
approaches typically frame it as a direct mapping which inputs a code diff and
produces a brief descriptive sentence as output. However, we argue that relying
solely on the code diff is insufficient, as raw code diff fails to capture the
full context needed for generating high-quality and informative commit
messages. In this paper, we propose a contextual code retrieval-based method
called C3Gen to enhance CMG by retrieving commit-relevant code snippets from
the repository and incorporating them into the model input to provide richer
contextual information at the repository scope. In the experiments, we
evaluated the effectiveness of C3Gen across various models using four objective
and three subjective metrics. Meanwhile, we design and conduct a human
evaluation to investigate how C3Gen-generated commit messages are perceived by
human developers. The results show that by incorporating contextual code into
the input, C3Gen enables models to effectively leverage additional information
to generate more comprehensive and informative commit messages with greater
practical value in real-world development scenarios. Further analysis
underscores concerns about the reliability of similaritybased metrics and
provides empirical insights for CMG.

</details>


### [142] [Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence](https://arxiv.org/abs/2507.17743)
*Andre Menolli,Bruno Strik*

Main category: cs.SE

TL;DR: 本研究通过分析代码坏味道和SOLID原则违规与面向对象编程学习困难的关系，构建了一个概念图模型，并得到专家验证，有助于识别和解决学生在学习面向对象编程时遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究通过源代码分析（如代码坏味道和SOLID原则）来识别面向对象编程中的设计和编码问题，但很少有研究探讨这些代码级别问题与面向对象编程学习困难之间的关系。

Method: 通过定性分析识别主要的学习困难类别，并通过文献综述将这些困难与代码坏味道和SOLID原则违规联系起来，最终开发了一个概念图模型。

Result: 开发了一个将代码问题与特定的面向对象编程学习挑战联系起来的概念图模型，并由专家进行了评估。

Conclusion: 本研究通过概念图将代码问题与特定的面向对象编程学习挑战联系起来，并经过专家评估，证明了其在教育环境中的相关性和适用性。

Abstract: Object-Oriented programming is frequently challenging for undergraduate
Computer Science students, particularly in understanding abstract concepts such
as encapsulation, inheritance, and polymorphism. Although the literature
outlines various methods to identify potential design and coding issues in
object-oriented programming through source code analysis, such as code smells
and SOLID principles, few studies explore how these code-level issues relate to
learning difficulties in Object-Oriented Programming. In this study, we explore
the relationship of the code issue indicators with common challenges
encountered during the learning of object-oriented programming. Using
qualitative analysis, we identified the main categories of learning
difficulties and, through a literature review, established connections between
these difficulties, code smells, and violations of the SOLID principles. As a
result, we developed a conceptual map that links code-related issues to
specific learning challenges in Object-Oriented Programming. The model was then
evaluated by an expert who applied it in the analysis of the student code to
assess its relevance and applicability in educational contexts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [143] [Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design](https://arxiv.org/abs/2507.16818)
*C. H. E. Jordaan,M. van der Stelt,T. J. J. Maal,V. M. A. Stirler,R. Leijendekkers,T. Kachman,G. A. de Jong*

Main category: cs.LG

TL;DR: 本研究使用人工智能来标准化截肢者的假肢接受腔设计，并发现随机森林模型在预测所需调整方面效果最好。


<details>
  <summary>Details</summary>
Motivation: 为了标准化假肢接受腔的设计，因为目前的设计很大程度上依赖于技师的技能和专业知识。

Method: 本研究使用三种不同的算法（3D神经网络、前馈神经网络和随机森林）来预测假肢接受腔的最终形状或适应性。通过测量AI生成的接受腔与技师设计的接受腔之间的表面到表面距离和距离图来评估每种算法的性能。

Result: 估计适应性比直接预测最终接受腔形状的性能更好。随机森林模型在适应性预测任务中表现最佳，中位表面到表面距离为1.24毫米。

Conclusion: 通过在适应性预测任务上使用随机森林模型，可以实现假肢接受腔设计的标准化，模型在中位表面到表面距离为1.24毫米，第一四分位数为1.03毫米，第三四分位数为1.54毫米。

Abstract: The quality of a transtibial prosthetic socket depends on the prosthetist's
skills and expertise, as the fitting is performed manually. This study
investigates multiple artificial intelligence (AI) approaches to help
standardize transtibial prosthetic socket design. Data from 118 patients were
collected by prosthetists working in the Dutch healthcare system. This data
consists of a three-dimensional (3D) scan of the residual limb and a
corresponding 3D model of the prosthetist-designed socket. Multiple data
pre-processing steps are performed for alignment, standardization and
optionally compression using Morphable Models and Principal Component Analysis.
Afterward, three different algorithms - a 3D neural network, Feedforward neural
network, and random forest - are developed to either predict 1) the final
socket shape or 2) the adaptations performed by a prosthetist to predict the
socket shape based on the 3D scan of the residual limb. Each algorithm's
performance was evaluated by comparing the prosthetist-designed socket with the
AI-generated socket, using two metrics in combination with the error location.
First, we measure the surface-to-surface distance to assess the overall surface
error between the AI-generated socket and the prosthetist-designed socket.
Second, distance maps between the AI-generated and prosthetist sockets are
utilized to analyze the error's location. For all algorithms, estimating the
required adaptations outperformed direct prediction of the final socket shape.
The random forest model applied to adaptation prediction yields the lowest
error with a median surface-to-surface distance of 1.24 millimeters, a first
quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.

</details>


### [144] [Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs](https://arxiv.org/abs/2507.16833)
*Qiuyu Shi,Kangming Li,Yao Fehlis,Daniel Persaud,Robert Black,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: 本研究提出了一个自动化的工作流，用于检测和纠正材料发现实验中的数据噪声，提高了数据质量和实验精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决自驱动实验室（SDLs）在输入参数捕获中可能出现的错误，这些错误会破坏用于模拟系统性能的特征，从而影响当前和未来的实验活动。

Method: 开发了一个自动化的工作流，用于系统地检测噪声特征，确定可以纠正的样本-特征配对，并最终恢复正确的特征值。

Result: 研究结果表明，高强度噪声和大型训练数据集有利于检测和纠正噪声特征。低强度噪声会降低检测和恢复能力，但可以通过更大的清洁训练数据集来补偿。检测和校正结果因特征而异，具有连续分布的特征比具有离散或狭窄分布的特征更具可恢复性。

Conclusion: 该研究提出了一个模型无关的框架，用于在存在噪声、有限数据和不同特征分布的情况下进行合理的数据恢复，并为 kNN 填充在材料数据集中的应用提供了实际基准。

Abstract: Self-driving laboratories (SDLs) have shown promise to accelerate materials
discovery by integrating machine learning with automated experimental
platforms. However, errors in the capture of input parameters may corrupt the
features used to model system performance, compromising current and future
campaigns. This study develops an automated workflow to systematically detect
noisy features, determine sample-feature pairings that can be corrected, and
finally recover the correct feature values. A systematic study is then
performed to examine how dataset size, noise intensity, and feature value
distribution affect both the detectability and recoverability of noisy
features. In general, high-intensity noise and large training datasets are
conducive to the detection and correction of noisy features. Low-intensity
noise reduces detection and recovery but can be compensated for by larger clean
training data sets. Detection and correction results vary between features with
continuous and dispersed feature distributions showing greater recoverability
compared to features with discrete or narrow distributions. This systematic
study not only demonstrates a model agnostic framework for rational data
recovery in the presence of noise, limited data, and differing feature
distributions but also provides a tangible benchmark of kNN imputation in
materials data sets. Ultimately, it aims to enhance data quality and
experimental precision in automated materials discovery.

</details>


### [145] [TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning](https://arxiv.org/abs/2507.16844)
*Jie He,Vincent Theo Willem Kenbeek,Zhantao Yang,Meixun Qu,Ezio Bartocci,Dejan Ničković,Radu Grosu*

Main category: cs.LG

TL;DR: TD-Interpreter is an ML tool that uses multimodal learning (fine-tuned LLaVA) and synthetic data to help engineers understand complex timing diagrams, outperforming GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To assist engineers in understanding complex timing diagrams (TDs) from third parties during the design and verification process.

Method: TD-Interpreter is implemented using multimodal learning by fine-tuning LLaVA (a 7B Multimodal Large Language Model) and a synthetic data generation workflow to align visual information with its textual interpretation.

Result: Experimental evaluation shows TD-Interpreter is useful and outperforms untuned GPT-4o by a large margin on the evaluated benchmarks.

Conclusion: TD-Interpreter is a useful tool that assists engineers in understanding complex timing diagrams by leveraging multimodal learning and a synthetic data generation workflow. It outperforms untuned GPT-4o on evaluated benchmarks.

Abstract: We introduce TD-Interpreter, a specialized ML tool that assists engineers in
understanding complex timing diagrams (TDs), originating from a third party,
during their design and verification process. TD-Interpreter is a visual
question-answer environment which allows engineers to input a set of TDs and
ask design and verification queries regarding these TDs. We implemented
TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B
Multimodal Large Language Model (MLLM). To address limited training data
availability, we developed a synthetic data generation workflow that aligns
visual information with its textual interpretation. Our experimental evaluation
demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o
by a large margin on the evaluated benchmarks.

</details>


### [146] [Efficient Neural Network Verification via Order Leading Exploration of Branch-and-Bound Trees](https://arxiv.org/abs/2507.17453)
*Guanqin Zhang,Kota Fukuda,Zhenya Zhang,H. M. N. Dilum Bandara,Shiping Chen,Jianjun Zhao,Yulei Sui*

Main category: cs.LG

TL;DR: Oliva框架通过对子问题进行优先级排序，有效提高了神经网路验证的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的"分而治之"（BaB）策略在探索子问题时效率低下，因为它采用"先到先服务"的方式，无法有效确定子问题的优先级。

Method: 提出了一种新的验证框架Oliva，它通过引入子问题优先级排序来改进传统的"先到先服务"的"分而治之"（BaB）策略。Oliva有两种变体：$Oliva^{GR}$（贪心策略）和$Oliva^{SA}$（模拟退火策略）。

Result: 在MNIST和CIFAR10数据集的5个模型共690个验证问题上，Oliva相比现有最先进的方法实现了显著的加速，MNIST上最高25倍，CIFAR10上最高80倍。

Conclusion: Oliva框架通过优先探索更可能包含反例的子问题，提高了神经网路验证的效率，在MNIST和CIFAR10数据集上分别实现了高达25倍和80倍的加速。

Abstract: The vulnerability of neural networks to adversarial perturbations has
necessitated formal verification techniques that can rigorously certify the
quality of neural networks. As the state-of-the-art, branch and bound (BaB) is
a "divide-and-conquer" strategy that applies off-the-shelf verifiers to
sub-problems for which they perform better. While BaB can identify the
sub-problems that are necessary to be split, it explores the space of these
sub-problems in a naive "first-come-first-serve" manner, thereby suffering from
an issue of inefficiency to reach a verification conclusion. To bridge this
gap, we introduce an order over different sub-problems produced by BaB,
concerning with their different likelihoods of containing counterexamples.
Based on this order, we propose a novel verification framework Oliva that
explores the sub-problem space by prioritizing those sub-problems that are more
likely to find counterexamples, in order to efficiently reach the conclusion of
the verification. Even if no counterexample can be found in any sub-problem, it
only changes the order of visiting different sub-problem and so will not lead
to a performance degradation. Specifically, Oliva has two variants, including
$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that
are more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy
inspired by simulated annealing that gradually shifts from exploration to
exploitation to locate the globally optimal sub-problems. We experimentally
evaluate the performance of Oliva on 690 verification problems spanning over 5
models with datasets MNIST and CIFAR10. Compared to the state-of-the-art
approaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up
to 80X in CIFAR10.

</details>


### [147] [Reinforcement Learning in hyperbolic space for multi-step reasoning](https://arxiv.org/abs/2507.16864)
*Tao Xu,Dung-Yang Lee,Momiao Xiong*

Main category: cs.LG

TL;DR: 该研究提出了一种新的双曲Transformer强化学习框架，以提高多步推理任务的准确性和效率，并在数学问题和最优控制任务上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习方法在处理复杂推理任务时遇到的信用分配、高维状态表示和稳定性问题。

Method: 提出了一种将双曲Transformer集成到强化学习中以实现多步推理的新框架，利用双曲嵌入有效模拟分层结构。

Result: 与使用标准Transformer的强化学习方法相比，双曲强化学习在FrontierMath基准测试的准确性上提高了32%~44%，在非线性最优控制基准测试上提高了43%~45%，同时在FrontierMath基准测试的计算时间上减少了16%~32%，在非线性最优控制基准测试上减少了16%~17%。

Conclusion: 该研究证明了双曲Transformer在强化学习中的潜力，特别是在涉及分层结构的多步推理任务中。

Abstract: Multi-step reasoning is a fundamental challenge in artificial intelligence,
with applications ranging from mathematical problem-solving to decision-making
in dynamic environments. Reinforcement Learning (RL) has shown promise in
enabling agents to perform multi-step reasoning by optimizing long-term
rewards. However, conventional RL methods struggle with complex reasoning tasks
due to issues such as credit assignment, high-dimensional state
representations, and stability concerns. Recent advancements in Transformer
architectures and hyperbolic geometry have provided novel solutions to these
challenges. This paper introduces a new framework that integrates hyperbolic
Transformers into RL for multi-step reasoning. The proposed approach leverages
hyperbolic embeddings to model hierarchical structures effectively. We present
theoretical insights, algorithmic details, and experimental results that
include Frontier Math and nonlinear optimal control problems. Compared to RL
with vanilla transformer, the hyperbolic RL largely improves accuracy by
(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control
benchmark, while achieving impressive reduction in computational time by
(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control
benchmark. Our work demonstrates the potential of hyperbolic Transformers in
reinforcement learning, particularly for multi-step reasoning tasks that
involve hierarchical structures.

</details>


### [148] [Eco-Friendly AI: Unleashing Data Power for Green Federated Learning](https://arxiv.org/abs/2507.17241)
*Mattia Sabella,Monica Vitali*

Main category: cs.LG

TL;DR: 本研究提出了一种通过数据选择和节点选择来减少联邦学习环境影响的绿色AI方法，在时间序列分类任务中取得成效。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和机器学习的广泛采用，其巨大的能源消耗和碳排放对环境造成了显著影响。特别是机器学习模型的训练数据集大小是影响能耗的关键因素。联邦学习（FL）虽然可以在不共享原始数据的情况下进行模型训练，但也面临数据异质性、节点能力和环境影响等挑战。因此，有必要研究如何减少联邦学习的环境足迹。

Method: 提出了一种数据驱动的方法，通过分析联邦数据集的特征，根据质量指标选择最佳数据子集，并选择环境影响最小的联邦节点。在此基础上，开发了一个交互式推荐系统，通过数据缩减来优化联邦学习配置，最大限度地减少训练期间的环境影响。

Result: 通过将所提出的方法应用于时间序列分类任务，证明了在减少联邦学习环境影响方面的有效性，并取得了有希望的结果。

Conclusion: 该研究提出了一种以数据为中心的绿色联邦学习方法，通过优化数据选择和节点选择来减少联邦学习的环境影响，并在时间序列分类任务中取得了显著成效。

Abstract: The widespread adoption of Artificial Intelligence (AI) and Machine Learning
(ML) comes with a significant environmental impact, particularly in terms of
energy consumption and carbon emissions. This pressing issue highlights the
need for innovative solutions to mitigate AI's ecological footprint. One of the
key factors influencing the energy consumption of ML model training is the size
of the training dataset. ML models are often trained on vast amounts of data
continuously generated by sensors and devices distributed across multiple
locations. To reduce data transmission costs and enhance privacy, Federated
Learning (FL) enables model training without the need to move or share raw
data. While FL offers these advantages, it also introduces challenges due to
the heterogeneity of data sources (related to volume and quality),
computational node capabilities, and environmental impact.
  This paper contributes to the advancement of Green AI by proposing a
data-centric approach to Green Federated Learning. Specifically, we focus on
reducing FL's environmental impact by minimizing the volume of training data.
Our methodology involves the analysis of the characteristics of federated
datasets, the selecting of an optimal subset of data based on quality metrics,
and the choice of the federated nodes with the lowest environmental impact. We
develop a comprehensive methodology that examines the influence of data-centric
factors, such as data quality and volume, on FL training performance and carbon
emissions. Building on these insights, we introduce an interactive
recommendation system that optimizes FL configurations through data reduction,
minimizing environmental impact during training. Applying this methodology to
time series classification has demonstrated promising results in reducing the
environmental impact of FL tasks.

</details>


### [149] [Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization](https://arxiv.org/abs/2507.16867)
*Yunyi Zhao,Wei Zhang,Cheng Xiang,Hongyang Du,Dusit Niyato,Shuhua Gao*

Main category: cs.LG

TL;DR: DiffCarl是一种用于多微电网系统智能运行的碳感知和风险感知强化学习算法，通过集成扩散模型来处理不确定性，并在降低成本和碳排放方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源的整合以及系统复杂性的增加，多微电网系统在不确定性下的实时能源调度和优化面临重大挑战。

Method: DiffCarl将扩散模型集成到深度强化学习（DRL）框架中，通过去噪生成过程学习动作分布，从而增强DRL策略的表现力，并实现碳感知和风险感知调度。

Result: 与经典算法和最先进的DRL解决方案相比，DiffCarl的运营成本降低了2.3%-30.1%，与未考虑碳排放的变体相比，碳排放量减少了28.7%，并降低了性能变异性。

Conclusion: DiffCarl是一种新颖的、面向实际的解决方案，其灵活的设计可适应不同的系统配置和目标，支持其在不断变化的能源系统中进行实际部署。

Abstract: This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware
reinforcement learning algorithm for intelligent operation of multi-microgrid
systems. With the growing integration of renewables and increasing system
complexity, microgrid communities face significant challenges in real-time
energy scheduling and optimization under uncertainty. DiffCarl integrates a
diffusion model into a deep reinforcement learning (DRL) framework to enable
adaptive energy scheduling under uncertainty and explicitly account for carbon
emissions and operational risk. By learning action distributions through a
denoising generation process, DiffCarl enhances DRL policy expressiveness and
enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid
environments. Extensive experimental studies demonstrate that it outperforms
classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower
operational cost. It also achieves 28.7% lower carbon emissions than those of
its carbon-unaware variant and reduces performance variability. These results
highlight DiffCarl as a practical and forward-looking solution. Its flexible
design allows efficient adaptation to different system configurations and
objectives to support real-world deployment in evolving energy systems.

</details>


### [150] [Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks](https://arxiv.org/abs/2507.16871)
*Pietro Giuseppe Fré,Federico Milanesio,Guido Sanguinetti,Matteo Santoro*

Main category: cs.LG

TL;DR: 介绍了 Cartan 神经网络，这是一种利用非紧对称空间 U/H 的几何结构的新型神经网络。该论文详细介绍了其数学基础，强调了其几何可解释性和协变性。


<details>
  <summary>Details</summary>
Motivation: 利用非紧对称空间 U/H 作为一种有前途的齐性流形，以开发一种几何上一致的神经网络理论。

Method: 详细介绍 Cartan 神经网络的数学结构，特别是层的几何特性以及层之间的映射如何与这些结构相互作用，以实现协变性和几何可解释性。

Result: 展示了 Cartan 神经网络的几何特性、层交互以及它们如何实现协变性和几何可解释性。

Conclusion: 这两个论文集是朝着利用群论结构实现完全几何可解释的神经网络理论迈出的第一步。

Abstract: Recent work has identified non-compact symmetric spaces U/H as a promising
class of homogeneous manifolds to develop a geometrically consistent theory of
neural networks. An initial implementation of these concepts has been presented
in a twin paper under the moniker of Cartan Neural Networks, showing both the
feasibility and the performance of these geometric concepts in a machine
learning context. The current paper expands on the mathematical structures
underpinning Cartan Neural Networks, detailing the geometric properties of the
layers and how the maps between layers interact with such structures to make
Cartan Neural Networks covariant and geometrically interpretable. Together,
these twin papers constitute a first step towards a fully geometrically
interpretable theory of neural networks exploiting group-theoretic structures

</details>


### [151] [Confidence Optimization for Probabilistic Encoding](https://arxiv.org/abs/2507.16881)
*Pengjiu Xia,Yidian Huang,Wenchao Wei,Yuwen Tan*

Main category: cs.LG

TL;DR: 通过置信优化概率编码（CPE）改善了概率编码在分类任务中的准确性，提高了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 高斯噪声会扭曲点估计距离测量，影响分类任务的准确性。现有方法未能有效解决此问题。

Method: 提出了一种置信优化概率编码（CPE）方法，该方法包含两个关键策略：1. 引入置信感知机制来调整距离计算，以确保概率编码分类任务中的一致性和可靠性。2. 使用L2正则化项直接约束方差，替代了依赖不可靠先验假设的KL散度。

Result: 所提出的CPE方法在自然语言分类任务上，于BERT和RoBERTa模型上均显著提高了性能和泛化能力。

Conclusion: 所提出的置信优化概率编码（CPE）方法通过引入置信感知机制和使用L2正则化项替代KL散度，提高了距离的可靠性并增强了表示学习能力。

Abstract: Probabilistic encoding introduces Gaussian noise into neural networks,
enabling a smooth transition from deterministic to uncertain states and
enhancing generalization ability. However, the randomness of Gaussian noise
distorts point-based distance measurements in classification tasks. To mitigate
this issue, we propose a confidence optimization probabilistic encoding (CPE)
method that improves distance reliability and enhances representation learning.
Specifically, we refine probabilistic encoding with two key strategies: First,
we introduce a confidence-aware mechanism to adjust distance calculations,
ensuring consistency and reliability in probabilistic encoding classification
tasks. Second, we replace the conventional KL divergence-based variance
regularization, which relies on unreliable prior assumptions, with a simpler L2
regularization term to directly constrain variance. The method we proposed is
model-agnostic, and extensive experiments on natural language classification
tasks demonstrate that our method significantly improves performance and
generalization on both the BERT and the RoBERTa model.

</details>


### [152] [SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling](https://arxiv.org/abs/2507.16884)
*Yi Guo,Wei Wang,Zhihang Yuan,Rong Cao,Kuan Chen,Zhengyang Chen,Yuanyuan Huo,Yang Zhang,Yuping Wang,Shouda Liu,Yuxuan Wang*

Main category: cs.LG

TL;DR: Flow Matching 模型（如 MeanFlow）在生成时计算成本高。MeanFlow 通过学习平均速度场来解决此问题，但其方法受限于微分算子。本文提出 SplitMeanFlow，通过一种新的代数恒等式“区间分割一致性”来学习平均速度场，避免了计算 JVP，效率更高、实现更简单、训练更稳定，并在语音合成产品中实现了 20 倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有的 Flow Matching 模型（如 MeanFlow）在生成过程中需要计算昂贵的迭代采样，而 MeanFlow 通过学习平均速度场来解决此问题，但其学习方法（基于微分恒等式）是一种更受限的特殊情况。

Method: 提出了一种名为“区间分割一致性”的新颖的、纯代数的恒等式，该恒等式不依赖于任何微分算子，而是利用定积分的可加性属性来建立平均速度场在不同时间间隔内的自指称关系。基于此原理，开发了一个名为 SplitMeanFlow 的新训练框架，将此代数一致性作为学习目标。

Result: SplitMeanFlow 的单步和两步模型已成功应用于语音合成产品（如 Doubao），实现了 20 倍的加速。从理论上讲，证明了当区间分割趋于无穷小时，SplitMeanFlow 的代数一致性可以恢复 MeanFlow 的微分恒等式。

Conclusion: SplitMeanFlow 是一个更通用、更高效的学习平均速度场的基础，它通过代数方法避免了计算 JVP，从而实现更简单的实现、更稳定的训练和更广泛的硬件兼容性。

Abstract: Generative models like Flow Matching have achieved state-of-the-art
performance but are often hindered by a computationally expensive iterative
sampling process. To address this, recent work has focused on few-step or
one-step generation by learning the average velocity field, which directly maps
noise to data. MeanFlow, a leading method in this area, learns this field by
enforcing a differential identity that connects the average and instantaneous
velocities. In this work, we argue that this differential formulation is a
limiting special case of a more fundamental principle. We return to the first
principles of average velocity and leverage the additivity property of definite
integrals. This leads us to derive a novel, purely algebraic identity we term
Interval Splitting Consistency. This identity establishes a self-referential
relationship for the average velocity field across different time intervals
without resorting to any differential operators. Based on this principle, we
introduce SplitMeanFlow, a new training framework that enforces this algebraic
consistency directly as a learning objective. We formally prove that the
differential identity at the core of MeanFlow is recovered by taking the limit
of our algebraic consistency as the interval split becomes infinitesimal. This
establishes SplitMeanFlow as a direct and more general foundation for learning
average velocity fields. From a practical standpoint, our algebraic approach is
significantly more efficient, as it eliminates the need for JVP computations,
resulting in simpler implementation, more stable training, and broader hardware
compatibility. One-step and two-step SplitMeanFlow models have been
successfully deployed in large-scale speech synthesis products (such as
Doubao), achieving speedups of 20x.

</details>


### [153] [P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices](https://arxiv.org/abs/2507.17228)
*Wei Fan,JinYi Yoon,Xiaochang Li,Huajie Shao,Bo Ji*

Main category: cs.LG

TL;DR: P3SL框架通过个性化分割和双层优化，解决了资源受限的异构边缘设备的隐私保护和模型定制问题。


<details>
  <summary>Details</summary>
Motivation: 现有的分裂学习（SL）框架在处理异构环境（设备在计算资源、通信能力、环境条件和隐私需求等方面存在差异）时，往往忽视了个性化的隐私需求和变化的地理环境下的本地模型定制化问题。这些框架虽然优化了具有不同资源限制的设备的分割点，但缺乏对个体差异的关注。

Method: 提出了一种名为P3SL的个性化隐私保护分裂学习框架，该框架采用个性化顺序分割学习流水线，允许客户端根据其计算资源、环境条件和隐私需求进行定制化隐私保护和本地模型维护。同时，利用双层优化技术使客户端能够在不共享敏感信息的情况下自行确定最佳的个性化分割点。

Result: 在包含4个Jetson Nano、2个Raspberry Pi和1个笔记本电脑的测试平台上，使用多样化的模型架构和数据集，并在不同的环境条件下进行了实现和评估，证明了P3SL的有效性。

Conclusion: P3SL框架通过个性化顺序分割学习流水线和双层优化技术，解决了异构环境中资源受限的边缘设备在隐私保护和模型定制方面的挑战，实现了隐私泄露风险和能源消耗的平衡，同时保持了高模型准确性。

Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning
technique that enables resource constrained edge devices to participate in
model training by partitioning a model into client-side and server-side
sub-models. While SL reduces computational overhead on edge devices, it
encounters significant challenges in heterogeneous environments where devices
vary in computing resources, communication capabilities, environmental
conditions, and privacy requirements. Although recent studies have explored
heterogeneous SL frameworks that optimize split points for devices with varying
resource constraints, they often neglect personalized privacy requirements and
local model customization under varying environmental conditions. To address
these limitations, we propose P3SL, a Personalized Privacy-Preserving Split
Learning framework designed for heterogeneous, resource-constrained edge device
systems. The key contributions of this work are twofold. First, we design a
personalized sequential split learning pipeline that allows each client to
achieve customized privacy protection and maintain personalized local models
tailored to their computational resources, environmental conditions, and
privacy needs. Second, we adopt a bi-level optimization technique that empowers
clients to determine their own optimal personalized split points without
sharing private sensitive information (i.e., computational resources,
environmental conditions, privacy requirements) with the server. This approach
balances energy consumption and privacy leakage risks while maintaining high
model accuracy. We implement and evaluate P3SL on a testbed consisting of 7
devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,
using diverse model architectures and datasets under varying environmental
conditions.

</details>


### [154] [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
*Steven K. Esser,Jeffrey L. McKinstry,Deepika Bablani,Rathinakumar Appuswamy,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 一种新的量化感知训练方法，可以提高大型语言模型的效率，同时保持准确性，并且易于实现。


<details>
  <summary>Details</summary>
Motivation: 为了在不损失准确性的情况下降低大型语言模型的推理延迟、模型大小和能耗，并确保兼容专门的推理加速器。

Method: 提出了一种简单的端到端量化感知训练方法，该方法仅在量化本身之外引入了额外的操作。

Result: 该方法在多个现代基准测试中，以不到0.1%的总模型训练预算的增加，显著优于现有的量化方法，并可推广到不同的模型架构、激活、缓存和权重。

Conclusion: 该方法是一种简单、端到端的量化感知训练方法，在不要求额外操作的情况下，以不到0.1%的总模型训练预算的增加，在多个现代基准测试中以显著优势优于现有的量化方法，并且可以应用于激活、缓存和权重，并推广到不同的模型架构。

Abstract: Large language models can be quantized to reduce inference time latency,
model size, and energy consumption, thereby delivering a better user experience
at lower cost. A challenge exists to deliver quantized models with minimal loss
of accuracy in reasonable time, and in particular to do so without requiring
mechanisms incompatible with specialized inference accelerators. Here, we
demonstrate a simple, end-to-end quantization-aware training approach that,
with an increase in total model training budget of less than 0.1%, outperforms
the leading published quantization methods by large margins on several modern
benchmarks, with both base and instruct model variants. The approach easily
generalizes across different model architectures, can be applied to
activations, cache, and weights, and requires the introduction of no additional
operations to the model other than the quantization itself.

</details>


### [155] [Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals](https://arxiv.org/abs/2507.16983)
*Sonny T. Jones,Grange M. Simpson,Patrick M. Pilarski,Ashley N. Dalrymple*

Main category: cs.LG

TL;DR: 通过使用分层强化学习和通用值函数，该研究提高了下肢外骨骼在不同地形上的行走能力和决策准确性。


<details>
  <summary>Details</summary>
Motivation: 康复技术是研究人与机器智能体共享学习和决策的自然场所。本研究旨在通过开发下肢外骨骼的自适应控制策略来增强运动功能障碍者的活动能力和自主性。

Method: 本文探索了使用分层强化学习（HRL）来开发下肢外骨骼的自适应控制策略，将复杂的控制任务分解为高级地形策略适应和低级预测信息提供两个框架，其中低级框架通过持续学习通用值函数（GVFs）来实现。GVFs 从多个可穿戴下肢传感器（包括肌电图、压力鞋垫和测角仪）生成时间抽象的未来信号值。文章研究了两种将实际和预测的传感器信号纳入策略网络的方法，以提高外骨骼控制系统在各种地形上的行走决策能力。

Result: 研究结果表明，加入由 GVFs 产生的预测信息可以提高策略网络的整体准确性。在平地、不平地、上下斜坡和转弯等地形上，外骨骼的性能均有所提升，这些地形在没有预测信息的情况下常常被错误分类。这表明预测信息可以在不确定性（例如，易被错误分类的地形）下辅助决策。

Conclusion: 该研究为 HRL 和外骨骼的未来发展提供了新的见解，以促进在不同行走环境中安全过渡和穿越。

Abstract: Rehabilitation technology is a natural setting to study the shared learning
and decision-making of human and machine agents. In this work, we explore the
use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control
strategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy
for individuals with motor impairments. Inspired by prominent models of
biological sensorimotor processing, our investigated HRL approach breaks down
the complex task of exoskeleton control adaptation into a higher-level
framework for terrain strategy adaptation and a lower-level framework for
providing predictive information; this latter element is implemented via the
continual learning of general value functions (GVFs). GVFs generated temporal
abstractions of future signal values from multiple wearable lower-limb sensors,
including electromyography, pressure insoles, and goniometers. We investigated
two methods for incorporating actual and predicted sensor signals into a policy
network with the intent to improve the decision-making capacity of the control
system of a lower-limb exoskeleton during ambulation across varied terrains. As
a key result, we found that the addition of predictions made from GVFs
increased overall network accuracy. Terrain-specific performance increases were
seen while walking on even ground, uneven ground, up and down ramps, and turns,
terrains that are often misclassified without predictive information. This
suggests that predictive information can aid decision-making during
uncertainty, e.g., on terrains that have a high chance of being misclassified.
This work, therefore, contributes new insights into the nuances of HRL and the
future development of exoskeletons to facilitate safe transitioning and
traversing across different walking environments.

</details>


### [156] [Enhancing Quantum Federated Learning with Fisher Information-Based Optimization](https://arxiv.org/abs/2507.17580)
*Amandeep Singh Bhatia,Sabre Kais*

Main category: cs.LG

TL;DR: 通过利用费舍尔信息来优化参数聚合，本研究提出了一种更有效的量子联邦学习算法，并在真实数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦学习（FL）在通信成本高、客户端数据异构、处理时间长和隐私泄露风险增加等方面的挑战，并结合量子计算在去中心化训练量子模型方面的潜力。

Method: 提出了一种利用费舍尔信息来识别对量子模型性能有显著影响的关键参数，并在聚合过程中对其进行保留的量子联邦学习（QFL）算法。

Result: 实验结果表明，所提出的QFL方法在ADNI和MNIST数据集上，相比于量子联邦平均方法，能够实现更好的性能和鲁棒性。

Conclusion: 本文提出的量子联邦学习（QFL）算法利用本地客户端模型上计算的费舍尔信息，并通过实验证明了其在提高模型性能和鲁棒性方面优于传统的量子联邦平均方法。

Abstract: Federated Learning (FL) has become increasingly popular across different
sectors, offering a way for clients to work together to train a global model
without sharing sensitive data. It involves multiple rounds of communication
between the global model and participating clients, which introduces several
challenges like high communication costs, heterogeneous client data, prolonged
processing times, and increased vulnerability to privacy threats. In recent
years, the convergence of federated learning and parameterized quantum circuits
has sparked significant research interest, with promising implications for
fields such as healthcare and finance. By enabling decentralized training of
quantum models, it allows clients or institutions to collaboratively enhance
model performance and outcomes while preserving data privacy. Recognizing that
Fisher information can quantify the amount of information that a quantum state
carries under parameter changes, thereby providing insight into its geometric
and statistical properties. We intend to leverage this property to address the
aforementioned challenges. In this work, we propose a Quantum Federated
Learning (QFL) algorithm that makes use of the Fisher information computed on
local client models, with data distributed across heterogeneous partitions.
This approach identifies the critical parameters that significantly influence
the quantum model's performance, ensuring they are preserved during the
aggregation process. Our research assessed the effectiveness and feasibility of
QFL by comparing its performance against other variants, and exploring the
benefits of incorporating Fisher information in QFL settings. Experimental
results on ADNI and MNIST datasets demonstrate the effectiveness of our
approach in achieving better performance and robustness against the quantum
federated averaging method.

</details>


### [157] [PyG 2.0: Scalable Learning on Real World Graphs](https://arxiv.org/abs/2507.16991)
*Matthias Fey,Jinu Sunil,Akihiro Nitta,Rishi Puri,Manan Shah,Blaž Stojanovič,Ramona Bendias,Alexandria Barghi,Vid Kocijan,Zecheng Zhang,Xinwei He,Jan Eric Lenssen,Jure Leskovec*

Main category: cs.LG

TL;DR: PyG 2.0 带来了重大更新，提高了可扩展性和真实应用能力，支持异构和时间图、可扩展的特征/图存储和优化，使处理大规模图学习问题更高效。


<details>
  <summary>Details</summary>
Motivation: PyG（PyTorch Geometric）作为图神经网络的领先框架，已发布 2.0 版本，旨在提高可扩展性和真实应用能力，以应对大规模图学习问题。

Method: 详细介绍 PyG 2.0 框架的增强架构，包括对异构和时间图的支持、可扩展的特征/图存储以及各种优化。

Result: PyG 2.0 提高了可扩展性和真实应用能力，支持异构和时间图、可扩展的特征/图存储以及各种优化。

Conclusion: PyG 2.0 及其后续次要版本带来了对异构和时间图的支持、可扩展的特征/图存储以及各种优化，使研究人员和实践者能够有效地处理大规模图学习问题。

Abstract: PyG (PyTorch Geometric) has evolved significantly since its initial release,
establishing itself as a leading framework for Graph Neural Networks. In this
paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive
update that introduces substantial improvements in scalability and real-world
application capabilities. We detail the framework's enhanced architecture,
including support for heterogeneous and temporal graphs, scalable feature/graph
stores, and various optimizations, enabling researchers and practitioners to
tackle large-scale graph learning problems efficiently. Over the recent years,
PyG has been supporting graph learning in a large variety of application areas,
which we will summarize, while providing a deep dive into the important areas
of relational deep learning and large language modeling.

</details>


### [158] [Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation](https://arxiv.org/abs/2507.17001)
*Yan Li,Guangyi Chen,Yunlong Deng,Zijian Li,Zeyu Tang,Anpeng Wu,Kun Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种新框架，通过利用偏见特征来提高模型在不同分布域的适应性，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖不变性表示学习来消除偏见特征的影响，但该研究旨在探讨偏见是否总应被消除，以及在什么情况下应保留偏见并如何利用它。

Method: 该研究首先进行了理论分析，探讨了在何种条件下可以识别并有效利用偏见特征。在此基础上，提出了一个包含两个关键组成部分的框架：1. 利用不变性作为指导，从偏见中提取预测成分；2. 利用识别出的偏见来估计环境条件，并据此探索适用的偏见感知预测器，以缓解环境差异。

Result: 实验结果一致表明，该方法在合成数据集和标准域泛化基准测试上的表现优于现有方法，证明了其鲁棒性和适应性。

Conclusion: 该研究提出了一个创新的框架，通过策略性地利用偏见来补充不变表示，以提高模型在不同分布（OOD）域的适应性，并在合成数据集和标准的域泛化基准测试中取得了优于现有方法的成果。

Abstract: Most existing methods for adapting models to out-of-distribution (OOD)
domains rely on invariant representation learning to eliminate the influence of
biased features. However, should bias always be eliminated -- and if not, when
should it be retained, and how can it be leveraged? To address these questions,
we first present a theoretical analysis that explores the conditions under
which biased features can be identified and effectively utilized. Building on
this theoretical foundation, we introduce a novel framework that strategically
leverages bias to complement invariant representations during inference. The
framework comprises two key components that leverage bias in both direct and
indirect ways: (1) using invariance as guidance to extract predictive
ingredients from bias, and (2) exploiting identified bias to estimate the
environmental condition and then use it to explore appropriate bias-aware
predictors to alleviate environment gaps. We validate our approach through
experiments on both synthetic datasets and standard domain generalization
benchmarks. Results consistently demonstrate that our method outperforms
existing approaches, underscoring its robustness and adaptability.

</details>


### [159] [laplax -- Laplace Approximations with JAX](https://arxiv.org/abs/2507.17013)
*Tobias Weber,Bálint Mucsányi,Lenard Rommel,Thomas Christie,Lars Kasüschke,Marvin Pförtner,Philipp Hennig*

Main category: cs.LG

TL;DR: laplax是一个新的开源Python包，使用Jax进行拉普拉斯近似，旨在促进贝叶斯深度学习和不确定性量化研究。


<details>
  <summary>Details</summary>
Motivation: 为了使贝叶斯工具（如预测不确定性和通过奥卡姆剃刀进行模型选择）能够应用于深度神经网络，并提供一个灵活、易于研究的框架。

Method: 通过介绍laplax这个开源Python包，使用Jax实现了拉普拉斯近似。

Result: laplax包提供了一个模块化、纯函数式且依赖性小的框架，支持快速原型设计和实验。

Conclusion: laplax包为Jax用户提供了一个可扩展且高效的贝叶斯深度学习框架，促进了不确定性量化和模型选择等研究。

Abstract: The Laplace approximation provides a scalable and efficient means of
quantifying weight-space uncertainty in deep neural networks, enabling the
application of Bayesian tools such as predictive uncertainty and model
selection via Occam's razor. In this work, we introduce laplax, a new
open-source Python package for performing Laplace approximations with jax.
Designed with a modular and purely functional architecture and minimal external
dependencies, laplax offers a flexible and researcher-friendly framework for
rapid prototyping and experimentation. Its goal is to facilitate research on
Bayesian neural networks, uncertainty quantification for deep learning, and the
development of improved Laplace approximation techniques.

</details>


### [160] [Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting](https://arxiv.org/abs/2507.17016)
*Omid Orang,Patricia O. Lucas,Gabriel I. F. Paiva,Petronio C. L. Silva,Felipe Augusto Rocha da Silva,Adriano Alonso Veloso,Frederico Gadelha Guimaraes*

Main category: cs.LG

TL;DR: 本研究提出了CGF-LLM，一个结合GPT-2、模糊时间序列和因果图的新型框架，用于预测多变量时间序列。该模型通过将时间序列转换为可解释的文本格式，实现了有效的预测，并在多个数据集上得到验证。


<details>
  <summary>Details</summary>
Motivation: 为了应对将大型语言模型（LLM）应用于时间序列预测（TSF）的挑战，本研究旨在提出一种新的框架，以提高预测的准确性和可解释性。

Method: 提出了一种名为CGF-LLM的新型LLM框架，该框架结合了GPT-2、模糊时间序列（FTS）和因果图，用于预测多变量时间序列。该方法通过并行应用模糊化和因果分析将数值时间序列转换为可解释的形式，为预训练的GPT-2模型提供语义理解和结构洞察力。

Result: CGF-LLM模型在四个不同的多变量时间序列数据集上取得了有效的预测结果。

Conclusion: 研究结果证实了所提出的基于LLM的时间序列预测模型的有效性，该模型在四个不同的多变量时间序列数据集上进行了验证。该研究为基于FTS的LLM在时间序列预测领域开辟了有前景的未来方向。

Abstract: In recent years, the application of Large Language Models (LLMs) to time
series forecasting (TSF) has garnered significant attention among researchers.
This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with
fuzzy time series (FTS) and causal graph to predict multivariate time series,
marking the first such architecture in the literature. The key objective is to
convert numerical time series into interpretable forms through the parallel
application of fuzzification and causal analysis, enabling both semantic
understanding and structural insight as input for the pretrained GPT-2 model.
The resulting textual representation offers a more interpretable view of the
complex dynamics underlying the original time series. The reported results
confirm the effectiveness of our proposed LLM-based time series forecasting
model, as demonstrated across four different multivariate time series datasets.
This initiative paves promising future directions in the domain of TSF using
LLMs based on FTS.

</details>


### [161] [BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation](https://arxiv.org/abs/2507.17019)
*Ray Zirui Zhang,Christopher E. Miles,Xiaohui Xie,John S. Lowengrub*

Main category: cs.LG

TL;DR: 提出了一种新的贝叶斯推理框架，用于解决受PDE约束的优化问题，并在不确定性量化方面取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 将双层局部算子学习（BiLO）方法扩展到贝叶斯推理框架，以解决受PDE约束的优化问题，并进行不确定性量化。

Method: 在较低层级，通过最小化局部算子损失来训练网络以逼近局部解算子。在较高层级，从后验分布中采样PDE参数。通过基于梯度的马尔可夫链蒙特卡洛（MCMC）方法和低秩自适应（LoRA）实现高效采样。

Result: 与基于贝叶斯神经网络的现有方法相比，该方法规避了在神经网络权重的高维空间中进行采样的挑战，并且不需要对神经网络解指定先验分布。通过强制执行强PDE约束，提高了参数推理和不确定性量化的准确性。分析了MCMC采样器中梯度的动态误差以及由于低层级问题不精确最小化导致后验分布中的静态误差，并证明了低层级问题求解容差与最终不确定性量化的准确性之间的直接联系。

Conclusion: 通过数值实验，证明了该方法在保持高计算效率的同时，能够准确地进行推理和量化不确定性。

Abstract: Uncertainty quantification and inverse problems governed by partial
differential equations (PDEs) are central to a wide range of scientific and
engineering applications. In this second part of a two part series, we extend
Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization
problems developed in Part 1 to the Bayesian inference framework. At the lower
level, we train a network to approximate the local solution operator by
minimizing the local operator loss with respect to the weights of the neural
network. At the upper level, we sample the PDE parameters from the posterior
distribution. We achieve efficient sampling through gradient-based Markov Chain
Monte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with
existing methods based on Bayesian neural networks, our approach bypasses the
challenge of sampling in the high-dimensional space of neural network weights
and does not require specifying a prior distribution on the neural network
solution. Instead, uncertainty propagates naturally from the data through the
PDE constraints. By enforcing strong PDE constraints, the proposed method
improves the accuracy of both parameter inference and uncertainty
quantification. We analyze the dynamic error of the gradient in the MCMC
sampler and the static error in the posterior distribution due to inexact
minimization of the lower level problem and demonstrate a direct link between
the tolerance for solving the lower level problem and the accuracy of the
resulting uncertainty quantification. Through numerical experiments across a
variety of PDE models, we demonstrate that our method delivers accurate
inference and quantification of uncertainties while maintaining high
computational efficiency.

</details>


### [162] [BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles](https://arxiv.org/abs/2507.17472)
*Junhua Liu,Roy Ka-Wei Lee,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 提出了一种名为BGM-HAN的新模型，该模型通过分层学习和注意力机制，提高了大学招生等高风险领域决策的准确性和可解释性，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在高风险领域中，专家决策易受难以检测的认知偏见影响，威胁公平性和长期结果的问题。

Method: 提出了一种名为BGM-HAN的增强型BPE、门控多头分层注意力网络，用于处理半结构化的申请者数据，并捕获多层次的表示以进行细致的评估。

Result: 实验结果表明，BGM-HAN在真实招生数据上显著优于传统的机器学习和大型语言模型等现有方法。

Conclusion: 该模型在大学招生等高风险领域具有应用潜力，可以提高决策的公平性和长期效益。

Abstract: Human decision-making in high-stakes domains often relies on expertise and
heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten
fairness and long-term outcomes. This work presents a novel approach to
enhancing complex decision-making workflows through the integration of
hierarchical learning alongside various enhancements. Focusing on university
admissions as a representative high-stakes domain, we propose BGM-HAN, an
enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,
designed to effectively model semi-structured applicant data. BGM-HAN captures
multi-level representations that are crucial for nuanced assessment, improving
both interpretability and predictive performance. Experimental results on real
admissions data demonstrate that our proposed model significantly outperforms
both state-of-the-art baselines from traditional machine learning to large
language models, offering a promising framework for augmenting decision-making
in domains where structure, context, and fairness matter. Source code is
available at: https://github.com/junhua/bgm-han.

</details>


### [163] [Pragmatic Policy Development via Interpretable Behavior Cloning](https://arxiv.org/abs/2507.17056)
*Anton Matsson,Yaochen Rao,Heather J. Litman,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本研究提出了一种从行为策略的可解释模型中提取治疗策略的替代方法，该方法利用树状模型解决了offline RL在安全关键领域的可解释性和评估问题，并在类风湿关节炎和败血症护理的实际案例中取得了优于当前实践的结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决offline RL在安全关键领域中可解释性和评估的挑战，本研究提出了一种简单的替代方法，即从行为策略的可解释模型中提取治疗策略。

Method: 本研究提出了一种基于树的模型，该模型从行为策略的可解释模型中提取治疗策略，其中考虑的动作数量可以调整，以控制与行为策略的重叠程度，从而实现可靠的策略评估。

Result: 本研究在类风湿关节炎和败血症护理的现实世界案例中，证明了该框架下的策略可以优于当前实践，并提供了可解释的替代方案。

Conclusion: 本研究提出了一种从行为策略的可解释模型中提取治疗策略的替代方法，该模型利用数据中存在的模式，并通过调整考虑的动作数量来控制与行为策略的重叠程度，从而实现可靠的策略评估。

Abstract: Offline reinforcement learning (RL) holds great promise for deriving optimal
policies from observational data, but challenges related to interpretability
and evaluation limit its practical use in safety-critical domains.
Interpretability is hindered by the black-box nature of unconstrained RL
policies, while evaluation -- typically performed off-policy -- is sensitive to
large deviations from the data-collecting behavior policy, especially when
using methods based on importance sampling. To address these challenges, we
propose a simple yet practical alternative: deriving treatment policies from
the most frequently chosen actions in each patient state, as estimated by an
interpretable model of the behavior policy. By using a tree-based model, which
is specifically designed to exploit patterns in the data, we obtain a natural
grouping of states with respect to treatment. The tree structure ensures
interpretability by design, while varying the number of actions considered
controls the degree of overlap with the behavior policy, enabling reliable
off-policy evaluation. This pragmatic approach to policy development
standardizes frequent treatment patterns, capturing the collective clinical
judgment embedded in the data. Using real-world examples in rheumatoid
arthritis and sepsis care, we demonstrate that policies derived under this
framework can outperform current practice, offering interpretable alternatives
to those obtained via offline RL.

</details>


### [164] [Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation](https://arxiv.org/abs/2507.17066)
*Jessup Byun,Xiaofeng Lin,Joshua Ward,Guang Cheng*

Main category: cs.LG

TL;DR: 在低数据场景下，GPT-4o-mini、LLaMA 3.3 70B和TabPFN等基础模型在生成合成表格数据时存在显著的隐私风险，因为它们可能重复训练数据中的记录。尽管如此，通过调整提示参数（如减小批量大小、降低温度和使用摘要统计数据）可以显著降低这种风险，同时保持高数据保真度。在隐私和效用之间取得最佳平衡的模型是CTGAN和GPT-4o-mini。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型在数据量不足的情况下存在过拟合、泄露敏感记录和需要频繁重新训练的问题。虽然基于Transformer的上下文学习（ICL）模型无需更新参数，但存在重复种子行并引入隐私风险的问题，尤其是在识别性强的表格数据中，这种风险的严重性尚不清楚。

Method: 本研究构建了一个基准测试，评估了三种基础模型（GPT-4o-mini、LLaMA 3.3 70B、TabPFN v2）和四种基线模型在35个真实世界表格数据集上的表现，涵盖健康、金融和政策领域。评估指标包括统计保真度、下游效用和成员推断泄露。

Result: 基础模型在隐私风险方面表现不佳，LLaMA 3.3 70B在1%的假阳性率下，其真阳性率比最安全的基线模型高出54个百分点。GPT-4o-mini和TabPFN也存在高度隐私脆弱性。在隐私-效用权衡方面，CTGAN和GPT-4o-mini表现更优。通过提示工程（小批量大小、低温度、使用汇总统计），可以在保持90%以上保真度的同时，将最坏情况下的AUC降低14个百分点，并将稀有类别泄露降低多达39个百分点。

Conclusion: 基础模型在低数据场景下生成合成表格数据时存在较高的隐私风险，尽管它们在数据增强方面表现出色。研究表明，通过调整提示（如减小批次大小、降低温度和使用汇总统计信息）可以有效降低隐私风险，同时保持较高的数据保真度。CTGAN和GPT-4o-mini在隐私和效用之间提供了更好的权衡。

Abstract: Synthetic tabular data is essential for machine learning workflows,
especially for expanding small or imbalanced datasets and enabling
privacy-preserving data sharing. However, state-of-the-art generative models
(GANs, VAEs, diffusion models) rely on large datasets with thousands of
examples. In low-data settings, often the primary motivation for synthetic
data, these models can overfit, leak sensitive records, and require frequent
retraining. Recent work uses large pre-trained transformers to generate rows
via in-context learning (ICL), which needs only a few seed examples and no
parameter updates, avoiding retraining. But ICL repeats seed rows verbatim,
introducing a new privacy risk that has only been studied in text. The severity
of this risk in tabular synthesis-where a single row may identify a
person-remains unclear. We address this gap with the first benchmark of three
foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four
baselines on 35 real-world tables from health, finance, and policy. We evaluate
statistical fidelity, downstream utility, and membership inference leakage.
Results show foundation models consistently have the highest privacy risk.
LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at
1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly
vulnerable. We plot the privacy-utility frontier and show that CTGAN and
GPT-4o-mini offer better tradeoffs. A factorial study finds that three
zero-cost prompt tweaks-small batch size, low temperature, and using summary
statistics-can reduce worst-case AUC by 14 points and rare-class leakage by up
to 39 points while maintaining over 90% fidelity. Our benchmark offers a
practical guide for safer low-data synthesis with foundation models.

</details>


### [165] [Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach](https://arxiv.org/abs/2507.17070)
*Adithya Mohan,Dominik Rößle,Daniel Cremers,Torsten Schön*

Main category: cs.LG

TL;DR: 深度强化学习在自动驾驶中面临对抗性攻击的鲁棒性挑战。本研究提出了一种集成防御架构，显著提高了DRL模型在对抗性攻击下的鲁棒性，优于单一防御方法。


<details>
  <summary>Details</summary>
Motivation: 探讨深度强化学习（DRL）模型在面对对抗性攻击时的鲁棒性问题，并解决了在自动驾驶场景中整合多种防御机制的研究空白。

Method: 提出了一种新颖的基于集成防御的架构，用于缓解自动驾驶中的对抗性攻击。

Result: 与基线相比，在FGSM攻击下，所提出的集成方法在高速公路和合并场景中，平均奖励从5.87提高到18.38（提高213%以上），平均碰撞率从0.50降低到0.09（降低82%），优于所有独立的防御策略。

Conclusion: 该研究提出了一种新颖的基于集成防御的架构，以减轻自动驾驶中的对抗性攻击，并证明了其有效性。

Abstract: Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated
its applicability across various domains, including robotics, healthcare,
energy optimization, and autonomous driving. However, a critical question
remains: How robust are DRL models when exposed to adversarial attacks? While
existing defense mechanisms such as adversarial training and distillation
enhance the resilience of DRL models, there remains a significant research gap
regarding the integration of multiple defenses in autonomous driving scenarios
specifically. This paper addresses this gap by proposing a novel ensemble-based
defense architecture to mitigate adversarial attacks in autonomous driving. Our
evaluation demonstrates that the proposed architecture significantly enhances
the robustness of DRL models. Compared to the baseline under FGSM attacks, our
ensemble method improves the mean reward from 5.87 to 18.38 (over 213%
increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82%
decrease) in the highway scenario and merge scenario, outperforming all
standalone defense strategies.

</details>


### [166] [Sensor Drift Compensation in Electronic-Nose-Based Gas Recognition Using Knowledge Distillation](https://arxiv.org/abs/2507.17071)
*Juntao Lin,Xianghao Zhan*

Main category: cs.LG

TL;DR: 本研究提出了一种新的知识蒸馏（KD）方法，用于解决电子鼻传感器漂移问题，实验结果表明KD方法在提高准确性和F1分数方面优于现有方法，显著提升了传感器在真实环境中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了解决电子鼻在实际应用中因环境变化和传感器老化导致的传感器漂移问题，并克服先前研究中统计验证不足和过度补偿传感器漂移的缺点，本研究旨在提高传感器漂移补偿的统计严谨性。

Method: 通过设计两种领域自适应任务（使用第一批数据预测剩余批次，以及使用所有先前批次预测下一个批次），并系统地测试了三种方法：知识蒸馏（KD）、领域正则化成分分析（DRCA）和混合方法KD-DRCA，在UCI数据集的30个随机测试集分区上进行了评估。

Result: KD方法在准确性和F1分数上始终优于DRCA和KD-DRCA，分别提高了18%和15%，证明了KD在漂移补偿方面的优越性。

Conclusion: 所提出的知识蒸馏（KD）方法在电子鼻传感器漂移补偿方面表现优于DRCA和KD-DRCA，在准确性和F1分数方面分别提高了18%和15%，显著提高了传感器漂移补偿在真实环境中的可靠性。

Abstract: Due to environmental changes and sensor aging, sensor drift challenges the
performance of electronic nose systems in gas classification during real-world
deployment. Previous studies using the UCI Gas Sensor Array Drift Dataset
reported promising drift compensation results but lacked robust statistical
experimental validation and may overcompensate for sensor drift, losing
class-related variance.To address these limitations and improve sensor drift
compensation with statistical rigor, we first designed two domain adaptation
tasks based on the same electronic nose dataset: using the first batch to
predict the remaining batches, simulating a controlled laboratory setting; and
predicting the next batch using all prior batches, simulating continuous
training data updates for online training. We then systematically tested three
methods: our proposed novel Knowledge Distillation (KD) method, the benchmark
method Domain Regularized Component Analysis (DRCA), and a hybrid method
KD-DRCA, across 30 random test set partitions on the UCI dataset. We showed
that KD consistently outperformed both DRCA and KD-DRCA, achieving up to an 18%
improvement in accuracy and 15% in F1-score, demonstrating KD's superior
effectiveness in drift compensation. This is the first application of KD for
electronic nose drift mitigation, significantly outperforming the previous
state-of-the-art DRCA method and enhancing the reliability of sensor drift
compensation in real-world environments.

</details>


### [167] [ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search](https://arxiv.org/abs/2507.17096)
*Olivia Dry,Timothy L. Molloy,Wanxin Jin,Iman Shames*

Main category: cs.LG

TL;DR: A new method called ZORMS-LfD can learn control problems from demonstrations without needing gradient information, working for both continuous and discrete time. It's faster and as good as or better than existing methods.


<details>
  <summary>Details</summary>
Motivation: To enable learning of costs, constraints, and dynamics for constrained optimal control problems (continuous and discrete time) from expert demonstrations without requiring a smooth learning-loss landscape, unlike existing first-order methods which require gradients and are often limited to discrete time.

Method: Zeroth-Order Random Matrix Search for Learning from Demonstrations (ZORMS-LfD)

Result: ZORMS-LfD matches or surpasses state-of-the-art methods in learning loss and compute time. It shows an 80% reduction in compute time compared to first-order methods on unconstrained continuous-time problems and outperforms Nelder-Mead on constrained continuous-time problems.

Conclusion: ZORMS-LfD matches or surpasses state-of-the-art methods in learning loss and compute time across various benchmark problems. It achieves similar loss performance to first-order methods on unconstrained continuous-time problems with an 80% reduction in compute time and outperforms Nelder-Mead on constrained continuous-time problems.

Abstract: We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations
(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of
constrained optimal control problems, in both continuous and discrete time, to
be learned from expert demonstrations without requiring smoothness of the
learning-loss landscape. In contrast, existing state-of-the-art first-order
methods require the existence and computation of gradients of the costs,
constraints, dynamics, and learning loss with respect to states, controls
and/or parameters. Most existing methods are also tailored to discrete time,
with constrained problems in continuous time receiving only cursory attention.
We demonstrate that ZORMS-LfD matches or surpasses the performance of
state-of-the-art methods in terms of both learning loss and compute time across
a variety of benchmark problems. On unconstrained continuous-time benchmark
problems, ZORMS-LfD achieves similar loss performance to state-of-the-art
first-order methods with an over $80$\% reduction in compute time. On
constrained continuous-time benchmark problems where there is no specialized
state-of-the-art method, ZORMS-LfD is shown to outperform the commonly used
gradient-free Nelder-Mead optimization method.

</details>


### [168] [Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models](https://arxiv.org/abs/2507.17107)
*Andrii Balashov*

Main category: cs.LG

TL;DR: RL微调只更新一小部分参数，而非全部，且这部分参数具有可转移性，仅更新这部分参数即可达到完全微调的效果。


<details>
  <summary>Details</summary>
Motivation: 挑战了RL微调需要更新大部分模型参数的普遍假设，发现RL微调只会修改模型参数的一个小子集。

Method: 通过实验和分析，研究了RL微调过程中参数更新的稀疏性现象，并验证了只更新这个稀疏子集可以达到与完全微调相当的效果。

Result: RL微调只会修改模型参数的一个小子集（通常为5%-30%的权重），并且这个小子集在不同算法、模型和实验条件下表现出高度的一致性，甚至具有一定的可转移性。仅微调这个稀疏子集即可恢复模型性能。

Conclusion: RL微调只修改模型参数的一个小子集，而不是全部参数。这个小子集在不同实验条件下表现出一致性，并且可以恢复模型的全部性能。

Abstract: Reinforcement learning (RL) is a key post-pretraining step for aligning large
language models (LLMs) with complex tasks and human preferences. While it is
often assumed that RL fine-tuning requires updating most of a model's
parameters, we challenge this assumption with a surprising finding: RL
fine-tuning consistently modifies only a small subnetwork (typically 5-30% of
weights), leaving most parameters unchanged. We call this phenomenon RL-induced
parameter update sparsity. It arises naturally, without any sparsity
constraints or parameter-efficient tuning, and appears across multiple RL
algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,
Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show
substantial overlap across different seeds, datasets, and algorithms-far
exceeding chance-suggesting a partially transferable structure in the
pretrained model. We show that fine-tuning only this sparse subnetwork recovers
full model performance and yields parameters nearly identical to the fully
fine-tuned model. Our analysis suggests this sparsity emerges because RL
operates near the model's original distribution, requiring only targeted
changes. KL penalties, gradient clipping, and on-policy dynamics have limited
effect on the sparsity pattern. These findings shed new light on how RL adapts
models: not by shifting all weights, but by focusing training on a small,
consistently updated subnetwork. This insight enables more efficient RL methods
and reframes sparsity through the lens of the lottery ticket hypothesis.

</details>


### [169] [Probabilistic Graphical Models: A Concise Tutorial](https://arxiv.org/abs/2507.17116)
*Jacqueline Maasch,Willie Neiswanger,Stefano Ermon,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: This tutorial introduces probabilistic graphical modeling, covering its theory, representation, learning, and inference algorithms.


<details>
  <summary>Details</summary>
Motivation: Probabilistic graphical modeling, a branch of machine learning, uses probability distributions to describe the world, make predictions, and support decision-making under uncertainty by bridging probability and graph theory.

Method: The tutorial reviews basic probability and graph theory, then delves into three themes: representation of multivariate distributions using graphs, algorithms for learning model parameters and structures from data, and algorithms for exact and approximate inference.

Result: The tutorial provides a concise introduction to the formalisms, methods, and applications of probabilistic graphical modeling.

Conclusion: The tutorial explores the formalisms, methods, and applications of probabilistic graphical modeling, covering representation, learning, and inference.

Abstract: Probabilistic graphical modeling is a branch of machine learning that uses
probability distributions to describe the world, make predictions, and support
decision-making under uncertainty. Underlying this modeling framework is an
elegant body of theory that bridges two mathematical traditions: probability
and graph theory. This framework provides compact yet expressive
representations of joint probability distributions, yielding powerful
generative models for probabilistic reasoning.
  This tutorial provides a concise introduction to the formalisms, methods, and
applications of this modeling framework. After a review of basic probability
and graph theory, we explore three dominant themes: (1) the representation of
multivariate distributions in the intuitive visual language of graphs, (2)
algorithms for learning model parameters and graphical structures from data,
and (3) algorithms for inference, both exact and approximate.

</details>


### [170] [Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems](https://arxiv.org/abs/2507.17123)
*Jacob M. Delgado-López,Ricardo A. Morell-Rodriguez,Sebastián O. Espinosa-Del Rosario,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 本文开发了一种基于NVIDIA Jetson Orin Nano和MobileNetV2的人工智能诊断工具，用于快速诊断猴痘。通过TensorRT优化，实现了高精度、低功耗和快速推理，并提供易于使用的Web界面，适用于资源匮乏地区。


<details>
  <summary>Details</summary>
Motivation: 为了有效遏制和治疗传染病（如猴痘），快速诊断至关重要，尤其是在资源受限的环境中。本研究旨在开发一种AI驱动的诊断工具，以解决这些地区的诊断挑战。

Method: 本文提出了一种由人工智能驱动的诊断工具，并将其部署在NVIDIA Jetson Orin Nano上，利用预训练的MobileNetV2模型进行二元分类。通过TensorRT框架优化模型，支持FP32、FP16和INT8格式，并进行了功耗分析。该系统还设置了Wi-Fi接入点和基于Web的界面，方便用户通过移动设备上传和分析图像。

Result: 该模型在猴痘皮肤病变数据集上进行了训练，取得了93.07%的F1分数。通过TensorRT优化后，模型大小减小，推理速度加快，功耗降低约一半，同时保持了原有的准确性。功耗分析证实，优化后的模型在推理过程中能耗显著降低。

Conclusion: 该诊断工具通过优化、可扩展且节能的解决方案，有效解决了资源匮乏地区的诊断挑战，有望在资源有限的医疗环境中得到更广泛的应用。

Abstract: The rapid diagnosis of infectious diseases, such as monkeypox, is crucial for
effective containment and treatment, particularly in resource-constrained
environments. This study presents an AI-driven diagnostic tool developed for
deployment on the NVIDIA Jetson Orin Nano, leveraging the pre-trained
MobileNetV2 architecture for binary classification. The model was trained on
the open-source Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score,
which reflects a well-balanced performance in precision and recall. To optimize
the model, the TensorRT framework was used to accelerate inference for FP32 and
to perform post-training quantization for FP16 and INT8 formats. TensorRT's
mixed-precision capabilities enabled these optimizations, which reduced the
model size, increased inference speed, and lowered power consumption by
approximately a factor of two, all while maintaining the original accuracy.
Power consumption analysis confirmed that the optimized models used
significantly less energy during inference, reinforcing their suitability for
deployment in resource-constrained environments. The system was deployed with a
Wi-Fi Access Point (AP) hotspot and a web-based interface, enabling users to
upload and analyze images directly through connected devices such as mobile
phones. This setup ensures simple access and seamless connectivity, making the
tool practical for real-world applications. These advancements position the
diagnostic tool as an efficient, scalable, and energy-conscious solution to
address diagnosis challenges in underserved regions, paving the way for broader
adoption in low-resource healthcare settings.

</details>


### [171] [Model Compression Engine for Wearable Devices Skin Cancer Diagnosis](https://arxiv.org/abs/2507.17125)
*Jacob M. Delgado-López,Andrea P. Seda-Hernandez,Juan D. Guadalupe-Rosado,Luis E. Fernandez Ramirez,Miguel Giboyeaux-Camilo,Wilfredo E. Lugo-Beauchamp*

Main category: cs.LG

TL;DR: 本研究成功开发并优化了一种AI皮肤癌诊断工具，能在资源受限的设备上高效运行，为医疗资源匮乏地区带来了福音。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是最普遍且可预防的癌症之一，但早期检测仍然是一个挑战，特别是在医疗资源有限且难以获得专业医疗服务的地区。本研究旨在通过开发一种适用于嵌入式系统的AI驱动诊断工具来解决这一问题。

Method: 本研究提出了一种人工智能驱动的诊断工具，并针对嵌入式系统进行了优化。研究采用了MobileNetV2架构和迁移学习技术，将模型改编为用于区分“皮肤癌”和“其他”的二元分类器。为了在NVIDIA Jetson Orin Nano上实现模型压缩和优化，研究使用了TensorRT框架，以平衡性能和能耗。通过模型大小、推理速度、吞吐量和功耗等多个基准进行了全面评估。

Result: 优化后的模型在INT8精度下，模型尺寸减小高达0.41倍，推理速度和吞吐量得到提升，能耗降低高达0.93倍，同时保持了87.18%的F1分数、93.18%的精确率和81.91%的召回率。

Conclusion: 该研究验证了在资源受限的边缘设备上部署高性能、高能效的诊断工具的可行性，并强调了优化人工智能系统在革新医疗诊断方面的潜力，从而缩小了先进技术与服务欠缺地区之间的差距。

Abstract: Skin cancer is one of the most prevalent and preventable types of cancer, yet
its early detection remains a challenge, particularly in resource-limited
settings where access to specialized healthcare is scarce. This study proposes
an AI-driven diagnostic tool optimized for embedded systems to address this
gap. Using transfer learning with the MobileNetV2 architecture, the model was
adapted for binary classification of skin lesions into "Skin Cancer" and
"Other." The TensorRT framework was employed to compress and optimize the model
for deployment on the NVIDIA Jetson Orin Nano, balancing performance with
energy efficiency. Comprehensive evaluations were conducted across multiple
benchmarks, including model size, inference speed, throughput, and power
consumption. The optimized models maintained their performance, achieving an
F1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.
Post-compression results showed reductions in model size of up to 0.41, along
with improvements in inference speed and throughput, and a decrease in energy
consumption of up to 0.93 in INT8 precision. These findings validate the
feasibility of deploying high-performing, energy-efficient diagnostic tools on
resource-constrained edge devices. Beyond skin cancer detection, the
methodologies applied in this research have broader applications in other
medical diagnostics and domains requiring accessible, efficient AI solutions.
This study underscores the potential of optimized AI systems to revolutionize
healthcare diagnostics, thereby bridging the divide between advanced technology
and underserved regions.

</details>


### [172] [Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance](https://arxiv.org/abs/2507.17131)
*Yufei He,Ruoyu Li,Alex Chen,Yue Liu,Yulin Chen,Yuan Sui,Cheng Chen,Yi Zhu,Luca Luo,Frank Yang,Bryan Hooi*

Main category: cs.LG

TL;DR: ARIA is a new LLM agent framework that continuously learns updated domain knowledge at test time by identifying its own uncertainties and requesting human guidance to update its knowledge repository. It has been successfully deployed in TikTok Pay.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents struggle in environments with frequently changing rules and domain knowledge, such as regulatory compliance and user risk screening. Offline fine-tuning and standard prompting are insufficient for adapting to new knowledge during operation.

Method: ARIA is an LLM agent framework that continuously learns updated domain knowledge at test time. It assesses its own uncertainty through structured self-dialogue, identifies knowledge gaps, requests targeted explanations or corrections from human experts, and systematically updates an internal, timestamped knowledge repository. It also detects and resolves conflicting or outdated knowledge through comparisons and clarification queries.

Result: ARIA shows significant improvements in adaptability and accuracy compared to baselines using standard offline fine-tuning and existing self-improving agents when evaluated on a realistic customer due diligence name screening task on TikTok Pay and publicly available dynamic knowledge tasks.

Conclusion: ARIA has been successfully deployed in TikTok Pay, serving over 150 million monthly active users, demonstrating its practicality and effectiveness for operational use in rapidly evolving environments. It significantly improves adaptability and accuracy compared to baselines.

Abstract: Large language model (LLM) agents often struggle in environments where rules
and required domain knowledge frequently change, such as regulatory compliance
and user risk screening. Current approaches, like offline fine-tuning and
standard prompting, are insufficient because they cannot effectively adapt to
new knowledge during actual operation. To address this limitation, we propose
the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework
designed specifically to continuously learn updated domain knowledge at test
time. ARIA assesses its own uncertainty through structured self-dialogue,
proactively identifying knowledge gaps and requesting targeted explanations or
corrections from human experts. It then systematically updates an internal,
timestamped knowledge repository with provided human guidance, detecting and
resolving conflicting or outdated knowledge through comparisons and
clarification queries. We evaluate ARIA on the realistic customer due diligence
name screening task on TikTok Pay, alongside publicly available dynamic
knowledge tasks. Results demonstrate significant improvements in adaptability
and accuracy compared to baselines using standard offline fine-tuning and
existing self-improving agents. ARIA is deployed within TikTok Pay serving over
150 million monthly active users, confirming its practicality and effectiveness
for operational use in rapidly evolving environments.

</details>


### [173] [SADA: Stability-guided Adaptive Diffusion Acceleration](https://arxiv.org/abs/2507.17135)
*Ting Jiang,Yixiao Wang,Hancheng Ye,Zishan Shao,Jingwei Sun,Jingyang Zhang,Zekai Chen,Jianyi Zhang,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: SADA是一种新的加速扩散模型采样的方法，通过统一的稳定性标准来适应性地调整稀疏度，显著提高了速度，同时保持了高保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的免训练加速策略虽然有效，但与原始基线相比保真度较低。作者假设保真度差距源于（a）不同的提示对应不同的去噪轨迹，以及（b）这些方法未考虑潜在的ODE公式及其数值解。

Method: SADA（Stability-guided Adaptive Diffusion Acceleration）是一种新颖的范式，通过单一稳定性标准统一步进和token级稀疏决策，以加速基于ODE的生成模型（扩散和流匹配）的采样。SADA根据采样轨迹自适应地分配稀疏度，并引入了利用数值ODE求解器精确梯度信息的原则性近似方案。

Result: 在SD-2、SDXL和Flux上，使用EDM和DPM++求解器进行了全面评估，与未修改的基线相比，SADA实现了$
≥1.8	imes$的持续加速，保真度下降极小（LPIPS ≤ 0.10且FID ≤ 4.5），显著优于现有方法。此外，SADA可以无缝适应其他流程和模态：它在不进行任何修改的情况下加速了ControlNet，并将MusicLDM的速度提高了$1.8	imes$，同时保持约0.01的频谱LPIPS。

Conclusion: SADA通过单一稳定性标准统一了步进和token级稀疏决策，以加速基于ODE的生成模型（扩散和流匹配）的采样。它根据采样轨迹自适应地分配稀疏度，并引入了利用数值ODE求解器精确梯度信息的原则性近似方案。

Abstract: Diffusion models have achieved remarkable success in generative tasks but
suffer from high computational costs due to their iterative sampling process
and quadratic attention costs. Existing training-free acceleration strategies
that reduce per-step computation cost, while effectively reducing sampling
time, demonstrate low faithfulness compared to the original baseline. We
hypothesize that this fidelity gap arises because (a) different prompts
correspond to varying denoising trajectory, and (b) such methods do not
consider the underlying ODE formulation and its numerical solution. In this
paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a
novel paradigm that unifies step-wise and token-wise sparsity decisions via a
single stability criterion to accelerate sampling of ODE-based generative
models (Diffusion and Flow-matching). For (a), SADA adaptively allocates
sparsity based on the sampling trajectory. For (b), SADA introduces principled
approximation schemes that leverage the precise gradient information from the
numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using
both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with
minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to
unmodified baselines, significantly outperforming prior methods. Moreover, SADA
adapts seamlessly to other pipelines and modalities: It accelerates ControlNet
without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim
0.01$ spectrogram LPIPS.

</details>


### [174] [PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training](https://arxiv.org/abs/2507.17151)
*Anirudh Satheesh,Anant Khandelwal,Mucong Ding,Radu Balan*

Main category: cs.LG

TL;DR: PICore是一个无监督框架，通过物理信息损失选择训练数据，减少了对昂贵模拟数据的需求，提高了神经算子训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的神经算子在训练时需要大量的标注数据，而这些数据通常通过昂贵的模拟获得，这限制了其应用。PICore旨在同时解决数据量需求和标注成本问题。

Method: PICore是一个无监督的coreset选择框架，通过物理信息损失来选择最有信息量的训练样本，然后对选定的样本进行模拟以生成标签，最后在减少的标注数据集上训练神经算子。

Result: PICore在四个不同的PDE基准测试中，平均将训练效率提高了78%，同时对精度的影响很小，并且相比于监督coreset选择方法，显著减少了标注成本和训练时间。aterina@了。com

Conclusion: PICore框架通过利用物理信息损失来选择最有信息量的训练样本，可以显著降低训练时间和标注成本，同时保持高精度。

Abstract: Neural operators offer a powerful paradigm for solving partial differential
equations (PDEs) that cannot be solved analytically by learning mappings
between function spaces. However, there are two main bottlenecks in training
neural operators: they require a significant amount of training data to learn
these mappings, and this data needs to be labeled, which can only be accessed
via expensive simulations with numerical solvers. To alleviate both of these
issues simultaneously, we propose PICore, an unsupervised coreset selection
framework that identifies the most informative training samples without
requiring access to ground-truth PDE solutions. PICore leverages a
physics-informed loss to select unlabeled inputs by their potential
contribution to operator learning. After selecting a compact subset of inputs,
only those samples are simulated using numerical solvers to generate labels,
reducing annotation costs. We then train the neural operator on the reduced
labeled dataset, significantly decreasing training time as well. Across four
diverse PDE benchmarks and multiple coreset selection strategies, PICore
achieves up to 78% average increase in training efficiency relative to
supervised coreset selection methods with minimal changes in accuracy. We
provide code at https://github.com/Asatheesh6561/PICore.

</details>


### [175] [Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection](https://arxiv.org/abs/2507.17161)
*Vinura Galwaduge,Jagath Samarabandu*

Main category: cs.LG

TL;DR: 提出了一种新的基于扩散的反事实解释方法，用于网络入侵检测，生成的解释更小、更多样且更易于操作，同时比现有方法更有效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度学习在网络入侵检测中的“黑盒”问题，并提供可操作的、易于转化为应对措施的解释，以增强检测决策的理解、信任和防御能力。

Method: 提出了一种基于扩散的反事实解释框架，并与现有的反事实解释算法进行了比较分析，在三个现代网络入侵数据集上进行了评估。

Result: 提出的方法在效率、解释的最小化和多样性方面优于其他反事实解释算法，并能生成可操作的全局规则，有效过滤攻击查询。

Conclusion: 该研究提出了一个新颖的基于扩散的反事实解释框架，用于网络入侵检测，该框架能够提供可操作的解释，并比现有算法更有效地生成最小化、多样化的反事实解释。此外，该研究还展示了如何将这些反事实解释转化为全局规则，以实现对入侵攻击的有效过滤和防御。

Abstract: Modern network intrusion detection systems (NIDS) frequently utilize the
predictive power of complex deep learning models. However, the "black-box"
nature of such deep learning methods adds a layer of opaqueness that hinders
the proper understanding of detection decisions, trust in the decisions and
prevent timely countermeasures against such attacks. Explainable AI (XAI)
methods provide a solution to this problem by providing insights into the
causes of the predictions. The majority of the existing XAI methods provide
explanations which are not convenient to convert into actionable
countermeasures. In this work, we propose a novel diffusion-based
counterfactual explanation framework that can provide actionable explanations
for network intrusion attacks. We evaluated our proposed algorithm against
several other publicly available counterfactual explanation algorithms on 3
modern network intrusion datasets. To the best of our knowledge, this work also
presents the first comparative analysis of existing counterfactual explanation
algorithms within the context of network intrusion detection systems. Our
proposed method provide minimal, diverse counterfactual explanations out of the
tested counterfactual explanation algorithms in a more efficient manner by
reducing the time to generate explanations. We also demonstrate how
counterfactual explanations can provide actionable explanations by summarizing
them to create a set of global rules. These rules are actionable not only at
instance level but also at the global level for intrusion attacks. These global
counterfactual rules show the ability to effectively filter out incoming attack
queries which is crucial for efficient intrusion detection and defense
mechanisms.

</details>


### [176] [Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems](https://arxiv.org/abs/2507.17189)
*Shaohan Li,Hao Yang,Min Chen,Xiaolin Qin*

Main category: cs.LG

TL;DR: 提出了一种新的两阶段隐式训练方法，通过为每个变量使用单独的编码器和解码器，并在潜在空间中使用自注意力机制，解决了天气预测中多变量表示不一致和依赖关系捕捉的挑战，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 由于全球气候变化导致极端天气事件的频率增加，因此需要准确的天气预测。现有的端到端方法在多变量集成中存在表示不一致的问题，并且难以有效捕捉复杂天气系统中变量之间的依赖关系。而从多模态模型中采用两阶段训练方法虽然可以部分缓解此问题，但由于两个阶段之间训练任务的不一致性，结果常常不是最优的。

Method: 提出了一种隐式两阶段训练方法，为每个变量配置单独的编码器和解码器。在第一阶段，冻结翻译器，编码器和解码器学习共享的潜在空间；在第二阶段，冻结编码器和解码器，翻译器捕捉交互以进行预测。此外，通过在潜在空间中引入自注意力机制来进行多变量融合，以进一步提高性能。

Result: 与现有方法相比，该模型在近地表气温和相对湿度预测方面，分别将均方误差降低了 28.82% 和 23.39%，达到了最先进的性能。

Conclusion: 该方法在近地表气温和相对湿度预测方面，将均方误差分别降低了 28.82% 和 23.39%，达到了最先进的性能。

Abstract: The increasing frequency of extreme weather events due to global climate
change urges accurate weather prediction. Recently, great advances have been
made by the \textbf{end-to-end methods}, thanks to deep learning techniques,
but they face limitations of \textit{representation inconsistency} in
multivariable integration and struggle to effectively capture the dependency
between variables, which is required in complex weather systems. Treating
different variables as distinct modalities and applying a \textbf{two-stage
training approach} from multimodal models can partially alleviate this issue,
but due to the inconformity in training tasks between the two stages, the
results are often suboptimal. To address these challenges, we propose an
implicit two-stage training method, configuring separate encoders and decoders
for each variable. In detailed, in the first stage, the Translator is frozen
while the Encoders and Decoders learn a shared latent space, in the second
stage, the Encoders and Decoders are frozen, and the Translator captures
inter-variable interactions for prediction. Besides, by introducing a
self-attention mechanism for multivariable fusion in the latent space, the
performance achieves further improvements. Empirically, extensive experiments
show the state-of-the-art performance of our method. Specifically, it reduces
the MSE for near-surface air temperature and relative humidity predictions by
28.82\% and 23.39\%, respectively. The source code is available at
https://github.com/ShremG/Met2Net.

</details>


### [177] [Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation](https://arxiv.org/abs/2507.17204)
*Zixuan Wang,Jinghao Shi,Hanzhong Liang,Xiang Shen,Vera Wen,Zhiqian Chen,Yifan Wu,Zhixin Zhang,Hongyu Xiong*

Main category: cs.LG

TL;DR: 该研究提出了一种高效的多模态大语言模型（MLLM）内容审核方法，通过少量数据微调和级联部署，在提高审核准确率的同时大幅降低了计算成本，实现了工业规模的应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统视频分类模型在处理隐晦有害内容和情境模糊等复杂审核场景时的局限性，并克服MLLM在工业应用中面临的高计算成本和从生成到判别任务适配的挑战。

Method: 1.提出一种高效方法，利用最少量的判别性训练数据将生成式MLLM转化为多模态分类器。
2.提出路由-排序级联系统，将MLLM与轻量级路由器模型集成，以实现工业规模的部署。

Result: 离线实验表明，所提出的基于MLLM的方法相比传统分类器F1分数提高了66.50%，仅需2%的微调数据。
在线评估显示，该系统将自动内容审核量提高了41%，同时级联部署将计算成本降低到直接全规模部署的1.5%。

Conclusion: 该研究提出了一种将生成式多模态大语言模型（MLLM）转化为多模态分类器的有效方法，并通过路由-排序级联系统实现了大规模部署，显著提高了内容审核的效率和准确性。

Abstract: Effective content moderation is essential for video platforms to safeguard
user experience and uphold community standards. While traditional video
classification models effectively handle well-defined moderation tasks, they
struggle with complicated scenarios such as implicit harmful content and
contextual ambiguity. Multimodal large language models (MLLMs) offer a
promising solution to these limitations with their superior cross-modal
reasoning and contextual understanding. However, two key challenges hinder
their industrial adoption. First, the high computational cost of MLLMs makes
full-scale deployment impractical. Second, adapting generative models for
discriminative classification remains an open research problem. In this paper,
we first introduce an efficient method to transform a generative MLLM into a
multimodal classifier using minimal discriminative training data. To enable
industry-scale deployment, we then propose a router-ranking cascade system that
integrates MLLMs with a lightweight router model. Offline experiments
demonstrate that our MLLM-based approach improves F1 score by 66.50% over
traditional classifiers while requiring only 2% of the fine-tuning data. Online
evaluations show that our system increases automatic content moderation volume
by 41%, while the cascading deployment reduces computational cost to only 1.5%
of direct full-scale deployment.

</details>


### [178] [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/abs/2507.17221)
*Youneng Bao,Yiping Liu,Zhuo Chen,Yongsheng Liang,Mu Li,Kede Ma*

Main category: cs.LG

TL;DR: 提出了一种联合率-效优化方法，通过优化潜在代码和使用bpc度量，实现了比现有方法高170倍的数据集压缩率，同时保持了相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法要么在固定的存储预算下最大化性能，要么在不联合优化这两个目标的情况下追求用于冗余去除的合适合成数据表示。现代机器学习需要越来越大的数据集和模型，这导致了高昂的计算和存储要求。数据集蒸馏通过压缩原始数据集为一小组合成样本来缓解这一问题，同时保留其全部效用。

Method: 提出了一种联合率-效优化方法，将合成样本参数化为可优化的潜在代码，并通过极轻量级的网络解码。通过量化潜在变量的香农熵作为率度量，并使用任何现有的蒸馏损失作为效度量，通过拉格朗日乘子进行权衡。引入了精确的存储度量bpc（bits per class），该度量考虑了样本、标签和解码器参数的成本。

Result: 在CIFAR-10、CIFAR-100和ImageNet-128数据集上，与标准蒸馏相比，实现了高达170倍的压缩率，同时保持了相当的准确性。该方法在不同的bpc预算、蒸馏损失和骨干网络架构下，都能持续建立更好的率-效权衡。

Conclusion: 该方法在CIFAR-10、CIFAR-100和ImageNet-128等数据集上实现了高达170倍的压缩率，同时保持了相当的准确性。在不同的bpc预算、蒸馏损失和骨干网络架构下，该方法都能持续建立更好的率-效权衡。

Abstract: Driven by the ``scale-is-everything'' paradigm, modern machine learning
increasingly demands ever-larger datasets and models, yielding prohibitive
computational and storage requirements. Dataset distillation mitigates this by
compressing an original dataset into a small set of synthetic samples, while
preserving its full utility. Yet, existing methods either maximize performance
under fixed storage budgets or pursue suitable synthetic data representations
for redundancy removal, without jointly optimizing both objectives. In this
work, we propose a joint rate-utility optimization method for dataset
distillation. We parameterize synthetic samples as optimizable latent codes
decoded by extremely lightweight networks. We estimate the Shannon entropy of
quantized latents as the rate measure and plug any existing distillation loss
as the utility measure, trading them off via a Lagrange multiplier. To enable
fair, cross-method comparisons, we introduce bits per class (bpc), a precise
storage metric that accounts for sample, label, and decoder parameter costs. On
CIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\times$
greater compression than standard distillation at comparable accuracy. Across
diverse bpc budgets, distillation losses, and backbone architectures, our
approach consistently establishes better rate-utility trade-offs.

</details>


### [179] [DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs](https://arxiv.org/abs/2507.17245)
*Haolin Jin,Mengbai Xiao,Yuan Yuan,Xiao Zhang,Dongxiao Yu,Guanghui Zhang,Haoliang Wang*

Main category: cs.LG

TL;DR: DistrAttention通过分组和局部敏感哈希优化了Transformer的自注意力机制，提高了效率和灵活性，并在多种任务中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在自然语言处理、计算机视觉和时间序列预测等领域取得了巨大成功，但其核心组件自注意力具有二次时间复杂度，限制了其可扩展性。现有的优化方法要么丢弃了全部上下文信息，要么缺乏灵活性。因此，需要一种能够保持全部上下文信息且高效灵活的自注意力机制。

Method: DistrAttention通过将数据按嵌入维度（d）进行分组来实现高效和灵活的自注意力。它采用轻量级的采样和融合方法，利用局部敏感哈希（LSH）对相似数据进行分组，并通过分块分组框架来限制LSH引入的错误。通过优化块大小，DistrAttention可以与FlashAttention-2集成，在现代GPU上实现高性能。

Result: DistrAttention在计算自注意力时比FlashAttention-2快37%。在ViT推理中，DistrAttention是所有近似自注意力机制中速度最快且准确率最高的。在Llama3-1B上，DistrAttention实现了最低的推理时间，准确率仅损失1%。

Conclusion: DistrAttention是一种高效且灵活的自注意力机制，它通过在嵌入维度上进行数据分组，并结合局部敏感哈希和分块分组框架，实现了在保持全部上下文信息的同时提高了效率。该方法可以与FlashAttention-2集成，并在现代GPU上获得高性能。实验结果表明，DistrAttention在计算自注意力时比FlashAttention-2快37%，在ViT推理中是最快和最准确的近似自注意力机制，并且在Llama3-1B上实现了最低的推理时间，仅有1%的准确率损失。

Abstract: The Transformer architecture has revolutionized deep learning, delivering the
state-of-the-art performance in areas such as natural language processing,
computer vision, and time series prediction. However, its core component,
self-attention, has the quadratic time complexity relative to input sequence
length, which hinders the scalability of Transformers. The exsiting approaches
on optimizing self-attention either discard full-contextual information or lack
of flexibility. In this work, we design DistrAttention, an effcient and
flexible self-attention mechanism with the full context. DistrAttention
achieves this by grouping data on the embedding dimensionality, usually
referred to as $d$. We realize DistrAttention with a lightweight sampling and
fusion method that exploits locality-sensitive hashing to group similar data. A
block-wise grouping framework is further designed to limit the errors
introduced by locality sensitive hashing. By optimizing the selection of block
sizes, DistrAttention could be easily integrated with FlashAttention-2, gaining
high-performance on modern GPUs. We evaluate DistrAttention with extensive
experiments. The results show that our method is 37% faster than
FlashAttention-2 on calculating self-attention. In ViT inference,
DistrAttention is the fastest and the most accurate among approximate
self-attention mechanisms. In Llama3-1B, DistrAttention still achieves the
lowest inference time with only 1% accuray loss.

</details>


### [180] [Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions](https://arxiv.org/abs/2507.17255)
*Songxuan Shi*

Main category: cs.LG

TL;DR: 本篇论文通过引入新的训练方法，探索了AE、VAE和VQ-VAE之间的联系，强调了编码空间特性对生成模型的重要性，并指出了现有模型在处理多向量输出时可能遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索自编码器（AE）的生成能力，并试图通过重新构建的训练框架建立变分自编码器（VAE）和向量量化-变分自编码器（VQ-VAE）之间的联系。

Method: 提出了一种新的类似VAE的训练方法，通过引入聚类中心来增强数据紧凑性并确保明确的潜在空间，无需依赖传统的KL散度或重参数化技巧。将此方法扩展到多个可学习向量，观察到在连续空间中自然地向VQ-VAE模型发展。

Result: 实验结果表明，在MNIST、CelebA和FashionMNIST数据集上，AE通过潜在空间插值和扰动表现出生成潜力，但受限于编码空间中未定义区域的影响。新方法实现了平滑的插值过渡，但仍然存在模糊性。当编码器输出多个向量时，模型退化为离散自编码器（VQ-AE），仅组合图像片段而未学习语义表示。

Conclusion: 本研究揭示了编码空间紧凑性和离散性在生成模型中的关键作用，并深入探讨了VAE和VQ-VAE之间的内在联系，为它们的设计和局限性提供了新的视角。

Abstract: This paper explores the generative capabilities of Autoencoders (AEs) and
establishes connections between Variational Autoencoders (VAEs) and Vector
Quantized-Variational Autoencoders (VQ-VAEs) through a reformulated training
framework. We demonstrate that AEs exhibit generative potential via latent
space interpolation and perturbation, albeit limited by undefined regions in
the encoding space. To address this, we propose a new VAE-like training method
that introduces clustering centers to enhance data compactness and ensure
well-defined latent spaces without relying on traditional KL divergence or
reparameterization techniques. Experimental results on MNIST, CelebA, and
FashionMNIST datasets show smooth interpolative transitions, though blurriness
persists. Extending this approach to multiple learnable vectors, we observe a
natural progression toward a VQ-VAE-like model in continuous space. However,
when the encoder outputs multiple vectors, the model degenerates into a
discrete Autoencoder (VQ-AE), which combines image fragments without learning
semantic representations. Our findings highlight the critical role of encoding
space compactness and dispersion in generative modeling and provide insights
into the intrinsic connections between VAEs and VQ-VAEs, offering a new
perspective on their design and limitations.

</details>


### [181] [Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance](https://arxiv.org/abs/2507.17273)
*Rishi Parekh,Saisubramaniam Gopalakrishnan,Zishan Ahmad,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 本研究提出了一种结合知识图谱（KG）和大型语言模型（LLM）代理的框架，用于分析仓库运营的离散事件模拟（DES）数据，以识别瓶颈和低效。该方法通过将数据转化为知识图谱，并利用LLM代理的迭代推理和自我纠正能力，实现了比传统方法更优越的诊断能力和更快的洞察时间。


<details>
  <summary>Details</summary>
Motivation: 分析离散事件模拟（DES）中仓库运营的大型、复杂输出数据，以识别瓶颈和低效是一个关键但充满挑战的任务，通常需要大量的人工努力或专门的分析工具。因此，需要一种更直观、更高效的方法来减少洞察时间，并实现仓库低效的自动化评估和诊断。

Method: 本研究提出的框架整合了知识图谱（KG）和基于大语言模型（LLM）的代理。首先，将离散事件模拟（DES）的原始数据转化为一个语义丰富的知识图谱，其中捕获了模拟事件和实体之间的关系。然后，利用一个LLM代理，通过迭代推理生成相互关联的子问题，并为每个子问题创建Cypher查询以进行知识图谱交互。代理会提取信息并进行自我反思以纠正错误，从而形成一个自适应、迭代和自我纠正的分析过程。

Result: 该方法在识别仓库瓶颈方面，通过使用设备故障和流程不规则性进行测试，其表现优于基线方法。对于操作性问题，它在查明低效方面达到了近乎完美的通过率。对于复杂的调查性问题，该研究证明了该框架具有优越的诊断能力，能够揭示微妙的、相互关联的问题。

Conclusion: 该框架通过整合知识图谱（KG）和基于大语言模型（LLM）的代理，能够分析复杂的离散事件模拟（DES）输出数据，以识别仓库运营中的瓶颈和低效问题。它将原始DES数据转化为丰富的语义知识图谱，并通过LLM代理的迭代推理、子问题生成、Cypher查询创建、信息提取和自我反思，实现了类似人类的分析过程，能够纠正错误并诊断微妙的、相互关联的问题。

Abstract: Analyzing large, complex output datasets from Discrete Event Simulations
(DES) of warehouse operations to identify bottlenecks and inefficiencies is a
critical yet challenging task, often demanding significant manual effort or
specialized analytical tools. Our framework integrates Knowledge Graphs (KGs)
and Large Language Model (LLM)-based agents to analyze complex Discrete Event
Simulation (DES) output data from warehouse operations. It transforms raw DES
data into a semantically rich KG, capturing relationships between simulation
events and entities. An LLM-based agent uses iterative reasoning, generating
interdependent sub-questions. For each sub-question, it creates Cypher queries
for KG interaction, extracts information, and self-reflects to correct errors.
This adaptive, iterative, and self-correcting process identifies operational
issues mimicking human analysis. Our DES approach for warehouse bottleneck
identification, tested with equipment breakdowns and process irregularities,
outperforms baseline methods. For operational questions, it achieves
near-perfect pass rates in pinpointing inefficiencies. For complex
investigative questions, we demonstrate its superior diagnostic ability to
uncover subtle, interconnected issues. This work bridges simulation modeling
and AI (KG+LLM), offering a more intuitive method for actionable insights,
reducing time-to-insight, and enabling automated warehouse inefficiency
evaluation and diagnosis.

</details>


### [182] [Decentralized Federated Learning of Probabilistic Generative Classifiers](https://arxiv.org/abs/2507.17285)
*Aritz Pérez,Carlos Echegoyen,Guzmán Santafé*

Main category: cs.LG

TL;DR: 提出了一种去中心化联邦学习方法，用于协作学习概率生成分类器，通过节点间共享统计信息实现模型收敛，并在多种复杂场景下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了在不共享私有数据的情况下，跨异构用户网络构建全局模型，特别关注去中心化架构下的模型学习，用户无需依赖中心服务器即可直接协作更新全局模型。

Method: 提出了一种在去中心化架构上学习概率生成分类器的新方法，通过在本地节点之间共享局部统计信息，并聚合邻居信息进行迭代学习，最终实现全局模型的收敛。

Result: 实验证明了该算法在多种场景下的一致收敛性，表明其能够达到具有全局竞争力的模型性能。

Conclusion: 该算法在各种网络拓扑、网络规模、本地数据集大小和极端非独立同分布数据分布下，能够持续收敛到具有全局竞争力的模型。

Abstract: Federated learning is a paradigm of increasing relevance in real world
applications, aimed at building a global model across a network of
heterogeneous users without requiring the sharing of private data. We focus on
model learning over decentralized architectures, where users collaborate
directly to update the global model without relying on a central server. In
this context, the current paper proposes a novel approach to collaboratively
learn probabilistic generative classifiers with a parametric form. The
framework is composed by a communication network over a set of local nodes,
each of one having its own local data, and a local updating rule. The proposal
involves sharing local statistics with neighboring nodes, where each node
aggregates the neighbors' information and iteratively learns its own local
classifier, which progressively converges to a global model. Extensive
experiments demonstrate that the algorithm consistently converges to a globally
competitive model across a wide range of network topologies, network sizes,
local dataset sizes, and extreme non-i.i.d. data distributions.

</details>


### [183] [DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD](https://arxiv.org/abs/2507.17501)
*Xianbiao Qi,Marco Chen,Wenjie Xiao,Jiaquan Ye,Yelin He,Chun-Guang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: DNT 是一种新型 Transformer 架构，通过深度归一化解决了梯度重尾问题，可以使用更简单的 mSGDW 优化器进行训练，并达到了与 AdamW 相当的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer 的训练通常需要像 AdamW 这样的高级优化器，而不是动量 SGDW (mSGDW)，这是因为梯度分布的重尾特性。本研究旨在克服这一限制。

Method: 通过在 Transformer 的关键位置集成归一化技术来调节雅可比矩阵，平衡权重、激活及其相互作用的影响，从而使梯度分布集中。

Result: DNT 使得 Transformer 能够使用 vanilla mSGDW 进行无缝训练，并取得与使用 AdamW 训练的 Transformer 相当的性能。

Conclusion: DNT 表现优于其对应模型（ViT 和 GPT），并且可以使用 vanilla mSGDW 进行有效训练。

Abstract: Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.

</details>


### [184] [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
*Zhuokun Chen,Zeren Chen,Jiahao He,Mingkui Tan,Jianfei Cai,Bohan Zhuang*

Main category: cs.LG

TL;DR: R-Stitch 是一种新的解码框架，通过在 SLM 和 LLM 之间智能切换，显著加速了 CoT 推理过程，同时保持了高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的 CoT 加速方法，如缩短序列长度或使用投机性解码，存在局限性。缩短序列长度会影响准确性，而投机性解码在小型模型和大型模型一致性较低时加速效果有限，并且未能充分利用小型模型生成简洁中间推理的优势。因此，需要一种新的方法来更有效地加速 CoT 推理。

Method: R-Stitch 是一种基于 token 级别、置信度的方法，它在推理轨迹中结合了 SLM 和 LLM。SLM 默认用于生成 token，当 SLM 的置信度低于某个阈值时，则将 LLM 调用于生成后续 token。这种方法避免了完全回滚，只在 SLM 不确定的步骤中调用 LLM。

Result: 实验结果表明，R-Stitch 在数学推理基准测试上实现了高达 85% 的推理延迟降低，同时准确性几乎没有下降。

Conclusion: R-Stitch 通过在推理过程中根据 SLM 的置信度在 SLM 和 LLM 之间进行切换，可以有效地减少 CoT 推理的延迟，同时保持可忽略的准确性损失。

Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing acceleration strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the LLM only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the LLM on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.

</details>


### [185] [Confounded Causal Imitation Learning with Instrumental Variables](https://arxiv.org/abs/2507.17309)
*Yan Zeng,Shenglan Nie,Feng Xie,Libo Huang,Peng Wu,Zhi Geng*

Main category: cs.LG

TL;DR: This paper introduces the Confounded Causal Imitation Learning (C2L) model to address confounding effects in imitation learning by using instrumental variables. It features a two-stage framework for identifying valid IVs and learning policies, demonstrating effectiveness in experiments.


<details>
  <summary>Details</summary>
Motivation: Imitation learning from demonstrations is often affected by unmeasured confounding variables, leading to biased policy estimation. This paper aims to address this confounding gap by utilizing instrumental variables (IVs).

Method: A two-stage imitation learning framework is developed. The first stage constructs a testing criterion using a defined pseudo-variable to identify a valid instrumental variable (IV) for C2L models, ensuring sufficient and necessary identifiability conditions for IV validity. The second stage proposes two policy learning approaches using the identified IV: one simulator-based and one offline.

Result: The proposed C2L model successfully accommodates confounders that influence actions across multiple timesteps. Experiments confirmed the effectiveness of both the IV identification method and the policy learning approaches.

Conclusion: The Confounded Causal Imitation Learning (C2L) model effectively identifies valid instrumental variables and learns policies, as verified by extensive experiments.

Abstract: Imitation learning from demonstrations usually suffers from the confounding
effects of unmeasured variables (i.e., unmeasured confounders) on the states
and actions. If ignoring them, a biased estimation of the policy would be
entailed. To break up this confounding gap, in this paper, we take the best of
the strong power of instrumental variables (IV) and propose a Confounded Causal
Imitation Learning (C2L) model. This model accommodates confounders that
influence actions across multiple timesteps, rather than being restricted to
immediate temporal dependencies. We develop a two-stage imitation learning
framework for valid IV identification and policy optimization. In particular,
in the first stage, we construct a testing criterion based on the defined
pseudo-variable, with which we achieve identifying a valid IV for the C2L
models. Such a criterion entails the sufficient and necessary identifiability
conditions for IV validity. In the second stage, with the identified IV, we
propose two candidate policy learning approaches: one is based on a simulator,
while the other is offline. Extensive experiments verified the effectiveness of
identifying the valid IV as well as learning the policy.

</details>


### [186] [EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents](https://arxiv.org/abs/2507.17311)
*Zijie Guo,Jiong Wang,Xiaoyu Yue,Wangxu Wei,Zhe Jiang,Wanghan Xu,Ben Fei,Wenlong Zhang,Xinyu Gu,Lijing Cheng,Jing-Jia Luo,Chao Li,Yaqiang Wang,Tao Chen,Wanli Ouyang,Fenghua Ling,Lei Bai*

Main category: cs.LG

TL;DR: EarthLink是首个用于地球科学的AI研究副驾驶，能自动化研究流程，学习用户反馈，并被评为可媲美初级研究员的分析能力。


<details>
  <summary>Details</summary>
Motivation: 地球科学面临数据量大、碎片化、复杂性高等问题，加上日益复杂的分析需求，对科学发现构成了重大瓶颈。

Method: EarthLink是一个AI代理，通过自动化研究工作流程（包括规划、代码生成和多场景分析）来协助地球科学家。它能够从用户交互中学习，并通过动态反馈循环不断改进其能力。

Result: 在针对气候变化核心科学任务（从模型-观测比较到复杂现象诊断）的验证中，EarthLink取得了科学上合理的分析结果，其分析能力在某些方面可与人类初级研究人员相媲美。其透明、可审计的工作流程和自然语言界面使科学家能够从繁琐的手动执行转向战略监督和假设生成。

Conclusion: EarthLink标志着迈向一个高效、可信和协作的地球系统研究范式的关键一步，以应对加速的全球变化。

Abstract: Modern Earth science is at an inflection point. The vast, fragmented, and
complex nature of Earth system data, coupled with increasingly sophisticated
analytical demands, creates a significant bottleneck for rapid scientific
discovery. Here we introduce EarthLink, the first AI agent designed as an
interactive copilot for Earth scientists. It automates the end-to-end research
workflow, from planning and code generation to multi-scenario analysis. Unlike
static diagnostic tools, EarthLink can learn from user interaction,
continuously refining its capabilities through a dynamic feedback loop. We
validated its performance on a number of core scientific tasks of climate
change, ranging from model-observation comparisons to the diagnosis of complex
phenomena. In a multi-expert evaluation, EarthLink produced scientifically
sound analyses and demonstrated an analytical competency that was rated as
comparable to specific aspects of a human junior researcher's workflow.
Additionally, its transparent, auditable workflows and natural language
interface empower scientists to shift from laborious manual execution to
strategic oversight and hypothesis generation. EarthLink marks a pivotal step
towards an efficient, trustworthy, and collaborative paradigm for Earth system
research in an era of accelerating global change.

</details>


### [187] [A Learning-based Domain Decomposition Method](https://arxiv.org/abs/2507.17328)
*Rui Wu,Nikola Kovachki,Burigede Liu*

Main category: cs.LG

TL;DR: 提出了一种名为L-DDM的学习域分解方法，该方法使用预训练的神经算子来高效地解决具有复杂几何形状和不连续微观结构的偏微分方程问题，性能优于现有方法且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了满足机械、航空和结构工程领域对更大、更复杂尺度结构进行建模和分析的需求，并解决传统数值方法（如有限元法）在处理大规模和几何复杂问题时的计算成本和可扩展性问题，以及现有神经网络方法在简单域上的局限性。

Method: 提出了一种基于学习的域分解方法（L-DDM），该方法使用单个预训练的神经算子作为域分解方案中的代理模型，以处理大规模和复杂的问题。理论上证明了在域分解求解抽象偏微分方程的背景下，神经算子逼近的存在性。

Result: 所提出的L-DDM方法能够准确地逼近具有不连续微观结构的复杂几何形状的椭圆偏微分方程的解，并且在性能上优于当前最先进的方法。该方法还具有分辨率不变性，并能很好地泛化到训练中未出现的微观结构模式。

Conclusion: 该方法在处理具有复杂几何形状和不连续微观结构的问题时，能够准确地逼近椭圆偏微分方程的解，并且优于当前最先进的方法，同时还提供分辨率不变性和对训练期间未见过的微观结构模式的强大泛化能力。

Abstract: Recent developments in mechanical, aerospace, and structural engineering have
driven a growing need for efficient ways to model and analyse structures at
much larger and more complex scales than before. While established numerical
methods like the Finite Element Method remain reliable, they often struggle
with computational cost and scalability when dealing with large and
geometrically intricate problems. In recent years, neural network-based methods
have shown promise because of their ability to efficiently approximate
nonlinear mappings. However, most existing neural approaches are still largely
limited to simple domains, which makes it difficult to apply to real-world PDEs
involving complex geometries. In this paper, we propose a learning-based domain
decomposition method (L-DDM) that addresses this gap. Our approach uses a
single, pre-trained neural operator-originally trained on simple domains-as a
surrogate model within a domain decomposition scheme, allowing us to tackle
large and complicated domains efficiently. We provide a general theoretical
result on the existence of neural operator approximations in the context of
domain decomposition solution of abstract PDEs. We then demonstrate our method
by accurately approximating solutions to elliptic PDEs with discontinuous
microstructures in complex geometries, using a physics-pretrained neural
operator (PPNO). Our results show that this approach not only outperforms
current state-of-the-art methods on these challenging problems, but also offers
resolution-invariance and strong generalization to microstructural patterns
unseen during training.

</details>


### [188] [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
*Anisha Gunjal,Anthony Wang,Elaine Lau,Vaskar Nath,Bing Liu,Sean Hendryx*

Main category: cs.LG

TL;DR: RaR框架使用结构化评分标准作为可解释的奖励信号，用于强化学习，在HealthBench-1k上取得了显著改进，并能更好地与人类偏好保持一致。


<details>
  <summary>Details</summary>
Motivation: 传统的基于偏好的方法依赖于不透明且易于产生虚假关联的奖励函数，而许多现实世界的任务缺乏单一、明确的地面真值，难以定义可靠的奖励信号。

Method: 提出了一种名为“Rubrics as Rewards”（RaR）的新框架，该框架使用结构化的、清单式的评分标准作为可解释的奖励信号，并将其与GRPO（一种策略内训练方法）结合使用。

Result: 在HealthBench-1k上的实验显示，RaR方法比基于Likert量表的简单方法取得了高达28%的相对改进，同时在性能上达到或超过了基于专家编写的参考资料得出的奖励信号。此外，RaR能够使规模较小的模型更好地与人类偏好保持一致，并在不同模型规模下保持稳健的性能。

Conclusion: 该研究提出了“Rubrics as Rewards”（RaR）框架，该框架使用结构化的、清单式的评分标准作为可解释的奖励信号，用于基于GRPO的策略内训练。研究表明，RaR能够更好地使模型与人类偏好保持一致，并在不同模型规模下保持稳健的性能。

Abstract: Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world
tasks often requires balancing objective and subjective evaluation criteria.
However, many such tasks lack a single, unambiguous ground truth-making it
difficult to define reliable reward signals for post-training language models.
While traditional preference-based methods offer a workaround, they rely on
opaque reward functions that are difficult to interpret and prone to spurious
correlations. We introduce $\textbf{Rubrics as Rewards}$ (RaR), a framework
that uses structured, checklist-style rubrics as interpretable reward signals
for on-policy training with GRPO. Our best RaR method yields up to a $28\%$
relative improvement on HealthBench-1k compared to simple Likert-based
approaches, while matching or surpassing the performance of reward signals
derived from expert-written references. By treating rubrics as structured
reward signals, we show that RaR enables smaller-scale judge models to better
align with human preferences and sustain robust performance across model
scales.

</details>


### [189] [DeCo-SGD: Joint Optimization of Delay Staleness and Gradient Compression Ratio for Distributed SGD](https://arxiv.org/abs/2507.17346)
*Rongwei Lu,Jingyan Jiang,Chunyang Li,Haotian Dong,Xingguang Wei,Delin Cai,Zhi Wang*

Main category: cs.LG

TL;DR: DeCo-SGD通过新的理论分析，解决了分布式SGD在低带宽、高延迟网络中的性能问题，实现了显著的加速。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式SGD在面临高延迟和低、可变带宽网络环境时吞吐量严重下降的问题，并弥补现有方法在理论指导方面的不足。

Method: 提出了一种新的理论工具，将联合优化问题分解为具有多个可分析噪声项的传统收敛率分析。

Result: DeCo-SGD在हरूको延迟和低、可变带宽网络中分别实现了比D-SGD和静态策略高5.07倍和1.37倍的加速。

Conclusion: 通过将联合优化问题分解为具有多个可分析噪声项的传统收敛率分析，并提出了一种将收敛率与面向网络的最小时间条件相结合的方法DeCo-SGD，根据实时网络条件和训练任务动态调整压缩率和延迟。

Abstract: Distributed machine learning in high end-to-end latency and low, varying
bandwidth network environments undergoes severe throughput degradation. Due to
its low communication requirements, distributed SGD (D-SGD) remains the
mainstream optimizer in such challenging networks, but it still suffers from
significant throughput reduction. To mitigate these limitations, existing
approaches typically employ gradient compression and delayed aggregation to
alleviate low bandwidth and high latency, respectively. To address both
challenges simultaneously, these strategies are often combined, introducing a
complex three-way trade-off among compression ratio, staleness (delayed
synchronization steps), and model convergence rate. To achieve the balance
under varying bandwidth conditions, an adaptive policy is required to
dynamically adjust these parameters. Unfortunately, existing works rely on
static heuristic strategies due to the lack of theoretical guidance, which
prevents them from achieving this goal. This study fills in this theoretical
gap by introducing a new theoretical tool, decomposing the joint optimization
problem into a traditional convergence rate analysis with multiple analyzable
noise terms. We are the first to reveal that staleness exponentially amplifies
the negative impact of gradient compression on training performance, filling a
critical gap in understanding how compressed and delayed gradients affect
training. Furthermore, by integrating the convergence rate with a network-aware
time minimization condition, we propose DeCo-SGD, which dynamically adjusts the
compression ratio and staleness based on the real-time network condition and
training task. DeCo-SGD achieves up to 5.07 and 1.37 speed-ups over D-SGD and
static strategy in high-latency and low, varying bandwidth networks,
respectively.

</details>


### [190] [TOC-UCO: a comprehensive repository of tabular ordinal classification datasets](https://arxiv.org/abs/2507.17348)
*Rafael Ayllón-Gavilán,David Guijo-Rubio,Antonio Manuel Gómez-Orellana,David Guijo-Rubio,Francisco Bérchez-Moreno,Víctor Manuel Vargas-Yun,Pedro A. Gutiérrez*

Main category: cs.LG

TL;DR: 由于缺乏用于基准测试新方法的综合数据集，来自科尔多瓦大学 (UCO) 的研究人员创建了一个名为 TOC-UCO 的公共表格数据仓库，其中包含 46 个有序分类数据集，并附有用于实验可重复性的基准测试指南。


<details>
  <summary>Details</summary>
Motivation: 有序分类（OC）领域缺乏用于基准测试新方法的综合数据集，这阻碍了该领域的发展。

Method: 提供了一个名为 TOC-UCO 的表格数据仓库，其中包含 46 个有序分类数据集，并在通用框架下进行了预处理。提供了数据集来源、预处理步骤和用于实验可重复性的 30 种不同随机训练/测试分区索引。

Result: 创建了一个名为 TOC-UCO 的公共表格数据仓库，其中包含 46 个预处理过的有序分类数据集，并附有用于实验可重复性的基准测试指南。

Conclusion: 该手稿提供了一个名为 TOC-UCO 的表格数据仓库，其中包含 46 个有序分类数据集，用于验证新的 OC 方法。

Abstract: An ordinal classification (OC) problem corresponds to a special type of
classification characterised by the presence of a natural order relationship
among the classes. This type of problem can be found in a number of real-world
applications, motivating the design and development of many ordinal
methodologies over the last years. However, it is important to highlight that
the development of the OC field suffers from one main disadvantage: the lack of
a comprehensive set of datasets on which novel approaches to the literature
have to be benchmarked. In order to approach this objective, this manuscript
from the University of C\'ordoba (UCO), which have previous experience on the
OC field, provides the literature with a publicly available repository of
tabular data for a robust validation of novel OC approaches, namely TOC-UCO
(Tabular Ordinal Classification repository of the UCO). Specifically, this
repository includes a set of $46$ tabular ordinal datasets, preprocessed under
a common framework and ensured to have a reasonable number of patterns and an
appropriate class distribution. We also provide the sources and preprocessing
steps of each dataset, along with details on how to benchmark a novel approach
using the TOC-UCO repository. For this, indices for $30$ different randomised
train-test partitions are provided to facilitate the reproducibility of the
experiments.

</details>


### [191] [Generalized Advantage Estimation for Distributional Policy Gradients](https://arxiv.org/abs/2507.17530)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: 提出了一种新的分布对估计（DGAE）方法，该方法利用最优传输理论和类似Wasserstein的方向度量来处理分布RL中的值分布，以提高策略梯度估计的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决GAE无法处理分布RL中的值分布的问题，并利用值分布来处理系统中的固随机性，从而提高对系统噪声的鲁棒性。

Method: 利用最优传输理论，引入了一个类似Wasserstein的方向度量，并利用指数加权估计来推导分布对估计（DGAE）。

Result: DGAE的整合表明，在各种OpenAI Gym环境中，与使用传统GAE的基线相比，所提出的DGAE在策略梯度方法中具有良好的性能。

Conclusion: 所提出的DGAE可用于策略梯度算法，并能在OpenAI Gym环境中与传统GAE基线相比，在各种环境中提供良好的性能。

Abstract: Generalized Advantage Estimation (GAE) has been used to mitigate the
computational complexity of reinforcement learning (RL) by employing an
exponentially weighted estimation of the advantage function to reduce the
variance in policy gradient estimates. Despite its effectiveness, GAE is not
designed to handle value distributions integral to distributional RL, which can
capture the inherent stochasticity in systems and is hence more robust to
system noises. To address this gap, we propose a novel approach that utilizes
the optimal transport theory to introduce a Wasserstein-like directional
metric, which measures both the distance and the directional discrepancies
between probability distributions. Using the exponentially weighted estimation,
we leverage this Wasserstein-like directional metric to derive distributional
GAE (DGAE). Similar to traditional GAE, our proposed DGAE provides a
low-variance advantage estimate with controlled bias, making it well-suited for
policy gradient algorithms that rely on advantage estimation for policy
updates. We integrated DGAE into three different policy gradient methods.
Algorithms were evaluated across various OpenAI Gym environments and compared
with the baselines with traditional GAE to assess the performance.

</details>


### [192] [DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2507.17365)
*Chuzhan Hao,Wenfeng Feng,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: DynaSearcher通过动态知识图谱和多奖励强化学习解决了多步检索系统中查询不一致和搜索效率低下的问题，在多跳问答任务上达到了最先进的性能，且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 多步代理检索系统在复杂信息搜索任务中表现出色，但在实际应用中仍面临挑战，尤其是在生成事实不一致的中间查询和低效的搜索轨迹方面，这可能导致推理偏差或冗余计算。

Method: 提出了一种名为DynaSearcher的创新搜索代理，该代理通过动态知识图谱和多奖励强化学习（RL）进行增强。具体来说，我们的系统利用知识图谱作为外部结构化知识来指导搜索过程，通过显式建模实体关系来确保中间查询的事实一致性，并减轻无关信息带来的偏差。此外，我们采用多奖励RL框架来精细控制检索准确性、效率和响应质量等训练目标。

Result: 实验结果表明，DynaSearcher在六个多跳问答数据集上取得了最先进的答案准确性，与前沿LLM相当，同时仅使用小型模型和有限的计算资源。该方法还展现了在不同检索环境和更大规模模型上的强大泛化能力和鲁棒性。

Conclusion: DynaSearcher在六个多跳问答数据集上实现了最先进的答案准确性，与前沿LLM相当，同时仅使用小型模型和有限的计算资源。此外，我们的方法在不同的检索环境和更大规模的模型中表现出强大的泛化能力和鲁棒性，显示了其广泛的适用性。

Abstract: Multi-step agentic retrieval systems based on large language models (LLMs)
have demonstrated remarkable performance in complex information search tasks.
However, these systems still face significant challenges in practical
applications, particularly in generating factually inconsistent intermediate
queries and inefficient search trajectories, which can lead to reasoning
deviations or redundant computations. To address these issues, we propose
DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs
and multi-reward reinforcement learning (RL). Specifically, our system
leverages knowledge graphs as external structured knowledge to guide the search
process by explicitly modeling entity relationships, thereby ensuring factual
consistency in intermediate queries and mitigating biases from irrelevant
information. Furthermore, we employ a multi-reward RL framework for
fine-grained control over training objectives such as retrieval accuracy,
efficiency, and response quality. This framework promotes the generation of
high-quality intermediate queries and comprehensive final answers, while
discouraging unnecessary exploration and minimizing information omissions or
redundancy. Experimental results demonstrate that our approach achieves
state-of-the-art answer accuracy on six multi-hop question answering datasets,
matching frontier LLMs while using only small-scale models and limited
computational resources. Furthermore, our approach demonstrates strong
generalization and robustness across diverse retrieval environments and
larger-scale models, highlighting its broad applicability.

</details>


### [193] [ViRN: Variational Inference and Distribution Trilateration for Long-Tailed Continual Representation Learning](https://arxiv.org/abs/2507.17368)
*Hao Dai,Chong Tang,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: 提出ViRN框架，通过结合变分推断和分布三边测量法，有效解决了长尾数据分布下的持续学习问题，显著提高了模型在样本稀缺情况下的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习方法在处理长尾数据分布时，难以平衡稳定性和可塑性，尤其是在样本稀缺的情况下，模型常常会失效。本研究旨在解决这一关键挑战。

Method: ViRN框架整合了变分推断（VI）和分布三边测量法，以实现鲁棒的长尾学习。首先，通过变分自编码器对类别条件分布进行建模，以减轻对头部类别的偏倚。其次，通过基于Wasserstein距离的邻域检索和几何融合来重建尾部类别分布，从而实现尾部类别表示的样本高效对齐。

Result: ViRN在语音（如稀有声学事件、口音）和图像任务的六个长尾分类基准上，平均准确率比现有最先进的方法提高了10.24%。

Conclusion: ViRN框架在语音和图像的六个长尾分类基准上进行了评估，在平均准确率上比最先进的方法高出10.24%。

Abstract: Continual learning (CL) with long-tailed data distributions remains a
critical challenge for real-world AI systems, where models must sequentially
adapt to new classes while retaining knowledge of old ones, despite severe
class imbalance. Existing methods struggle to balance stability and plasticity,
often collapsing under extreme sample scarcity. To address this, we propose
ViRN, a novel CL framework that integrates variational inference (VI) with
distributional trilateration for robust long-tailed learning. First, we model
class-conditional distributions via a Variational Autoencoder to mitigate bias
toward head classes. Second, we reconstruct tail-class distributions via
Wasserstein distance-based neighborhood retrieval and geometric fusion,
enabling sample-efficient alignment of tail-class representations. Evaluated on
six long-tailed classification benchmarks, including speech (e.g., rare
acoustic events, accents) and image tasks, ViRN achieves a 10.24% average
accuracy gain over state-of-the-art methods.

</details>


### [194] [Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective](https://arxiv.org/abs/2507.17382)
*Hao Dai,Jagmohan Chauhan*

Main category: cs.LG

TL;DR: VB-CGCD是一种新框架，通过变分贝叶斯方法解决持续泛化类别发现（C-GCD）中的遗忘问题，提高了新旧类别的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的C-GCD方法在增量学习新类别和保持旧类别知识方面存在困难，尤其是在无标签数据混合了已知和新类别的情况下，会引发灾难性遗忘。本文旨在通过贝叶斯方法分析C-GCD的遗忘动态，揭示协方差错位是导致性能下降的关键因素。

Method: 提出了一种名为VB-CGCD（变分贝叶斯C-GCD）的新框架，该框架集成了变分推断和注意协方差的最近类平均分类。通过随机变分更新，VB-CGCD能够自适应地对齐类分布并抑制伪标签噪声。

Result: VB-CGCD在标准基准测试的最后阶段，整体准确性比现有方法高出+15.21%。在仅有10%标记数据的新挑战性基准测试和扩展在线阶段，VB-CGCD实现了67.86%的最终准确性，显著优于现有最先进水平（38.55%），证明了其在不同场景下的鲁棒适用性。

Conclusion: VB-CGCD框架通过结合变分推断和注意协方差的最近类平均分类，有效解决了C-GCD中的灾难性遗忘问题，并在混合了已知和新类的标记数据流上实现了知识的增量学习。实验证明，VB-CGCD在整体准确性方面比现有方法提高了+15.21%，并在新的具有挑战性的基准测试中取得了显著的性能提升。

Abstract: Continual Generalized Category Discovery (C-GCD) faces a critical challenge:
incrementally learning new classes from unlabeled data streams while preserving
knowledge of old classes. Existing methods struggle with catastrophic
forgetting, especially when unlabeled data mixes known and novel categories. We
address this by analyzing C-GCD's forgetting dynamics through a Bayesian lens,
revealing that covariance misalignment between old and new classes drives
performance degradation. Building on this insight, we propose Variational Bayes
C-GCD (VB-CGCD), a novel framework that integrates variational inference with
covariance-aware nearest-class-mean classification. VB-CGCD adaptively aligns
class distributions while suppressing pseudo-label noise via stochastic
variational updates. Experiments show VB-CGCD surpasses prior art by +15.21%
with the overall accuracy in the final session on standard benchmarks. We also
introduce a new challenging benchmark with only 10% labeled data and extended
online phases, VB-CGCD achieves a 67.86% final accuracy, significantly higher
than state-of-the-art (38.55%), demonstrating its robust applicability across
diverse scenarios. Code is available at: https://github.com/daihao42/VB-CGCD

</details>


### [195] [A Comprehensive Evaluation on Quantization Techniques for Large Language Models](https://arxiv.org/abs/2507.17417)
*Yutong Liu,Cairong Zhao,Guosheng Hu*

Main category: cs.LG

TL;DR: 本文对LLM的量化方法进行了全面的评估和分析，提出了预量化转换和量化误差补偿的解耦框架，并对现有技术进行了公平比较，为LLM的量化研究提供了有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM量化研究缺乏统一的实验平台和理论分析，导致不同方法之间难以公平比较，并且对量化方法的理论联系理解不足。本文旨在通过进行公平的广泛评估和理论分析来弥合这些差距。

Method: 对LLM的PTQ技术进行了广泛的回顾，并将现有方法解耦为预量化转换和量化误差补偿两个步骤。通过在相同实验基础上进行全面的评估和分析，比较不同组件的影响，并对MXFP4数据格式进行了实验评估。

Result: 优化的旋转和缩放预量化转换策略在预量化转换中表现最佳；低秩补偿与GPTQ的结合有时优于单独使用GPTQ；INT4的最佳预量化转换策略不适用于MXFP4。

Conclusion: 本文通过解耦量化方法、消融实验和公平的消融研究，对LLM的PTQ进行了全面的分析和评估。研究结果表明，优化的旋转和缩放预量化转换策略效果最佳，低秩补偿与GPTQ的结合有时优于单独使用GPTQ。此外，研究还发现INT4的最佳预量化转换策略并不适用于MXFP4，为后续研究提供了方向。

Abstract: For large language models (LLMs), post-training quantization (PTQ) can
significantly reduce memory footprint and computational overhead. Model
quantization is a rapidly evolving research field. Though many papers have
reported breakthrough performance, they may not conduct experiments on the same
ground since one quantization method usually contains multiple components. In
addition, analyzing the theoretical connections among existing methods is
crucial for in-depth understanding. To bridge these gaps, we conduct an
extensive review of state-of-the-art methods and perform comprehensive
evaluations on the same ground to ensure fair comparisons. To our knowledge,
this fair and extensive investigation remains critically important yet
underexplored. To better understand the theoretical connections, we decouple
the published quantization methods into two steps: pre-quantization
transformation and quantization error mitigation. We define the former as a
preprocessing step applied before quantization to reduce the impact of
outliers, making the data distribution flatter and more suitable for
quantization. Quantization error mitigation involves techniques that offset the
errors introduced during quantization, thereby enhancing model performance. We
evaluate and analyze the impact of different components of quantization
methods. Additionally, we analyze and evaluate the latest MXFP4 data format and
its performance. Our experimental results demonstrate that optimized rotation
and scaling yield the best performance for pre-quantization transformation, and
combining low-rank compensation with GPTQ occasionally outperforms using GPTQ
alone for quantization error mitigation. Furthermore, we explore the potential
of the latest MXFP4 quantization and reveal that the optimal pre-quantization
transformation strategy for INT4 does not generalize well to MXFP4, inspiring
further investigation.

</details>


### [196] [Persistent Patterns in Eye Movements: A Topological Approach to Emotion Recognition](https://arxiv.org/abs/2507.17450)
*Arsha Niksa,Hooman Zare,Ali Shahrabi,Hanieh Hatami,Mohammadreza Razvan*

Main category: cs.LG

TL;DR: 一种新的拓扑方法，利用眼动追踪数据和持久同源性，以高达75.6%的准确率识别四种情感。


<details>
  <summary>Details</summary>
Motivation: 提出一种用于从眼动追踪数据自动识别多类情感的拓扑管线。

Method: 使用持久同源性分析延迟嵌入的注视轨迹，并从持久性图中提取基于形状的特征（例如平均持久性、最大持久性、熵）。使用随机森林分类器在这些特征上进行训练。

Result: 在四个情感类别（包括环状情感模型中的象限）上，随机森林分类器的准确率高达75.6%。

Conclusion: 拓扑学方法为情感识别提供了有前景的途径，能够有效编码区分性的注视动力学。

Abstract: We present a topological pipeline for automated multiclass emotion
recognition from eye-tracking data. Delay embeddings of gaze trajectories are
analyzed using persistent homology. From the resulting persistence diagrams, we
extract shape-based features such as mean persistence, maximum persistence, and
entropy. A random forest classifier trained on these features achieves up to
$75.6\%$ accuracy on four emotion classes, which are the quadrants the
Circumplex Model of Affect. The results demonstrate that persistence diagram
geometry effectively encodes discriminative gaze dynamics, suggesting a
promising topological approach for affective computing and human behavior
analysis.

</details>


### [197] [C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning](https://arxiv.org/abs/2507.17454)
*Shusen Ma,Yun-Bo Zhao,Yu Kang*

Main category: cs.LG

TL;DR: C3RL是一个结合了通道混合（CM）和通道独立（CI）策略的新型多变量时间序列预测框架，通过孪生网络和对比学习，显著提升了现有模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多变量时间序列预测方法要么采用通道混合（CM）策略，要么采用通道独立（CI）策略。CM策略能捕捉变量间的依赖关系但忽略变量自身的时间模式；CI策略能识别变量自身模式但未能充分利用跨变量依赖。混合策略在泛化性和可解释性方面存在局限。本研究旨在解决这些问题。

Method: 提出了一种名为C3RL的新型表示学习框架，该框架采用孪生网络架构，联合建模通道混合（CM）和通道独立（CI）策略。通过结合对比学习和预测损失，并进行自适应加权，以平衡表示和预测性能。

Result: 在七个数据集上的广泛实验表明，C3RL能够将基于CI策略的模型的最优性能提升至81.4%，将基于CM策略的模型的最优性能提升至76.3%，证明了其强大的泛化能力和有效性。

Conclusion: C3RL框架在多变量时间序列预测任务上展现出强大的泛化性和有效性，能够同时提升基于CI和CM策略的模型性能。

Abstract: Multivariate time series forecasting has drawn increasing attention due to
its practical importance. Existing approaches typically adopt either
channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can
capture inter-variable dependencies but fails to discern variable-specific
temporal patterns. CI strategy improves this aspect but fails to fully exploit
cross-variable dependencies like CM. Hybrid strategies based on feature fusion
offer limited generalization and interpretability. To address these issues, we
propose C3RL, a novel representation learning framework that jointly models
both CM and CI strategies. Motivated by contrastive learning in computer
vision, C3RL treats the inputs of the two strategies as transposed views and
builds a siamese network architecture: one strategy serves as the backbone,
while the other complements it. By jointly optimizing contrastive and
prediction losses with adaptive weighting, C3RL balances representation and
forecasting performance. Extensive experiments on seven models show that C3RL
boosts the best-case performance rate to 81.4\% for models based on CI strategy
and to 76.3\% for models based on CM strategy, demonstrating strong
generalization and effectiveness. The code will be available once the paper is
accepted.

</details>


### [198] [HOTA: Hamiltonian framework for Optimal Transport Advection](https://arxiv.org/abs/2507.17513)
*Nazar Buzun,Daniil Shlenskii,Maxim Bobrin,Dmitry V. Dylov*

Main category: cs.LG

TL;DR: HOTA是一种新的最优传输方法，通过解决对偶动力学问题来优化轨迹，无需显式密度建模，并在各种基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大多数生成模型依赖于平凡几何和强密度估计假设，导致轨迹不符合潜在流形上的最优性原理。

Method: HOTA是一种基于Hamilton-Jacobi-Bellman的方法，通过Kantorovich势来明确处理对偶动力学最优传输问题，从而实现高效且可扩展的轨迹优化。

Result: HOTA有效规避了显式密度建模的需要，即使在成本泛函不光滑的情况下也能执行。

Conclusion: HOTA在标准基准和自定义数据集上均优于所有基线，包括在非可微成本方面，在可行性和最优性方面均表现出色。

Abstract: Optimal transport (OT) has become a natural framework for guiding the
probability flows. Yet, the majority of recent generative models assume trivial
geometry (e.g., Euclidean) and rely on strong density-estimation assumptions,
yielding trajectories that do not respect the true principles of optimality in
the underlying manifold. We present Hamiltonian Optimal Transport Advection
(HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical
OT problem explicitly through Kantorovich potentials, enabling efficient and
scalable trajectory optimization. Our approach effectively evades the need for
explicit density modeling, performing even when the cost functionals are
non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks,
as well as in custom datasets with non-differentiable costs, both in terms of
feasibility and optimality.

</details>


### [199] [Generalized Low-Rank Matrix Contextual Bandits with Graph Information](https://arxiv.org/abs/2507.17528)
*Yao Wang,Jiannan Li,Yue Kang,Shanxing Gao,Zhenxin Xiao*

Main category: cs.LG

TL;DR: 一种新的矩阵上下文随机算法框架，它结合了低秩结构和图信息，并通过理论和实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的矩阵上下文随机算法（CB）未能利用图信息，因此难以制定有效的决策策略。为了解决这个问题，我们提出了一种新的矩阵CB算法框架，该框架可以有效地以统一的方式结合低秩结构和图信息。

Method: 提出一种新的矩阵上下文随机算法框架，该框架建立在经典的上限置信界（UCB）框架之上。具体来说，它首先解决联合核范数和矩阵拉普拉斯正则化问题，然后实现基于图的广义线性UCB算法。

Result: 通过严格的理论分析和一系列合成及真实世界数据实验证明，该方法能够有效利用图信息，从而在累积遗憾界限方面优于几种流行的方法。

Conclusion: 该方法通过联合核范数和图拉普拉斯正则化问题，并实现基于图的广义线性UCB算法，能够有效结合低秩结构和图信息，并在累积遗憾界限方面优于现有方法。

Abstract: The matrix contextual bandit (CB), as an extension of the well-known
multi-armed bandit, is a powerful framework that has been widely applied in
sequential decision-making scenarios involving low-rank structure. In many
real-world scenarios, such as online advertising and recommender systems,
additional graph information often exists beyond the low-rank structure, that
is, the similar relationships among users/items can be naturally captured
through the connectivity among nodes in the corresponding graphs. However,
existing matrix CB methods fail to explore such graph information, and thereby
making them difficult to generate effective decision-making policies. To fill
in this void, we propose in this paper a novel matrix CB algorithmic framework
that builds upon the classical upper confidence bound (UCB) framework. This new
framework can effectively integrate both the low-rank structure and graph
information in a unified manner. Specifically, it involves first solving a
joint nuclear norm and matrix Laplacian regularization problem, followed by the
implementation of a graph-based generalized linear version of the UCB
algorithm. Rigorous theoretical analysis demonstrates that our procedure
outperforms several popular alternatives in terms of cumulative regret bound,
owing to the effective utilization of graph information. A series of synthetic
and real-world data experiments are conducted to further illustrate the merits
of our procedure.

</details>


### [200] [Federated Majorize-Minimization: Beyond Parameter Aggregation](https://arxiv.org/abs/2507.17534)
*Aymeric Dieuleveut,Gersende Fort,Mahmoud Hegazy,Hoi-To Wai*

Main category: cs.LG

TL;DR: 该论文提出了一种名为\SSMM
的统一随机优化算法框架，该框架适用于联邦学习。通过聚合表征Majorizing函数的信息，\QSMM
能够有效解决联邦学习中的数据异质性、部分参与和通信限制等问题。最后，该方法在计算联邦最优传输图方面得到了成功应用。


<details>
  <summary>Details</summary>
Motivation: 为了设计能够稳健地扩展到联邦学习环境的随机优化算法，并为Majorize-Minimization（MM）问题提供一个统一的框架。

Method: 提出了一种统一的框架来设计随机优化算法，适用于联邦学习。该框架基于Majorize-Minimization（MM）问题，该问题具有线性参数化的Majorizing函数族。由此产生的算法\SSMM
包含以前的随机MM过程作为特例。此外，研究还将\SSMM
扩展到联邦环境，称为\QSMM\，以解决数据异质性、部分参与和通信限制等常见挑战。

Result: 开发了一种名为\SSMM
的统一算法，并将其扩展到联邦设置，称为\QSMM\。\QSMM
的创新之处在于聚合表征Majorizing函数的信息，而不是聚合原始参数。此外，研究还展示了该方法在计算联邦最优传输图中的应用。

Conclusion: 该研究提出了一种新颖的联邦学习优化方法\SSMM，并通过将其应用于最优传输图计算来展示其灵活性。

Abstract: This paper proposes a unified approach for designing stochastic optimization
algorithms that robustly scale to the federated learning setting. Our work
studies a class of Majorize-Minimization (MM) problems, which possesses a
linearly parameterized family of majorizing surrogate functions. This framework
encompasses (proximal) gradient-based algorithms for (regularized) smooth
objectives, the Expectation Maximization algorithm, and many problems seen as
variational surrogate MM. We show that our framework motivates a unifying
algorithm called Stochastic Approximation Stochastic Surrogate MM (\SSMM),
which includes previous stochastic MM procedures as special instances. We then
extend \SSMM\ to the federated setting, while taking into consideration common
bottlenecks such as data heterogeneity, partial participation, and
communication constraints; this yields \QSMM. The originality of \QSMM\ is to
learn locally and then aggregate information characterizing the
\textit{surrogate majorizing function}, contrary to classical algorithms which
learn and aggregate the \textit{original parameter}. Finally, to showcase the
flexibility of this methodology beyond our theoretical setting, we use it to
design an algorithm for computing optimal transport maps in the federated
setting.

</details>


### [201] [XStacking: Explanation-Guided Stacked Ensemble Learning](https://arxiv.org/abs/2507.17650)
*Moncef Garouani,Ayah Barhrhouj,Olivier Teste*

Main category: cs.LG

TL;DR: XStacking 是一种新的集成机器学习框架，它通过结合动态特征转换和 Shapley 值，解决了堆叠模型的解释性问题，同时保持了其预测性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决集成机器学习（EML）技术（尤其是堆叠）缺乏可解释性的问题。

Method: 通过整合动态特征转换和模型无关的 Shapley 加法解释，将 XStacking 框架与 Shapley 值相结合，以实现堆叠模型的解释。

Result: 在 29 个数据集上证明了该框架的有效性，在学习空间的预测有效性和所得模型的可解释性方面均取得了改进。

Conclusion: XStacking 提供了一个实用的、可扩展的解决方案，用于负责任的机器学习，它在保持预测准确性的同时，使堆叠模型具有内在的可解释性。

Abstract: Ensemble Machine Learning (EML) techniques, especially stacking, have been
shown to improve predictive performance by combining multiple base models.
However, they are often criticized for their lack of interpretability. In this
paper, we introduce XStacking, an effective and inherently explainable
framework that addresses this limitation by integrating dynamic feature
transformation with model-agnostic Shapley additive explanations. This enables
stacked models to retain their predictive accuracy while becoming inherently
explainable. We demonstrate the effectiveness of the framework on 29 datasets,
achieving improvements in both the predictive effectiveness of the learning
space and the interpretability of the resulting models. XStacking offers a
practical and scalable solution for responsible ML.

</details>


### [202] [How Should We Meta-Learn Reinforcement Learning Algorithms?](https://arxiv.org/abs/2507.17668)
*Alexander David Goldie,Zilin Wang,Jakob Nicolaus Foerster,Shimon Whiteson*

Main category: cs.LG

TL;DR: Meta-learning is popular for RL but lacks comparison. This paper compares meta-learning algorithms (evolution, LLMs) for RL, evaluating performance, interpretability, sample cost, and train time, and provides guidelines for future research.


<details>
  <summary>Details</summary>
Motivation: The growing popularity of meta-learning for RL and the lack of comparison between different meta-learning approaches.

Method: Empirical comparison of different meta-learning algorithms (e.g., evolution, LLMs) applied to various parts of the RL pipeline.

Result: The paper investigates performance, interpretability, sample cost, and train time for each meta-learning algorithm, leading to proposed guidelines for future meta-learning of RL algorithms.

Conclusion: The paper provides guidelines for meta-learning RL algorithms based on an empirical comparison of different approaches, considering performance, interpretability, sample cost, and train time.

Abstract: The process of meta-learning algorithms from data, instead of relying on
manual design, is growing in popularity as a paradigm for improving the
performance of machine learning systems. Meta-learning shows particular promise
for reinforcement learning (RL), where algorithms are often adapted from
supervised or unsupervised learning despite their suboptimality for RL.
However, until now there has been a severe lack of comparison between different
meta-learning algorithms, such as using evolution to optimise over black-box
functions or LLMs to propose code. In this paper, we carry out this empirical
comparison of the different approaches when applied to a range of meta-learned
algorithms which target different parts of the RL pipeline. In addition to
meta-train and meta-test performance, we also investigate factors including the
interpretability, sample cost and train time for each meta-learning algorithm.
Based on these findings, we propose several guidelines for meta-learning new RL
algorithms which will help ensure that future learned algorithms are as
performant as possible.

</details>


### [203] [Generalized Dual Discriminator GANs](https://arxiv.org/abs/2507.17684)
*Penukonda Naga Chandana,Tejas Srivastava,Gowtham R. Kurri,V. Lalitha*

Main category: cs.LG

TL;DR: 本研究提出了广义对偶判别器生成对抗网络（generalized dual discriminator generative adversarial networks），结合了对偶判别器和 $\alpha$-loss 的优点，克服了模式崩溃问题，并通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了缓解生成对抗网络（GANs）中的模式崩溃（mode collapse）问题，并提供一种结合对偶判别器和可调损失函数的灵活方法。

Method: 提出对偶判别器 alpha-GANs (D2 $\alpha$-GANs)，结合了对偶判别器和 alpha-loss 的灵活性，并推广到更广泛的广义对偶判别器生成对抗网络。通过理论分析，将相关的最小-最大优化（min-max optimization）简化为 f 散度和反向 f 散度的线性组合。

Result: 在 2D 合成数据上进行了实验，并使用多种性能指标来捕捉所提出的 GANs 的各种优势。

Conclusion: 这项工作提出了广义对偶判别器生成对抗网络（generalized dual discriminator generative adversarial networks），该网络将对偶判别器的优点与可调损失函数 $\alpha$-loss 的灵活性相结合，并推广到更广泛的模型类别。通过理论分析，证明了这些模型与 f 散度及反向 f 散度（f-divergence and reverse f-divergence）的线性组合相关联。

Abstract: Dual discriminator generative adversarial networks (D2 GANs) were introduced
to mitigate the problem of mode collapse in generative adversarial networks. In
D2 GANs, two discriminators are employed alongside a generator: one
discriminator rewards high scores for samples from the true data distribution,
while the other favors samples from the generator. In this work, we first
introduce dual discriminator $\alpha$-GANs (D2 $\alpha$-GANs), which combines
the strengths of dual discriminators with the flexibility of a tunable loss
function, $\alpha$-loss. We further generalize this approach to arbitrary
functions defined on positive reals, leading to a broader class of models we
refer to as generalized dual discriminator generative adversarial networks. For
each of these proposed models, we provide theoretical analysis and show that
the associated min-max optimization reduces to the minimization of a linear
combination of an $f$-divergence and a reverse $f$-divergence. This generalizes
the known simplification for D2-GANs, where the objective reduces to a linear
combination of the KL-divergence and the reverse KL-divergence. Finally, we
perform experiments on 2D synthetic data and use multiple performance metrics
to capture various advantages of our GANs.

</details>


### [204] [Towards Effective Open-set Graph Class-incremental Learning](https://arxiv.org/abs/2507.17687)
*Jiazhen Chen,Zheng Ma,Sichao Fu,Mingbin Feng,Tony S. Wirjanto,Weihua Ou*

Main category: cs.LG

TL;DR: 本研究提出了OGCIL框架，通过伪样本嵌入生成来解决开放集图类增量学习中的灾难性遗忘和开放集识别问题，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的图类增量学习（GCIL）方法主要关注闭集假设，限制了其在现实世界场景中的应用。本研究探索了一个更具挑战性的开放集图类增量学习场景，其中存在灾难性遗忘和不充分的开放集识别两个相互交织的挑战。

Method: 设计了一个原型条件变分自编码器来合成旧类别的节点嵌入，实现无需存储原始图数据的知识回放。采用了一种基于混合的策略来生成伪类内分布和当前节点嵌入的分布外（OOD）样本。提出了一个新颖的原型超球分类损失，将类内分布嵌入锚定到各自的原型，并将OOD嵌入推开，通过感知原型回退区域将所有未知样本显式地建模为离群值。

Result: 在五个基准测试上的大量实验证明了OGCIL相对于现有的GCIL和开放集GNN方法的有效性。

Conclusion: 所提出的OGCIL框架通过伪样本嵌入生成有效缓解了灾难性遗忘问题，并实现了对未知类的鲁棒检测。实验证明，OGCIL在GCIL和开放集GNN方法方面均表现出优越性。

Abstract: Graph class-incremental learning (GCIL) allows graph neural networks (GNNs)
to adapt to evolving graph analytical tasks by incrementally learning new class
knowledge while retaining knowledge of old classes. Existing GCIL methods
primarily focus on a closed-set assumption, where all test samples are presumed
to belong to previously known classes. Such an assumption restricts their
applicability in real-world scenarios, where unknown classes naturally emerge
during inference, and are absent during training. In this paper, we explore a
more challenging open-set graph class-incremental learning scenario with two
intertwined challenges: catastrophic forgetting of old classes, which impairs
the detection of unknown classes, and inadequate open-set recognition, which
destabilizes the retention of learned knowledge. To address the above problems,
a novel OGCIL framework is proposed, which utilizes pseudo-sample embedding
generation to effectively mitigate catastrophic forgetting and enable robust
detection of unknown classes. To be specific, a prototypical conditional
variational autoencoder is designed to synthesize node embeddings for old
classes, enabling knowledge replay without storing raw graph data. To handle
unknown classes, we employ a mixing-based strategy to generate
out-of-distribution (OOD) samples from pseudo in-distribution and current node
embeddings. A novel prototypical hypersphere classification loss is further
proposed, which anchors in-distribution embeddings to their respective class
prototypes, while repelling OOD embeddings away. Instead of assigning all
unknown samples into one cluster, our proposed objective function explicitly
models them as outliers through prototype-aware rejection regions, ensuring a
robust open-set recognition. Extensive experiments on five benchmarks
demonstrate the effectiveness of OGCIL over existing GCIL and open-set GNN
methods.

</details>


### [205] [Joint Asymmetric Loss for Learning with Noisy Labels](https://arxiv.org/abs/2507.17692)
*Jialiang Wang,Xianming Liu,Xiong Zhou,Gangfeng Hu,Deming Zhai,Junjun Jiang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本研究提出了一种名为联合不对称损失（JAL）的新框架，通过引入一种名为AMSE（不对称均方误差）的新型不对称损失函数，来解决带噪声标签的深度学习问题。该方法成功地将不对称损失与APL（主动被动损失）框架结合起来，解决了传统对称损失函数容易欠拟合的问题，并通过广泛的实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于解决深度神经网络在带噪声标签下训练的准确性问题。尽管现有的对称损失函数（如APL框架中的）在一定程度上能减轻标签噪声，但它们通常会因约束过于严格而导致欠拟合。同时，新兴的理论分析表明不对称损失函数可能具有比对称损失更优越的特性，但现有不对称损失与APL等高级优化框架不兼容，限制了其应用潜力。因此，本研究旨在弥合这一理论差距，并将不对称损失扩展到更复杂的被动损失场景，以期获得更好的鲁棒性。

Method: 本研究提出的方法是联合不对称损失（JAL）框架，它通过将新提出的不对称损失函数AMSE（不对称均方误差）替换掉APL（主动被动损失）框架中的传统对称被动损失来实现。研究中还严格证明了AMSE满足不对称条件的充要条件。

Result: 通过将AMSE（不对称均方误差）作为APL（主动被动损失）框架中的被动损失，我们提出了联合不对称损失（JAL）框架。大量实验结果表明，与现有方法相比，JAL在减轻标签噪声方面表现出优越的性能。

Conclusion: 该研究提出了联合不对称损失（JAL）框架，通过引入不对称损失函数AMSE（不对称均方误差）来解决标签噪声问题，并成功地将其集成到APL（主动被动损失）框架中，有效解决了传统对称损失的欠拟合问题，并在实验中证明了其在缓解标签噪声方面的有效性。

Abstract: Learning with noisy labels is a crucial task for training accurate deep
neural networks. To mitigate label noise, prior studies have proposed various
robust loss functions, particularly symmetric losses. Nevertheless, symmetric
losses usually suffer from the underfitting issue due to the overly strict
constraint. To address this problem, the Active Passive Loss (APL) jointly
optimizes an active and a passive loss to mutually enhance the overall fitting
ability. Within APL, symmetric losses have been successfully extended, yielding
advanced robust loss functions. Despite these advancements, emerging
theoretical analyses indicate that asymmetric losses, a new class of robust
loss functions, possess superior properties compared to symmetric losses.
However, existing asymmetric losses are not compatible with advanced
optimization frameworks such as APL, limiting their potential and
applicability. Motivated by this theoretical gap and the prospect of asymmetric
losses, we extend the asymmetric loss to the more complex passive loss scenario
and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We
rigorously establish the necessary and sufficient condition under which AMSE
satisfies the asymmetric condition. By substituting the traditional symmetric
passive loss in APL with our proposed AMSE, we introduce a novel robust loss
framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate
the effectiveness of our method in mitigating label noise. Code available at:
https://github.com/cswjl/joint-asymmetric-loss

</details>


### [206] [HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging](https://arxiv.org/abs/2507.17706)
*Taha Ceritli,Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: HydraOpt 是一种新的模型合并技术，通过利用低秩适配器矩阵的内在相似性，在存储效率和性能之间取得了良好的平衡，显著减少了存储空间，同时保持了具有竞争力的性能，并且优于现有的合并技术。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLMs）为适应下游任务而使用单独适配器（如低秩适配器）导致内存需求增加的问题，尤其是在移动设备等资源受限的环境中。现有的模型合并技术虽然能降低存储成本，但通常会严重影响性能。

Method: HydraOpt 通过利用低秩适配器矩阵的内在相似性来进行模型合并，允许在存储大小和性能之间进行权衡。

Result: HydraOpt 相比存储所有适配器，显著减少了存储空间（48% 的减少），同时实现了具有竞争力的性能（0.2-1.8% 的性能下降）。此外，在相同的存储效率下，其性能优于现有的合并技术。

Conclusion: HydraOpt 是一种新的模型合并技术，通过利用低秩适配器矩阵的内在相似性，在存储效率和性能之间取得了良好的平衡，显著减少了存储空间（减少 48%），同时保持了具有竞争力的性能（仅下降 0.2-1.8%），并且优于现有的合并技术。

Abstract: Large language models (LLMs) often leverage adapters, such as low-rank-based
adapters, to achieve strong performance on downstream tasks. However, storing a
separate adapter for each task significantly increases memory requirements,
posing a challenge for resource-constrained environments such as mobile
devices. Although model merging techniques can reduce storage costs, they
typically result in substantial performance degradation. In this work, we
introduce HydraOpt, a new model merging technique that capitalizes on the
inherent similarities between the matrices of low-rank adapters. Unlike
existing methods that produce a fixed trade-off between storage size and
performance, HydraOpt allows us to navigate this spectrum of efficiency and
performance. Our experiments show that HydraOpt significantly reduces storage
size (48% reduction) compared to storing all adapters, while achieving
competitive performance (0.2-1.8% drop). Furthermore, it outperforms existing
merging techniques in terms of performance at the same or slightly worse
storage efficiency.

</details>


### [207] [On the Interaction of Compressibility and Adversarial Robustness](https://arxiv.org/abs/2507.17725)
*Melih Barsbey,Antônio H. Ribeiro,Umut Şimşekli,Tolga Birdal*

Main category: cs.LG

TL;DR: 可压缩性（如稀疏性）可能通过引入敏感方向来降低神经网络的鲁棒性，这在各种压缩技术和对抗性训练中都存在，揭示了效率和安全性的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管可压缩性和鲁棒性都得到了广泛研究，但它们之间的相互作用仍不明确。本研究旨在弥合这一差距，分析可压缩性如何影响对抗性鲁棒性。

Method: 本文提出一个原则性框架来分析不同形式的可压缩性（如神经元稀疏性和谱可压缩性）如何影响对抗性鲁棒性。通过理论分析和实证评估，我们揭示了可压缩性可能导致表示空间中少数高度敏感的方向，使攻击者能够构建有效的扰动，从而影响模型的鲁棒性。

Result: 研究结果表明，神经元稀疏性和谱可压缩性会影响 $L_	ext{inf}$ 和 $L_2$ 鲁棒性。这些漏洞即使在对抗性训练和迁移学习下也依然存在，并促成了通用对抗性扰动的出现。

Conclusion: 该研究揭示了结构化可压缩性与鲁棒性之间的基本权衡，并为设计兼具效率和安全性的模型提供了新的途径。

Abstract: Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.

</details>


### [208] [Flow Matching Meets Biology and Life Science: A Survey](https://arxiv.org/abs/2507.17731)
*Zihao Li,Zhichen Zeng,Xiao Lin,Feihao Fang,Yanru Qu,Zhe Xu,Zhining Liu,Xuying Ning,Tianxin Wei,Ge Liu,Hanghang Tong,Jingrui He*

Main category: cs.LG

TL;DR: 本文是关于流匹配模型在生物学中应用的首次全面调查，涵盖了其基础、变体和在序列、分子、肽/蛋白质生成中的应用，并讨论了未来方向。


<details>
  <summary>Details</summary>
Motivation: 鉴于流匹配模型作为扩散模型替代品的潜力和日益增长的兴趣，以及其在生物学和生命科学领域的应用，本文旨在全面 survey 近期在生物领域应用流匹配模型的进展。

Method: 对流匹配模型的基础和变体进行了系统性回顾，并将其应用分为三个主要领域：生物序列建模、分子生成与设计、肽和蛋白质生成，并对每个领域进行了深入回顾。

Result: 本文提供了对流匹配模型及其在生物学领域应用的全面调查，涵盖了其基础、变体和在三个主要领域的应用。此外，还总结了常用的数据集和软件工具，并对未来方向进行了讨论。

Conclusion: 本文对流匹配模型在生物学领域的应用进行了全面的调查，总结了其在生物序列建模、分子生成与设计、肽和蛋白质生成方面的进展，并讨论了未来的发展方向。

Abstract: Over the past decade, advances in generative modeling, such as generative
adversarial networks, masked autoencoders, and diffusion models, have
significantly transformed biological research and discovery, enabling
breakthroughs in molecule design, protein generation, drug discovery, and
beyond. At the same time, biological applications have served as valuable
testbeds for evaluating the capabilities of generative models. Recently, flow
matching has emerged as a powerful and efficient alternative to diffusion-based
generative modeling, with growing interest in its application to problems in
biology and life sciences. This paper presents the first comprehensive survey
of recent developments in flow matching and its applications in biological
domains. We begin by systematically reviewing the foundations and variants of
flow matching, and then categorize its applications into three major areas:
biological sequence modeling, molecule generation and design, and peptide and
protein generation. For each, we provide an in-depth review of recent progress.
We also summarize commonly used datasets and software tools, and conclude with
a discussion of potential future directions. The corresponding curated
resources are available at
https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.

</details>


### [209] [Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility](https://arxiv.org/abs/2507.17748)
*Melih Barsbey,Lucas Prieto,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.LG

TL;DR: High learning rates help machine learning models be both robust and efficient, outperforming other techniques by improving feature utilization, class separation, and activation sparsity, potentially by addressing hidden dataset biases.


<details>
  <summary>Details</summary>
Motivation: Achieving robustness and resource-efficiency simultaneously in machine learning models is a significant challenge. This paper explores the potential of high learning rates to address this challenge.

Method: The study positions high learning rates as a key factor for achieving robustness and resource-efficiency, demonstrating their positive effects on representation properties like invariant feature utilization, class separation, and activation sparsity. The research validates these findings across diverse datasets, models, and optimizers.

Result: High learning rates were shown to facilitate both robustness to spurious correlations and network compressibility. They also contribute to desirable representation properties, including invariant feature utilization, class separation, and activation sparsity, outperforming other methods in achieving these properties collectively. The research provides evidence that the benefits of high learning rates in standard classification tasks are likely due to their impact on hidden spurious correlations.

Conclusion: The paper argues that high learning rates can simultaneously achieve robustness to spurious correlations and network compressibility, outperforming other hyperparameters and regularization methods in consistently satisfying these properties. It also suggests that the success of high learning rates in standard classification tasks may stem from their ability to address hidden spurious correlations.

Abstract: Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [210] [Mapple: A Domain-Specific Language for Mapping Distributed Heterogeneous Parallel Programs](https://arxiv.org/abs/2507.17087)
*Anjiang Wei,Rohan Yadav,Hang Song,Wonchan Lee,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: Mapple is a high-level interface that simplifies creating efficient parallel programs for distributed systems, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Optimizing parallel programs for distributed heterogeneous systems is complex and requires significant code modifications. Task-based systems improve modularity but have low-level mapping interfaces.

Method: Introduced Mapple, a high-level, declarative programming interface for mapping distributed applications, with transformation primitives like 'decompose' to minimize communication. Implemented on Legion by translating Mapple mappers to its C++ interface.

Result: Across nine applications, Mapple reduced mapper code size by 14x and enabled performance improvements of up to 1.34x over expert-written C++ mappers. The 'decompose' primitive achieved up to 1.83x improvement over existing dimensionality-resolution heuristics.

Conclusion: Mapple simplifies the development of high-performance mappers for distributed applications, reducing code size and improving performance compared to expert-written C++ mappers and existing heuristics.

Abstract: Optimizing parallel programs for distributed heterogeneous systems remains a
complex task, often requiring significant code modifications. Task-based
programming systems improve modularity by separating performance decisions from
core application logic, but their mapping interfaces are often too low-level.
In this work, we introduce Mapple, a high-level, declarative programming
interface for mapping distributed applications. Mapple provides transformation
primitives to resolve dimensionality mismatches between iteration and processor
spaces, including a key primitive, decompose, that helps minimize communication
volume. We implement Mapple on top of the Legion runtime by translating Mapple
mappers into its low-level C++ interface. Across nine applications, including
six matrix multiplication algorithms and three scientific computing workloads,
Mapple reduces mapper code size by 14X and enables performance improvements of
up to 1.34X over expert-written C++ mappers. In addition, the decompose
primitive achieves up to 1.83X improvement over existing
dimensionality-resolution heuristics. These results demonstrate that Mapple
simplifies the development of high-performance mappers for distributed
applications.

</details>


### [211] [PathWeaver: A High-Throughput Multi-GPU System for Graph-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.17094)
*Sukjin Kim,Seongyeon Park,Si Ung Noh,Junguk Hong,Taehee Kwon,Hunseong Lim,Jinho Lee*

Main category: cs.DC

TL;DR: PathWeaver是一个新颖的多GPU框架，通过创新的技术（如流水线化路径扩展、虚拟节点预取和定向选择）解决了现有ANNS方法在处理大规模数据集时的多GPU可扩展性问题，实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GPU的近似最近邻搜索（ANNS）方法在处理不断增长的数据集规模时，面临多GPU可扩展性不足的问题。现有方法通常将额外的GPU仅用于扩展内存容量，并在每个GPU上独立搜索，效率低下。

Method: PathWeaver框架结合了三种关键技术：1. 流水线化路径扩展：利用GPU到GPU的通信来减少冗余搜索迭代。2. 虚拟节点预取：通过使用代表性数据集来识别最佳查询起点，从而缩小搜索空间。3. 定向选择：在搜索过程早期过滤掉不相关的点，以最小化不必要的内存访问和距离计算。

Result: PathWeaver在95%召回率下，实现了3.24倍的几何平均加速和高达5.30倍的峰值加速，在各种数据集的评估中均优于最先进的多GPU ANNS框架。

Conclusion: PathWeaver通过流水线化路径扩展、虚拟节点预取和定向选择等技术，显著提高了多GPU环境下近似最近邻搜索的性能和可扩展性，在各种数据集上实现了比现有最先进的多GPU近似最近邻搜索框架高达5.30倍的加速。

Abstract: Graph-based Approximate Nearest Neighbor Search (ANNS) is widely adopted in
numerous applications, such as recommendation systems, natural language
processing, and computer vision. While recent works on GPU-based acceleration
have significantly advanced ANNS performance, the ever-growing scale of
datasets now demands efficient multi-GPU solutions. However, the design of
existing works overlooks multi-GPU scalability, resulting in naive approaches
that treat additional GPUs as a means to extend memory capacity for large
datasets. This inefficiency arises from partitioning the dataset and
independently searching for data points similar to the queries in each GPU. We
therefore propose PathWeaver, a novel multi-GPU framework designed to scale and
accelerate ANNS for large datasets. First, we propose pipelining-based path
extension, a GPU-aware pipelining mechanism that reduces prior work's redundant
search iterations by leveraging GPU-to-GPU communication. Second, we design
ghost staging that leverages a representative dataset to identify optimal query
starting points, reducing the search space for challenging queries. Finally, we
introduce direction-guided selection, a data selection technique that filters
irrelevant points early in the search process, minimizing unnecessary memory
accesses and distance computations. Comprehensive evaluations across diverse
datasets demonstrate that PathWeaver achieves 3.24$\times$ geomean speedup and
up to 5.30$\times$ speedup on 95% recall rate over state-of-the-art
multi-GPU-based ANNS frameworks.

</details>


### [212] [BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving](https://arxiv.org/abs/2507.17120)
*Wanyi Zheng,Minxian Xu,Shengye Song,Kejiang Ye*

Main category: cs.DC

TL;DR: BucketServe 是一种创新的 LLM 推理框架，通过其独特的分桶动态批处理方法，显著提高了吞吐量和效率，解决了现有 LLM 服务系统的痛点。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 服务系统在处理资源密集型和延迟敏感型 LLM 推理时，由于其静态或连续批处理策略，存在 GPU 内存利用率低和延迟增加的问题，尤其是在异构工作负载和动态变化的情况下，这可能导致吞吐量不佳和 SLO 违反。

Method: BucketServe 框架通过将请求分组到基于序列长度的同类分桶中，并进行实时批处理大小调整，从而优化 LLM 推理性能。它还采用自适应分桶拆分/合并和优先级感知调度来解决资源碎片和 SLO 合规性问题。

Result: 实验表明，BucketServe 的吞吐量比 UELLM 高 3.58 倍，在满足 80% SLO 的情况下，其处理请求负载的能力比 DistServe 强 1.93 倍，并比 UELLM 展现出 1.975 倍更高的系统负载能力。

Conclusion: BucketServe 通过分桶动态批处理、自适应分桶拆分/合并和优先级感知调度，显著提高了 LLM 推理性能、GPU 内存利用率和 SLO 合规性，在吞吐量和系统负载能力方面均优于现有系统。

Abstract: Large language models (LLMs) have become increasingly popular in various
areas, traditional business gradually shifting from rule-based systems to
LLM-based solutions. However, the inference of LLMs is resource-intensive or
latency-sensitive, posing significant challenges for serving systems. Existing
LLM serving systems often use static or continuous batching strategies, which
can lead to inefficient GPU memory utilization and increased latency,
especially under heterogeneous workloads. These methods may also struggle to
adapt to dynamic workload fluctuations, resulting in suboptimal throughput and
potential service level objective (SLO) violations. In this paper, we introduce
BucketServe, a bucket-based dynamic batching framework designed to optimize LLM
inference performance. By grouping requests into size-homogeneous buckets based
on sequence length, BucketServe minimizes padding overhead and optimizes GPU
memory usage through real-time batch size adjustments preventing out-of-memory
(OOM) errors. It introduces adaptive bucket splitting/merging and
priority-aware scheduling to mitigate resource fragmentation and ensure SLO
compliance. Experiment shows that BucketServe significantly outperforms UELLM
in throughput, achieving up to 3.58x improvement. It can also handle 1.93x more
request load under the SLO attainment of 80% compared with DistServe and
demonstrates 1.975x higher system load capacity compared to the UELLM.

</details>


### [213] [Auto-scaling Approaches for Cloud-native Applications: A Survey and Taxonomy](https://arxiv.org/abs/2507.17128)
*Minxian Xu,Linfeng Wen,Junhan Liao,Huaming Wu,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 对云原生应用的自动扩展技术进行了全面的文献回顾和分类，指出了当前面临的挑战和未来的发展方向，特别是在大型模型、微服务依赖管理和元学习方面。


<details>
  <summary>Details</summary>
Motivation: 云原生应用程序内的交互复杂，服务和负载数量不断变化，对自动扩展方法提出了更高的要求，主要涉及微服务依赖分析、性能分析、异常检测、工作负载表征和任务协同定位等挑战。

Method: 通过系统性地回顾自2020年以来的文献，并提出一个详细的分类法，从基础设施、架构、扩展方法、优化目标和行为建模五个视角对现有研究进行分类。然后，对各种方法的关键特性、优点、局限性和应用场景进行全面比较和深入讨论。

Result: 对云原生应用程序的自动扩展方法进行了全面的文献综述和技术演进探索，并提出了一个包含五个维度的详细分类法，对现有研究进行了深入的比较和讨论，指出了该领域的差距和未来的研究方向。

Conclusion: 本篇论文系统地回顾了云原生应用程序的自动扩展方法，并探讨了技术演进。最后，总结了该领域的 current research state，识别了存在的差距和未解决的挑战，并强调了未来有希望的研究方向，特别是在大型模型应用、微服务依赖管理和元学习技术以增强模型在不同环境中的适用性和适应性方面。

Abstract: The interactions within cloud-native applications are complex, with a
constantly changing number of services and loads, posing higher demands on
auto-scaling approach. This mainly involves several challenges such as
microservices dependency analysis, performance profiling, anomaly detection,
workload characterization and task co-location. Therefore, some advanced
algorithms have been investigated into auto-scaling cloud-native applications
to optimize system and application performance. These algorithms can learn from
historical data and appropriately adjust resource allocation based on the
current environment and load conditions to optimize resource utilization and
system performance. In this paper, we systematically review the literature on
state-of-the-art auto-scaling approaches for cloud-native applications from
2020, and further explore the technological evolution. Additionally, we propose
a detailed taxonomy to categorize current research from five perspectives,
including infrastructure, architecture, scaling methods, optimization
objectives, and behavior modeling. Then, we provide a comprehensive comparison
and in-depth discussion of the key features, advantages, limitations, and
application scenarios of each approach, considering their performance in
diverse environments and under various conditions. Finally, we summarize the
current state of research in this field, identify the gaps and unresolved
challenges, and emphasize promising directions for future exploration,
particularly in areas such as the application of large models, microservice
dependency management, and the use of meta-learning techniques to enhance model
applicability and adaptability across different environments.

</details>


### [214] [BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for MoE-based LLMs](https://arxiv.org/abs/2507.17133)
*Jianmin Hu,Minxian Xu,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: BrownoutServe通过联合专家和动态熔断优化MoE模型推理，显著提升吞吐量并减少延迟和SLO违规。


<details>
  <summary>Details</summary>
Motivation: 现有的混合专家（MoE）模型服务系统存在静态模型放置和缺乏动态工作负载适应性的问题，导致资源利用率不高和延迟增加，尤其是在请求量大的时期。

Method: BrownoutServe提出了一种新的服务框架，其核心在于引入“联合专家”（united experts）和动态“熔断”（brownout）机制。联合专家整合了多个专家知识，减少了专家访问次数和推理延迟。动态熔断机制则能够自适应地调整部分token的处理，从而在满足服务等级目标的前提下优化推理性能。

Result: 在各种工作负载下，BrownoutServe相比vLLM实现了高达2.07倍的吞吐量提升，并将SLO违规减少了90.28%，证明了其在突发流量下的鲁棒性，同时保持了可接受的推理准确性。

Conclusion: BrownoutServe通过引入“联合专家”和动态“熔断”机制，有效提高了MoE模型的推理效率和服务的可靠性，在动态负载和突发流量下表现出色，实现了更高的吞吐量并显著减少了服务等级目标（SLO）的违规。

Abstract: In recent years, the Mixture-of-Experts (MoE) architecture has been widely
applied to large language models (LLMs), providing a promising solution that
activates only a subset of the model's parameters during computation, thereby
reducing overall memory requirements and allowing for faster inference compared
to dense models. Despite these advantages, existing systems still face issues
of low efficiency due to static model placement and lack of dynamic workloads
adaptation. This leads to suboptimal resource utilization and increased
latency, especially during bursty requests periods.
  To address these challenges, this paper introduces BrownoutServe, a novel
serving framework designed to optimize inference efficiency and maintain
service reliability for MoE-based LLMs under dynamic computational demands and
traffic conditions. BrownoutServe introduces "united experts" that integrate
knowledge from multiple experts, reducing the times of expert access and
inference latency. Additionally, it proposes a dynamic brownout mechanism to
adaptively adjust the processing of certain tokens, optimizing inference
performance while guaranteeing service level objectives (SLOs) are met. Our
evaluations show the effectiveness of BrownoutServe under various workloads: it
achieves up to 2.07x throughput improvement compared to vLLM and reduces SLO
violations by 90.28%, showcasing its robustness under bursty traffic while
maintaining acceptable inference accuracy.

</details>


### [215] [Efficient Column-Wise N:M Pruning on RISC-V CPU](https://arxiv.org/abs/2507.17301)
*Chi-Wei Chu,Ding-Yong Hong,Jan-Jan Wu*

Main category: cs.DC

TL;DR: 通过列方向N:M剪枝、XNNPACK优化、im2col与数据打包融合以及AITemplate分析，显著提升了RISC-V平台上CNN的推理性能，同时保持了准确率。


<details>
  <summary>Details</summary>
Motivation: 为了提高深度学习框架中模型计算效率，特别是在CNN的性能瓶颈卷积算子上，通过权重剪枝来减小模型尺寸，并针对RISC-V向量架构进行优化。

Method: 提出了一种列方向N:M剪枝策略，并应用到tile级别，修改XNNPACK以支持RISC-V向量架构上的剪枝模型高效执行，融合了im2col和数据打包操作以减少内存访问和开销，并利用AITemplate的分析技术为每个卷积算子选择最优实现。

Result: 将ResNet的推理吞吐量提高了4.0倍，同时在ImageNet top-1准确率上与稠密基线相当（仅下降2.1%）。

Conclusion: 该方法在ResNet上实现了高达4.0倍的推理吞吐量提升，并将ImageNet top-1准确率在稠密基线内保持在2.1%以内。

Abstract: In deep learning frameworks, weight pruning is a widely used technique for
improving computational efficiency by reducing the size of large models. This
is especially critical for convolutional operators, which often act as
performance bottlenecks in convolutional neural networks (CNNs). However, the
effectiveness of pruning heavily depends on how it is implemented, as different
methods can significantly impact both computational performance and memory
footprint. In this work, we propose a column-wise N:M pruning strategy applied
at the tile level and modify XNNPACK to enable efficient execution of pruned
models on the RISC-V vector architecture. Additionally, we propose fusing the
operations of im2col and data packing to minimize redundant memory accesses and
memory overhead. To further optimize performance, we incorporate AITemplate's
profiling technique to identify the optimal implementation for each
convolutional operator. Our proposed approach effectively increases ResNet
inference throughput by as much as 4.0x, and preserves ImageNet top-1 accuracy
within 2.1\% of the dense baseline.

</details>


### [216] [Multiprocessor Scheduling with Memory Constraints: Fundamental Properties and Finding Optimal Solutions](https://arxiv.org/abs/2507.17411)
*Pál András Papp,Toni Böhnlein,A. N. Yzelman*

Main category: cs.DC

TL;DR: ILP方法在多处理器2级内存调度中优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 研究在多处理器2级内存层次结构中调度通用计算DAG的问题，以同时处理工作负载平衡、通信和缓存限制导致的数据移动。

Method: 提出一种基于整数线性规划（ILP）的整体调度算法，并实验验证其性能。

Result: 证明了分别优化并行化和内存管理可能导致次优解，ILP方法比基线方法能找到更好的解决方案。

Conclusion: 该ILP方法能找到比经典方法更好的解决方案。

Abstract: We study the problem of scheduling a general computational DAG on multiple
processors in a 2-level memory hierarchy. This setting is a natural
generalization of several prominent models in the literature, and it
simultaneously captures workload balancing, communication, and data movement
due to cache size limitations. We first analyze the fundamental properties of
this problem from a theoretical perspective, such as its computational
complexity. We also prove that optimizing parallelization and memory management
separately, as done in many applications, can result in a solution that is a
linear factor away from the optimum.
  On the algorithmic side, we discuss a natural technique to represent and
solve the problem as an Integer Linear Program (ILP). We develop a holistic
scheduling algorithm based on this approach, and we experimentally study its
performance and properties on a small benchmark of computational tasks. Our
results confirm that the ILP-based method can indeed find considerably better
solutions than a baseline which combines classical scheduling algorithms and
memory management policies.

</details>


### [217] [Distributed P2P quantile tracking with relative value error](https://arxiv.org/abs/2507.17458)
*Marco Pulimeno,Italo Epicoco,Massimo Cafaro*

Main category: cs.DC

TL;DR: DUDDSketch is a distributed version of UDDSketch for accurate quantile tracking using a decentralized, gossip-based protocol in P2P networks. It is proven correct and experimentally shown to converge to sequential algorithm results.


<details>
  <summary>Details</summary>
Motivation: Accurate tracking of quantiles in a distributed manner.

Method: A fully decentralized, gossip-based distributed protocol working in the context of unstructured P2P networks.

Result: The algorithm converges to the results provided by the sequential algorithm, which is a fundamental and highly desirable property.

Conclusion: The algorithm converges to the results provided by the sequential algorithm.

Abstract: In this paper we present \textsc{DUDDSketch}, a distributed version of the
\textsc{UDDSketch} algorithm for accurate tracking of quantiles. The algorithm
is a fully decentralized, gossip-based distributed protocol working in the
context of unstructured P2P networks. We discuss the algorithm's design and
formally prove its correctness. We also show, through extensive experimental
results, that the algorithm converges to the results provided by the sequential
algorithm, which is a fundamental and highly desirable property.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [218] [Fractional Spike Differential Equations Neural Network with Efficient Adjoint Parameters Training](https://arxiv.org/abs/2507.16937)
*Chengjie Ge,Yufeng Peng,Xueyang Fu,Qiyu Kang,Xuhao Li,Qixin Zhang,Junhao Ren,Zheng-Jun Zha*

Main category: cs.NE

TL;DR: fspikeDE是一种新的SNN模型，利用分数阶动力学来捕捉长期依赖性，从而提高准确性和鲁棒性，同时保持能效。


<details>
  <summary>Details</summary>
Motivation: 现有的SNN模型通常假设单一时间常数，导致其电压状态仅依赖于其最近的过去值，这可能限制了网络的表达能力。然而，真实神经元表现出受长期相关性和树突结构影响的复杂动力学，表明存在非马尔可夫行为。

Method: 提出fspikeDE模型，使用分数阶动力学捕捉长期依赖性，并通过反向传播优化参数，利用伴随敏感性方法向后求解增强分数阶ODE。

Result: fspikeDE在图像和图数据集上表现优于传统SNN，准确率更高，能效相当，训练内存使用更少，并且对噪声的鲁棒性更强。

Conclusion: fspikeDE通过分数阶动力学捕捉膜电压和脉冲序列中的长期依赖性，在准确性、能效、训练内存和鲁棒性方面优于传统SNN。

Abstract: Spiking Neural Networks (SNNs) draw inspiration from biological neurons to
create realistic models for brain-like computation, demonstrating effectiveness
in processing temporal information with energy efficiency and biological
realism. Most existing SNNs assume a single time constant for neuronal membrane
voltage dynamics, modeled by first-order ordinary differential equations (ODEs)
with Markovian characteristics. Consequently, the voltage state at any time
depends solely on its immediate past value, potentially limiting network
expressiveness. Real neurons, however, exhibit complex dynamics influenced by
long-term correlations and fractal dendritic structures, suggesting
non-Markovian behavior. Motivated by this, we propose the Fractional SPIKE
Differential Equation neural network (fspikeDE), which captures long-term
dependencies in membrane voltage and spike trains through fractional-order
dynamics. These fractional dynamics enable more expressive temporal patterns
beyond the capability of integer-order models. For efficient training of
fspikeDE, we introduce a gradient descent algorithm that optimizes parameters
by solving an augmented fractional-order ODE (FDE) backward in time using
adjoint sensitivity methods. Extensive experiments on diverse image and graph
datasets demonstrate that fspikeDE consistently outperforms traditional SNNs,
achieving superior accuracy, comparable energy efficiency, reduced training
memory usage, and enhanced robustness against noise. Our approach provides a
novel open-sourced computational toolbox for fractional-order SNNs, widely
applicable to various real-world tasks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [219] [Assessing Medical Training Skills via Eye and Head Movements](https://arxiv.org/abs/2507.16819)
*Kayhan Latifzadeh,Luis A. Leiva,Klen Čopič Pucihar,Matjaž Kljun,Iztok Devetak,Lili Steblovnik*

Main category: cs.HC

TL;DR: 通过追踪眼球和头部运动，研究表明该技术能有效区分临床技能水平，并为开发辅助技能评估和培训的计算模型提供了基础。


<details>
  <summary>Details</summary>
Motivation: 为了深入了解临床环境中的技能发展，我们研究了眼球和头部的运动。

Method: 研究共纳入24名参与者，在模拟的婴儿分娩培训课程中，我们追踪了他们的眼球和头部运动，并计算了包括瞳孔反应率、注视时长或角速度在内的关键指标。

Result: 研究结果显示，眼动和头动追踪技术能够有效区分训练有素和未训练的从业者。头部相关特征在F1分数和AUC方面表现为0.85和0.86，而瞳孔相关特征在F1分数和AUC方面表现为0.77和0.85。

Conclusion: 眼动和头动追踪技术可以有效区分训练有素和未训练的从业者，尤其是在分娩任务中。该研究为使用商品眼动追踪眼镜作为传统评估方法的补充设备，在临床环境中支持隐式技能评估和培训的计算模型奠定了基础。

Abstract: We examined eye and head movements to gain insights into skill development in
clinical settings. A total of 24 practitioners participated in simulated baby
delivery training sessions. We calculated key metrics, including pupillary
response rate, fixation duration, or angular velocity. Our findings indicate
that eye and head tracking can effectively differentiate between trained and
untrained practitioners, particularly during labor tasks. For example,
head-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas
pupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results
lay the groundwork for computational models that support implicit skill
assessment and training in clinical settings by using commodity eye-tracking
glasses as a complementary device to more traditional evaluation methods such
as subjective scores.

</details>


### [220] [Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances](https://arxiv.org/abs/2507.17024)
*Chase Stokes,Kylie Lin,Cindy Xiong Bearfield*

Main category: cs.HC

TL;DR: 研究人员正在寻找更有效的方法来了解可视化设计如何影响人们的理解。他们尝试了几种不同的方法，包括让人们对图表进行排名或对图表得出的结论进行评分，并发现这些方法可以部分复制自由回应所产生的结果。他们还尝试使用 AI (GPT-4o) 来完成这项工作，发现 AI 在某些方面有所帮助，但仍有局限性。总的来说，选择和结合正确的方法非常重要。


<details>
  <summary>Details</summary>
Motivation: 这项研究的目的是找到一种可扩展的方法来分析信息可视化中的设计选择，以及它如何影响读者对数据的解读。现有的方法，如众包研究，耗时且成本高昂。

Method: 本研究探讨了四种数据收集方法：自由回应、可视化排序、结论排序和显著性评分，以评估图表的设计。此外，还研究了使用 GPT-4o 作为人类受试者的代理来收集数据和分析的有效性。

Result: 研究发现，排序和评分方法的组合可以作为一种可扩展的替代方案，但不能完全复制自由回应得出的结论。GPT-4o 在显著性评分方面表现最佳，但在其他方面存在局限性。

Conclusion: 有多种数据收集和分析方法可以作为研究图表设计和读者解读之间关系的有效替代方案，但没有一种方法能够完全复制自由回应得出的结论。

Abstract: A growing body of work on visualization affordances highlights how specific
design choices shape reader takeaways from information visualizations. However,
mapping the relationship between design choices and reader conclusions often
requires labor-intensive crowdsourced studies, generating large corpora of
free-response text for analysis. To address this challenge, we explored
alternative scalable research methodologies to assess chart affordances. We
test four elicitation methods from human-subject studies: free response,
visualization ranking, conclusion ranking, and salience rating, and compare
their effectiveness in eliciting reader interpretations of line charts, dot
plots, and heatmaps. Overall, we find that while no method fully replicates
affordances observed in free-response conclusions, combinations of ranking and
rating methods can serve as an effective proxy at a broad scale. The two
ranking methodologies were influenced by participant bias towards certain chart
types and the comparison of suggested conclusions. Rating conclusion salience
could not capture the specific variations between chart types observed in the
other methods. To supplement this work, we present a case study with GPT-4o,
exploring the use of large language models (LLMs) to elicit human-like chart
interpretations. This aligns with recent academic interest in leveraging LLMs
as proxies for human participants to improve data collection and analysis
efficiency. GPT-4o performed best as a human proxy for the salience rating
methodology but suffered from severe constraints in other areas. Overall, the
discrepancies in affordances we found between various elicitation
methodologies, including GPT-4o, highlight the importance of intentionally
selecting and combining methods and evaluating trade-offs.

</details>


### [221] [Evaluation of the effects of frame time variation on VR task performance](https://arxiv.org/abs/2507.17139)
*Benjamin Watson,Victoria Spaulding,Neff Walker,William Ribarsky*

Main category: cs.HC

TL;DR: 帧时间变化会影响 VR 任务性能，尤其是在较低帧率下。


<details>
  <summary>Details</summary>
Motivation: 为了了解帧时间变化（包括偏差和波动周期）对虚拟现实（VR）中任务性能的影响，为 VR 和沉浸式应用的设计师提供指导，帮助他们控制由于图形等复杂性的大幅波动而导致的帧时间变化。

Method: 研究了帧时间变化（在平均帧时间的偏差和波动周期方面）对虚拟环境中任务性能的影响，选择了对当前或未来应用具有代表性的开放和闭环任务。

Result: 在 VR 应用的可接受帧时间范围内，帧时间变化（在振幅和周期方面）对任务性能影响不大。但在被认为是沉浸式 VR 最低要求的帧时间下，帧时间变化会对闭环任务性能产生显著影响。

Conclusion: 在许多应用的可接受帧时间范围内，相当大的振幅偏差和相当宽的周期变化不会显著影响任务性能。然而，在通常被认为是沉浸式 VR 最低要求的帧时间下，帧时间变化确实会对闭环任务性能产生显著影响。

Abstract: We present a first study of the effects of frame time variations, in both
deviation around mean frame times and period of fluctuation, on task
performance in a virtual environment (VE). Chosen are open and closed loop
tasks that are typical for current applications or likely to be prominent in
future ones. The results show that at frame times in the range deemed
acceptable for many applications, fairly large deviations in amplitude over a
fairly wide range of periods do not significantly affect task performance.
However, at a frame time often considered a minimum for immersive VR, frame
time variations do produce significant effects on closed loop task performance.
The results will be of use to designers of VEs and immersive applications, who
often must control frame time variations due to large fluctuations of
complexity (graphical and otherwise) in the VE.

</details>


### [222] [HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery](https://arxiv.org/abs/2507.17209)
*Haoran Jiang,Shaohan Shi,Yunjie Yao,Chang Jiang,Quan Li*

Main category: cs.HC

TL;DR: HypoChainer框架通过结合人类智慧、LLM推理和知识图谱，解决了科学发现中处理海量数据和复杂性生成的挑战，提高了假设生成和验证的效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代科学发现面临着整合海量异构知识的挑战，这对于生物医学和药物开发的突破至关重要。传统假设驱动的研究虽然有效，但受到人类认知限制、生物系统复杂性以及反复试验的高成本的制约。尽管深度学习模型（特别是GNNs）和大型语言模型（LLMs）在加速预测生成和假设生成方面展现了潜力，但它们在可扩展性、可靠性（幻觉问题）和结构化知识接地方面存在局限性。因此，需要一个能克服这些挑战的框架。

Method: HypoChainer框架通过三个阶段进行工作：1. 探索和情境化：利用检索增强LLM（RAGs）和降维技术，结合交互式解释，帮助专家导航大规模GNN预测。2. 假设链形成：专家通过检查围绕预测的KG关系和语义链接的实体，并结合LLM和KG的建议，迭代地完善假设。3. 验证优先级排序：基于KG支持的证据对完善后的假设进行过滤，以识别高优先级的实验候选者，并利用视觉分析进一步加强推理中的薄弱环节。

Result: 通过在两个领域的案例研究和专家访谈，证明了HypoChainer的有效性，突显了其在支持可解释、可扩展和知识驱动的科学发现方面的潜力。

Conclusion: HypoChainer是一个协作可视化框架，通过整合人类专业知识、LLM驱动的推理和知识图谱（KGs），能够增强药物开发等领域的假设生成和验证能力，从而支持可解释、可扩展且基于知识的科学发现。

Abstract: Modern scientific discovery faces growing challenges in integrating vast and
heterogeneous knowledge critical to breakthroughs in biomedicine and drug
development. Traditional hypothesis-driven research, though effective, is
constrained by human cognitive limits, the complexity of biological systems,
and the high cost of trial-and-error experimentation. Deep learning models,
especially graph neural networks (GNNs), have accelerated prediction
generation, but the sheer volume of outputs makes manual selection for
validation unscalable. Large language models (LLMs) offer promise in filtering
and hypothesis generation, yet suffer from hallucinations and lack grounding in
structured knowledge, limiting their reliability. To address these issues, we
propose HypoChainer, a collaborative visualization framework that integrates
human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance
hypothesis generation and validation. HypoChainer operates in three stages:
First, exploration and contextualization -- experts use retrieval-augmented
LLMs (RAGs) and dimensionality reduction to navigate large-scale GNN
predictions, assisted by interactive explanations. Second, hypothesis chain
formation -- experts iteratively examine KG relationships around predictions
and semantically linked entities, refining hypotheses with LLM and KG
suggestions. Third, validation prioritization -- refined hypotheses are
filtered based on KG-supported evidence to identify high-priority candidates
for experimentation, with visual analytics further strengthening weak links in
reasoning. We demonstrate HypoChainer's effectiveness through case studies in
two domains and expert interviews, highlighting its potential to support
interpretable, scalable, and knowledge-grounded scientific discovery.

</details>


### [223] [OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena](https://arxiv.org/abs/2507.17218)
*Yang Ouyang,Yuchen Wu,Xiyuan Wang,Laixin Xie,Weicong Cheng,Jianping Gan,Quan Li,Xiaojuan Ma*

Main category: cs.HC

TL;DR: OceanVive是一个沉浸式可视化系统，用于沟通复杂的海洋现象，如缺氧和酸化。它使用交互式元素和空间叙事来帮助公众理解，并通过专家访谈进行了验证。


<details>
  <summary>Details</summary>
Motivation: 沟通海洋现象（如缺氧和酸化）的复杂性对海洋科学来说是一个持续的挑战。尽管传感技术和计算模型取得了进展，但传统的静态可视化和基于文本的报告等格式在传达海洋变化动态方面往往不足。

Method: 提出了一种名为OceanVive的沉浸式和交互式可视化系统，该系统将复杂的海洋数据集转化为可导航的空间叙事。该系统包括一个用于在大型屏幕上管理沉浸式内容的平板电脑探索面板，并集成了自适应视觉编码、情境化叙事和直观的导航路径。

Result: OceanVive系统能够将复杂的海洋数据集转化为可导航的空间叙事，并有效传达海洋变化动态。

Conclusion: 该系统通过专家访谈得到验证，证明了其在加强科学传播和促进公众更深入理解方面的潜力。

Abstract: Communicating the complexity of oceanic phenomena-such as hypoxia and
acidification-poses a persistent challenge for marine science. Despite advances
in sensing technologies and computational models, conventional formats like
static visualizations and text-based reports often fall short in conveying the
dynamics of ocean changes. To address this gap, we present OceanVive, an
immersive and interactive visualization system that transforms complex ocean
datasets into navigable spatial narratives. OceanVive incorporates an
exploratory panel on a table-sized tablet for managing immersive content on a
large screen and integrates adaptive visual encodings, contextual storytelling,
and intuitive navigation pathways to support effective communication. We
validate the system through expert interviews, demonstrating its potential to
enhance science communication and promote deeper public understanding.

</details>


### [224] [A "watch your replay videos" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection](https://arxiv.org/abs/2507.17226)
*Sarah "Magz" Fernandez,Greg L Nelson*

Main category: cs.HC

TL;DR: 生成式人工智能正在颠覆计算机教育。本研究提出了一种比较视频反思的方法，帮助学生理解人工智能如何改变他们的编程过程，并培养了他们的元认知技能。


<details>
  <summary>Details</summary>
Motivation: 大多数干预措施侧重于教授生成式人工智能的使用，而不是帮助学生理解人工智能如何改变他们的编程过程。

Method: "本研究设计并部署了一个新颖的比较视频反思作业，该作业改编了描述、检查、然后阐述（DEAL）学习框架。在入门级软件工程课程中，学生在团队项目期间录制了两次编程过程：第一次不使用生成式人工智能，第二次使用生成式人工智能。然后，学生使用一套结构化的反思问题来分析自己的视频，问题内容包括编程过程以及寻求人类、互联网和人工智能的帮助。我们对反思进行了定性主题分析。"

Result: 学生们对规划、调试和寻求帮助的行为有了超越人工智能使用的认识。学生们报告说，他们学会了在编写或生成代码之前放慢速度并进行理解，认识到自己解决问题方法的模式，并阐述了具体的改进过程。学生们还学习并反思了人工智能的局限性和缺点，以及更批判性地使用人工智能的策略，包括更好地进行提示，以及利用人工智能使学习受益，而不仅仅是为了完成任务。出乎意料的是，比较反思也促进了对不涉及人工智能使用的编程的反思，甚至导致学生自发地设定未来目标，采用视频和其他定期反思。

Conclusion: 结构化反思视频编程过程可以培养元认知技能，这对于使用和不使用生成式人工智能进行编程以及在我们不断发展的领域中进行终身学习至关重要。

Abstract: Generative AI is disrupting computing education. Most interventions focus on
teaching GenAI use rather than helping students understand how AI changes their
programming process. We designed and deployed a novel comparative video
reflection assignment adapting the Describe, Examine, then Articulate Learning
(DEAL) framework. In an introductory software engineering course, students
recorded themselves programming during their team project two times: first
without, then with using generative AI. Students then analyzed their own videos
using a scaffolded set of reflection questions, including on their programming
process and human, internet, and AI help-seeking. We conducted a qualitative
thematic analysis of the reflections, finding students developed insights about
planning, debugging, and help-seeking behaviors that transcended AI use.
Students reported learning to slow down and understand before writing or
generating code, recognized patterns in their problem-solving approaches, and
articulated specific process improvements. Students also learned and reflected
on AI limits and downsides, and strategies to use AI more critically, including
better prompting but also to benefit their learning instead of just completing
tasks. Unexpectedly, the comparative reflection also scaffolded reflection on
programming not involving AI use, and even led to students spontaneously
setting future goals to adopt video and other regular reflection. This work
demonstrates structured reflection on programming session videos can develop
metacognitive skills essential for programming with and without generative AI
and also lifelong learning in our evolving field.

</details>


### [225] [Designing for Learning with Generative AI is a Wicked Problem: An Illustrative Longitudinal Qualitative Case Series](https://arxiv.org/abs/2507.17230)
*Clara Scalzer,Saurav Pokhrel,Sara Hunt,Greg L Nelson*

Main category: cs.HC

TL;DR: 在AI时代，培养学生在学习、技能、伦理和职业发展方面面临复杂挑战，需要综合考量，而非单独优化。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨在生成式人工智能（GenAI）日益影响未来职业的背景下，计算教育者如何有效培养学生，以支持他们的学习、动机、伦理观和职业发展，应对这一“棘手”的挑战。

Method: 本研究采用纵向定性研究方法，分析了学生在整合了生成式人工智能（GenAI）的创意媒体课程中的学习经历，重点关注了他们对GenAI技能、伦理和职业发展的影响。

Result: 研究发现，提高GenAI使用技能可能会降低学生的伦理意识，例如，有学生从避免使用GenAI转变为过度依赖，并承认学习效果下降。反之，提高伦理意识可能会阻碍GenAI技能的学习，例如，有学生因担忧环境问题而限制使用，从而影响了技能发展，并对GenAI可能取代创意职业感到担忧。此外，即使提高了GenAI熟练度，学生的职业信心并未得到提升。

Conclusion: 支持学生在生成式人工智能时代的学习、技能、伦理和职业发展是一个“棘手”的难题，需要进行多维度评估和设计，而不是单独优化任何一个方面。

Abstract: Students continue their education when they feel their learning is meaningful
and relevant for their future careers. Computing educators now face the
challenge of preparing students for careers increasingly shaped by generative
AI (GenAI) with the goals of supporting their learning, motivation, ethics, and
career development. Our longitudinal qualitative study of students in a
GenAI-integrated creative media course shows how this is a "wicked" problem:
progress on one goal can then impede progress on other goals. Students
developed concerning patterns despite extensive instruction in critical and
ethical GenAI use including prompt engineering, ethics and bias, and industry
panels on GenAI's career impact. We present an analysis of two students'
experiences to showcase this complexity. Increasing GenAI use skills can lower
ethics; for example, Pat started from purposefully avoiding GenAI use, to
dependency. He described himself as a "notorious cheater" who now uses GenAi to
"get all the right answers" while acknowledging he's learning less. Increasing
ethical awareness can lower the learning of GenAI use skills; for example,
Jay's newfound environmental concerns led to self-imposed usage limits that
impeded skill development, and new serious fears that GenAI would eliminate
creative careers they had been passionate about. Increased GenAI proficiency, a
potential career skill, did not improve their career confidence. These findings
suggest that supporting student development in the GenAI era is a "wicked"
problem requiring multi-dimensional evaluation and design, rather than
optimizing learning, GenAI skills, ethics, or career motivation individually.

</details>


### [226] [High-Density EEG Enables the Fastest Visual Brain-Computer Interfaces](https://arxiv.org/abs/2507.17242)
*Gege Ming,Weihua Pei,Sen Tian,Xiaogang Chen,Xiaorong Gao,Yijun Wang*

Main category: cs.HC

TL;DR: 本研究提出了一种新的BCI编码方法，并结合高密度EEG，显著提高了信息传输率，为开发实用的高速BCI系统铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉BCI系统信息传输率（ITR）不足以满足实际应用的需求，空间信息的利用不足，因为记录方法的空间分辨率有限，限制了对大脑信号丰富的时空动态的捕获。

Method: 提出了一种频率-相位-空间融合编码方法，并结合了256通道高密度脑电图（EEG）记录，以开发高速BCI系统。

Result: 在经典的频率-相位编码40目标BCI范式中，256-66、128-32和64-21电极配置带来了理论ITR比传统的64-9设置分别提高了83.66%、79.99%和55.50%。在提出的频率-相位-空间编码200目标BCI范式中，这些增幅分别达到了195.56%、153.08%和103.07%。在线BCI系统实现了平均472.7 bpm的实际ITR。

Conclusion: 这项研究证明了高密度脑电图在解码视觉刺激的时空信息方面的重要作用和巨大潜力。

Abstract: Brain-computer interface (BCI) technology establishes a direct communication
pathway between the brain and external devices. Current visual BCI systems
suffer from insufficient information transfer rates (ITRs) for practical use.
Spatial information, a critical component of visual perception, remains
underexploited in existing systems because the limited spatial resolution of
recording methods hinders the capture of the rich spatiotemporal dynamics of
brain signals. This study proposed a frequency-phase-space fusion encoding
method, integrated with 256-channel high-density electroencephalogram (EEG)
recordings, to develop high-speed BCI systems. In the classical frequency-phase
encoding 40-target BCI paradigm, the 256-66, 128-32, and 64-21 electrode
configurations brought theoretical ITR increases of 83.66%, 79.99%, and 55.50%
over the traditional 64-9 setup. In the proposed frequency-phase-space encoding
200-target BCI paradigm, these increases climbed to 195.56%, 153.08%, and
103.07%. The online BCI system achieved an average actual ITR of 472.7 bpm.
This study demonstrates the essential role and immense potential of
high-density EEG in decoding the spatiotemporal information of visual stimuli.

</details>


### [227] [Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations](https://arxiv.org/abs/2507.17248)
*Xiaoan Liu,Difan Jia,Xianhao Carton Liu,Mar Gonzalez-Franco,Chen Zhu-Tian*

Main category: cs.HC

TL;DR: Reality Proxy 通过使用代理（物理对象的抽象表示）来克服混合现实中与拥挤、遥远或部分遮挡的物理对象的交互挑战，从而实现更流畅、更强大的交互。


<details>
  <summary>Details</summary>
Motivation: 在混合现实（MR）中与现实世界物体进行交互时，当物体密集、遥远或部分被遮挡时，选择和操作会变得困难，因为交互直接受限于物理对象的物理约束。

Method: 通过引入代理（现实世界对象的抽象表示）将交互目标从物理对象无缝地转移到代理。利用人工智能丰富代理，包含与相应物理对象相关的语义属性和分层空间关系，从而实现诸如浏览、基于属性的过滤、导航嵌套组和复杂的多对象选择等新颖的交互。

Result: Reality Proxy 系统支持新颖的交互，例如浏览、基于属性的过滤、导航嵌套组和复杂的多对象选择，而无需新的手势或菜单系统。该系统在办公信息检索、大规模空间导航和多无人机控制等多种场景中得到了验证。

Conclusion: 代理式抽象为未来的混合现实系统提供了一种强大且可推广的交互范式。

Abstract: Interacting with real-world objects in Mixed Reality (MR) often proves
difficult when they are crowded, distant, or partially occluded, hindering
straightforward selection and manipulation. We observe that these difficulties
stem from performing interaction directly on physical objects, where input is
tightly coupled to their physical constraints. Our key insight is to decouple
interaction from these constraints by introducing proxies-abstract
representations of real-world objects. We embody this concept in Reality Proxy,
a system that seamlessly shifts interaction targets from physical objects to
their proxies during selection. Beyond facilitating basic selection, Reality
Proxy uses AI to enrich proxies with semantic attributes and hierarchical
spatial relationships of their corresponding physical objects, enabling novel
and previously cumbersome interactions in MR - such as skimming,
attribute-based filtering, navigating nested groups, and complex multi object
selections - all without requiring new gestures or menu systems. We demonstrate
Reality Proxy's versatility across diverse scenarios, including office
information retrieval, large-scale spatial navigation, and multi-drone control.
An expert evaluation suggests the system's utility and usability, suggesting
that proxy-based abstractions offer a powerful and generalizable interaction
paradigm for future MR systems.

</details>


### [228] [EventLines: Time Compression for Discrete Event Timelines](https://arxiv.org/abs/2507.17320)
*Yuet Ling Wong,Niklas Elmqvist*

Main category: cs.HC

TL;DR: EventLines是一种新的技术，它通过调整时间轴来更好地显示事件序列，解决了传统时间轴在表示突发事件时的不足。


<details>
  <summary>Details</summary>
Motivation: 标准的时间轴图表无法充分表示离散事件序列中的突发行为，导致在事件爆发期间图表混乱，而在其他区域则利用不足。

Method: EventLines是一种新颖的技术，通过动态调整时间尺度来匹配潜在的事件分布，从而更有效地利用屏幕空间。为解决非线性时间缩放的挑战，EventLines采用时间轴的视觉表示本身来传达变化的尺度。

Result: 在通过众包图形感知研究中，我们研究了不同的时间尺度表示如何影响时间感知，并展示了EventLines的效果。

Conclusion: EventLines通过动态调整时间尺度以匹配事件分布，实现了更高效的屏幕空间利用，并能有效处理非线性时间尺度带来的挑战，其时间轴的视觉表示方式能够传达变化的尺度。

Abstract: Discrete event sequences serve as models for numerous real-world datasets,
including publications over time, project milestones, and medication dosing
during patient treatments. These event sequences typically exhibit bursty
behavior, where events cluster together in rapid succession, interspersed with
periods of inactivity. Standard timeline charts with linear time axes fail to
adequately represent such data, resulting in cluttered regions during event
bursts while leaving other areas unutilized. We introduce EventLines, a novel
technique that dynamically adjusts the time scale to match the underlying event
distribution, enabling more efficient use of screen space. To address the
challenges of non-linear time scaling, EventLines employs the time axis's
visual representation itself to communicate the varying scale. We present
findings from a crowdsourced graphical perception study that examines how
different time scale representations influence temporal perception.

</details>


### [229] [Layered Interactions: Exploring Non-Intrusive Digital Craftsmanship Design Through Lacquer Art Interfaces](https://arxiv.org/abs/2507.17430)
*Yan Dong,Hanjie Yu,Yanran Chen,Zipeng Zhang,Qiong Wu*

Main category: cs.HC

TL;DR: 本研究提出“分层交互”方法，将HCI技术融入传统漆器，创造出融合技术与工艺的交互界面，增强了实用性和情感体验，促进了跨界合作。


<details>
  <summary>Details</summary>
Motivation: 将技术与工艺的特点相结合是数字工艺领域的一个关键问题。本研究旨在探索如何在传统漆器工艺中融合HCI技术，以增强传统工艺在现代数字环境中的适应性和实用性。

Method: 本研究提出了一种名为“分层交互”（Layered Interactions）的设计方法，该方法将人体-计算机交互（HCI）技术与传统漆器工艺相结合。通过利用漆器的多层结构和材料特性，将交互式电路和可编程硬件嵌入到漆器层中，创造出支持多样化交互的实体界面。此外，研究还开发了漆器工具包，并进行了用户实验和半结构化访谈。

Result: 研究表明，分层交互方法不仅使技术更容易被传统工匠接受，还增强了交互界面的物质属性和情感吸引力。此外，该方法促进了工匠和技术专家之间的跨学科学习与协作。该研究为HCI社区提供了新的跨学科视角，拓宽了交互界面的材料和设计选择。

Conclusion: 该研究将HCI技术与传统漆器工艺相结合，通过分层交互设计方法，在漆器中嵌入交互电路和可编程硬件，创造出支持多样化交互的实体界面。该方法增强了传统工艺在现代数字环境中的适应性和实用性，使技术更容易被传统工匠接受，同时提升了交互界面的物质性和情感品质，促进了工匠与技术专家之间的相互学习与合作。该研究为HCI领域带来了跨学科视角，拓宽了交互界面的物质和设计可能性。

Abstract: Integrating technology with the distinctive characteristics of craftsmanship
has become a key issue in the field of digital craftsmanship. This paper
introduces Layered Interactions, a design approach that seamlessly merges
Human-Computer Interaction (HCI) technologies with traditional lacquerware
craftsmanship. By leveraging the multi-layer structure and material properties
of lacquerware, we embed interactive circuits and integrate programmable
hardware within the layers, creating tangible interfaces that support diverse
interactions. This method enhances the adaptability and practicality of
traditional crafts in modern digital contexts. Through the development of a
lacquerware toolkit, along with user experiments and semi-structured
interviews, we demonstrate that this approach not only makes technology more
accessible to traditional artisans but also enhances the materiality and
emotional qualities of interactive interfaces. Additionally, it fosters mutual
learning and collaboration between artisans and technologists. Our research
introduces a cross-disciplinary perspective to the HCI community, broadening
the material and design possibilities for interactive interfaces.

</details>


### [230] [SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2507.17524)
*Jiahao Tang,Youjun Li,Xiangting Fan,Yangxuan Zheng,Siyuan Lu,Xueping Li,Peng Fang,Chenxi Li,Zi-Gang Huang*

Main category: cs.HC

TL;DR: 提出了一种名为SDC-Net的新型无监督域适应网络，用于解决脑电图情感识别中的受试间差异和标签稀疏问题，并在多个数据集上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 解决了脑电图情感识别中存在的显著的受试间差异和目标域缺乏标签的问题。

Method: 提出了一种名为SDC-Net的无监督语义-动态一致性域适应网络，并采用了同主体同试验的Mixup策略、动态分布对齐模块以及双域相似性一致性学习机制。

Result: 在SEED、SEED-IV和Faced三个广泛使用的脑电图基准数据集上进行了大量实验，与现有的无监督域适应方法相比，SDC-Net在情感识别方面取得了最先进的性能。

Conclusion: SDC-Net在跨主语和跨会话的脑电图情感识别方面取得了最先进的性能，提高了准确性和泛化能力，为个性化情感脑机接口的实际应用奠定了基础。

Abstract: Electroencephalography(EEG) based emotion recognition holds great promise for
affective brain-computer interfaces (aBCIs), yet practical deployment remains
challenging due to substantial inter-subject variability and the lack of
labeled data in target domains. To overcome these limitations, we present a
novel unsupervised Semantic-Dynamic Consistency domain adaptation network for
fully label-free cross-subject EEG emotion recognition. First, we introduce a
Same-Subject Same-Trial Mixup strategy that generates augmented samples via
intra-trial interpolation, enhancing data diversity while explicitly preserving
individual identity to mitigate label ambiguity. Second, we construct a dynamic
distribution alignment module in reproducing kernel Hilbert space (RKHS),
jointly aligning marginal and conditional distributions through multi-objective
kernel mean embedding, and leveraging a confidence-aware pseudo-labeling
strategy to ensure stable adaptation. Third, we propose a dual-domain
similarity consistency learning mechanism that enforces cross-domain structural
constraints based on latent pairwise similarities, enabling semantic boundary
learning without relying on temporal synchronization or label priors. To
validate the effectiveness and robustness of the proposed SDC-Net, extensive
experiments are conducted on three widely used EEG benchmark datasets: SEED,
SEED-IV, and Faced. Comparative results against existing unsupervised domain
adaptation methods demonstrate that SDC-Net achieves state-of-the-art
performance in emotion recognition under both cross-subject and cross-session
conditions. This advancement significantly improves the accuracy and
generalization capability of emotion decoding, and lays a solid foundation for
real-world applications of personalized affective brain-computer interfaces
(aBCIs). The source code will be released at
https://github.com/XuanSuTrum/SDC-Net.

</details>


### [231] [Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams](https://arxiv.org/abs/2507.17543)
*Xue Wen Tan,Kenneth See,Stanley Kok*

Main category: cs.HC

TL;DR: 本研究提出了一种名为ASR的AI框架，利用ScamGPT-J模型来检测和理解短信诈骗，提高了安全性，但发现最脆弱的用户反而最不愿意接受AI帮助。


<details>
  <summary>Details</summary>
Motivation: 解决日益增长的短信诈骗对用户安全和财务造成的威胁。

Method: 提出了一种名为“预期、模拟、推理”（ASR）的生成式AI框架，利用大型语言模型（LLM）预测诈骗者回复、创建模拟诈骗对话，并为用户提供实时、可解释的支持。该框架通过微调特定领域的语言模型ScamGPT-J，并在包含多种诈骗类型的新型高质量诈骗对话数据集上进行了训练。

Result: ASR框架显著提高了诈骗检测能力，尤其是在应对诸如求职诈骗等具有挑战性的场景时。实验还发现了用户易感性和对AI辅助看法的关键人口统计学模式，并揭示了风险最高的用户群体往往对AI支持的接受度最低。

Conclusion: ASR框架通过生成式AI（特别是ScamGPT-J）能够有效识别和理解短信诈骗，提高了诈骗检测能力，并揭示了用户在AI辅助防范中的易感性和接受度模式，强调了以用户为中心的设计在AI驱动的欺诈防范中的重要性。

Abstract: The rapid growth of messaging scams creates an escalating challenge for user
security and financial safety. In this paper, we present the Anticipate,
Simulate, Reason (ASR) framework, a generative AI method that enables users to
proactively identify and comprehend scams within instant messaging platforms.
Using large language models, ASR predicts scammer responses, creates realistic
scam conversations, and delivers real-time, interpretable support to end-users.
We develop ScamGPT-J, a domain-specific language model fine-tuned on a new,
high-quality dataset of scam conversations covering multiple scam types.
Thorough experimental evaluation shows that the ASR framework substantially
enhances scam detection, particularly in challenging contexts such as job
scams, and uncovers important demographic patterns in user vulnerability and
perceptions of AI-generated assistance. Our findings reveal a contradiction
where those most at risk are often least receptive to AI support, emphasizing
the importance of user-centered design in AI-driven fraud prevention. This work
advances both the practical and theoretical foundations for interpretable,
human-centered AI systems in combating evolving digital threats.

</details>


### [232] [Explainable AI for Collaborative Assessment of 2D/3D Registration Quality](https://arxiv.org/abs/2507.17597)
*Sue Min Cho,Alexander Do,Russell H. Taylor,Mathias Unberath*

Main category: cs.HC

TL;DR: 该研究提出了一种用于2D/3D配准质量验证的人工智能框架，并结合了可解释性功能。结果表明，可解释性对性能的提升有限，但未来的研究方向具有潜力。


<details>
  <summary>Details</summary>
Motivation: 随着手术的数字化转型，算法的准确性至关重要，因为即使是很小的错误也可能导致严重的后果。然而，传统的基于可视化的方法不足以可靠地检测2D/3D配准的错误。因此，需要一种新的方法来确保配准的准确性，并为人类操作员提供决策支持。

Method: 提出了一种专门用于2D/3D配准质量验证的人工智能（AI）框架，并辅以可解释性功能以阐明模型的决策过程。通过以算法为中心和以人为中心的人工智能评估，系统地比较了仅人工智能、仅人工、人工-人工智能和人工-XAI四种条件。

Result: 在仅人工智能、仅人工、人工-人工智能和人工-XAI这四种条件下，可解释性特征在用户信任度和覆盖人工智能错误方面有所适度提高，但在总体性能上并未超过单独的人工智能。

Conclusion: 虽然可解释性特征在用户信任度和覆盖人工智能错误错误率方面有所适度提高，但它们在总体性能上并未超过单独的人工智能。然而，扩展算法设计和人类-XAI协作元素以实现更稳健的2D/3D配准质量保证仍然充满希望。

Abstract: As surgery embraces digital transformation--integrating sophisticated
imaging, advanced algorithms, and robotics to support and automate complex
sub-tasks--human judgment of system correctness remains a vital safeguard for
patient safety. This shift introduces new "operator-type" roles tasked with
verifying complex algorithmic outputs, particularly at critical junctures of
the procedure, such as the intermediary check before drilling or implant
placement. A prime example is 2D/3D registration, a key enabler of image-based
surgical navigation that aligns intraoperative 2D images with preoperative 3D
data. Although registration algorithms have advanced significantly, they
occasionally yield inaccurate results. Because even small misalignments can
lead to revision surgery or irreversible surgical errors, there is a critical
need for robust quality assurance. Current visualization-based strategies alone
have been found insufficient to enable humans to reliably detect 2D/3D
registration misalignments. In response, we propose the first artificial
intelligence (AI) framework trained specifically for 2D/3D registration quality
verification, augmented by explainability features that clarify the model's
decision-making. Our explainable AI (XAI) approach aims to enhance informed
decision-making for human operators by providing a second opinion together with
a rationale behind it. Through algorithm-centric and human-centered
evaluations, we systematically compare four conditions: AI-only, human-only,
human-AI, and human-XAI. Our findings reveal that while explainability features
modestly improve user trust and willingness to override AI errors, they do not
exceed the standalone AI in aggregate performance. Nevertheless, future work
extending both the algorithmic design and the human-XAI collaboration elements
holds promise for more robust quality assurance of 2D/3D registration.

</details>


### [233] [Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills](https://arxiv.org/abs/2507.17688)
*Mohammad Nur Hossain Khan,David creswell,Jordan Albert,Patrick O'Connell,Shawn Fallon,Mathew Polowitz,Xuhai "orson" Xu,Bashima islam*

Main category: cs.HC

TL;DR: 该研究提出了一种利用智能手机加速度计跟踪呼吸和估计正念技能的方法，并通过用户研究证明了其在提高可用性和技能发展方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索呼吸生物信号反馈和正念技能估计是否能增强系统可用性和技能发展，以解决智能手机应用程序用户长期参与度低的问题。

Method: 开发了一种基于智能手机加速度计的呼吸跟踪算法，无需额外的可穿戴设备。引入了第一个基于加速度计衍生的呼吸数据来估计正念技能（专注、感官清晰度和情绪平等）的量化框架。在受控和现实世界的环境中，对 261 次正念课程的算法进行了开发和测试。

Result: 与使用标准应用程序的对照组相比，接受生物信号反馈的实验组的用户研究表明，呼吸反馈提高了系统可用性。呼吸跟踪模型实现了 1.6 次/分钟的平均绝对误差（MAE），与真实数据非常吻合。正念技能估计在跟踪技能进展方面达到了 80%-84% 的 F1 分数。

Conclusion: 通过将呼吸跟踪和正念技能估计集成到商业应用程序中，证明了智能手机传感器在增强数字正念培训方面的潜力。

Abstract: Mindfulness training is widely recognized for its benefits in reducing
depression, anxiety, and loneliness. With the rise of smartphone-based
mindfulness apps, digital meditation has become more accessible, but sustaining
long-term user engagement remains a challenge. This paper explores whether
respiration biosignal feedback and mindfulness skill estimation enhance system
usability and skill development. We develop a smartphone's accelerometer-based
respiration tracking algorithm, eliminating the need for additional wearables.
Unlike existing methods, our approach accurately captures slow breathing
patterns typical of mindfulness meditation. Additionally, we introduce the
first quantitative framework to estimate mindfulness skills-concentration,
sensory clarity, and equanimity-based on accelerometer-derived respiration
data. We develop and test our algorithms on 261 mindfulness sessions in both
controlled and real-world settings. A user study comparing an experimental
group receiving biosignal feedback with a control group using a standard app
shows that respiration feedback enhances system usability. Our respiration
tracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,
closely aligning with ground truth data, while our mindfulness skill estimation
attains F1 scores of 80-84% in tracking skill progression. By integrating
respiration tracking and mindfulness estimation into a commercial app, we
demonstrate the potential of smartphone sensors to enhance digital mindfulness
training.

</details>


### [234] [DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models](https://arxiv.org/abs/2507.17734)
*Liwenhan Xie,Yanna Lin,Can Liu,Huamin Qu,Xinhuan Shu*

Main category: cs.HC

TL;DR: DataWink是一个能让普通用户通过改编优秀案例来创建定制化可视化图表的系统。它使用大型多模态模型提取设计信息，并允许用户通过对话和控件进行修改，用户研究表明它易于学习且效果好。


<details>
  <summary>Details</summary>
Motivation: 为解决缺乏设计专业知识或不熟悉可视化工具的用户在创建美观数据可视化方面的挑战。

Method: DataWink系统通过大型多模态模型（LMM）提取SVG可视化示例中的数据编码，并建立一个介于SVG和可视化程序之间的中间表示。用户可以通过对话代理表达改编目标，并通过动态生成的控件来调整数据映射和视觉设计元素。

Result: 用户研究（N=12）结果表明，DataWink在学习性和个性化创作任务方面表现出色，证明了基于示例的方法在普及可视化创作方面的潜力。

Conclusion: DataWink利用基于示例的、由大型多模态模型驱动的方法，能够让没有设计专业知识的用户也能创建出美观的数据可视化，并在用户研究中被证明具有良好的可学习性和有效性。

Abstract: Creating aesthetically pleasing data visualizations remains challenging for
users without design expertise or familiarity with visualization tools. To
address this gap, we present DataWink, a system that enables users to create
custom visualizations by adapting high-quality examples. Our approach combines
large multimodal models (LMMs) to extract data encoding from existing SVG-based
visualization examples, featuring an intermediate representation of
visualizations that bridges primitive SVG and visualization programs. Users may
express adaptation goals to a conversational agent and control the visual
appearance through widgets generated on demand. With an interactive interface,
users can modify both data mappings and visual design elements while
maintaining the original visualization's aesthetic quality. To evaluate
DataWink, we conduct a user study (N=12) with replication and free-form
exploration tasks. As a result, DataWink is recognized for its learnability and
effectiveness in personalized authoring tasks. Our results demonstrate the
potential of example-driven approaches for democratizing visualization
creation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [235] [Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems](https://arxiv.org/abs/2507.16843)
*Zhongsheng Wang,Sijie Wang,Jia Wang,Yung-I Liang,Yuxi Zhang,Jiamou Liu*

Main category: cs.SD

TL;DR: 提出一种微调ASR模型的方法，以提高其在特定行业CRM系统中的性能，并已成功应用于实际工业场景。


<details>
  <summary>Details</summary>
Motivation: 通用预训练ASR模型难以有效处理特定行业语音识别任务，而准确识别客户类型和提供个性化服务是CRM系统设计的关键，这需要克服识别客户声音和意图的挑战。

Method: 提出了一种微调特定行业ASR模型的方法。

Result: 实验结果表明，该方法大幅提升了ASR模型在行业CRM系统中的辅助作用。

Conclusion: 所提出的针对特定行业的ASR模型微调方法显著提高了ASR模型在行业应用中的性能，并已被实际工业应用所采纳。

Abstract: In the design of customer relationship management (CRM) systems, accurately
identifying customer types and offering personalized services are key to
enhancing customer satisfaction and loyalty. However, this process faces the
challenge of discerning customer voices and intentions, and general pre-trained
automatic speech recognition (ASR) models make it difficult to effectively
address industry-specific speech recognition tasks. To address this issue, we
innovatively proposed a solution for fine-tuning industry-specific ASR models,
which significantly improved the performance of the fine-tuned ASR models in
industry applications. Experimental results show that our method substantially
improves the crucial auxiliary role of the ASR model in industry CRM systems,
and this approach has also been adopted in actual industrial applications.

</details>


### [236] [On Temporal Guidance and Iterative Refinement in Audio Source Separation](https://arxiv.org/abs/2507.17297)
*Tobias Morocutti,Jonathan Greif,Paul Primus,Florian Schmid,Gerhard Widmer*

Main category: cs.SD

TL;DR: 提出一种新的S5方法，通过Transformer进行声学类别检测和提供时变引导信息，并结合迭代优化机制提升声源分离质量，在DCASE挑战赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间语义分割（S5）中，由于两阶段流水线（音频标签提取后进行条件源分离）缺乏精细的时间信息，导致分离效果受限。本研究旨在通过增强事件检测与声源分离阶段的协同作用来解决此问题。

Method: 提出了一种新的空间语义分割方法，通过微调预训练Transformer模型进行活动声学类别检测，并利用该模型提供时变引导信息进行声源分离，同时引入迭代优化机制以逐步提升分离质量。

Result: 该方法在音频标签提取和声源分离方面均取得了显著的性能提升，并在DCASE挑战赛2025的Task 4中获得第二名。

Conclusion: 该方法通过引入迭代优化机制，实现了声源分离性能的显著提升，并在DCASE挑战赛中取得了第二名的成绩。

Abstract: Spatial semantic segmentation of sound scenes (S5) involves the accurate
identification of active sound classes and the precise separation of their
sources from complex acoustic mixtures. Conventional systems rely on a
two-stage pipeline - audio tagging followed by label-conditioned source
separation - but are often constrained by the absence of fine-grained temporal
information critical for effective separation. In this work, we address this
limitation by introducing a novel approach for S5 that enhances the synergy
between the event detection and source separation stages. Our key contributions
are threefold. First, we fine-tune a pre-trained Transformer to detect active
sound classes. Second, we utilize a separate instance of this fine-tuned
Transformer to perform sound event detection (SED), providing the separation
module with detailed, time-varying guidance. Third, we implement an iterative
refinement mechanism that progressively enhances separation quality by
recursively reusing the separator's output from previous iterations. These
advancements lead to significant improvements in both audio tagging and source
separation performance, as demonstrated by our system's second-place finish in
Task 4 of the DCASE Challenge 2025. Our implementation and model checkpoints
are available in our GitHub repository: https://github.com/theMoro/dcase25task4 .

</details>


### [237] [Application of Whisper in Clinical Practice: the Post-Stroke Speech Assessment during a Naming Task](https://arxiv.org/abs/2507.17326)
*Milena Davudova,Ziyuan Cai,Valentina Giunchiglia,Dragos C. Gruia,Giulia Sanguedolce,Adam Hampshire,Fatemeh Geranmayeh*

Main category: cs.SD

TL;DR: 本研究发现，经过微调的Whisper模型可以有效转录和分析卒中患者的语音，并能预测其语言功能，但其泛化能力有限，需要针对特定人群进行调整。


<details>
  <summary>Details</summary>
Motivation: 卒中后语言障碍的评估复杂且耗时，限制了及时和可扩展的诊断。自动语音识别（ASR）Foundation Model在辅助人类评估方面显示出潜力，但其在有语言障碍的患者群体中的有效性尚不确定。

Method: 本研究评估了最先进的自动语音识别（ASR）Foundation Model“Whisper”在转录和分析卒中患者图片命名任务中的语音方面的有效性。研究人员评估了逐字转录的准确性以及模型支持语言功能下游预测的能力，并通过在临床数据上微调Whisper来提高其准确性，并评估其在未见过的数据集上的泛化能力。

Result: 微调后的Whisper模型在转录准确性方面有了显著提高（在健康语音和患者语音中的词错误率分别降低了87.72%和71.22%）。此外，从模型中学到的表征能够准确预测语音质量（在健康语音和患者语音中的平均F1 Macro分别为0.74和0.75）。然而，在未见过的数据集（TORGO）上的评估显示出有限的泛化能力，表明Whisper在零样本转录中的局限性，并强调了将模型适应特定临床人群的必要性。

Conclusion: 尽管在跨领域泛化方面仍存在挑战，但研究结果表明，经过适当微调的Foundation Model有潜力在自动化卒中后语言障碍的语音和语言评估和康复方面取得进展。

Abstract: Detailed assessment of language impairment following stroke remains a
cognitively complex and clinician-intensive task, limiting timely and scalable
diagnosis. Automatic Speech Recognition (ASR) foundation models offer a
promising pathway to augment human evaluation through intelligent systems, but
their effectiveness in the context of speech and language impairment remains
uncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASR
foundation model, can be applied to transcribe and analyze speech from patients
with stroke during a commonly used picture-naming task. We assess both verbatim
transcription accuracy and the model's ability to support downstream prediction
of language function, which has major implications for outcomes after stroke.
Our results show that the baseline Whisper model performs poorly on single-word
speech utterances. Nevertheless, fine-tuning Whisper significantly improves
transcription accuracy (reducing Word Error Rate by 87.72% in healthy speech
and 71.22% in speech from patients). Further, learned representations from the
model enable accurate prediction of speech quality (average F1 Macro of 0.74
for healthy, 0.75 for patients). However, evaluations on an unseen (TORGO)
dataset reveal limited generalizability, highlighting the inability of Whisper
to perform zero-shot transcription of single-word utterances on out-of-domain
clinical speech and emphasizing the need to adapt models to specific clinical
populations. While challenges remain in cross-domain generalization, these
findings highlight the potential of foundation models, when appropriately
fine-tuned, to advance automated speech and language assessment and
rehabilitation for stroke-related impairments.

</details>


### [238] [BoSS: Beyond-Semantic Speech](https://arxiv.org/abs/2507.17563)
*Qing Wang,Zehan Li,Hang Lv,Hongjie Chen,Yaodong Song,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.SD

TL;DR: 现代语音技术在理解语音中的情感、语境等“超语义”信息方面能力不足。本研究提出了“超语义语音”（BoSS）框架和评估方法，旨在提升人机通信的丰富度和语境感知能力，但发现现有模型仍难以完全解读这些信息。


<details>
  <summary>Details</summary>
Motivation: 现代语音技术（如ASR和TTS）在理解和生成超越显性语义的通信维度（如情感、语境和隐晦线索）方面存在局限。为了推动语音智能的发展，需要一个能够表征和评估这些“超语义”能力的框架和方法。

Method: 本研究提出了“超语义语音”（BoSS）的概念，并构建了一个形式化框架。该框架利用认知相关性理论和机器学习模型来分析时态和语境语音动态，以量化和评估语音通信中超越显性语义的信息。研究评估了BoSS相关属性，并将其应用于L1-L5的“口语交互系统能力等级”框架中。

Result: 目前的口语语言模型（SLMs）在完全解读超语义信号方面面临困难。评估结果揭示了在情感线索、语境动态和隐晦语义等方面的不足，表明在这些领域需要进一步的研究和改进。

Conclusion: 现代语音技术在捕捉除了显性语义之外的隐晦信号和语境线索方面存在不足。为更好地表征和评估口语智能的进展，我们提出了L1-L5的“口语交互系统能力等级”框架，并定义了“超语义语音”（BoSS）的概念，即语音通信中包含但又超越显性语义的信息，它传达情感、语境，并通过多维特征（如情感线索、语境动态和隐晦语义）修改或扩展意义。我们提出了一个利用认知相关性理论和机器学习模型来分析时态和语境语音动态的BoSS形式化框架，并评估了BoSS相关属性。结果表明，当前的口语语言模型（SLMs）难以完全解读超语义信号。这些发现强调了推进BoSS研究的必要性，以实现更丰富、更具语境感知能力的人机通信。

Abstract: Human communication involves more than explicit semantics, with implicit
signals and contextual cues playing a critical role in shaping meaning.
However, modern speech technologies, such as Automatic Speech Recognition (ASR)
and Text-to-Speech (TTS) often fail to capture these beyond-semantic
dimensions. To better characterize and benchmark the progression of speech
intelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),
a hierarchical framework illustrated the evolution of spoken dialogue systems
from basic command recognition to human-like social interaction. To support
these advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which
refers to the set of information in speech communication that encompasses but
transcends explicit semantics. It conveys emotions, contexts, and modifies or
extends meanings through multidimensional features such as affective cues,
contextual dynamics, and implicit semantics, thereby enhancing the
understanding of communicative intentions and scenarios. We present a
formalized framework for BoSS, leveraging cognitive relevance theories and
machine learning models to analyze temporal and contextual speech dynamics. We
evaluate BoSS-related attributes across five different dimensions, reveals that
current spoken language models (SLMs) are hard to fully interpret
beyond-semantic signals. These findings highlight the need for advancing BoSS
research to enable richer, more context-aware human-machine communication.

</details>


### [239] [Audio-Vision Contrastive Learning for Phonological Class Recognition](https://arxiv.org/abs/2507.17682)
*Daiqi Liu,Tomás Arias-Vergara,Jana Hutter,Andreas Maier,Paula Andrea Pérez-Toro*

Main category: cs.SD

TL;DR: 提出了一种新颖的多模态深度学习框架，利用实时MRI和语音信号进行语音分类。该框架通过对比学习实现了最先进的性能，平均F1分数达到0.81，比单一模态基线提高了0.23。


<details>
  <summary>Details</summary>
Motivation: 准确分类发音-语音特征对于理解人类语音产生和开发鲁棒的语音技术至关重要，特别是在临床环境中，靶向语音分析和治疗可以提高疾病诊断的准确性和个性化康复。

Method: 提出了一种结合实时MRI和语音信号的多模态深度学习框架，用于分类发音方式、发音部位和发声三个关键的发音维度，并对15个音类进行了分类。

Result: 在USC-TIMIT数据集上，对比学习方法实现了0.81的平均F1分数，比单一模态基线提高了0.23。

Conclusion: 所提出的对比学习框架在多模态发音分析方面是有效的，并且在USC-TIMIT数据集上达到了最先进的性能。

Abstract: Accurate classification of articulatory-phonological features plays a vital
role in understanding human speech production and developing robust speech
technologies, particularly in clinical contexts where targeted phonemic
analysis and therapy can improve disease diagnosis accuracy and personalized
rehabilitation. In this work, we propose a multimodal deep learning framework
that combines real-time magnetic resonance imaging (rtMRI) and speech signals
to classify three key articulatory dimensions: manner of articulation, place of
articulation, and voicing. We perform classification on 15 phonological classes
derived from the aforementioned articulatory dimensions and evaluate the system
with four audio/vision configurations: unimodal rtMRI, unimodal audio signals,
multimodal middle fusion, and contrastive learning-based audio-vision fusion.
Experimental results on the USC-TIMIT dataset show that our contrastive
learning-based approach achieves state-of-the-art performance, with an average
F1-score of 0.81, representing an absolute increase of 0.23 over the unimodal
baseline. The results confirm the effectiveness of contrastive representation
learning for multimodal articulatory analysis. Our code and processed dataset
will be made publicly available at
https://github.com/DaE-plz/AC_Contrastive_Phonology to support future research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [240] [Summarizing Normative Driving Behavior From Large-Scale NDS Datasets for Vehicle System Development](https://arxiv.org/abs/2507.16839)
*Gregory Beale,Gibran Ali*

Main category: cs.RO

TL;DR: 本研究提出了一种分析大规模自然驾驶研究（NDS）数据的方法，以描述不同群体（按道路特征、车辆类别和驾驶员人口统计信息划分）的驾驶行为，并开发了相应的在线可视化工具。研究结果揭示了不同人口统计学群体在超速和车头时距方面的差异，为提升车辆安全和智能交通系统提供了数据支持。


<details>
  <summary>Details</summary>
Motivation: 描述驾驶行为，为车辆安全和智能交通系统的开发提供支持。

Method: 本研究提出了一种处理大规模自然驾驶研究（NDS）的方法，利用车辆、GPS和前向雷达数据，结合道路特征、车辆类别和驾驶员人口统计信息，描述了速度、超速、车道保持、跟车距离和车头时距等五种车辆指标的驾驶行为。此外，还开发了交互式在线分析工具，通过动态数据选择和分组来可视化和比较不同群体的驾驶行为。

Result: 研究结果显示，在SHRP 2 NDS数据中，年龄在16-19岁的女性驾驶员在限速65英里/小时的道路上超速的频率比同龄男性高7.5至15英里/小时，并且年轻驾驶员比年长驾驶员更频繁地保持小于1.5秒的车头时距。

Conclusion: 该研究通过量化规范驾驶行为并提供分析NDS数据集以进行跨组比较的方法，支持更好的车辆系统和更安全的基础设施。

Abstract: This paper presents a methodology to process large-scale naturalistic driving
studies (NDS) to describe the driving behavior for five vehicle metrics,
including speed, speeding, lane keeping, following distance, and headway,
contextualized by roadway characteristics, vehicle classes, and driver
demographics. Such descriptions of normative driving behaviors can aid in the
development of vehicle safety and intelligent transportation systems. The
methodology is demonstrated using data from the Second Strategic Highway
Research Program (SHRP 2) NDS, which includes over 34 million miles of driving
across more than 3,400 drivers. Summaries of each driving metric were generated
using vehicle, GPS, and forward radar data. Additionally, interactive online
analytics tools were developed to visualize and compare driving behavior across
groups through dynamic data selection and grouping. For example, among drivers
on 65-mph roads for the SHRP 2 NDS, females aged 16-19 exceeded the speed limit
by 7.5 to 15 mph slightly more often than their male counterparts, and younger
drivers maintained headways under 1.5 seconds more frequently than older
drivers. This work supports better vehicle systems and safer infrastructure by
quantifying normative driving behaviors and offers a methodology for analyzing
NDS datasets for cross group comparisons.

</details>


### [241] [AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of Aquaculture Net Pens](https://arxiv.org/abs/2507.16841)
*Waseem Akram,Muhayy Ud Din,Abdelhaleem Saad,Irfan Hussain*

Main category: cs.RO

TL;DR: AquaChat是一个利用大型语言模型（LLM）驱动的新型ROV框架，用于智能水产养殖网箱检查。它通过自然语言理解、任务规划和精确控制，提高了检查的灵活性、准确性和效率，为可持续水产养殖开辟了道路。


<details>
  <summary>Details</summary>
Motivation: 传统的网箱检查方法依赖于预编程任务或手动控制，难以适应动态的水下环境和用户的特定需求。因此，需要一种更智能、更灵活的检查方法。

Method: 提出了一种名为AquaChat的新型遥控无人机（ROV）框架，该框架集成了大型语言模型（LLM）来实现智能和自适应的网箱检查。该系统采用多层架构：高层规划层利用LLM解释自然语言指令并生成任务计划；中层任务管理器将计划转换为ROV控制序列；底层运动控制层精确执行导航和检查任务。实时反馈和事件触发的重新规划提高了在复杂水产养殖环境中的鲁棒性。

Result: 实验结果表明，AquaChat在模拟和受控水生环境中均表现出色，提高了任务的灵活性、检查的准确性和操作效率。

Conclusion: AquaChat通过整合语言模型与海洋机器人技术，为可持续水产养殖提供了智能、用户交互式的检查解决方案。

Abstract: Inspection of aquaculture net pens is essential for maintaining the
structural integrity, biosecurity, and operational efficiency of fish farming
systems. Traditional inspection approaches rely on pre-programmed missions or
manual control, offering limited adaptability to dynamic underwater conditions
and user-specific demands. In this study, we propose AquaChat, a novel Remotely
Operated Vehicle (ROV) framework that integrates Large Language Models (LLMs)
for intelligent and adaptive net pen inspection. The system features a
multi-layered architecture: (1) a high-level planning layer that interprets
natural language user commands using an LLM to generate symbolic task plans;
(2) a mid-level task manager that translates plans into ROV control sequences;
and (3) a low-level motion control layer that executes navigation and
inspection tasks with precision. Real-time feedback and event-triggered
replanning enhance robustness in challenging aquaculture environments. The
framework is validated through experiments in both simulated and controlled
aquatic environments representative of aquaculture net pens. Results
demonstrate improved task flexibility, inspection accuracy, and operational
efficiency. AquaChat illustrates the potential of integrating language-based AI
with marine robotics to enable intelligent, user-interactive inspection systems
for sustainable aquaculture operations.

</details>


### [242] [Sensor-Space Based Robust Kinematic Control of Redundant Soft Manipulator by Learning](https://arxiv.org/abs/2507.16842)
*Yinan Meng,Kun Qian,Jiong Yang,Renbo Su,Zhenhong Li,Charlie C. L. Wang*

Main category: cs.RO

TL;DR: 针对软体操作器在密闭环境和执行器饱和下的运动学控制挑战，提出了一种结合强化学习和模仿学习的传感器空间模仿学习运动学控制（SS-ILKC）框架，并通过仿真到实际的传输机制实现了零样本的真实世界部署，实验验证了其在精确路径跟踪和物体操作方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 冗余软操作器的固有顺应性和高自由度（DoF）有利于安全交互和灵活的任务执行。然而，有效的运动学控制仍然具有挑战性，因为它必须处理由未知外部载荷引起的变形，并避免由于不当的零空间调节而导致的执行器饱和，尤其是在密闭环境中。

Method: 提出了一种基于传感器空间模仿学习的运动学控制（SS-ILKC）框架，该框架采用双学习策略：一个基于强化学习原理在仿真中训练的多目标传感器空间控制框架，用于在开放空间中开发鲁棒的控制策略；另一个生成对抗模仿学习方法，用于从稀疏的专家演示中学习在密闭空间中有效的策略。此外，还提出了一种预处理的仿真到实际的传输机制，以缩小仿真与现实之间的差距并准确表征执行器饱和限制。

Result: 实验结果表明，该方法能够有效控制气动软驱动机械臂，在未知载荷条件下，在密闭环境中实现精确的路径跟踪和物体操作。

Conclusion: 该方法能够有效控制气动软驱动机械臂，在未知载荷条件下，在密闭环境中实现精确的路径跟踪和物体操作。

Abstract: The intrinsic compliance and high degree of freedom (DoF) of redundant soft
manipulators facilitate safe interaction and flexible task execution. However,
effective kinematic control remains highly challenging, as it must handle
deformations caused by unknown external loads and avoid actuator saturation due
to improper null-space regulation - particularly in confined environments. In
this paper, we propose a Sensor-Space Imitation Learning Kinematic Control
(SS-ILKC) framework to enable robust kinematic control under actuator
saturation and restrictive environmental constraints. We employ a dual-learning
strategy: a multi-goal sensor-space control framework based on reinforcement
learning principle is trained in simulation to develop robust control policies
for open spaces, while a generative adversarial imitation learning approach
enables effective policy learning from sparse expert demonstrations for
confined spaces. To enable zero-shot real-world deployment, a pre-processed
sim-to-real transfer mechanism is proposed to mitigate the
simulation-to-reality gap and accurately characterize actuator saturation
limits. Experimental results demonstrate that our method can effectively
control a pneumatically actuated soft manipulator, achieving precise
path-following and object manipulation in confined environments under unknown
loading conditions.

</details>


### [243] [Analytical Formulation of Autonomous Vehicle Freeway Merging Control with State-Dependent Discharge Rates](https://arxiv.org/abs/2507.16846)
*Qing Tang,Xianbiao Hu*

Main category: cs.RO

TL;DR: 本文提出了一种新的分析方法和动态规划模型，用于控制自动驾驶车辆在高速公路汇入过程中的效率和安全性，并通过实验证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统车道汇入控制忽略了交通拥堵期间由于交叉交通影响导致的流量下降，本文旨在解决这一问题，并提高交通效率和安全性。

Method: 提出了一种分析方法来表征和控制多阶段汇入，推导了汇入点有效排出率的解析表达式，并将其构建为动态规划模型，使用逆向归纳法求解。

Result: 数值实验表明，所提出的模型比两个基准算法更优越，能够实现更高效、更安全的汇入过程，并验证了所推导的有效排出率。

Conclusion: 该模型通过逆向归纳法优化了车辆汇入的决策变量（汇入位置和速度），从而同时最小化了延迟和碰撞风险，并在数值实验中验证了其有效性，优于基准算法。

Abstract: The core of the freeway merging control problem lies in dynamic queue
propagation and dissipation linked to merging vehicle behavior. Traditionally,
queuing is modeled through demand-supply interactions with time varying demand
and fixed capacity. However, field observations show flow rates decrease during
congestion at freeway merges due to the impact of intersecting traffic, a
factor overlooked in fundamental diagrams. This manuscript introduces an
analytical approach to characterize and control the dynamic multi-stage merging
of autonomous vehicles, prioritizing traffic efficiency and safety. For the
first time, the effective discharge rate at the merging point, reduced by the
multi-stage dynamic merging process, is analytically derived using a closed
form formulation. Leveraging this expression, performance metrics such as queue
length and traffic delay are derived as the first objective. Additionally, a
crash risk function is established to quantitatively assess potential
collisions during the merging process, serving as the second objective.
Finally, the problem is formulated as a dynamic programming model to jointly
minimize delay and crash risk, with the merging location and speed as decision
variables. Given the terminal state, the ramp vehicle merging task is
formulated as a recursive optimization problem, employing backward induction to
find the minimum cost solution. Numerical experiments using the NGSIM dataset
validate the derived effective discharge rate. The results indicate that the
proposed model outperforms two benchmark algorithms, leading to a more
efficient and safer merging process.

</details>


### [244] [MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous Mobile Operation](https://arxiv.org/abs/2507.16853)
*Ning Li,Xiangmou Qu,Jiamu Zhou,Jun Wang,Muning Wen,Kounianhua Du,Xingyu Lou,Qiuying Peng,Jun Wang,Weinan Zhang*

Main category: cs.RO

TL;DR: MobileUse是一个用于移动设备任务自动执行的GUI代理，通过改进的架构和探索方法解决了长期任务、错误恢复和冷启动等挑战。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有MLLM在移动场景下执行长期任务、错误恢复和冷启动方面面临的挑战。

Method: 提出了一种名为MobileUse的GUI代理，采用分层反射架构来实现自我监控、检测和错误恢复，并通过主动探索模块来解决冷启动问题。

Result: MobileUse在AndroidWorld和AndroidLab基准测试中分别达到了62.9%和44.2%的成功率，确立了新的最先进性能。

Conclusion: MobileUse通过其分层反射架构和主动探索模块，在长期任务执行、错误恢复和冷启动问题方面取得了显著进展，并在AndroidWorld和AndroidLab基准测试中创下了新的最先进性能。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled the
development of mobile agents that can understand visual inputs and follow user
instructions, unlocking new possibilities for automating complex tasks on
mobile devices. However, applying these models to real-world mobile scenarios
remains a significant challenge due to the long-horizon task execution,
difficulty in error recovery, and the cold-start problem in unfamiliar
environments. To address these challenges, we propose MobileUse, a GUI agent
designed for robust and adaptive mobile task execution. To improve resilience
in long-horizon tasks and dynamic environments, we introduce a hierarchical
reflection architecture that enables the agent to self-monitor, detect, and
recover from errors across multiple temporal scales-ranging from individual
actions to overall task completion-while maintaining efficiency through a
reflection-on-demand strategy. To tackle cold-start issues, we further
introduce a proactive exploration module, which enriches the agent's
understanding of the environment through self-planned exploration. Evaluations
on AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse
establishes new state-of-the-art performance, achieving success rates of 62.9%
and 44.2%, respectively. To facilitate real-world applications, we release an
out-of-the-box toolkit for automated task execution on physical mobile devices,
which is available at https://github.com/MadeAgents/mobile-use.

</details>


### [245] [Leveraging multi-source and heterogeneous signals for fatigue detection](https://arxiv.org/abs/2507.16859)
*Luobin Cui,Yanlai Wu,Tang Ying,Weikai Li*

Main category: cs.RO

TL;DR: 提出了一种新颖的疲劳检测框架，用于解决真实世界中传感器受限的挑战，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的疲劳检测方法大多依赖高端传感器和受控环境，限制了其在真实世界中的应用。本文旨在解决在传感器配置不同甚至受限的真实世界场景中进行疲劳检测的实际问题。

Method: 提出了一种异构、多源的疲劳检测框架，能够自适应地利用目标域中可用的模态，并受益于源域中存在的不同配置。

Result: 实验结果表明，该方法在真实的现场部署传感器设置和两个公开可用的数据集上，具有实用性、鲁棒性和改进的泛化能力。

Conclusion: 该框架在传感器受限的情况下，为有效的疲劳监测提供了实用的方法，证明了其实用性、鲁棒性和泛化能力的提高。

Abstract: Fatigue detection plays a critical role in safety-critical applications such
as aviation, mining, and long-haul transport. However, most existing methods
rely on high-end sensors and controlled environments, limiting their
applicability in real world settings. This paper formally defines a practical
yet underexplored problem setting for real world fatigue detection, where
systems operating with context-appropriate sensors aim to leverage knowledge
from differently instrumented sources including those using impractical sensors
deployed in controlled environments. To tackle this challenge, we propose a
heterogeneous and multi-source fatigue detection framework that adaptively
utilizes the available modalities in the target domain while benefiting from
the diverse configurations present in source domains. Our experiments,
conducted using a realistic field-deployed sensor setup and two publicly
available datasets, demonstrate the practicality, robustness, and improved
generalization of our approach, paving the practical way for effective fatigue
monitoring in sensor-constrained scenarios.

</details>


### [246] [ResKACNNet: A Residual ChebyKAN Network for Inertial Odometry](https://arxiv.org/abs/2507.16865)
*Shanshan Zhang,Tianshui Wen,Siyue Wang,Qi Zhang,Ziheng Zhou,Huiru Zheng,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: 提出了一种名为ResChebyKAN的新型惯性定位网络，通过利用Chebyshev多项式和EKSA模块来解决传统方法在处理IMU数据中的非线性运动和长期依赖性问题，并在多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于CNN的惯性定位方法难以捕捉IMU数据中的非线性运动特征和长期依赖性。

Method: 提出了一种名为ResChebyKAN的新型惯性定位网络，该网络利用Chebyshev多项式的非线性逼近能力来建模运动模式，并引入了高效核基自注意力（EKSA）模块来捕捉上下文信息和增强长期依赖性建模。

Result: 在RIDI、RoNIN、RNIN-VIO、OxIOD、IMUNet和TLIO等公开数据集上，与现有基准方法相比，所提出的方法将绝对轨迹误差降低了3.79%至42.32%。此外，经验性地证明，从加速度数据中移除重力分量可以显著提高惯性定位性能。

Conclusion: 提出了一种新颖的惯性定位网络ResChebyKAN，结合了Chebyshev多项式和EKSA模块，以捕捉非线性运动特征和长期依赖性。实验结果表明，该方法在多个公开数据集上显著降低了绝对轨迹误差。

Abstract: Inertial Measurement Unit (IMU) has become a key technology for achieving
low-cost and precise positioning. However, traditional CNN-based inertial
positioning methods struggle to capture the nonlinear motion characteristics
and long-term dependencies in IMU data. To address this limitation, we propose
a novel inertial positioning network with a generic backbone called
ResChebyKAN, which leverages the nonlinear approximation capabilities of
Chebyshev polynomials to model complex motion patterns. Additionally, we
introduce an Efficient Kernel-based Self-Attention (EKSA) module to effectively
capture contextual information and enhance long-term dependency modeling.
Experimental results on public datasets (e.g., RIDI, RoNIN, RNIN-VIO, OxIOD,
IMUNet, and TLIO) demonstrate that our method reduces the absolute trajectory
error by 3.79% to 42.32% compared to existing benchmark methods. Furthermore,
we release a preprocessed dataset and empirically show that removing the
gravity component from acceleration data significantly improves inertial
positioning performance.

</details>


### [247] [Multi-agent Reinforcement Learning for Robotized Coral Reef Sample Collection](https://arxiv.org/abs/2507.16941)
*Daniel Correa,Tero Kaarlela,Jose Fuentes,Paulo Padrao,Alain Duran,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: 该研究提出了一种创新的强化学习方法，用于训练水下机器人进行珊瑚取样，以实现珊瑚礁保护。


<details>
  <summary>Details</summary>
Motivation: 开发用于珊瑚礁保护和研究任务的自主水下机器人取样剂。

Method: 利用软件在回路（SIL）和硬件在回路（HIL）进行开发，并使用通用游戏引擎、深度强化学习和实时水下动作捕捉。

Result: 成功开发了一个使用数字孪生进行训练的强化学习人工智能控制器，并通过物理实验进行了验证。

Conclusion: 该方法通过结合游戏引擎、深度强化学习和实时水下动作捕捉，实现了有效的零样本模拟到现实策略，为珊瑚礁保护和研究任务开发了自主水下机器人取样剂。

Abstract: This paper presents a reinforcement learning (RL) environment for developing
an autonomous underwater robotic coral sampling agent, a crucial coral reef
conservation and research task. Using software-in-the-loop (SIL) and
hardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI)
controller is developed using a digital twin (DT) in simulation and
subsequently verified in physical experiments. An underwater motion capture
(MOCAP) system provides real-time 3D position and orientation feedback during
verification testing for precise synchronization between the digital and
physical domains. A key novelty of this approach is the combined use of a
general-purpose game engine for simulation, deep RL, and real-time underwater
motion capture for an effective zero-shot sim-to-real strategy.

</details>


### [248] [RAPTAR: Radar Radiation Pattern Acquisition through Automated Collaborative Robotics](https://arxiv.org/abs/2507.16988)
*Maaz Qureshi,Mohammad Omid Bagheri,Abdelrahman Elbadrawy,William Melek,George Shaker*

Main category: cs.RO

TL;DR: RAPTAR是一个基于协作机器人的自主系统，可以对片上天线进行3D辐射模式测量，解决了传统方法的局限性，并且在准确性和效率方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前的探针台技术在角度覆盖范围、专用硬件依赖和手动校准频率方面存在局限性，难以精确表征现代片上天线。RAPTAR系统的开发旨在解决在车辆、无人机、AR/VR头显和生物医疗设备等实际应用场景中，传统测量方法不适用的测试雷达模块的挑战。

Method: 本研究介绍了一种名为RAPTAR（Radiation Pattern Acquisition through Robotic Automation）的便携式、先进的自主系统，该系统基于协作机器人技术，能够对现代片上天线进行3D辐射模式测量，无需专门的消声室。

Result: RAPTAR系统能够对集成雷达模块进行3D辐射模式测量，覆盖半球空间域，角度分辨率高达2.5度，并且实现了低于0.9毫米的均方根误差的校准精度。实验结果表明，与全波电磁仿真相比，测量误差小于2 dB，并且比基线方法提高了36.5%的准确性。

Conclusion: RAPTAR系统在60 GHz雷达模块的测量中，与全波电磁仿真相比，平均绝对误差低于2 dB，并且相比基线方法，平均绝对误差降低了36.5%，证明了其准确性和可重复性。

Abstract: Accurate characterization of modern on-chip antennas remains challenging, as
current probe-station techniques offer limited angular coverage, rely on
bespoke hardware, and require frequent manual alignment. This research
introduces RAPTAR (Radiation Pattern Acquisition through Robotic Automation), a
portable, state-of-the-art, and autonomous system based on collaborative
robotics. RAPTAR enables 3D radiation-pattern measurement of integrated radar
modules without dedicated anechoic facilities. The system is designed to
address the challenges of testing radar modules mounted in diverse real-world
configurations, including vehicles, UAVs, AR/VR headsets, and biomedical
devices, where traditional measurement setups are impractical. A
7-degree-of-freedom Franka cobot holds the receiver probe and performs
collision-free manipulation across a hemispherical spatial domain, guided by
real-time motion planning and calibration accuracy with RMS error below 0.9 mm.
The system achieves an angular resolution upto 2.5 degree and integrates
seamlessly with RF instrumentation for near- and far-field power measurements.
Experimental scans of a 60 GHz radar module show a mean absolute error of less
than 2 dB compared to full-wave electromagnetic simulations ground truth.
Benchmarking against baseline method demonstrates 36.5% lower mean absolute
error, highlighting RAPTAR accuracy and repeatability.

</details>


### [249] [Shared Control of Holonomic Wheelchairs through Reinforcement Learning](https://arxiv.org/abs/2507.17055)
*Jannis Bähler,Diego Paez-Granados,Jorge Peña-Queralta*

Main category: cs.RO

TL;DR: 智能轮椅共享控制的强化学习方法在全向移动平台上实现了更好的用户体验和全向行驶潜力。


<details>
  <summary>Details</summary>
Motivation: 解决现有智能轮椅共享控制方法在全向移动平台上存在的用户体验不直观和未能充分发挥全向行驶潜力的问题。

Method: 采用基于强化学习的方法，将二维用户输入转换为三维运动，以提高用户舒适度和降低驾驶员认知负荷。在Isaac Gym中训练，在Gazebo中进行模拟测试，并与先前非学习方法进行比较。

Result: 所提出的强化学习方法在模拟中实现了无碰撞导航，智能定向轮椅，并且在平稳性方面优于或不逊于先前非学习方法。此外，该研究首次成功实现了强化学习驱动的共享控制在全向移动平台上的实际应用。

Conclusion: 提出了一种基于强化学习的方法，该方法将二维用户输入转换为三维运动，同时确保用户舒适度和降低驾驶员的认知负荷。该方法在Isaac Gym中训练，在Gazebo中进行模拟测试，并与先前非学习方法进行比较。结果表明，该方法可确保无碰撞导航，智能定向轮椅，并具有更好的或具有竞争力的平稳性。此外，研究人员还进行了仿真到现实的传输，并展示了强化学习驱动的共享控制在全向移动平台上的首次实际应用。

Abstract: Smart electric wheelchairs can improve user experience by supporting the
driver with shared control. State-of-the-art work showed the potential of
shared control in improving safety in navigation for non-holonomic robots.
However, for holonomic systems, current approaches often lead to unintuitive
behavior for the user and fail to utilize the full potential of omnidirectional
driving. Therefore, we propose a reinforcement learning-based method, which
takes a 2D user input and outputs a 3D motion while ensuring user comfort and
reducing cognitive load on the driver. Our approach is trained in Isaac Gym and
tested in simulation in Gazebo. We compare different RL agent architectures and
reward functions based on metrics considering cognitive load and user comfort.
We show that our method ensures collision-free navigation while smartly
orienting the wheelchair and showing better or competitive smoothness compared
to a previous non-learning-based method. We further perform a sim-to-real
transfer and demonstrate, to the best of our knowledge, the first real-world
implementation of RL-based shared control for an omnidirectional mobility
platform.

</details>


### [250] [Deformable Cluster Manipulation via Whole-Arm Policy Learning](https://arxiv.org/abs/2507.17085)
*Jayadeep Jacob,Wenzheng Zhang,Houston Warren,Paulo Borges,Tirthankar Bandyopadhyay,Fabio Ramos*

Main category: cs.RO

TL;DR: 机器人技术在操纵易变形物体方面取得了进展，通过结合3D视觉和触觉，能够更有效地进行抓取和清理任务，甚至能实现从模拟到现实的无缝转移。


<details>
  <summary>Details</summary>
Motivation: 解决可变形物体操纵的挑战，包括模型合成能力有限、感知不确定性高以及缺乏有效的空间抽象。

Method: 提出了一种新的无模型策略学习框架，结合了3D点云和本体感觉触觉信号，并利用了核均值嵌入和分布状态表示来提高训练效率和实时推理能力。此外，还提出了一种新的与上下文无关的遮挡启发式方法。

Result: 机器人能够生成利用多个手臂部件进行遮挡清除的策略，并成功地将训练好的策略从模拟环境转移到真实世界，以处理未知的遮挡模式、未见过的拓扑结构和不确定的动力学。

Conclusion: 该框架成功实现了机器人抓取带状物体，并能有效处理未知的遮挡和动力学。

Abstract: Manipulating clusters of deformable objects presents a substantial challenge
with widespread applicability, but requires contact-rich whole-arm
interactions. A potential solution must address the limited capacity for
realistic model synthesis, high uncertainty in perception, and the lack of
efficient spatial abstractions, among others. We propose a novel framework for
learning model-free policies integrating two modalities: 3D point clouds and
proprioceptive touch indicators, emphasising manipulation with full body
contact awareness, going beyond traditional end-effector modes. Our
reinforcement learning framework leverages a distributional state
representation, aided by kernel mean embeddings, to achieve improved training
efficiency and real-time inference. Furthermore, we propose a novel
context-agnostic occlusion heuristic to clear deformables from a target region
for exposure tasks. We deploy the framework in a power line clearance scenario
and observe that the agent generates creative strategies leveraging multiple
arm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy
transfer, allowing the arm to clear real branches with unknown occlusion
patterns, unseen topology, and uncertain dynamics.

</details>


### [251] [MARSCalib: Multi-robot, Automatic, Robust, Spherical Target-based Extrinsic Calibration in Field and Extraterrestrial Environments](https://arxiv.org/abs/2507.17130)
*Seokhwan Jeong,Hogyun Kim,Younggun Cho*

Main category: cs.RO

TL;DR: 提出了一种新颖的鲁棒的激光雷达-相机外参标定方法，该方法使用球形目标，能够处理目标和传感器损坏的情况，并在多种传感器和环境下进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为室外多机器人系统设计了一种新颖的基于球形目标的激光雷达-相机外参标定方法，并考虑了目标和传感器损坏的情况。

Method: 提出了一种新颖的基于球形目标的激光雷达-相机外参标定方法，该方法考虑了目标和传感器损坏。具体来说，首先使用SAM分解图像，然后提出了一种从潜在损坏的球体中提取椭圆并纠正透视投影模型引起的误差的新算法。对于激光雷达点云，由于缺乏平坦区域，球体上的点往往噪声较大，为此我们对累积点云应用了分层加权和来准确提取球体。

Result: 通过实验证明，该方法在目标和传感器都损坏的情况下，仍能鲁棒地检测到球体，并且性能优于其他目标。使用三种不同的激光雷达（旋转式、固态式和非重复式）和三种不同的相机位置进行了评估。通过在行星测试和野外环境中对不同程度损坏的球体进行实验，验证了该方法对目标损坏的鲁棒性。

Conclusion: 该方法在目标和传感器损坏的情况下都能鲁棒地检测到球体，并且优于其他目标。

Abstract: This paper presents a novel spherical target-based LiDAR-camera extrinsic
calibration method designed for outdoor environments with multi-robot systems,
considering both target and sensor corruption. The method extracts the 2D
ellipse center from the image and the 3D sphere center from the pointcloud,
which are then paired to compute the transformation matrix. Specifically, the
image is first decomposed using the Segment Anything Model (SAM). Then, a novel
algorithm extracts an ellipse from a potentially corrupted sphere, and the
extracted center of ellipse is corrected for errors caused by the perspective
projection model. For the LiDAR pointcloud, points on the sphere tend to be
highly noisy due to the absence of flat regions. To accurately extract the
sphere from these noisy measurements, we apply a hierarchical weighted sum to
the accumulated pointcloud. Through experiments, we demonstrated that the
sphere can be robustly detected even under both types of corruption,
outperforming other targets. We evaluated our method using three different
types of LiDARs (spinning, solid-state, and non-repetitive) with cameras
positioned in three different locations. Furthermore, we validated the
robustness of our method to target corruption by experimenting with spheres
subjected to various types of degradation. These experiments were conducted in
both a planetary test and a field environment. Our code is available at
https://github.com/sparolab/MARSCalib.

</details>


### [252] [Dynamic Modeling and Dimensional Optimization of Legged Mechanisms for Construction Robot](https://arxiv.org/abs/2507.17132)
*Xiao Liu,Xianlong Yang,Weijun Wang,Wei Feng*

Main category: cs.RO

TL;DR: 本研究提出了一种基于蚂蚁腿部结构并结合拉格朗日方法进行优化的建筑机器人腿部设计，通过仿真验证了该设计能够显著降低能耗和关节扭矩，为重载、高性能建筑机器人的设计提供了支持。


<details>
  <summary>Details</summary>
Motivation: 随着建筑行业的快速发展，恶劣的工作环境、高强度和高风险的任务以及劳动力短缺等问题日益突出，这促使人们对建筑机器人的低能耗、高移动性和高负载能力提出了更高的要求。本研究旨在提高建筑机器人腿部结构的动力学性能、降低能耗并增强承载能力。

Method: 首先，基于蚂蚁的腿部构型设计了机器人腿部结构，然后提出了一种新颖的结构优化方法，利用拉格朗日方法建立了腿部动力学模型，并结合腿部运动轨迹、多个动力学评估指标对各腿部段的几何参数进行了全面的优化研究。

Result: 优化的腿部结构将峰值关节扭矩和能耗降低了20%以上。通过ADAMS的动态仿真实验结果表明，优化后各关节的驱动功率显著降低，验证了所提出策略的有效性和合理性。

Conclusion: 该研究为重载、高性能建筑机器人的设计提供了理论基础和技术支撑。

Abstract: With the rapid development of the construction industry, issues such as harsh
working environments, high-intensity and high-risk tasks, and labor shortages
have become increasingly prominent. This drives higher demands for construction
robots in terms of low energy consumption, high mobility, and high load
capacity. This paper focuses on the design and optimization of leg structures
for construction robots, aiming to improve their dynamic performance, reduce
energy consumption, and enhance load-bearing capabilities. Firstly, based on
the leg configuration of ants in nature, we design a structure for the robot's
leg. Secondly, we propose a novel structural optimization method. Using the
Lagrangian approach, a dynamic model of the leg was established. Combining the
dynamic model with the leg's motion trajectory, we formulated multiple dynamic
evaluation metrics and conducted a comprehensive optimization study on the
geometric parameters of each leg segment. The results show that the optimized
leg structure reduces peak joint torques and energy consumption by over 20%.
Finally, dynamic simulation experiments were conducted using ADAMS. The results
demonstrate a significant reduction in the driving power of each joint after
optimization, validating the effectiveness and rationality of the proposed
strategy. This study provides a theoretical foundation and technical support
for the design of heavy-load, high-performance construction robots.

</details>


### [253] [Dynamic Parameter Identification of a Curtain Wall Installation Robotic Arm](https://arxiv.org/abs/2507.17136)
*Xiao Liu,Yunxiao Cheng,Weijun Wang,Tianlun Huang,Wei Feng*

Main category: cs.RO

TL;DR: 提出了一种用于幕墙安装机器人的高精度动态参数辨识方法，通过改进建模和辨识策略，显著提升了安装作业的智能化水平。


<details>
  <summary>Details</summary>
Motivation: 传统幕墙安装方法效率和质量低下，无法满足现代建筑需求。需要更智能化的解决方案来提高效率和质量。

Method: 提出了一种基于D-H模型的机器人动力学建模方法，并考虑了液压缸的动态特性和Stribeck摩擦模型。通过设计优化的激励信号和采用分层递进的最小二乘估计策略，分别辨识液压缸和机器人臂的动态参数，并进行联合标定。

Result: 实验验证表明，该方法能够实现对液压驱动幕墙安装机器人的高精度动态参数辨识，理论与实测关节力矩间的残差标准偏差低于0.4 Nm。

Conclusion: 该研究为液压驱动的幕墙安装机器人设计了一种动态参数辨识方法，通过构建复合参数系统并采用分层递进策略进行辨识，实验结果表明该方法能实现高精度的动态参数辨识，从而提升幕墙安装作业的智能化水平。

Abstract: In the construction industry, traditional methods fail to meet the modern
demands for efficiency and quality. The curtain wall installation is a critical
component of construction projects. We design a hydraulically driven robotic
arm for curtain wall installation and a dynamic parameter identification
method. We establish a Denavit-Hartenberg (D-H) model based on measured robotic
arm structural parameters and integrate hydraulic cylinder dynamics to
construct a composite parametric system driven by a Stribeck friction model. By
designing high-signal-to-noise ratio displacement excitation signals for
hydraulic cylinders and combining Fourier series to construct optimal
excitation trajectories that satisfy joint constraints, this method effectively
excites the characteristics of each parameter in the minimal parameter set of
the dynamic model of the robotic arm. On this basis, a hierarchical progressive
parameter identification strategy is proposed: least squares estimation is
employed to separately identify and jointly calibrate the dynamic parameters of
both the hydraulic cylinder and the robotic arm, yielding Stribeck model curves
for each joint. Experimental validation on a robotic arm platform demonstrates
residual standard deviations below 0.4 Nm between theoretical and measured
joint torques, confirming high-precision dynamic parameter identification for
the hydraulic-driven curtain wall installation robotic arm. This significantly
contributes to enhancing the intelligence level of curtain wall installation
operations.

</details>


### [254] [Multi-Objective Trajectory Planning for a Robotic Arm in Curtain Wall Installation](https://arxiv.org/abs/2507.17140)
*Xiao Liu,Yunxiao Cheng,Weijun Wang,Tianlun Huang,Zhiyong Wang,Wei Feng*

Main category: cs.RO

TL;DR: 提出了一种用于幕墙安装的机器人手臂和NSGA-III-FO算法，以解决传统方法在复杂建筑环境中优化机器人轨迹的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对劳动力短缺和成本上升的背景，为了确保建筑机器人在复杂的建筑环境中高效准确地执行任务，传统的单目标轨迹优化方法难以满足不断变化的建筑环境的复杂需求。

Method: 提出了一种用于幕墙安装的机器人手臂，并设计了一种包含聚焦操作符筛选机制的NSGA-III-FO算法，以加速算法向帕累托前沿收敛，从而有效平衡建筑机器人多目标约束。

Result: 所提出的NSGA-III-FO算法在DTLZ3和WFG3测试函数上进行了十次连续试验，与NSGA-III、MOEA/D和MSOPS-II算法相比，显示出更优越的收敛效率。

Conclusion: 通过设计的机械臂平台实验，验证了NSGA-III-FO算法在幕墙安装任务中的多目标轨迹规划问题的有效性和实用性。

Abstract: In the context of labor shortages and rising costs, construction robots are
regarded as the key to revolutionizing traditional construction methods and
improving efficiency and quality in the construction industry. In order to
ensure that construction robots can perform tasks efficiently and accurately in
complex construction environments, traditional single-objective trajectory
optimization methods are difficult to meet the complex requirements of the
changing construction environment. Therefore, we propose a multi-objective
trajectory optimization for the robotic arm used in the curtain wall
installation. First, we design a robotic arm for curtain wall installation,
integrating serial, parallel, and folding arm elements, while considering its
physical properties and motion characteristics. In addition, this paper
proposes an NSGA-III-FO algorithm (NSGA-III with Focused Operator, NSGA-III-FO)
that incorporates a focus operator screening mechanism to accelerate the
convergence of the algorithm towards the Pareto front, thereby effectively
balancing the multi-objective constraints of construction robots. The proposed
algorithm is tested against NSGA-III, MOEA/D, and MSOPS-II in ten consecutive
trials on the DTLZ3 and WFG3 test functions, showing significantly better
convergence efficiency than the other algorithms. Finally, we conduct two sets
of experiments on the designed robotic arm platform, which confirm the
efficiency and practicality of the NSGA-III-FO algorithm in solving
multi-objective trajectory planning problems for curtain wall installation
tasks.

</details>


### [255] [Towards Human-level Intelligence via Human-like Whole-Body Manipulation](https://arxiv.org/abs/2507.17141)
*Guang Gao,Jianan Wang,Jinbo Zuo,Junnan Jiang,Jingfan Zhang,Xianwen Zeng,Yuejiang Zhu,Lianyang Ma,Ke Chen,Minhua Sheng,Ruirui Zhang,Zhaohui An*

Main category: cs.RO

TL;DR: Unexpected Error


<details>
  <summary>Details</summary>
Motivation: 'NoneType' object has no attribute 'model_dump'

Method: N/A

Result: N/A

Conclusion: N/A

Abstract: Building general-purpose intelligent robots has long been a fundamental goal
of robotics. A promising approach is to mirror the evolutionary trajectory of
humans: learning through continuous interaction with the environment, with
early progress driven by the imitation of human behaviors. Achieving this goal
presents three core challenges: (1) designing safe robotic hardware with
human-level physical capabilities; (2) developing an intuitive and scalable
whole-body teleoperation interface for data collection; and (3) creating
algorithms capable of learning whole-body visuomotor policies from human
demonstrations. To address these challenges in a unified framework, we propose
Astribot Suite, a robot learning suite for whole-body manipulation aimed at
general daily tasks across diverse environments. We demonstrate the
effectiveness of our system on a wide range of activities that require
whole-body coordination, extensive reachability, human-level dexterity, and
agility. Our results show that Astribot's cohesive integration of embodiment,
teleoperation interface, and learning pipeline marks a significant step towards
real-world, general-purpose whole-body robotic manipulation, laying the
groundwork for the next generation of intelligent robots.

</details>


### [256] [Falconry-like palm landing by a flapping-wing drone based on the human gesture interaction and distance-aware flight planning](https://arxiv.org/abs/2507.17144)
*Kazuki Numazato,Keiichiro Kan,Masaki Kitagawa,Yunong Li,Johannes Kubel,Moju Zhao*

Main category: cs.RO

TL;DR: 本研究提出了一种类似训鹰的扑翼无人机手部着陆交互系统，该系统通过考虑人类安全因素的轨迹规划，实现了首个扑翼无人机与人类的接触式交互，并取得了安全、平稳的着陆效果。


<details>
  <summary>Details</summary>
Motivation: 扑翼无人机因其仿生飞行而备受关注，但很少有研究探索人与扑翼无人机之间的实际交互。从训鹰者那里获得灵感，他们引导猛禽降落在手臂上，将人体的动态着陆平台用于各种场景，如拥挤或空间受限的环境。

Method: 提出了一种类似训鹰的交互系统，其中扑翼无人机执行在人手上的掌心着陆运动。为了实现安全的接近人类，设计了一种同时考虑无人机速度和与用户距离等人类安全物理和心理因素的轨迹规划方法。

Result: 实验证明，该方法能够实现安全、平稳的手部着陆交互。

Conclusion: 该研究首次实现了扑翼无人机与人类之间的基于接触的交互。

Abstract: Flapping-wing drones have attracted significant attention due to their
biomimetic flight. They are considered more human-friendly due to their
characteristics such as low noise and flexible wings, making them suitable for
human-drone interactions. However, few studies have explored the practical
interaction between humans and flapping-wing drones. On establishing a physical
interaction system with flapping-wing drones, we can acquire inspirations from
falconers who guide birds of prey to land on their arms. This interaction
interprets the human body as a dynamic landing platform, which can be utilized
in various scenarios such as crowded or spatially constrained environments.
Thus, in this study, we propose a falconry-like interaction system in which a
flapping-wing drone performs a palm landing motion on a human hand. To achieve
a safe approach toward humans, we design a trajectory planning method that
considers both physical and psychological factors of the human safety such as
the drone's velocity and distance from the user. We use a commercial flapping
platform with our implemented motion planning and conduct experiments to
evaluate the palm landing performance and safety. The results demonstrate that
our approach enables safe and smooth hand landing interactions. To the best of
our knowledge, it is the first time to achieve a contact-based interaction
between flapping-wing drones and humans.

</details>


### [257] [JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction](https://arxiv.org/abs/2507.17152)
*Fangze Lin,Ying He,Fei Yu,Hong Zhang*

Main category: cs.RO

TL;DR: JAM是一种新的两阶段预测框架，用于解决多智能体联合预测中的低概率模式生成问题，并在Waymo数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决多智能体联合预测中低概率模式生成质量低的问题。

Method: 提出了一种名为“JAM”的两阶段多智能体交互预测框架。第一阶段将轨迹点分类，第二阶段利用场景上下文和第一阶段的边际轨迹点来学习联合分布，并引入关键航点来指导联合预测。

Result: JAM在Waymo开放运动数据集交互预测基准上取得了具有竞争力的性能，并在框架比较实验中优于其他预测框架，在交互式轨迹预测方面取得了最先进的性能。

Conclusion: JAM在交互式轨迹预测方面取得了具有竞争力的性能，并在框架比较实验中取得了最先进的性能。

Abstract: Predicting the future motion of road participants is a critical task in
autonomous driving. In this work, we address the challenge of low-quality
generation of low-probability modes in multi-agent joint prediction. To tackle
this issue, we propose a two-stage multi-agent interactive prediction framework
named \textit{keypoint-guided joint prediction after classification-aware
marginal proposal} (JAM). The first stage is modeled as a marginal prediction
process, which classifies queries by trajectory type to encourage the model to
learn all categories of trajectories, providing comprehensive mode information
for the joint prediction module. The second stage is modeled as a joint
prediction process, which takes the scene context and the marginal proposals
from the first stage as inputs to learn the final joint distribution. We
explicitly introduce key waypoints to guide the joint prediction module in
better capturing and leveraging the critical information from the initial
predicted trajectories. We conduct extensive experiments on the real-world
Waymo Open Motion Dataset interactive prediction benchmark. The results show
that our approach achieves competitive performance. In particular, in the
framework comparison experiments, the proposed JAM outperforms other prediction
frameworks and achieves state-of-the-art performance in interactive trajectory
prediction. The code is available at https://github.com/LinFunster/JAM to
facilitate future research.

</details>


### [258] [The Wilhelm Tell Dataset of Affordance Demonstrations](https://arxiv.org/abs/2507.17401)
*Rachel Ringe,Mihai Pomarlan,Nikolaos Tsiogkas,Stefano De Giorgis,Maria Hedblom,Rainer Malaka*

Main category: cs.RO

TL;DR: 该研究提出了一个包含视频和元数据的新数据集，用于训练机器人识别家庭任务中的 affordance，并能研究任务准备活动。


<details>
  <summary>Details</summary>
Motivation: 为了让机器人在人类环境中运行，需要让机器人感知 affordance，但现有方法在静态图像或形状上训练能力，而忽略了动态的、多视角的真实场景。

Method: 通过收集包含七小时人类活动的多视角视频，并进行标注，构建 affordance 数据集。

Result: 该数据集不仅可以用于训练感知系统识别 affordance，还可以用于研究人们在执行任务前进行的准备活动，例如如何布置任务空间，这对于协作服务机器人也很重要。

Conclusion: 该研究提出了一个用于学习家庭日常任务中的 affordance 的新数据集，该数据集包含视频序列和元数据。

Abstract: Affordances - i.e. possibilities for action that an environment or objects in
it provide - are important for robots operating in human environments to
perceive. Existing approaches train such capabilities on annotated static
images or shapes. This work presents a novel dataset for affordance learning of
common household tasks. Unlike previous approaches, our dataset consists of
video sequences demonstrating the tasks from first- and third-person
perspectives, along with metadata about the affordances that are manifested in
the task, and is aimed towards training perception systems to recognize
affordance manifestations. The demonstrations were collected from several
participants and in total record about seven hours of human activity. The
variety of task performances also allows studying preparatory maneuvers that
people may perform for a task, such as how they arrange their task space, which
is also relevant for collaborative service robots.

</details>


### [259] [Reconfigurable Tendon-Driven Robots: Eliminating Inter-segmental Coupling via Independently Lockable Joints](https://arxiv.org/abs/2507.17163)
*Botao Lin,Shuang Song,Jiaole Wang*

Main category: cs.RO

TL;DR: RTR通过可锁定关节消除了段间耦合，简化了控制，仅用6个电机驱动7个关节，提高了效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统肌腱驱动机器人（TDR）在增加机器人段以提高灵活性和工作空间时出现的段间耦合问题，以及由此带来的复杂模型和额外电机需求。

Method: 通过设计带有可锁定关节的RTR，并利用一对拮抗肌腱来控制关节状态（锁定/自由），从而实现对特定机器人段的选择性驱动，消除段间耦合，无需复杂的协调控制。研究推导了RTR的运动学和静力学模型，并通过实验验证了模型的准确性。

Result: 通过仿真比较了RTR与传统TDR的工作空间，证明了RTR的优势。使用一个包含六个电机的七关节RTR原型进行了演示实验，展示了其重构能力和在复杂环境中的移动能力。

Conclusion: 该研究提出了一种名为“可重构肌腱驱动机器人”（RTR）的新型机器人，它通过创新的可锁定关节消除了传统肌腱驱动机器人（TDR）中因结构扩展而加剧的段间耦合问题，从而简化了控制并提高了效率。

Abstract: With a slender redundant body, the tendon-driven robot (TDR) has a large
workspace and great maneuverability while working in complex environments. TDR
comprises multiple independently controlled robot segments, each with a set of
driving tendons. While increasing the number of robot segments enhances
dexterity and expands the workspace, this structural expansion also introduces
intensified inter-segmental coupling. Therefore, achieving precise TDR control
requires more complex models and additional motors. This paper presents a
reconfigurable tendon-driven robot (RTR) equipped with innovative lockable
joints. Each joint's state (locked/free) can be individually controlled through
a pair of antagonistic tendons, and its structure eliminates the need for a
continuous power supply to maintain the state. Operators can selectively
actuate the targeted robot segments, and this scheme fundamentally eliminates
the inter-segmental coupling, thereby avoiding the requirement for complex
coordinated control between segments. The workspace of RTR has been simulated
and compared with traditional TDRs' workspace, and RTR's advantages are further
revealed. The kinematics and statics models of the RTR have been derived and
validation experiments have been conducted. Demonstrations have been performed
using a seven-joint RTR prototype to show its reconfigurability and moving
ability in complex environments with an actuator pack comprising only six
motors.

</details>


### [260] [FAST-Calib: LiDAR-Camera Extrinsic Calibration in One Second](https://arxiv.org/abs/2507.17210)
*Chunran Zheng,Fu Zhang*

Main category: cs.RO

TL;DR: FAST-Calib 是一个快速、准确且用户友好的 LiDAR-相机外参数标定工具，使用定制 3D 目标，优于现有方法，并已开源。


<details>
  <summary>Details</summary>
Motivation: 本研究提出了一种快速、用户友好的 LiDAR-相机外参数标定方法，以解决现有方法在精度、鲁棒性和效率方面的不足。

Method: FAST-Calib 采用了一种高效且可靠的、与 LiDAR 扫描模式无关的边缘提取算法，并通过椭圆拟合来补偿由 LiDAR 斑点扩展引起的边缘扩张伪影，同时支持跨多个场景的联合优化。

Result: 实验结果表明，FAST-Calib 实现了优于现有方法的精度和鲁棒性，点到点注册误差低于 6.5mm，处理时间小于 0.7s，为机器人社区提供了高效、准确且基于目标的自动标定流程。

Conclusion: FAST-Calib 是一种基于定制 3D 目标、用于 LiDAR-相机外参数标定的工具，在三种 LiDAR 模型（Ouster、Avia 和 Mid360）上进行了验证，与现有方法相比，在精度和鲁棒性方面表现更优，点到点注册误差持续低于 6.5mm，总处理时间不到 0.7s。

Abstract: This paper proposes FAST-Calib, a fast and user-friendly LiDAR-camera
extrinsic calibration tool based on a custom-made 3D target. FAST-Calib
supports both mechanical and solid-state LiDARs by leveraging an efficient and
reliable edge extraction algorithm that is agnostic to LiDAR scan patterns. It
also compensates for edge dilation artifacts caused by LiDAR spot spread
through ellipse fitting, and supports joint optimization across multiple
scenes. We validate FAST-Calib on three LiDAR models (Ouster, Avia, and
Mid360), each paired with a wide-angle camera. Experimental results demonstrate
superior accuracy and robustness compared to existing methods. With
point-to-point registration errors consistently below 6.5mm and total
processing time under 0.7s, FAST-Calib provides an efficient, accurate, and
target-based automatic calibration pipeline. We have open-sourced our code and
dataset on GitHub to benefit the robotics community.

</details>


### [261] [Optimizing Delivery Logistics: Enhancing Speed and Safety with Drone Technology](https://arxiv.org/abs/2507.17253)
*Maharshi Shastri,Ujjval Shrivastav*

Main category: cs.RO

TL;DR: 该研究开发了一个集成人工智能的无人机配送系统，利用 YOLOv4 Tiny、GPS 和 SIM 模块进行优化、检测和通信。它解决了效率、安全和法规问题，与地面配送相比，展示了更快的配送速度和更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 日益增长的快速、低成本的最后一英里配送解决方案的需求，促进了无人机物流的重大进展。

Method: 该系统利用 YOLOv4 Tiny 进行物体检测，NEO 6M GPS 模块进行导航，A7670 SIM 模块进行实时通信。对轻量级人工智能模型和硬件组件进行了比较分析，以确定无人机实时配送的最佳配置。本论文提出了所提出系统的架构、设计和初步模拟结果。

Result: 与传统的地面配送相比，配送时间有所改善，并且通过面部识别实现了高精度的收件人身份验证。

Conclusion: 该研究描述了一个集成人工智能的无人机配送系统的开发，重点是路线优化、物体检测、安全包裹处理和实时跟踪。该系统通过集成机器学习技术、物联网设备和加密协议来解决电池效率、法规遵从性和安全考虑等关键挑战。初步研究表明，与传统的地面配送相比，配送时间有所改善，并且通过面部识别实现了高精度的收件人身份验证。该研究还讨论了无人机配送的道德影响和社会接受度，确保符合 FAA、EASA 和 DGCA 监管标准。

Abstract: The increasing demand for fast and cost effective last mile delivery
solutions has catalyzed significant advancements in drone based logistics. This
research describes the development of an AI integrated drone delivery system,
focusing on route optimization, object detection, secure package handling, and
real time tracking. The proposed system leverages YOLOv4 Tiny for object
detection, the NEO 6M GPS module for navigation, and the A7670 SIM module for
real time communication. A comparative analysis of lightweight AI models and
hardware components is conducted to determine the optimal configuration for
real time UAV based delivery. Key challenges including battery efficiency,
regulatory compliance, and security considerations are addressed through the
integration of machine learning techniques, IoT devices, and encryption
protocols. Preliminary studies demonstrate improvement in delivery time
compared to conventional ground based logistics, along with high accuracy
recipient authentication through facial recognition. The study also discusses
ethical implications and societal acceptance of drone deliveries, ensuring
compliance with FAA, EASA and DGCA regulatory standards. Note: This paper
presents the architecture, design, and preliminary simulation results of the
proposed system. Experimental results, simulation benchmarks, and deployment
statistics are currently being acquired. A comprehensive analysis will be
included in the extended version of this work.

</details>


### [262] [Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning](https://arxiv.org/abs/2507.17275)
*Po-Yen Wu,Cheng-Yu Kuo,Yuki Kadokawa,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 机器人强化学习框架结合有限元分析和 Miner 规则，通过自适应奖励归一化来学习寿命引导型工具使用策略，以延长通用工具的寿命。


<details>
  <summary>Details</summary>
Motivation: 在任务需求不确定的不可及环境中，机器人通常依赖没有预定使用策略的通用工具。这些工具并非为特定操作量身定制，其寿命高度依赖于使用方式。由此产生了一个基本挑战：机器人如何学习一种既能完成任务又能延长工具寿命的工具使用策略？

Method: 提出一个强化学习（RL）框架，该框架在策略优化中考虑了工具寿命。该框架利用有限元分析（FEA）和 Miner 规则根据累积应力估计剩余有用寿命（RUL），并将 RUL 整合到 RL 奖励中，以指导策略学习以实现寿命引导行为。为了处理 RUL 只能在任务执行后估计这一事实，我们引入了一种自适应奖励归一化（ARN）机制，该机制根据估计的 RUL 动态调整奖励缩放，确保稳定的学习信号。

Result: 在模拟和现实世界的工具使用任务中，例如物体移动和开门，所学策略一致地延长了工具寿命（在模拟中最多延长 8.01 倍），并能有效地迁移到现实世界中。

Conclusion: 所学策略在模拟和现实世界的工具使用任务中，例如物体移动和开门，都一致地延长了工具寿命（在模拟中最多延长 8.01 倍），并且能有效地迁移到现实世界中，证明了学习寿命引导型工具使用策略的实际价值。

Abstract: In inaccessible environments with uncertain task demands, robots often rely
on general-purpose tools that lack predefined usage strategies. These tools are
not tailored for particular operations, making their longevity highly sensitive
to how they are used. This creates a fundamental challenge: how can a robot
learn a tool-use policy that both completes the task and prolongs the tool's
lifespan? In this work, we address this challenge by introducing a
reinforcement learning (RL) framework that incorporates tool lifespan as a
factor during policy optimization. Our framework leverages Finite Element
Analysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based
on accumulated stress, and integrates the RUL into the RL reward to guide
policy learning toward lifespan-guided behavior. To handle the fact that RUL
can only be estimated after task execution, we introduce an Adaptive Reward
Normalization (ARN) mechanism that dynamically adjusts reward scaling based on
estimated RULs, ensuring stable learning signals. We validate our method across
simulated and real-world tool use tasks, including Object-Moving and
Door-Opening with multiple general-purpose tools. The learned policies
consistently prolong tool lifespan (up to 8.01x in simulation) and transfer
effectively to real-world settings, demonstrating the practical value of
learning lifespan-guided tool use strategies.

</details>


### [263] [VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback](https://arxiv.org/abs/2507.17294)
*Jianxin Bi,Kevin Yuchen Ma,Ce Hao,Mike Zheng Shou,Harold Soh*

Main category: cs.RO

TL;DR: VLA-Touch通过利用预训练的触觉-语言模型和扩散模型控制器，在不微调现有VLA模型的情况下，将触觉反馈整合到机器人策略中，从而提高了机器人处理富接触任务的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在处理需要物理交互的任务时，由于缺乏解读和利用触觉信号的能力，其有效性受到限制。然而，整合触觉反馈到这些系统中存在挑战，因为缺少大规模的多模态数据集。

Method: VLA-Touch提出了一种在不微调基础VLA模型的情况下，利用触觉感知增强通用机器人策略的方法。其创新点包括：1.设计了一个利用预训练的触觉-语言模型来提供语义触觉反馈，以支持高级任务规划；2.引入了一个基于扩散的模型控制器，利用触觉信号来优化VLA模型生成的动作，以实现精确的富接触操作。

Result: 通过实际世界实验证明，VLA-Touch在两个层面整合触觉反馈，能够提升任务规划效率，并增强操作精度。

Conclusion: VLA-Touch通过在两个层面整合触觉反馈，提升了任务规划效率和操作精度，解决了现有VLA模型在处理富接触任务时对触觉信号的解读和利用能力不足的问题。

Abstract: Tactile feedback is generally recognized to be crucial for effective
interaction with the physical world. However, state-of-the-art
Vision-Language-Action (VLA) models lack the ability to interpret and use
tactile signals, limiting their effectiveness in contact-rich tasks.
Incorporating tactile feedback into these systems is challenging due to the
absence of large multi-modal datasets. We present VLA-Touch, an approach that
enhances generalist robot policies with tactile sensing \emph{without
fine-tuning} the base VLA. Our method introduces two key innovations: (1) a
pipeline that leverages a pretrained tactile-language model that provides
semantic tactile feedback for high-level task planning, and (2) a
diffusion-based controller that refines VLA-generated actions with tactile
signals for contact-rich manipulation. Through real-world experiments, we
demonstrate that our dual-level integration of tactile feedback improves task
planning efficiency while enhancing execution precision. Code is open-sourced
at \href{https://github.com/jxbi1010/VLA-Touch}{this URL}.

</details>


### [264] [HuNavSim 2.0](https://arxiv.org/abs/2507.17317)
*Miguel Escudero-Jiménez,Noé Pérez-Higueras,Andrés Martínez-Silva,Fernando Caballero,Luis Merino*

Main category: cs.RO

TL;DR: HuNavSim v2 is out! Now with better behavior Trees for more realistic human simulations with robots. ROS 2, Gazebo, Isaac Sim compatible.


<details>
  <summary>Details</summary>
Motivation: To facilitate the development and evaluation of human-aware robot navigation systems in simulation by presenting an improved and feature-enhanced iteration of the Human Navigation Simulator (HuNavSim).

Method: The Human Navigation Simulator (HuNavSim) is an open-source tool programmed under the ROS 2 framework, compatible with robotics simulators like Gazebo and NVidia Isaac Sim. This version introduces an extended set of actions and conditions for Behavior Trees to create complex human behaviors.

Result: The new version of HuNavSim includes improvements to existing features and the addition of new ones, such as an expanded set of actions and conditions that can be combined in Behavior Trees to create complex and realistic human behaviors.

Conclusion: The new version of HuNavSim offers improved features and extended capabilities for simulating human-agent navigation behaviors with mobile robots, facilitating the development and evaluation of human-aware robot navigation systems.

Abstract: This work presents a new iteration of the Human Navigation Simulator
(HuNavSim), a novel open-source tool for the simulation of different
human-agent navigation behaviors in scenarios with mobile robots. The tool,
programmed under the ROS 2 framework, can be used together with different
well-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main
goal is to facilitate the development and evaluation of human-aware robot
navigation systems in simulation. In this new version, several features have
been improved and new ones added, such as the extended set of actions and
conditions that can be combined in Behavior Trees to compound complex and
realistic human behaviors.

</details>


### [265] [Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks](https://arxiv.org/abs/2507.17338)
*Corrado Pezzato,Ozan Çatal,Toon Van de Maele,Riddhi J. Pitliya,Tim Verbelen*

Main category: cs.RO

TL;DR: 主动推理可以扩展到现代机器人基准的复杂性。


<details>
  <summary>Details</summary>
Motivation: 填补主动推理在复杂、长时任务中的应用空白。

Method: 提出了一种完全分层的 ao 架构，该架构结合了选择离散技能的 ao 模型和通过全身 ao 控制器实现的技能。

Result: 在 Habitat 基准测试中的三个长时任务上，与最先进的基线相比，性能有所提高。

Conclusion: 该研究首次证明了主动推理可以扩展到现代机器人基准的复杂性，在 Habitat 基准测试中的三个长时任务上均优于最先进的基线。

Abstract: Despite growing interest in active inference for robotic control, its
application to complex, long-horizon tasks remains untested. We address this
gap by introducing a fully hierarchical active inference architecture for
goal-directed behavior in realistic robotic settings. Our model combines a
high-level active inference model that selects among discrete skills realized
via a whole-body active inference controller. This unified approach enables
flexible skill composition, online adaptability, and recovery from task
failures without requiring offline training. Evaluated on the Habitat Benchmark
for mobile manipulation, our method outperforms state-of-the-art baselines
across the three long-horizon tasks, demonstrating for the first time that
active inference can scale to the complexity of modern robotics benchmarks.

</details>


### [266] [An Exploratory Study on Human-Robot Interaction using Semantics-based Situational Awareness](https://arxiv.org/abs/2507.17376)
*Tianshu Ruan,Aniketh Ramesh,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本文研究了高层语义对人机交互的影响，发现在灾难响应任务中，语义有助于减轻操作员工作量，提高信任度，并加快反应时间。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索高层语义对移动机器人部署背景下的人机团队（HRT）和人机交互（HRI）的影响，因为在AI领域，高层语义如何使HRT范式受益的研究尚不充分、模糊且难以处理。

Method: 本文采用了一个基于语义的框架，在模拟的灾难响应任务中，揭示了环境中不同的指标（即语义信息的多少）。

Result: 研究结果表明，所提出的语义能够减轻人类操作员的感知工作量，提高操作员对态势的信任度，并帮助减少在需要时切换自主性级别时的反应时间。

Conclusion: 该研究表明，高层语义可以减轻人类操作员的感知工作量，提高操作员对态势的信任度，并减少在需要时切换自主性级别时的反应时间。此外，研究还发现，与系统有更高信任度的参与者更倾向于使用远程操作模式。

Abstract: In this paper, we investigate the impact of high-level semantics (evaluation
of the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction
(HRI) in the context of mobile robot deployments. Although semantics has been
widely researched in AI, how high-level semantics can benefit the HRT paradigm
is underexplored, often fuzzy, and intractable. We applied a semantics-based
framework that could reveal different indicators of the environment (i.e. how
much semantic information exists) in a mock-up disaster response mission. In
such missions, semantics are crucial as the HRT should handle complex
situations and respond quickly with correct decisions, where humans might have
a high workload and stress. Especially when human operators need to shift their
attention between robots and other tasks, they will struggle to build
Situational Awareness (SA) quickly. The experiment suggests that the presented
semantics: 1) alleviate the perceived workload of human operators; 2) increase
the operator's trust in the SA; and 3) help to reduce the reaction time in
switching the level of autonomy when needed. Additionally, we find that
participants with higher trust in the system are encouraged by high-level
semantics to use teleoperation mode more.

</details>


### [267] [Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models](https://arxiv.org/abs/2507.17379)
*Shen Tan,Dong Zhou,Xiangyu Shao,Junqiao Wang,Guanghui Sun*

Main category: cs.RO

TL;DR: LOVMM是一个结合了LLM和VLM的新型框架，用于解决具有自然语言指令的开放词汇移动操作任务，并在家庭环境中展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开放词汇移动操作（OVMM）在处理新颖、未见过的物体以及跨不同工作空间操作方面，对于实际机器人应用仍然是一个重大挑战。

Method: 提出了一种名为LOVMM的新型语言条件开放词汇移动操作框架，该框架结合了大型语言模型（LLM）和视觉语言模型（VLM），用于处理家庭环境中的各种移动操作任务。

Result: LOVMM能够通过自由形式的自然语言指令（例如，“将食品盒从办公室桌子扔到角落的垃圾桶中”和“将床上的瓶子打包到客房的箱子里”）来解决各种OVMM任务。

Conclusion: 该框架在复杂的家庭环境中进行了广泛的实验，展示了强大的零样本泛化和多任务学习能力，并且在多种桌面操作任务上表现优于其他最先进的方法。

Abstract: Open-vocabulary mobile manipulation (OVMM) that involves the handling of
novel and unseen objects across different workspaces remains a significant
challenge for real-world robotic applications. In this paper, we propose a
novel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named
LOVMM, incorporating the large language model (LLM) and vision-language model
(VLM) to tackle various mobile manipulation tasks in household environments.
Our approach is capable of solving various OVMM tasks with free-form natural
language instructions (e.g. "toss the food boxes on the office room desk to the
trash bin in the corner", and "pack the bottles from the bed to the box in the
guestroom"). Extensive experiments simulated in complex household environments
show strong zero-shot generalization and multi-task learning abilities of
LOVMM. Moreover, our approach can also generalize to multiple tabletop
manipulation tasks and achieve better success rates compared to other
state-of-the-art methods.

</details>


### [268] [Confidence Calibration in Vision-Language-Action Models](https://arxiv.org/abs/2507.17383)
*Thomas P Zollo,Richard Zemel*

Main category: cs.RO

TL;DR: 本研究探索了视觉-语言-动作（VLA）基础模型在机器人任务中的置信度校准问题，提出了提示集成和逐行动性Platt缩放等方法，以提高机器人行为的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 信任机器人行为不仅需要高水平的任务成功，还需要机器人能够可靠地量化其成功的可能性。

Method: 本研究包括了对VLA模型进行信心校准的系统性研究，采用了提示集成（prompt ensembles）和逐行动性Platt缩放（action-wise Platt scaling）等方法，并通过广泛的基准测试来理解任务成功与校准误差之间的关系。

Result: 研究发现任务表现和校准之间并不存在冲突，提示集成能够持续改进校准，并且在任务进行一段时间后置信度量化更为可靠，逐行动性Platt缩放能够独立地对每个行动维度进行重新校准，以产生更好的置信度估计。

Conclusion: 本研究旨在为使VLA模型在提供可靠的不确定性量化方面既表现良好又值得信赖，奠定必要的工具和概念基础。

Abstract: Trustworthy robot behavior requires not only high levels of task success but
also that the robot can reliably quantify how likely it is to succeed. To this
end, we present the first systematic study of confidence calibration in
vision-language-action (VLA) foundation models, which map visual observations
and natural-language instructions to low-level robot motor commands. We begin
with extensive benchmarking to understand the critical relationship between
task success and calibration error across multiple datasets and VLA variants,
finding that task performance and calibration are not in tension. Next, we
introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm
that averages confidence across paraphrased instructions and consistently
improves calibration. We further analyze calibration over the task time
horizon, showing that confidence is often most reliable after making some
progress, suggesting natural points for risk-aware intervention. Finally, we
reveal differential miscalibration across action dimensions and propose
action-wise Platt scaling, a method to recalibrate each action dimension
independently to produce better confidence estimates. Our aim in this study is
to begin to develop the tools and conceptual understanding necessary to render
VLAs both highly performant and highly trustworthy via reliable uncertainty
quantification.

</details>


### [269] [IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception](https://arxiv.org/abs/2507.17445)
*Haichuan Li,Changda Tian,Panos Trahanias,Tomi Westerlund*

Main category: cs.RO

TL;DR: IndoorBEV 是一种新的基于掩膜的鸟瞰图（BEV）方法，用于室内机器人感知，可以有效地检测各种静态和动态物体，解决了传统边界框方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了解决在复杂室内三维点云中检测各种物体（包括形状各异、存在杂乱以及静态和动态元素共存的物体）的挑战，因为传统的边界框方法在这种情况下效果不佳。

Method: 提出了一种新颖的基于掩膜的鸟瞰图（BEV）方法，称为 IndoorBEV。该方法使用轴向紧凑编码器和基于窗口的骨干网络从 BEV 地图中提取特征，并利用基于查询的解码器头来预测物体的类别和实例掩膜。

Result: 在自定义的室内数据集上证明了 IndoorBEV 的有效性，该数据集包含各种物体类别，包括静态物体以及机器人和杂项物品等动态元素，展示了其在鲁棒的室内场景理解方面的潜力。

Conclusion: 所提出的 IndoorBEV 方法通过其掩膜中心的方法，能够有效识别三维点云中的各种静态和动态物体，为室内移动机器人提供了鲁棒的解决方案。

Abstract: Detecting diverse objects within complex indoor 3D point clouds presents
significant challenges for robotic perception, particularly with varied object
shapes, clutter, and the co-existence of static and dynamic elements where
traditional bounding box methods falter. To address these limitations, we
propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor
mobile robots.
  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles
naturally occlusions and provides a consistent top-down view aiding to
distinguish static obstacles from dynamic agents. The obtained 2D BEV results
is directly usable to downstream robotic tasks like navigation, motion
prediction, and planning. Our architecture utilizes an axis compact encoder and
a window-based backbone to extract rich spatial features from this BEV map. A
query-based decoder head then employs learned object queries to concurrently
predict object classes and instance masks in the BEV space. This mask-centric
formulation effectively captures the footprint of both static and dynamic
objects regardless of their shape, offering a robust alternative to bounding
box regression. We demonstrate the effectiveness of IndoorBEV on a custom
indoor dataset featuring diverse object classes including static objects
  and dynamic elements like robots and miscellaneous items, showcasing its
potential for robust indoor scene understanding.

</details>


### [270] [Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners](https://arxiv.org/abs/2507.17519)
*Kostas Karakontis,Thanos Petsanis,Athanasios Ch. Kapoutsis,Pavlos Ch. Kapoutsis,Elias B. Kosmatopoulos*

Main category: cs.RO

TL;DR: 该研究提出了一种名为DARP-3D的三维覆盖路径规划算法，能够提升三维重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的多无人机覆盖路径规划（mCPP）算法通常将感兴趣区域（RoI）视为二维平面，忽略了重要的三维结构特征，导致三维重建不完整，尤其是在遮挡或垂直表面附近。

Method: 提出了一种模块化算法，可以将现有的二维路径规划器扩展到三维空间，通过调整无人机的高度和相机方向来实现地形感知规划。

Result: 与基线方法相比，所提出的方法在多个三维环境的模拟结果和真实世界的飞行测试中，持续捕获了更高质量的三维重建。

Conclusion: 该算法能够提高三维重建的质量，尤其是在处理具有显著垂直特征的区域时。

Abstract: Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial
software typically treat a Region of Interest (RoI) only as a 2D plane,
ignoring important3D structure characteristics. This leads to incomplete
3Dreconstructions, especially around occluded or vertical surfaces. In this
paper, we propose a modular algorithm that can extend commercial
two-dimensional path planners to facilitate terrain-aware planning by adjusting
altitude and camera orientations. To demonstrate it, we extend the well-known
DARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm
and produce DARP-3D. We present simulation results in multiple 3D environments
and a real-world flight test using DJI hardware. Compared to baseline, our
approach consistently captures improved 3D reconstructions, particularly in
areas with significant vertical features. An open-source implementation of the
algorithm is available here:https://github.com/konskara/TerraPlan

</details>


### [271] [InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation](https://arxiv.org/abs/2507.17520)
*Shuai Yang,Hao Li,Yilun Chen,Bin Wang,Yang Tian,Tai Wang,Hanqing Wang,Feng Zhao,Yiyi Liao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: InstructVLA 是一个端到端的 VLA 模型，通过 VLA-IT 训练范式，在保持 VLM 灵活推理能力的同时，实现了领先的操作性能，并在多项基准测试和真实世界应用中展现出优越的泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 VLA 模型往往牺牲了推理或动作生成的精确性，能力局限于特定任务的操作数据，并且会遗忘预训练的视觉-语言能力。InstructVLA 旨在解决这些问题。

Method: 提出了一种名为 InstructVLA 的端到端视觉-语言-动作 (VLA) 模型，并引入了一种新的训练范式：视觉-语言-动作指令调优 (VLA-IT)。该范式结合了多模态训练和混合专家适应，以联合优化文本推理和动作生成。

Result: 在 SimplerEnv 任务上，InstructVLA 比 SpatialVLA 提高了 30.5%。在 SimplerEnv-Instruct 基准测试中，InstructVLA 的表现比微调的 OpenVLA 高出 92%，比 GPT-4o 辅助的操作专家高出 29%。此外，InstructVLA 在多模态任务上超越了基线 VLM。

Conclusion: InstructVLA 在模拟和真实世界环境中，通过利用文本推理来提高操作性能，展示了弥合直观和可控的人机交互与高效策略学习之间差距的潜力。

Abstract: To operate effectively in the real world, robots must integrate multimodal
reasoning with precise action generation. However, existing
vision-language-action (VLA) models often sacrifice one for the other, narrow
their abilities to task-specific manipulation data, and suffer catastrophic
forgetting of pre-trained vision-language capabilities. To bridge this gap, we
introduce InstructVLA, an end-to-end VLA model that preserves the flexible
reasoning of large vision-language models (VLMs) while delivering leading
manipulation performance. InstructVLA introduces a novel training paradigm,
Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal
training with mixture-of-experts adaptation to jointly optimize textual
reasoning and action generation on both standard VLM corpora and a curated
650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves
30.5% improvement over SpatialVLA. To evaluate generalization, we introduce
SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and
high-level instruction understanding, where it outperforms a fine-tuned OpenVLA
by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA
surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling
by leveraging textual reasoning to boost manipulation performance in both
simulated and real-world settings. These results demonstrate InstructVLA's
potential for bridging intuitive and steerable human-robot interaction with
efficient policy learning.

</details>


### [272] [When and Where Localization Fails: An Analysis of the Iterative Closest Point in Evolving Environment](https://arxiv.org/abs/2507.17531)
*Abdel-Raouf Dannaoui,Johann Laconte,Christophe Debain,Francois Pomerleau,Paul Checchin*

Main category: cs.RO

TL;DR: 本研究解决了在动态户外环境中短期环境变化对3D激光雷达定位的挑战，提出了一个多时相数据集和ICP评估框架，强调了点到平面在提高定位精度和稳定性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 在动态的户外环境中实现鲁棒的重定位是依赖3D激光雷达的自主系统的关键挑战。尽管长期定位已被广泛研究，但实际意义重大的短期环境变化（在几天或几周内发生）仍未得到充分探索。

Method: 本研究提出了一种高分辨率、短期、多时相的数据集，该数据集于2025年2月至4月每周在自然和半城市环境中收集。每个会话包括高密度点云图、360度全景图像和轨迹数据。使用从点云图派生的激光雷达扫描，并结合传感器精确遮挡进行建模，以评估点到点和点到平面两种迭代最近点（ICP）变体的对齐精度。

Result: 结果表明，点到平面在注册方面提供了明显更稳定和准确的性能，特别是在特征稀疏或植被茂密的区域。

Conclusion: 该研究提供了一个结构化的数据集，用于评估短期定位鲁棒性，一个用于在噪声下分析扫描到地图配准的可复现框架，以及在不断变化的户外环境中对ICP性能进行比较评估。研究结果强调了局部几何和环境可变性如何影响定位成功率，为设计更具弹性的机器人系统提供了见解。

Abstract: Robust relocalization in dynamic outdoor environments remains a key challenge
for autonomous systems relying on 3D lidar. While long-term localization has
been widely studied, short-term environmental changes, occurring over days or
weeks, remain underexplored despite their practical significance. To address
this gap, we present a highresolution, short-term multi-temporal dataset
collected weekly from February to April 2025 across natural and semi-urban
settings. Each session includes high-density point cloud maps, 360 deg
panoramic images, and trajectory data. Projected lidar scans, derived from the
point cloud maps and modeled with sensor-accurate occlusions, are used to
evaluate alignment accuracy against the ground truth using two Iterative
Closest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show
that Point-to-Plane offers significantly more stable and accurate registration,
particularly in areas with sparse features or dense vegetation. This study
provides a structured dataset for evaluating short-term localization
robustness, a reproducible framework for analyzing scan-to-map alignment under
noise, and a comparative evaluation of ICP performance in evolving outdoor
environments. Our analysis underscores how local geometry and environmental
variability affect localization success, offering insights for designing more
resilient robotic systems.

</details>


### [273] [Robot-mediated physical Human-Human Interaction in Neurorehabilitation: a position paper](https://arxiv.org/abs/2507.17561)
*Lorenzo Vianello,Matthew Short,Julia Manczurowsky,Emek Barış Küçüktabak,Francesco Di Tommaso,Alessia Noccaro,Laura Bandini,Shoshana Clark,Alaina Fiorenza,Francesca Lunardini,Alberto Canton,Marta Gandolla,Alessandra L. G. Pedrocchi,Emilia Ambrosini,Manuel Murie-Fernandez,Carmen B. Roman,Jesus Tornero,Natacha Leon,Andrew Sawers,Jim Patton,Domenico Formica,Nevio Luigi Tagliamonte,Georg Rauter,Kilian Baur,Fabian Just,Christopher J. Hasson,Vesna D. Novak,Jose L. Pons*

Main category: cs.RO

TL;DR: 该研究提出了一种名为“机器人辅助的物理人际交互”的新方法，该方法结合了物理治疗师的专业知识和机器人的精确性，以改善神经康复。


<details>
  <summary>Details</summary>
Motivation: 目前的神经康复方法虽然有机器人系统的辅助，但未能充分利用治疗师的适应性和临床专业知识。该研究旨在通过机器人辅助的物理人际交互来解决这一不足，以整合治疗师的专业知识和机器人的优势。

Method: 本研究提出了一种多学科方法，并利用了一个统一的分类法来描述机器人辅助康复，一个基于社会心理学的交互框架，以及一种使机器人系统成为自然人际交互的无缝促进者的技术方法。

Result: 通过整合治疗师的专业知识和机器人技术的优势，RMNHI 框架有潜力改善患者的康复效果，并使康复过程更加自然和高效。

Conclusion: 这项工作提出了机器人辅助的物理人际交互（RMNHI）的概念，这是一种旨在弥合传统手动治疗与康复机器人技术之间差距的框架，使治疗师能够利用其临床专业知识和细致的决策能力，结合机器人技术的优势，如力量、精度和可重复性。

Abstract: Neurorehabilitation conventionally relies on the interaction between a
patient and a physical therapist. Robotic systems can improve and enrich the
physical feedback provided to patients after neurological injury, but they
under-utilize the adaptability and clinical expertise of trained therapists. In
this position paper, we advocate for a novel approach that integrates the
therapist's clinical expertise and nuanced decision-making with the strength,
accuracy, and repeatability of robotics: Robot-mediated physical Human-Human
Interaction. This framework, which enables two individuals to physically
interact through robotic devices, has been studied across diverse research
groups and has recently emerged as a promising link between conventional manual
therapy and rehabilitation robotics, harmonizing the strengths of both
approaches. This paper presents the rationale of a multidisciplinary
team-including engineers, doctors, and physical therapists-for conducting
research that utilizes: a unified taxonomy to describe robot-mediated
rehabilitation, a framework of interaction based on social psychology, and a
technological approach that makes robotic systems seamless facilitators of
natural human-human interaction.

</details>


### [274] [KernelSOS for Global Sampling-Based Optimal Control and Estimation via Semidefinite Programming](https://arxiv.org/abs/2507.17572)
*Antoine Groudiev,Fabian Schramm,Éloïse Berthier,Justin Carpentier,Frederike Dümbgen*

Main category: cs.RO

TL;DR: KernelSOS框架通过结合多项式优化和核方法，在解决控制和估计问题方面表现出色，尤其擅长处理具有挑战性的局部极小值。


<details>
  <summary>Details</summary>
Motivation: 解决具有不良局部极小值的控制和估计问题。

Method: Kernel Sum of Squares (KernelSOS) 框架，结合了多项式优化和核方法。

Result: KernelSOS在估计问题上与其他平方和方法具有竞争力，并且能够处理非多项式和非参数化问题，还可以应用于带有集成模拟器的轨迹优化问题。

Conclusion: KernelSOS框架在控制和估计问题上表现良好，能够解决非多项式和非参数化问题，并且可以作为局部求解器的初始化方法，有助于发现更好的解决方案。

Abstract: Global optimization has gained attraction over the past decades, thanks to
the development of both theoretical foundations and efficient numerical
routines to cope with optimization problems of various complexities. Among
recent methods, Kernel Sum of Squares (KernelSOS) appears as a powerful
framework, leveraging the potential of sum of squares methods from the
polynomial optimization community with the expressivity of kernel methods
widely used in machine learning. This paper applies the kernel sum of squares
framework for solving control and estimation problems, which exhibit poor local
minima. We demonstrate that KernelSOS performs well on a selection of problems
from both domains. In particular, we show that KernelSOS is competitive with
other sum of squares approaches on estimation problems, while being applicable
to non-polynomial and non-parametric formulations. The sample-based nature of
KernelSOS allows us to apply it to trajectory optimization problems with an
integrated simulator treated as a black box, both as a standalone method and as
a powerful initialization method for local solvers, facilitating the discovery
of better solutions.

</details>


### [275] [Event Detection for Active Lower Limb Prosthesis](https://arxiv.org/abs/2507.17649)
*J. D. Clark,P. Ellison*

Main category: cs.RO

TL;DR: 该研究表明，通过使用具有前后交叉韧带模拟物的双髁膝关节设计，可以根据韧带拉伸情况精确检测假肢步态周期中的关键事件，从而提高假肢的控制精度。


<details>
  <summary>Details</summary>
Motivation: 精确的事件检测对于成功设计半被动和动力假肢至关重要。研究十字韧带拉伸在事件检测中的作用，以解决现有铰链式膝关节设计丢失部分运动行为的问题。

Method: 使用铰链式膝关节设计，并引入前后交叉韧带的类似物进行约束。通过与韧带平行的 LVDT 记录韧带的拉伸情况，在三种不同速度下，在拐杖屈膝装置上进行数据采集。

Result: 研究发现在步态周期的约 5% 和 80% 处，后交叉韧带和前交叉韧带的拉伸存在速度依赖性。此外，在步态周期的约 90% 和 95% 处，分别存在后交叉韧带和前交叉韧带的转折点，可用于预测初始接触和足平。

Conclusion: 使用双髁膝关节设计可以改善步态周期中事件的检测，从而提高后续动力假肢控制器的准确性。

Abstract: Accurate event detection is key to the successful design of semi-passive and
powered prosthetics. Kinematically, the natural knee is complex, with
translation and rotation components that have a substantial impact on gait
characteristics. When simplified to a pin joint, some of this behaviour is
lost. This study investigates the role of cruciate ligament stretch in event
detection. A bicondylar knee design was used, constrained by analogues of the
anterior and posterior cruciate ligaments. This offers the ability to
characterize knee kinematics by the stretch of the ligaments. The ligament
stretch was recorded using LVDTs parallel to the ligaments of the Russell knee
on a bent knee crutch. Which was used to capture data on a treadmill at 3
speeds. This study finds speed dependence within the stretch of the cruciate
ligaments, prominently around 5\% and 80\% of the gait cycle for the posterior
and anterior. The cycle profile remains consistent with speed; therefore, other
static events such as the turning point feature at around 90\% and 95\% of the
cycle, for the posterior and anterior, respectively, could be used as a
predictive precursor for initial contact. Likewise at 90\% and 95\%, another
pair of turning points that in this case could be used to predict foot flat.
This concludes that the use of a bicondylar knee design could improve the
detection of events during the gait cycle, and therefore could increase the
accuracy of subsequent controllers for powered prosthetics.

</details>


### [276] [Safety Assurance for Quadrotor Kinodynamic Motion Planning](https://arxiv.org/abs/2507.17679)
*Theodoros Tavoulareas,Marzia Cescon*

Main category: cs.RO

TL;DR: Drones need to be safe! This paper adds a safety layer to drone navigation planning so they don't break their own rules or crash. Tested successfully on a small drone.


<details>
  <summary>Details</summary>
Motivation: Existing motion planning methods for drones, while generating collision-free paths, do not account for the safe operational region of the system, potentially leading to safety violations. This research aims to address this gap by integrating runtime safety assurance into the planning process.

Method: The proposed method uses a sampling-based geometric planner for high-level pathfinding and a low-level safety assurance filter for LQR control input, ensuring safety constraints are met during trajectory tracking. Tested on a Crazyflie 2.0 drone model in a 3D simulation.

Result: The method was demonstrated in a restricted 3D simulation environment using a Crazyflie 2.0 drone model, showing successful integration of safety assurance into kinodynamic motion planning.

Conclusion: Autonomous drones require robust safety mechanisms to prevent accidents. This paper proposes a method combining motion planning with runtime safety assurance to ensure safe operation within the drone's constraints.

Abstract: Autonomous drones have gained considerable attention for applications in
real-world scenarios, such as search and rescue, inspection, and delivery. As
their use becomes ever more pervasive in civilian applications, failure to
ensure safe operation can lead to physical damage to the system, environmental
pollution, and even loss of human life. Recent work has demonstrated that
motion planning techniques effectively generate a collision-free trajectory
during navigation. However, these methods, while creating the motion plans, do
not inherently consider the safe operational region of the system, leading to
potential safety constraints violation during deployment. In this paper, we
propose a method that leverages run time safety assurance in a kinodynamic
motion planning scheme to satisfy the system's operational constraints. First,
we use a sampling-based geometric planner to determine a high-level
collision-free path within a user-defined space. Second, we design a low-level
safety assurance filter to provide safety guarantees to the control input of a
Linear Quadratic Regulator (LQR) designed with the purpose of trajectory
tracking. We demonstrate our proposed approach in a restricted 3D simulation
environment using a model of the Crazyflie 2.0 drone.

</details>


### [277] [CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation](https://arxiv.org/abs/2507.17727)
*Robel Mamo,Taeyeong Choi*

Main category: cs.RO

TL;DR: CA-Cut 通过在作物行附近遮挡图像区域来提高视觉导航模型的鲁棒性，在公共玉米地数据集上可将预测误差最多降低 36.9%。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉模型在现实田野部署中的可靠性需要大量训练数据，而数据收集成本高昂。此外，仅使用颜色抖动、高斯模糊和水平翻转等数据增强技术可能导致性能不佳，尤其是在存在遮挡、碎片和作物间距不均匀的复杂树冠下环境中。

Method: 提出了一种名为 Crop-Aligned Cutout (CA-Cut) 的新颖数据增强方法，通过在作物行两侧的输入图像中遮挡随机区域来模拟遮挡，以鼓励训练模型捕获高层上下文特征。

Result: 实验证明，CA-Cut 显著提高了模型的鲁棒性，在语义关键点预测方面表现出色，预测误差最多可降低 36.9%。通过消融研究确定了最优的遮挡数量、大小和空间分布，以最大化整体性能。

Conclusion: CA-Cut (Crop-Aligned Cutout) 是一种新颖的数据增强方法，通过在作物行周围遮挡随机区域来鼓励模型捕获高层上下文特征，即使在精细信息被遮挡的情况下也能如此。该方法在公共玉米地数据集上的实验证明了其有效性，可将预测误差最多降低 36.9%，并提高了跨不同环境的预测准确性和泛化能力。

Abstract: State-of-the-art visual under-canopy navigation methods are designed with
deep learning-based perception models to distinguish traversable space from
crop rows. While these models have demonstrated successful performance, they
require large amounts of training data to ensure reliability in real-world
field deployment. However, data collection is costly, demanding significant
human resources for in-field sampling and annotation. To address this
challenge, various data augmentation techniques are commonly employed during
model training, such as color jittering, Gaussian blur, and horizontal flip, to
diversify training data and enhance model robustness. In this paper, we
hypothesize that utilizing only these augmentation techniques may lead to
suboptimal performance, particularly in complex under-canopy environments with
frequent occlusions, debris, and non-uniform spacing of crops. Instead, we
propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)
which masks random regions out in input images that are spatially distributed
around crop rows on the sides to encourage trained models to capture high-level
contextual features even when fine-grained information is obstructed. Our
extensive experiments with a public cornfield dataset demonstrate that
masking-based augmentations are effective for simulating occlusions and
significantly improving robustness in semantic keypoint predictions for visual
navigation. In particular, we show that biasing the mask distribution toward
crop rows in CA-Cut is critical for enhancing both prediction accuracy and
generalizability across diverse environments achieving up to a 36.9% reduction
in prediction error. In addition, we conduct ablation studies to determine the
number of masks, the size of each mask, and the spatial distribution of masks
to maximize overall performance.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [278] [Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature](https://arxiv.org/abs/2507.16820)
*Ngan Tran,Haihua Chen,Ana Cleveland,Yuhan Zhou*

Main category: cs.SI

TL;DR: 本研究利用AI分析了2020-2022年灾害信息学文献，发现COVID-19影响了研究重点（偏向公共卫生），国家/地区/作者合作模式，以及领域发展趋势（韧性、数据共享）。研究提供了战略见解以提升灾害信息学能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在全面分析灾害信息学文献，以识别该领域的关键趋势、发展模式和研究重点的演变，特别是COVID-19大流行对该领域的影响。通过提供战略性见解，帮助政策制定者、从业者和学者提升灾害信息学能力，以应对复杂多变的风险环境。

Method: 本研究采用文献计量和主题分析方法，利用预训练语言模型和生成式人工智能技术，分析了2020年1月至2022年9月期间的灾害信息学文献。具体包括识别活跃国家、机构、作者、合作网络、新兴主题、重要主题之间的模式以及COVID-19大流行引起的研究重点转移。

Result: 研究结果显示，受COVID-19影响大的国家研究活跃度高，且各具研究特色。地理位置相近或语言相同的国家与机构之间存在合作倾向。顶尖作者倾向于与少数关键伙伴合作，而作者个人研究领域相对集中，机构研究兴趣则更为广泛。COVID-19大流行促使研究重点转向公共卫生。灾害信息学领域正趋向于多维度韧性策略和跨部门数据共享。研究中还提出了数据分析、主题提取和结果可视化的方法。

Conclusion: 该研究通过对2020年1月至2022年9月间发表的灾害信息学文献进行文献计量和主题分析，揭示了该领域的关键趋势和发展方向。研究结果表明，受COVID-19影响最大的国家在研究活跃度方面也名列前茅，并且各国在研究兴趣上具有独特性。同一地区或使用相同语言的国家和机构倾向于合作。顶尖活跃作者通常与一到两个关键伙伴形成紧密合作关系，而作者通常专注于一到两个特定主题，机构则展现出更广泛的主题兴趣。COVID-19大流行对灾害信息学的研究重点产生了影响，更加关注公共卫生。此外，该领域正朝着多维度韧性策略和跨部门数据共享合作的方向发展，这反映了对全球脆弱性和相互依存性的日益关注。研究中提出的数据收集、质量保证、分析方法、主题提取与总结以及结果可视化工具可应用于类似的数据集或分析问题。通过描绘灾害信息学的发展趋势，本研究为政策制定者、从业者和学者提供了战略见解，以应对日益不确定的复杂风险环境，从而增强灾害信息学能力。

Abstract: This study presents a comprehensive bibliometric and topic analysis of the
disaster informatics literature published between January 2020 to September
2022. Leveraging a large-scale corpus and advanced techniques such as
pre-trained language models and generative AI, we identify the most active
countries, institutions, authors, collaboration networks, emergent topics,
patterns among the most significant topics, and shifts in research priorities
spurred by the COVID-19 pandemic. Our findings highlight (1) countries that
were most impacted by the COVID-19 pandemic were also among the most active,
with each country having specific research interests, (2) countries and
institutions within the same region or share a common language tend to
collaborate, (3) top active authors tend to form close partnerships with one or
two key partners, (4) authors typically specialized in one or two specific
topics, while institutions had more diverse interests across several topics,
and (5) the COVID-19 pandemic has influenced research priorities in disaster
informatics, placing greater emphasis on public health. We further demonstrate
that the field is converging on multidimensional resilience strategies and
cross-sectoral data-sharing collaborations or projects, reflecting a heightened
awareness of global vulnerability and interdependency. Collecting and quality
assurance strategies, data analytic practices, LLM-based topic extraction and
summarization approaches, and result visualization tools can be applied to
comparable datasets or solve similar analytic problems. By mapping out the
trends in disaster informatics, our analysis offers strategic insights for
policymakers, practitioners, and scholars aiming to enhance disaster
informatics capacities in an increasingly uncertain and complex risk landscape.

</details>


### [279] [EVOLVE-X: Embedding Fusion and Language Prompting for User Evolution Forecasting on Social Media](https://arxiv.org/abs/2507.16847)
*Ismail Hossain,Sai Puppala,Md Jahangir Alam,Sajedul Talukder*

Main category: cs.SI

TL;DR: 本研究提出了一种结合多种语言模型（包括 Llama-3、Mistral、Gemma、GPT-2、BERT、RoBERTa）的新方法，用于分析和预测用户在社交媒体上的行为演变。实验证明该方法有效，特别是 GPT-2 在跨模态配置下表现最佳，能够预测网络变化、新连接和活动转移，并为用户提供风险预警。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台是分享个人情感、日常活动和生活事件的重要媒介，使用户能够及时了解最新动态。用户在平台上的行为会随着人口统计学属性和所形成的网络而演变。本研究的动机在于分析和预测这种用户行为的演变，以应对社交媒体中的好友推荐和活动预测等挑战，并为用户提供潜在负面结果的早期预警。

Method: 本研究提出了一种新颖的方法，该方法结合了开源模型（Llama-3-Instruct、Mistral-7B-Instruct、Gemma-7B-IT）和预训练模型（GPT-2、BERT、RoBERTa）。具体来说，它利用提示工程来处理开源模型，并采用联合嵌入技术处理预训练模型，以分析和预测用户在社交媒体上的终身行为演变。

Result: 实验结果表明，本研究提出的方法能够有效地预测用户社交演变的未来阶段，包括网络变化、未来连接和用户活动的变化。具体而言，GPT-2 在跨模态配置下取得了最低的困惑度（8.21），优于 RoBERTa（9.11）和 BERT，证明了跨模态配置在提升性能方面的重要性。

Conclusion: 本研究提出了一种利用 Llama-3-Instruct、Mistral-7B-Instruct、Gemma-7B-IT 等开源模型，结合 GPT-2、BERT 和 RoBERTa 模型，通过提示工程和联合嵌入技术来分析和预测用户在社交媒体上终身行为演变的新方法。实验结果表明，该方法能够预测用户网络的未来变化、新的连接以及活动的变化，其中 GPT-2 在跨模态配置下实现了最低的困惑度（8.21），优于 RoBERTa（9.11）和 BERT，凸显了跨模态配置的重要性。该方法有望解决社交媒体中的好友推荐和活动预测等关键挑战，并为用户提供潜在负面结果的早期预警，从而帮助用户做出明智的决策并降低长期风险。

Abstract: Social media platforms serve as a significant medium for sharing personal
emotions, daily activities, and various life events, ensuring individuals stay
informed about the latest developments. From the initiation of an account,
users progressively expand their circle of friends or followers, engaging
actively by posting, commenting, and sharing content. Over time, user behavior
on these platforms evolves, influenced by demographic attributes and the
networks they form. In this study, we present a novel approach that leverages
open-source models Llama-3-Instruct, Mistral-7B-Instruct, Gemma-7B-IT through
prompt engineering, combined with GPT-2, BERT, and RoBERTa using a joint
embedding technique, to analyze and predict the evolution of user behavior on
social media over their lifetime. Our experiments demonstrate the potential of
these models to forecast future stages of a user's social evolution, including
network changes, future connections, and shifts in user activities.
Experimental results highlight the effectiveness of our approach, with GPT-2
achieving the lowest perplexity (8.21) in a Cross-modal configuration,
outperforming RoBERTa (9.11) and BERT, and underscoring the importance of
leveraging Cross-modal configurations for superior performance. This approach
addresses critical challenges in social media, such as friend recommendations
and activity predictions, offering insights into the trajectory of user
behavior. By anticipating future interactions and activities, this research
aims to provide early warnings about potential negative outcomes, enabling
users to make informed decisions and mitigate risks in the long term.

</details>


### [280] [Weak Links in LinkedIn: Enhancing Fake Profile Detection in the Age of LLMs](https://arxiv.org/abs/2507.16860)
*Apoorva Gulati,Rajesh Kumar,Vinti Agarwal,Aditya Sharma*

Main category: cs.SI

TL;DR: LLM生成的虚假个人资料对现有的检测器构成了重大风险，但GPT辅助对抗性训练可以提高检测器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）使得在LinkedIn等平台上创建逼真的虚假个人资料变得更加容易，这对现有的文本虚假个人资料检测器构成了重大风险。

Method: 本研究使用GPT辅助对抗性训练来提高文本虚假个人资料检测器对LLM生成虚假个人资料的鲁棒性。通过消融研究，我们发现结合数值和文本嵌入的检测器是最鲁棒的，其次是仅使用数值嵌入的检测器，最后是仅使用文本嵌入的检测器。

Result: 现有的检测器在检测手动创建的虚假个人资料方面非常有效（错误接受率：6%-7%），但在识别GPT生成的虚假个人资料方面失败（错误接受率：42%-52%）。本研究提出的GPT辅助对抗性训练方法将错误接受率恢复到1%-7%，而不会影响错误拒绝率（0.5%-2%）。

Conclusion: 本研究提出了GPT辅助对抗性训练作为一种对抗LLM生成虚假个人资料的方法，并将错误接受率恢复到1%-7%，同时保持了较低的错误拒绝率（0.5%-2%）。结合数值和文本嵌入的检测器表现出最高的鲁棒性。

Abstract: Large Language Models (LLMs) have made it easier to create realistic fake
profiles on platforms like LinkedIn. This poses a significant risk for
text-based fake profile detectors. In this study, we evaluate the robustness of
existing detectors against LLM-generated profiles. While highly effective in
detecting manually created fake profiles (False Accept Rate: 6-7%), the
existing detectors fail to identify GPT-generated profiles (False Accept Rate:
42-52%). We propose GPT-assisted adversarial training as a countermeasure,
restoring the False Accept Rate to between 1-7% without impacting the False
Reject Rates (0.5-2%). Ablation studies revealed that detectors trained on
combined numerical and textual embeddings exhibit the highest robustness,
followed by those using numerical-only embeddings, and lastly those using
textual-only embeddings. Complementary analysis on the ability of prompt-based
GPT-4Turbo and human evaluators affirms the need for robust automated detectors
such as the one proposed in this study.

</details>


### [281] [Dynamic Simulation Framework for Disinformation Dissemination and Correction With Social Bots](https://arxiv.org/abs/2507.16848)
*Boyu Qiao,Kun Li,Wei Zhou,Songlin Hu*

Main category: cs.SI

TL;DR: MADD是一个多智能体框架，通过更真实的模拟来研究虚假信息传播及其纠正策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究在模拟虚假信息传播时，往往依赖于简化的用户和网络模型，忽视了机器人的动态行为，并且缺乏对纠正策略的量化评估。因此，有必要开发一个更全面的框架来解决这些问题。

Method: MADD框架结合了Barabasi Albert模型和Stochastic Block模型来构建更真实的传播网络，并整合了恶意和合法机器人，允许对纠正策略进行量化分析。研究使用个体和群体层面的指标进行评估，并验证了MADD的用户属性和网络结构的真实世界一致性。

Result: 实验模拟了六个虚假信息主题的传播，证明了基于事实和基于叙述的纠正策略具有不同的效果。MADD框架在用户属性和网络结构上与真实世界数据具有一致性。

Conclusion: 该研究提出了一个名为MADD的多智能体框架，用于模拟和分析信息生态系统中的虚假信息传播，并评估纠正策略的有效性。研究通过结合多种网络模型和考虑动态用户行为，为理解和控制虚假信息提供了更现实的模拟环境。

Abstract: In the human-bot symbiotic information ecosystem, social bots play key roles
in spreading and correcting disinformation. Understanding their influence is
essential for risk control and better governance. However, current studies
often rely on simplistic user and network modeling, overlook the dynamic
behavior of bots, and lack quantitative evaluation of correction strategies. To
fill these gaps, we propose MADD, a Multi Agent based framework for
Disinformation Dissemination. MADD constructs a more realistic propagation
network by integrating the Barabasi Albert Model for scale free topology and
the Stochastic Block Model for community structures, while designing node
attributes based on real world user data. Furthermore, MADD incorporates both
malicious and legitimate bots, with their controlled dynamic participation
allows for quantitative analysis of correction strategies. We evaluate MADD
using individual and group level metrics. We experimentally verify the real
world consistency of MADD user attributes and network structure, and we
simulate the dissemination of six disinformation topics, demonstrating the
differential effects of fact based and narrative based correction strategies.

</details>


### [282] [Cross-Subreddit Behavior as Open-Source Indicators of Coordinated Influence: A Case Study of r/Sino & r/China](https://arxiv.org/abs/2507.16857)
*Manon Pilaud,Ian McCulloh*

Main category: cs.SI

TL;DR: 本研究分析了Reddit上两个关于中国政治讨论的社区（r/Sino和r/China）的用户活动，以识别协调影响行为的迹象。研究人员利用话题建模、情感分析和行为分析，结合用户活动历史和元数据，发现了可能存在的不真实或战略性参与模式。研究结果表明，结合内容和活动信号分析对于理解在线信息环境中的影响行为非常有效。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在调查在r/Sino和r/China这两个意识形态上存在分歧的Reddit社区中，参与者之间可能存在的协调影响活动的指标。

Method: 本研究利用话题建模和情感分析来构建用户-话题情感矩阵，并结合用户活动历史和元数据进行行为分析，以识别可能存在的不真实或战略性参与模式。

Result: 通过结合语言和行为分析，本研究能够识别出与不真实或战略性参与一致的模式。

Conclusion: 本研究表明，整合内容和活动信号对于分析在线影响行为在信息环境中的应用非常有用。

Abstract: This study investigates potential indicators of coordinated influence
activity among users participating in both r/Sino and r/China, two
ideologically divergent Reddit communities focused on Chinese political
discourse. Topic modeling and sentiment analysis are applied to all posts and
comments authored by dual-subreddit users to construct a user-topic sentiment
matrix. Individual sentiment patterns are compared to global topic baselines
derived from the broader r/Sino and r/China populations. Behavioral profiling
is performed using full user activity histories and metadata, incorporating
measures such as lexical diversity, language consistency, account age, posting
frequency, and karma distribution. Users exhibiting multiple behavioral
anomalies are identified and examined within a subreddit co-participation
network to assess structural overlap. The combined linguistic and behavioral
analysis enables the identification of patterns consistent with inauthentic or
strategically structured participation. These findings demonstrate the utility
of integrating content and activity-based signals in the analysis of online
influence behavior within contested information environments.

</details>


### [283] [Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky](https://arxiv.org/abs/2507.16858)
*Gordon Hew,Ian McCulloh*

Main category: cs.SI

TL;DR: 本研究分析了国会民主党议员如何利用Bluesky平台进行政治传播和建立影响力。研究发现，尽管一些知名领袖获得了更高的可见度，但一些不太知名的议员在网络中扮演着关键角色。研究还指出了意识形态、州别和领导层在网络形成中的同质性，并识别了不同群体之间的特定话语模式。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在调查国会民主党议员如何利用Bluesky平台在缺乏算法放大的环境中形成影响力网络和传播政治信息。

Method: 本研究采用混合方法，结合了社交网络分析、指数随机图模型（ERGM）和基于Transformer的主题建模（BERTopic），分析了182名经认证的国会民主党议员之间的关注、提及、转发和话语模式。

Result: 研究发现，虽然Hakeem Jeffries和Elizabeth Warren等党内领袖在可见度指标上占主导地位，但Marcy Kaptur、Donald Beyer和Dwight Evans等被忽视的人物占据了结构中心位置，表明在数字党内生态系统中存在潜在影响力。ERGM结果显示，在意识形态、州和领导层方面存在明显的同质性，而参议院领导层的连接性较低。主题分析确定了共同主题（例如，生殖权利、外国冲突）和子群体特定问题，其中“The Squad”展示了最独特的话语特征。

Conclusion: 本研究揭示了去中心化平台（如Bluesky）在重塑党内沟通动态方面的潜力，并强调了在新兴数字环境中对精英政治行为进行持续计算研究的必要性。

Abstract: Following the 2024 U.S. presidential election, Democratic lawmakers and their
supporters increasingly migrated from mainstream social media plat-forms like X
(formerly Twitter) to decentralized alternatives such as Bluesky. This study
investigates how Congressional Democrats use Bluesky to form networks of
influence and disseminate political messaging in a platform environment that
lacks algorithmic amplification. We employ a mixed-methods approach that
combines social network analysis, expo-nential random graph modeling (ERGM),
and transformer-based topic mod-eling (BERTopic) to analyze follows, mentions,
reposts, and discourse pat-terns among 182 verified Democratic members of
Congress. Our findings show that while party leaders such as Hakeem Jeffries
and Elizabeth War-ren dominate visibility metrics, overlooked figures like
Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central
positions, suggesting latent influence within the digital party ecosystem. ERGM
re-sults reveal significant homophily along ideological, state, and leadership
lines, with Senate leadership exhibiting lower connectivity. Topic analysis
identifies both shared themes (e.g., reproductive rights, foreign conflicts)
and subgroup-specific issues, with The Squad showing the most distinct
discourse profile. These results demonstrate the potential of decentralized
platforms to reshape intra-party communication dynamics and highlight the need
for continued computational research on elite political behavior in emerging
digital environments.

</details>


### [284] [Dynamics of temporal influence in polarised networks](https://arxiv.org/abs/2507.17177)
*Caroline B. Pena,David J. P. O'Sullivan,Pádraig MacCarron,Akrati Saxena*

Main category: cs.SI

TL;DR: 本研究旨在研究极化社交网络中用户影响力的时间动态，并比较了使用时间中心性度量（扩展以考虑社区结构和网络演化行为）的影响力排序的稳定性。研究结果表明，改进后的时间独立级联模型和时间度中心性在该设置中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 识别社交网络中能够成功传播信息最有影响力的用户，尤其是在极化网络和信息传播动态的背景下。用户的影响力和排名会随着时间的推移而变化。

Method: 研究了在极化社交网络中用户影响力的时间动态。通过扩展时间中心性度量以考虑社区结构和网络演化行为，并比较了不同影响力排序的稳定性。

Result: 成功地将节点聚合为影响力层级，并聚合中心性分数以随时间分析社区的影响力。

Conclusion: 本研究表明，改进后的时间独立级联模型和时间度中心性在考虑社区结构和网络演化行为方面表现最佳，能够可靠地将节点划分为影响力层级。

Abstract: In social networks, it is often of interest to identify the most influential
users who can successfully spread information to others. This is particularly
important for marketing (e.g., targeting influencers for a marketing campaign)
and to understand the dynamics of information diffusion (e.g., who is the most
central user in the spreading of a certain type of information). However,
different opinions often split the audience and make the network polarised. In
polarised networks, information becomes soiled within communities in the
network, and the most influential user within a network might not be the most
influential across all communities. Additionally, influential users and their
influence may change over time as users may change their opinion or choose to
decrease or halt their engagement on the subject. In this work, we aim to study
the temporal dynamics of users' influence in a polarised social network. We
compare the stability of influence ranking using temporal centrality measures,
while extending them to account for community structure across a number of
network evolution behaviours. We show that we can successfully aggregate nodes
into influence bands, and how to aggregate centrality scores to analyse the
influence of communities over time. A modified version of the temporal
independent cascade model and the temporal degree centrality perform the best
in this setting, as they are able to reliably isolate nodes into their bands.

</details>


### [285] [Quotegraph: A Social Network Extracted from Millions of News Quotations](https://arxiv.org/abs/2507.17626)
*Marko Čuljak,Robert West,Andreas Spitz,Akhil Arora*

Main category: cs.SI

TL;DR: Quotegraph 是一个基于新闻引述构建的新型社交网络，包含人物关系及其传记和上下文信息，可用于社会科学研究。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个与在线社交网络互补的、大规模的社交网络资源，用于计算社会科学研究，以深入了解公众人物的行为及其在新闻中的呈现方式。

Method: 通过分析 2008 年至 2020 年英语新闻文章中的发言人归属引述来构建 Quotegraph 社交网络。该过程包括识别发言人、他们提及的人物以及相关的上下文信息，并将节点链接到 Wikidata 以获取传记实体信息。

Result: 构建了一个名为 Quotegraph 的新型大规模社交网络，包含 528,000 个节点和 8.63 百万个有向边，这些边代表了公众人物之间的提及关系，并附带了 Wikidata 的生物传记信息和 Quotebank 的上下文信息。

Conclusion: Quotegraph 是一个包含 528,000 个节点和 8.63 百万个有向边的新型社交网络，源自 2008 年至 2020 年间英语新闻文章中归属于发言人的引述。它通过链接到 Wikidata，为节点提供了详细的传记信息，并利用 Quotebank 的语料库丰富了关系及其上下文信息。Quotegraph 的构建流程语言无关，可用于构建非英语新闻语料库的类似数据集。该网络为计算社会科学家提供了一个有价值的资源，可用于研究公众人物的行为及其在新闻中的体现。

Abstract: We introduce Quotegraph, a novel large-scale social network derived from
speaker-attributed quotations in English news articles published between 2008
and 2020. Quotegraph consists of 528 thousand unique nodes and 8.63 million
directed edges, pointing from speakers to persons they mention. The nodes are
linked to their corresponding items in Wikidata, thereby endowing the dataset
with detailed biographic entity information, including nationality, gender, and
political affiliation. Being derived from Quotebank, a massive corpus of
quotations, relations in Quotegraph are additionally enriched with the
information about the context in which they are featured. Each part of the
network construction pipeline is language agnostic, enabling the construction
of similar datasets based on non-English news corpora. We believe Quotegraph is
a compelling resource for computational social scientists, complementary to
online social networks, with the potential to yield novel insights into the
behavior of public figures and how it is captured in the news.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [286] [Triadic First-Order Logic Queries in Temporal Networks](https://arxiv.org/abs/2507.17215)
*Omkar Bhalerao,Yunjie Pan,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: 该研究提出了FOLTY算法，用于分析大规模时间网络中的复杂模式，并在实际应用中取得了高效的结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模时间网络中的模式计数问题，并挖掘比传统三元组查询更丰富的信息，该研究引入了阈值一阶逻辑（FOL）模式分析。

Method: FOLTY算法，一种用于挖掘阈值FOL三元组查询的算法，该算法在稀疏图上的理论运行时间与已知的最快的时间三角形计数算法相当，并使用了专门的时间数据结构来实现。

Result: FOLTY算法在具有近7000万条边的图上，能在商品硬件上不到一小时的时间内回答三元组FOL查询，并且在实际应用中表现优异。

Conclusion: 该研究将阈值一阶逻辑（FOL）的模式分析引入了大规模时间网络，设计了首个用于挖掘阈值FOL三元组查询的算法FOLTY。

Abstract: Motif counting is a fundamental problem in network analysis, and there is a
rich literature of theoretical and applied algorithms for this problem. Given a
large input network $G$, a motif $H$ is a small "pattern" graph indicative of
special local structure. Motif/pattern mining involves finding all matches of
this pattern in the input $G$. The simplest, yet challenging, case of motif
counting is when $H$ has three vertices, often called a "triadic" query. Recent
work has focused on "temporal graph mining", where the network $G$ has edges
with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of
"thresholded First Order Logic (FOL) Motif Analysis" for massive temporal
networks. A typical triadic motif query asks for the existence of three
vertices that form a desired temporal pattern. An "FOL" motif query is obtained
by having both existential and thresholded universal quantifiers. This allows
for query semantics that can mine richer information from networks. A typical
triadic query would be "find all triples of vertices $u,v,w$ such that they
form a triangle within one hour". A thresholded FOL query can express "find all
pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$
also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic
queries. The theoretical running time of FOLTY matches the best known running
time for temporal triangle counting in sparse graphs. We give an efficient
implementation of FOLTY using specialized temporal data structures. FOLTY has
excellent empirical behavior, and can answer triadic FOL queries on graphs with
nearly 70M edges is less than hour on commodity hardware. Our work has the
potential to start a new research direction in the classic well-studied problem
of motif analysis.

</details>


### [287] [Unfolding Data Quality Dimensions in Practice: A Survey](https://arxiv.org/abs/2507.17507)
*Vasileios Papastergios,Lisa Ehrlinger,Anastasios Gounaris*

Main category: cs.DB

TL;DR: 本研究对七个开源数据质量工具进行了调查，将它们的功能映射到数据质量维度，以弥合理论与实践之间的差距。


<details>
  <summary>Details</summary>
Motivation: 弥合数据质量理论与实践之间的差距，阐明数据质量维度的实际应用。

Method: 通过检查七个开源数据质量工具，对它们的功能和数据质量维度进行了全面的映射。

Result: 提供了数据质量工具的功能与数据质量维度之间的映射，展示了功能如何部分地促进单个维度的评估。

Conclusion: 本论文系统性地将数据质量工具的低级功能与高级维度联系起来，揭示了它们的多对多关系，为数据质量评估提供了一个统一的视图和可操作的见解。

Abstract: Data quality describes the degree to which data meet specific requirements
and are fit for use by humans and/or downstream tasks (e.g., artificial
intelligence). Data quality can be assessed across multiple high-level concepts
called dimensions, such as accuracy, completeness, consistency, or timeliness.
While extensive research and several attempts for standardization (e.g.,
ISO/IEC 25012) exist for data quality dimensions, their practical application
often remains unclear. In parallel to research endeavors, a large number of
tools have been developed that implement functionalities for the detection and
mitigation of specific data quality issues, such as missing values or outliers.
With this paper, we aim to bridge this gap between data quality theory and
practice by systematically connecting low-level functionalities offered by data
quality tools with high-level dimensions, revealing their many-to-many
relationships. Through an examination of seven open-source data quality tools,
we provide a comprehensive mapping between their functionalities and the data
quality dimensions, demonstrating how individual functionalities and their
variants partially contribute to the assessment of single dimensions. This
systematic survey provides both practitioners and researchers with a unified
view on the fragmented landscape of data quality checks, offering actionable
insights for quality assessment across multiple dimensions.

</details>


### [288] [SHINE: A Scalable HNSW Index in Disaggregated Memory](https://arxiv.org/abs/2507.17647)
*Manuel Widmoser,Daniel Kocher,Nikolaus Augsten*

Main category: cs.DB

TL;DR: 一种用于分布式内存HNSW索引的新方法，通过缓存和合并缓存来提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了在超过十亿个高维向量的场景下扩展内存图方法（如HNSW），需要将索引进行分布式部署。然而，在计算和内存分离的硬件架构（disaggregated memory）中，RDMA网络使得计算节点可以直接访问远程内存，这对分布式索引提出了独特的挑战。

Method: 提出了一种可扩展的HNSW索引，该索引通过高效的缓存机制和逻辑上合并的计算节点缓存来克服网络带宽限制，从而在不分区图的情况下达到与单机HNSW相同的准确性。

Result: 所提出的方法达到了与单机HNSW相同的准确性，并通过评估证明了其效率和可扩展性。

Conclusion: 该方法在评估中被证明是高效且可扩展的，它通过在不损害准确性的情况下构建一个保留图的索引来解决分布式HNSW索引的挑战。

Abstract: Approximate nearest neighbor (ANN) search is a fundamental problem in
computer science for which in-memory graph-based methods, such as Hierarchical
Navigable Small World (HNSW), perform exceptionally well. To scale beyond
billions of high-dimensional vectors, the index must be distributed. The
disaggregated memory architecture physically separates compute and memory into
two distinct hardware units and has become popular in modern data centers. Both
units are connected via RDMA networks that allow compute nodes to directly
access remote memory and perform all the computations, posing unique challenges
for disaggregated indexes.
  In this work, we propose a scalable HNSW index for ANN search in
disaggregated memory. In contrast to existing distributed approaches, which
partition the graph at the cost of accuracy, our method builds a
graph-preserving index that reaches the same accuracy as a single-machine HNSW.
Continuously fetching high-dimensional vector data from remote memory leads to
severe network bandwidth limitations, which we overcome by employing an
efficient caching mechanism. Since answering a single query involves processing
numerous unique graph nodes, caching alone is not sufficient to achieve high
scalability. We logically combine the caches of the compute nodes to increase
the overall cache effectiveness and confirm the efficiency and scalability of
our method in our evaluation.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [289] [Social media uptake of scientific journals: A comparison between X and WeChat](https://arxiv.org/abs/2507.17114)
*Ting Cong,Er-Te Zheng,Zekun Han,Zhichao Fang,Rodrigo Costas*

Main category: cs.DL

TL;DR: 本研究对比了SCIE期刊在X和CSCD期刊在微信上的使用情况，发现微信使用率远高于X。生命科学领域期刊表现突出。用户参与度普遍较低，且文献计量与社交媒体指标相关性不强。研究强调了包含本土平台的社交媒体指标框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在考察科学期刊在X和微信这两个不同社交媒体平台上的采纳情况，并比较它们在用户参与度上的差异，以期为更全面的科学传播评估提供依据。

Method: 通过比较科学引文索引扩展版（SCIE）收录期刊在X平台的使用情况与中国科学引文数据库（CSCD）收录期刊在微信平台的使用情况，来考察科学期刊在两个不同平台上的社交媒体采纳情况。

Result: 研究发现，84.4%的CSCD期刊拥有微信公众号，而仅有22.7%的SCIE期刊拥有X账号。生命科学与生物医学领域的期刊在两个平台的采纳率上均领先。用户参与度方面，低参与度的互动行为（如点赞、转发）占主导。文献计量指标与社交媒体指标之间存在弱到中度的相关性，表明在线参与度是期刊影响力的一个独特维度。

Conclusion: 本研究揭示了科学期刊在X和微信这两个不同社交媒体平台上的采纳情况存在显著差异，并受到当地环境的影响。研究结果强调了整合本土主流平台以构建更广泛的社交媒体指标框架的必要性，从而更全面地理解跨不同社交媒体和环境的科学传播实践。

Abstract: This study examines the social media uptake of scientific journals on two
different platforms - X and WeChat - by comparing the adoption of X among
journals indexed in the Science Citation Index-Expanded (SCIE) with the
adoption of WeChat among journals indexed in the Chinese Science Citation
Database (CSCD). The findings reveal substantial differences in platform
adoption and user engagement, shaped by local contexts. While only 22.7% of
SCIE journals maintain an X account, 84.4% of CSCD journals have a WeChat
official account. Journals in Life Sciences & Biomedicine lead in uptake on
both platforms, whereas those in Technology and Physical Sciences show high
WeChat uptake but comparatively lower presence on X. User engagement on both
platforms is dominated by low-effort interactions rather than more
conversational behaviors. Correlation analyses indicate weak-to-moderate
relationships between bibliometric indicators and social media metrics,
confirming that online engagement reflects a distinct dimension of journal
impact, whether on an international or a local platform. These findings
underscore the need for broader social media metric frameworks that incorporate
locally dominant platforms, thereby offering a more comprehensive understanding
of science communication practices across diverse social media and contexts.

</details>


### [290] [Do male leading authors retract more articles than female leading authors?](https://arxiv.org/abs/2507.17127)
*Er-Te Zheng,Hui-Zhen Fu,Mike Thelwall,Zhichao Fang*

Main category: cs.DL

TL;DR: 本研究发现男性作者的撤稿率高于女性，尤其是在学术不端行为方面，但性别差异因撤稿原因、学科和国家而异。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨不同科学领域中男性和女性作者在撤稿率方面的差异，特别是与研究生产力的关系，以期为减少撤稿率的策略提供参考。

Method: 本研究使用来自Web of Science和Retraction Watch的11,622篇被撤文章和19,475,437篇未被撤文章的数据集，考察了性别差异在撤稿原因、学科领域和国家等方面的差异。

Result: 男性挂名作者的撤稿率更高，尤其是在学术不端行为方面。在因错误导致的撤稿方面，未发现显著的性别差异。男性挂名作者在生物医学与健康科学以及生命与地球科学领域的可显著更高的撤稿率，而女性挂名作者在数学与计算机科学领域的可显著更高的撤稿率。

Conclusion: 本研究发现，男性挂名作者的撤稿率高于女性，尤其是在抄袭、作者身份争议、伦理问题、重复发表和伪造/篡改数据等学术不端行为方面。在因错误导致的撤稿方面，未发现显著的性别差异。此外，男性挂名作者在生物医学与健康科学以及生命与地球科学领域的可显著更高的撤稿率，而女性挂名作者在数学与计算机科学领域的可显著更高的撤稿率。通讯作者也观察到类似的模式。了解这些性别化的撤稿模式可能有助于制定减少其发生率的策略。

Abstract: Scientific retractions reflect issues within the scientific record, arising
from human error or misconduct. Although gender differences in retraction rates
have been previously observed in various contexts, no comprehensive study has
explored this issue across all fields of science. This study examines gender
disparities in scientific misconduct or errors, specifically focusing on
differences in retraction rates between male and female first authors in
relation to their research productivity. Using a dataset comprising 11,622
retracted articles and 19,475,437 non-retracted articles from the Web of
Science and Retraction Watch, we investigate gender differences in retraction
rates from the perspectives of retraction reasons, subject fields, and
countries. Our findings indicate that male first authors have higher retraction
rates, particularly for scientific misconduct such as plagiarism, authorship
disputes, ethical issues, duplication, and fabrication/falsification. No
significant gender differences were found in retractions attributed to
mistakes. Furthermore, male first authors experience significantly higher
retraction rates in biomedical and health sciences, as well as in life and
earth sciences, whereas female first authors have higher retraction rates in
mathematics and computer science. Similar patterns are observed for
corresponding authors. Understanding these gendered patterns of retraction may
contribute to strategies aimed at reducing their prevalence.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [291] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: 多模态AI代理和数据驱动方法革新LCA，大幅提升效率和精度，解决数据难题。


<details>
  <summary>Details</summary>
Motivation: 近年来，对可持续性信息的需求激增，但用于生命周期评估（LCA）的必要数据往往难以获得。传统LCA方法耗时耗力，且存在数据缺失的问题。

Method: 本文引入了多模态AI代理，通过模拟LCA专家与产品经理、工程师的交互来计算电子产品的碳排放量。该代理利用自定义数据抽象和从在线文本、维修社区图片及政府认证中提取信息的软件工具，迭代生成详细的生命周期清单。此外，本文还开发了一种直接估算环境影响（EI）的方法，通过比较输入产品与具有相似描述和已知碳足迹的产品集群。最后，本文提出了一种数据驱动方法来生成排放因子，将未知材料表示为相似材料排放因子的加权和。

Result: 该方法将专家所需数周或数月的时间缩短到一分钟以内，弥补了数据可用性差距，同时碳足迹估算结果与专家LCA的误差在19%以内，且无需专有数据。直接估算EI的方法在笔记本电脑上运行仅需3毫秒，对电子产品的平均绝对百分比误差（MAPE）为12.28%。数据驱动的排放因子生成方法相比人类专家选择最接近的LCA数据库条目，MAPE 提高了120.26%。

Conclusion: 本文提出了一种利用多模态人工智能代理和数据驱动方法来革新传统生命周期评估（LCA）的方法，显著提高了效率、弥合了数据差距，并实现了与专家级LCA相当的碳排放估算精度。

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [292] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: 本文提出改进的 EECBS 算法，通过新的弹性分发机制提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 为了加速EECBS算法，以往的研究采用了弹性分发来提高阈值。然而，这可能导致部分最优解的成本超过 w·LB，从而降低了算法效率。本文旨在解决这一问题。

Method: 本文提出了基于冲突的弹性分发（Conflict-Based Flex Distribution）和基于延迟的弹性分发（Delay-Based Flex Distribution），并结合了两者提出了混合策略弹性分发（Mixed-Strategy Flex Distribution）。

Result: EECBS 结合我们新提出的基于冲突和基于延迟的弹性分发机制，可以保证算法的完备性和有界次优性。实验结果表明，我们提出的方法优于原始的（贪心）弹性分发方法。

Conclusion: EECBS 结合我们新提出的基于冲突和基于延迟的弹性分发机制，可以保证算法的完备性和有界次优性。实验结果表明，我们提出的方法优于原始的（贪心）弹性分发方法。

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [293] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: LoRA 结合拒绝数据集微调 LLMs，能在保证安全性的同时不牺牲推理能力，解决了“安全税”问题。


<details>
  <summary>Details</summary>
Motivation: 为了在对大型语言模型（LLMs）进行安全对齐微调时，避免“安全税”现象（即安全对齐损害推理能力）。

Method: 使用 LoRA（低秩适应）结合拒绝数据集进行有监督微调（SFT）。

Result: 该方法在数学、科学和编程四个基准测试中表现出色，生成的 LLMs 安全性高，且推理能力未受影响。LoRA 产生的权重更新与初始权重重叠较小，正则化或权重合并可进一步优化。

Conclusion: 使用 LoRA 结合拒绝数据集进行 SFT 可以有效地使模型在安全对齐方面达到与全模型微调相当的水平，同时不损害其推理能力。此外，LoRA 产生的权重更新与初始权重具有较小的重叠，通过正则化或权重合并可以进一步减少这种重叠，并在某些任务上取得改进。

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [294] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: AI copilots in vehicles enable proactive maintenance by bridging machine and driver communication.


<details>
  <summary>Details</summary>
Motivation: To integrate AI copilots in vehicles for proactive maintenance and to spark interdisciplinary dialogue.

Method: Conceptual and technical perspective offered to guide future research and development.

Result: Highlights the potential of AI-powered user interaction and predictive maintenance in intelligent vehicle systems.

Conclusion: AI in vehicles can transform maintenance from reactive to proactive by integrating AI copilots that communicate with both machines and drivers.

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [295] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: 本研究评估了FMEA和FTA等传统安全分析方法在AI驱动的安全关键系统（如自动驾驶）中的适用性，并提出了HySAFE-AI混合框架来改进这些分析，以适应基础模型的复杂性，并为未来的AI安全标准演进提供指导。


<details>
  <summary>Details</summary>
Motivation: AI在安全关键领域（如自动驾驶系统和机器人）的应用日益广泛，而当前这些系统倾向于采用端到端（E2E）的整体架构（如大型语言模型和视觉语言模型）。因此，有必要评估和改进现有安全分析技术（如FMEA和FTA）以适应这些复杂模型。HySAFE-AI框架旨在解决这一挑战。

Method: 本篇论文回顾了不同的架构解决方案，并评估了故障模式与影响分析（FMEA）和故障树分析（FTA）等通用安全分析的有效性。HySAFE-AI是一个混合安全架构分析框架，它适应传统方法来评估AI系统的安全性。

Result: HySAFE-AI框架被提出，用于改进FMEA和FTA等传统安全分析技术，以更好地评估基于AI的系统（特别是基础模型）的安全性。

Conclusion: AI在自动驾驶系统（ADS）和机器人等安全关键领域变得不可或缺。本篇论文回顾了不同的架构解决方案，并评估了故障模式与影响分析（FMEA）和故障树分析（FTA）等通用安全分析的有效性。我们展示了如何针对基础模型的复杂性改进这些技术，特别是它们如何形成和利用潜在表征。我们引入了HySAFE-AI，一个用于AI系统的混合安全架构分析框架，该框架适应传统方法来评估AI系统的安全性。最后，我们为未来的工作提供了线索和建议，以指导未来AI安全标准的演进。

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [296] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 研究了本体受控查询评估（CQE）及其信息披露的安全性，结合了信息披露依赖（EDs）和最优GA censors。对于DL-Lite_R本体和完全EDs，提出了一种在AC^0中可计算的安全查询方法，并通过实验验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 为了解决本体信息披露的安全性问题，并探索一种既能保证强安全又能兼顾计算效率的CQE方法。

Method: 结合了最优GA censors（可安全披露的最大原子集）和信息披露依赖（EDs）来研究受控查询评估（CQE）。重点关注布尔连接查询（BUCQs）的回答，并提出了一个一阶重写算法，用于DL-Lite_R本体和特定EDs子类的情况。

Result: 确定了基于最优GA censors交集的CQE方法的安全边界，并针对DL-Lite_R本体和一类EDs（完全EDs）证明了其安全性。开发了一种一阶重写算法，将CQE问题在数据复杂度上定位在AC^0，并通过实验验证了该方法的实际可行性。

Conclusion: 研究表明，对于一类特定的信息披露规则（完全ED）和DL-Lite_R本体，基于最优GA censors交集的方法在数据复杂度方面是安全的，并且可以通过给定的一阶重写算法在AC^0中进行评估。实验证明了该重写函数在实际应用中的可行性。

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [297] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: 该研究通过使用包含109亿token的图问题推理（GPR）数据集GraphPile对大型语言模型（LLMs）进行继续预训练（CPT），成功提升了它们的通用推理能力，特别是在数学和非数学推理任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理新颖和复杂推理任务时表现不佳，而现有的领域特定继续预训练（CPT）方法（如数学推理）缺乏广泛的迁移性。为了解决这一问题，本研究旨在探索利用图问题推理（GPR）来提升LLMs的通用推理能力，因为GPR任务需要复杂的逻辑和关系推理，能够教会模型多样化的推理模式。

Method: 提出了一种利用图问题推理（GPR）来增强大型语言模型（LLMs）通用推理能力的方法。该方法的核心是引入了GraphPile，一个包含109亿token的、专门为GPR数据设计的继续预训练（CPT）语料库，涵盖了路径查找、网络分析、数值计算和拓扑推理等23种图任务。研究人员使用GraphPile在Llama 3、3.1和Gemma 2等基础模型上训练了GraphMind模型。

Result: 通过在Llama 3、3.1和Gemma 2模型上使用GraphPile进行训练，所提出的GraphMind模型在数学推理任务上的准确率提高了高达4.9%，在逻辑推理和常识推理等非数学推理任务上的准确率则提高了高达21.2%。这证明了GPR方法和GraphPile数据集在增强LLM通用推理能力方面的有效性。

Conclusion: 这项工作通过引入图问题推理（GPR）和GraphPile数据集，成功提升了大型语言模型（LLMs）的通用推理能力。研究表明，基于GPR的继续预训练（CPT）能够显著提高模型在数学和非数学推理任务上的表现，为实现更广泛、更强大的LLM推理能力开辟了新途径。

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [298] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: HoPeS框架利用LLM驱动的代理来模拟社会生态系统中的不同利益相关者视角，以促进理解和跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 为了理解社会生态系统，需要获得难以获取的各种利益相关者视角。HoPeS框架旨在通过基于仿真的方法来探索不同的利益相关者视角。

Method: 开发了HoPeS（以人为本的视角转换）建模框架，该框架使用由大型语言模型（LLM）驱动的代理来表示不同的利益相关者。该框架还包括一个模拟协议，以简化多视角模拟。

Result: HoPeS框架成功地模拟了机构动态和土地利用变化中的利益相关者视角。在一个示例性实验中，用户能够体验不同利益相关者的视角，并观察到尽管有基于证据的政策建议，但由于利益相关者的利益冲突，政策执行仍存在差异。

Conclusion: HoPeS框架有潜力促进新的跨学科合作形式，用于社会生态模拟。尽管存在用户在研究人员角色中经历挫败感和失望等挑战，但该系统在探索不同视角方面显示出巨大潜力。

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [299] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: AIE 是一个评估 LMA 身份保持能力的框架，解决了 LLMs 的固有问题，有助于设计更好的 LMA。


<details>
  <summary>Details</summary>
Motivation: LMAs 继承了 LLMs 的一些固有问题（如无状态性、随机性、对提示的敏感性等），这些问题会削弱其身份的可识别性、连续性、持久性和一致性，从而影响其作为代理的能力（如推理、规划和行动），进而侵蚀其可靠性、可信度和效用。

Method: AIE 评估框架，包含了一系列新颖的统计驱动的实证指标，用于衡量 LMA 随时间保持其代理身份的能力。

Result: AIE 框架提供了一套正式的定义和方法，可以应用于 LMA 生命周期中的每个阶段，并给出了应用实例，有助于设计最优的 LMA 基础设施和脚手架。

Conclusion: 该研究提出了一个名为 AIE（Agent Identity Evals）的评估框架，用于衡量语言模型代理（LMAs）随时间保持其代理身份的能力，包括能力、属性以及从状态扰动中恢复过来的能力。AIE 包含了一系列新颖的指标，可以与性能、能力和代理稳健性等其他度量结合使用，以辅助设计最优的 LMA 基础设施和脚手架，例如记忆和工具。

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [300] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: 一项关于SCRIPT（一个基于ChatGPT-4o-mini的编程教育聊天机器人）的研究表明，该工具在满足初学者的反馈需求方面效果良好，其响应与学生偏好一致性达75%，为未来AI学习工具的设计提供了参考。


<details>
  <summary>Details</summary>
Motivation: 为了支持初学者，我们开发了一个基于ChatGPT-4o-mini的聊天机器人SCRIPT，它允许通过预定义的提示进行开放式交互和结构化指导。

Method: 通过一项涉及136名学生（来自一所大型德国大学的入门编程课程）的实验来评估该工具，并分析学生在解决编程任务时与SCRIPT的互动方式，重点关注他们的反馈偏好。

Result: 结果表明，学生的反馈请求似乎遵循特定的顺序，并且聊天机器人的响应在75%的情况下与学生请求的反馈类型一致，并且符合系统提示的约束。

Conclusion: 该研究为基于生成式AI的学习支持系统的设计提供了信息，并强调了在AI辅助工具中平衡引导性和灵活性的挑战。

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [301] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: CBA是一种AI助手，通过智能路由提高了合规任务的效率和质量，实验结果优于传统LLM。


<details>
  <summary>Details</summary>
Motivation: 为了提高企业环境中日常合规任务的效率，并平衡响应质量和延迟。

Method: 通过设计一个用户查询路由器，智能地在“快速通道”模式（处理简单请求）和“完全代理”模式（处理复杂请求，需要复合操作和工具调用）之间进行选择，以平衡响应质量和延迟。

Result: 实验评估表明，CBA在平均关键词匹配率（83.7% vs 41.7%）和LLM-评判通过率（82.0% vs 20.0%）等指标上显著优于开箱即用的LLM。同时，基于路由器的设计在保持运行时间不变的情况下，取得了更好的平均匹配率和通过率，验证了路由机制在两种模式之间取得了良好的权衡。

Conclusion: 该研究提出了一种名为合规大脑助手（CBA）的对话式AI助手，旨在提高企业环境中日常合规任务的效率。

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [302] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: Ctx2TrajGen, a context-aware trajectory generation framework using GAIL, PPO, and WGAN-GP, synthesizes realistic urban driving behaviors by conditioning on surrounding vehicles and road geometry, outperforming existing methods on the DRIFT dataset.


<details>
  <summary>Details</summary>
Motivation: Precise modeling of microscopic vehicle trajectories is critical for traffic behavior analysis and autonomous driving systems.

Method: Ctx2TrajGen framework uses GAIL, PPO, and WGAN-GP to synthesize realistic urban driving behaviors, conditioning on surrounding vehicles and road geometry.

Result: Experiments on the DRIFT dataset show Ctx2TrajGen outperforms existing methods in realism, behavioral diversity, and contextual fidelity, addressing data scarcity and domain shift without simulation.

Conclusion: Ctx2TrajGen generates interaction-aware trajectories aligned with real-world context, demonstrating superior performance over existing methods in realism, behavioral diversity, and contextual fidelity on the DRIFT dataset. It offers a robust solution to data scarcity and domain shift without simulation.

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [303] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: UDASA是一种全自动的LLM对齐框架，通过量化和利用输出的不确定性，无需人类标注即可提高LLM的对齐质量。


<details>
  <summary>Details</summary>
Motivation: 在缺乏人类标注的情况下，实现LLM与人类意图和安全规范的高质量对齐是一个根本性的挑战。

Method: UDASA框架首先为每个输入生成多个响应，并从语义、事实性和价值对齐三个维度量化输出不确定性。然后，基于不确定性得分构建偏好对，并将训练样本根据其不确定性差异分为保守、适度和探索三个阶段，对模型进行分阶段优化。

Result: 实验结果表明，UDASA在无害性、有用性、真实性和可控情感生成等多个任务上超越了现有的对齐方法，显著提高了模型性能。

Conclusion: UDASA框架在无人类标注的情况下，通过量化和利用输出的不确定性，实现了LLM与人类意图和安全规范的高质量对齐，并在无害性、有用性、真实性和可控情感生成等多个任务上显著优于现有对齐方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [304] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: LTLZinc是一个新的基准测试框架，用于评估神经符号和持续学习方法在时间推理任务上的表现。现有方法存在局限性，需要更统一的框架。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的神经符号人工智能方法仅限于静态场景，很少有研究探索在需要时间维度推理的动态场景下的应用。因此，有必要开发一个能够评估方法在时间推理和约束驱动能力上的基准测试框架。

Method: 开发了一个名为LTLZinc的基准测试框架，该框架能够根据线性时序逻辑（LTL）规范和MiniZinc约束以及图像分类数据集，生成包含时间推理和持续学习任务的数据集。该框架支持对同一数据集进行多种神经和神经符号训练设置，并提供了十个现成可用的任务。

Result: 在LTLZinc生成的六个神经符号序列分类任务和四个类别持续学习任务上的实验结果表明，时间学习和推理具有挑战性，并且暴露了当前最先进方法的局限性。

Conclusion: 该研究提出了LTLZinc框架，用于生成和评估神经符号和持续学习方法在时间推理和约束驱动维度上的性能。实验表明，当前最先进的方法在处理时间学习和推理方面存在局限性，并强调了统一时间学习和推理框架的必要性。

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [305] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 混合接地通过自动选择最佳接地策略来解决接地瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决混合接地何时使用体分离接地以及何时使用标准接地这两种技术的未解问题。

Method: 开发了自动混合接地，引入了基于数据结构启发式（基于规则结构和包含实例数据的估计过程）的拆分算法，以检测何时使用体分离接地以及何时使用标准接地。

Result: 在难以接地的情况下，性能有所提高，在难以求解的实例中，性能接近最先进的水平。

Conclusion: 混合接地通过结合标准的自底向上接地和最近提出的规则体在接地过程中分离的技术来缓解接地瓶颈，并且通过基于数据结构启发式的拆分算法解决了何时使用这两种技术的混合接地问题。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [306] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 本研究使用GRPO和Qwen-2.5-7B模型，通过单领域和跨领域训练，研究了LLM在数学、代码和逻辑推理中的交互作用，并分析了SFT、课程学习、奖励设计和语言因素的影响，为提升LLM的多领域推理能力提供了指导。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有研究主要集中于孤立的推理领域，而忽略了现实世界推理中多认知技能的集成应用这一问题，本研究旨在理解在强化学习背景下这些推理技能的相互作用。

Method: 利用GRPO算法和Qwen-2.5-7B模型系列，对模型在单一领域和跨领域训练下的表现进行了评估。研究了交叉训练中的相互促进和冲突，比较了SFT对RL的影响，并深入分析了课程学习策略、奖励设计和语言因素等关键RL训练细节。

Result: 通过广泛的实验，研究结果揭示了领域交互的动态规律，并识别了影响专业化和泛化推理性能的关键因素。

Conclusion: 本研究系统地探讨了在RLVR框架下多领域推理的相互作用，重点关注数学推理、代码生成和逻辑谜题解决。实验结果揭示了领域交互的动态规律，以及影响专业化和泛化推理能力的关键因素。这些发现为优化RL方法以培养LLM的综合多领域推理能力提供了宝贵的指导。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [307] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: 一个基于RAG的AI自我评估工具，用于简化AI法律合规性检查，特别是针对AI法案。该工具通过预筛选和评估两个步骤，能够预测风险等级并检索相关法律条款，定性评估显示出有前景的结果。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能够简化AI自我评估流程的工具，特别是为了协助用户满足AI法案的合规性要求。

Method: 该工具采用基于检索增强生成（RAG）的框架，包括预筛选和评估两个步骤。评估阶段利用检索到的信息来确定AI系统的风险等级，并引用相关的法律条款以协助合规。

Result: 在用例场景的定性评估中，该工具能够准确预测AI系统的风险等级，并成功检索到与三种不同语义组相关的法律条款。

Conclusion: 定性评估结果显示，该工具能够正确预测风险等级，并在三个不同的语义组中检索相关文章，表明其在法律合规性评估方面具有潜力。

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [308] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: 本研究提出了 FundusExpert，一个针对眼科的专用多模态大语言模型，并构建了 FundusGen 数据集。该模型在眼科问答和报告生成任务上表现优于现有模型，并揭示了数据质量对模型能力的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医学诊断领域具有巨大潜力，但在眼科等专业领域面临着标注粒度碎片化和临床推理逻辑不一致的挑战，这阻碍了精确的跨模态理解。因此，有必要开发专门针对眼科领域、具备精准推理能力的模型。

Method: 本研究引入了 FundusExpert，一个集成了定位-诊断推理能力的眼科领域多模态大语言模型，并构建了 FundusGen 数据集。FundusGen 数据集通过 Fundus-Engine 系统自动完成眼底图像的定位，并利用多模态大语言模型进行语义扩展，整合了全局疾病分类、局部目标检测和细粒度特征分析。此外，通过构建临床对齐的认知链来指导模型生成可解释的推理路径。FundusExpert 使用 FundusGen 中的指令数据进行微调。

Result: FundusExpert 在眼科问答任务上的平均准确率比 40B MedRegA 高出 26.6%。在零样本报告生成任务中，其临床一致性达到了 77.0%，显著优于 GPT-4o 的 47.6%。研究还发现了数据质量与模型能力之间的扩展规律（$L 
eq N^{0.068}$），表明 FundusGen 中的认知对齐标注提高了数据的利用效率。

Conclusion: 该研究成功开发了一个名为 FundusExpert 的眼科领域专用多模态大语言模型，并通过 FundusGen 数据集进行了微调。该模型在眼科问答任务上取得了显著优于现有模型的性能，并在零样本报告生成任务中展现出更高的临床一致性。此外，研究还揭示了数据质量与模型能力之间的扩展规律，证明了认知对齐标注能提高数据利用效率。该工作为构建可扩展、临床对齐的多模态大语言模型以及弥合特定领域多模态大语言模型中的视觉-语言鸿沟提供了新的途径。

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [309] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: LLM-based agents enhanced with real-time optimization (symbiotic agents) improve decision-making, reduce resource usage (even with smaller models), and enable efficient resource allocation in networks, paving the way for trustworthy AGI-driven systems.


<details>
  <summary>Details</summary>
Motivation: To leverage Large Language Models (LLMs) for autonomous agents in 6G networks, enabling real-time decision-making for management and service provisioning, and to transition towards Artificial General Intelligence (AGI)-driven networks with broader reasoning capabilities.

Method: The paper introduces a novel agentic paradigm combining LLMs with real-time optimization algorithms. It designs two agent types: Radio Access Network (RAN) optimizers and multi-agent negotiators for Service-Level Agreements (SLAs). An end-to-end architecture for AGI networks is proposed and evaluated on a 5G testbed.

Result: Symbiotic agents reduce decision errors fivefold compared to standalone LLM agents. Smaller language models (SLMs) achieve similar accuracy with a 99.9% reduction in GPU overhead and near-real-time loops of 82 ms. A multi-agent demonstration reduced RAN over-utilization by approximately 44%.

Conclusion: Symbiotic agents, combining LLMs with real-time optimization, represent a foundational paradigm for future AGI-driven networks. This approach enhances trustworthiness and adaptability, offering significant improvements in decision accuracy, resource efficiency, and real-time performance.

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [310] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: 近期研究认为大型推理模型（LRMs）可能无法提升推理能力，但本研究发现，通过集成Python解释器和记事本等工具，LRMs在各种复杂任务上均优于非推理模型，证明了工具增强LRMs的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）作为LLM研究的焦点，旨在通过输出逐步思考过程来处理复杂推理任务。然而，近期研究表明，这种思考过程可能并未真正提升推理能力，甚至在某些任务上表现不如不依赖显式推理的LLM。

Method: 在本研究中，我们重新审视了LRMs的局限性，并探究了在引入工具增强时这些局限性是否依然存在。我们结合了Python解释器和记事本这两种工具，并在Apple的基准推理谜题上评估了三种代表性LLMs及其LRM对应版本。

Result: 研究结果显示，当引入工具增强后，LRMs在所有复杂程度的任务上都持续优于非推理模型。

Conclusion: 研究结果表明，通过正确使用工具，大型推理模型（LRMs）在所有复杂程度的任务上都持续优于非推理模型，挑战了近期关于推理是幻觉的说法，并凸显了工具增强型LRMs在解决复杂问题方面的潜力。

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [311] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 研究界在不同领域开发了基准数据集来比较算法和技术的性能。然而，由于出版物同时出现在不同的地方，并且许多声称代表了最先进的技术，因此跟踪这些研究领域的进展并不容易。为了解决这个问题，研究界经常组织定期的竞赛来评估各种算法和技术的性能，从而跟踪该领域的进展。然而，这些竞赛给组织者带来了巨大的运营负担。组织者必须管理和评估大量的提交。此外，参与者通常在各种环境中开发他们的解决方案，这在评估他们的提交时会导致兼容性问题。本文提出了一个在线竞赛系统，可以自动执行竞赛的提交和评估过程。该竞赛系统允许组织者有效地管理大量的提交，并利用隔离的环境来评估提交。该系统已成功用于包括基于网格的寻路竞赛和机器人跑步者联赛竞赛在内的几项竞赛。


<details>
  <summary>Details</summary>
Motivation: 为了解决跟踪研究进展的困难，因为出版物出现在不同的地方，并且许多声称代表了最先进的技术，以及为了减轻组织者管理和评估大量提交的负担，并解决参与者在各种环境中开发解决方案而导致的兼容性问题。

Method: 提出一个在线竞赛系统，自动执行竞赛的提交和评估过程，并利用隔离环境来评估提交的内容。

Result: 该系统允许组织者有效地管理大量的提交，并使用隔离的环境来评估提交。

Conclusion: 该系统已成功用于包括基于网格的寻路竞赛和机器人跑步者联赛竞赛在内的几项竞赛。

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [312] [Ethics through the Facets of Artificial Intelligence](https://arxiv.org/abs/2507.17020)
*Flavio Soares Correa da Silva*

Main category: cs.CY

TL;DR: 本文探讨了人工智能的伦理问题，认为社会对其理解模糊，并提出了一个伦理评估框架。


<details>
  <summary>Details</summary>
Motivation: 文章认为，人工智能引发的伦理担忧源于社会对人工智能的理解模糊不清，包括其用途和被接受的方式。

Method: 文章基于三个描述性方面探讨了人工智能的概念，并思考了与每个方面相关的伦理问题。

Result: 文章探讨了人工智能的概念及其相关的伦理问题。

Conclusion: 最终，我们提出了一个用于人工智能伦理评估的框架。

Abstract: Artificial Intelligence (AI) has received unprecedented attention in recent
years, raising ethical concerns about the development and use of AI technology.
In the present article, we advocate that these concerns stem from a blurred
understanding of AI, how it can be used, and how it has been interpreted in
society. We explore the concept of AI based on three descriptive facets and
consider ethical issues related to each facet. Finally, we propose a framework
for the ethical assessment of the use of AI.

</details>


### [313] [AI in Design Education at College Level-Educators' Perspectives and Challenges](https://arxiv.org/abs/2507.17481)
*Lizhu Zhang,Cecilia X. Wang*

Main category: cs.CY

TL;DR: AI在设计教育中越来越重要，教师们正在接受它，但他们担心伦理和版权问题，并强调学生需要先掌握基本技能。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能日益渗透设计领域，本研究旨在探讨AI对大学设计教育的影响，了解设计教育者对AI在教学和研究中作用的看法、融合AI的意愿以及对相关挑战的担忧。

Method: 本研究通过对美国七位设计学院教师进行定性、半结构化和深入访谈，旨在了解他们对AI在大学设计教育中的作用的看法、对将AI融入教学和研究的看法，以及对AI在设计教育和研究中潜在挑战的担忧。

Result: 研究发现，AI已成为设计教育不可或缺的工具和信息来源，其功能越来越多地被集成到设计软件中，教育者也在积极地将AI作为教学的理论框架。此外，研究强调了设计教育者与AI建立合作关系的重要性。

Conclusion: 人工智能已成为设计教育的重要组成部分，但教育者强调在掌握基本设计原则和技能的基础上，引导学生使用AI工具的重要性。同时，设计教育者期待在AI相关的伦理、原创性和版权问题上取得进展。

Abstract: Artificial intelligence has deeply permeated numerous fields, especially the
design area which relies on technology as a tool for innovation. This change
naturally extends to the field of design education, which is closest to design
practice. This has led to further exploration of the impact of AI on
college-level education in the design discipline. This study aims to examine
how current design educators perceive the role of AI in college-level design
education, their perspectives on integrating AI into teaching and research, and
their concerns regarding its potential challenges in design education and
research. Through qualitative, semi-structured, in-depth interviews with seven
faculties in U.S. design colleges, the findings reveal that AI, as a tool and
source of information, has become an integral part of design education. AI-
derived functionalities are increasingly utilized in design software, and
educators are actively incorporating AI as a theoretical framework in their
teaching. Educators can guide students in using AI tools, but only if they
first acquire a strong foundation in basic design principles and skills. This
study also indicates the importance of promoting a cooperative relationship
between design educators and AI. At the same time, educators express
anticipation for advancements in ethical standards, authenticity, and the
resolution of copyright issues related to AI.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [314] [A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models](https://arxiv.org/abs/2507.16826)
*Qikai Wei,Huansheng Ning,Chunlong Han,Jianguo Ding*

Main category: cs.IR

TL;DR: QMKGF通过构建和融合多路径知识图谱来改进RAG，提升了生成内容的准确性和相关性，并在多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）研究主要关注使用相似性匹配方法检索孤立的文本片段，忽略了这些片段之间的内在联系，这限制了RAG任务的性能。为了解决这个问题，本研究提出了一种新的方法来改进RAG。

Method: 本研究提出了一种名为QMKGF（查询感知多路径知识图谱融合）的方法。该方法首先利用提示模板和通用语言模型提取实体和关系，高效构建知识图谱（KG）。接着，设计了一种多路径子图构建策略，包含单跳、多跳及基于重要性的关系，以增强检索文档与用户查询间的语义相关性。随后，设计了一个查询感知的注意力奖励模型，根据子图三元组与查询的语义相关性对其进行评分，并选择得分最高的子图，同时从其他高度相关的子图中补充三元组以丰富信息。最后，利用更新后的子图中的实体、关系和三元组扩展原始查询，增强其语义表示，从而提高语言模型生成内容的质量。

Result: 在SQuAD、IIRC、Culture、HotpotQA和MuSiQue数据集上进行了评估。在HotpotQA数据集上，QMKGF方法实现了64.98%的ROUGE-1得分，比BGE-Rerank方法（55.26%）高出9.72个百分点。实验结果证明了QMKGF方法的有效性和优越性。

Conclusion: 本研究提出的QMKGF方法通过构建查询感知的多路径知识图谱并进行融合，有效提升了检索增强生成（RAG）的性能。在HotpotQA等数据集上的实验结果表明，QMKGF相比现有方法具有显著优势。

Abstract: Retrieval Augmented Generation (RAG) has gradually emerged as a promising
paradigm for enhancing the accuracy and factual consistency of content
generated by large language models (LLMs). However, existing RAG studies
primarily focus on retrieving isolated segments using similarity-based matching
methods, while overlooking the intrinsic connections between them. This
limitation hampers performance in RAG tasks. To address this, we propose QMKGF,
a Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing
Retrieval Augmented Generation. First, we design prompt templates and employ
general-purpose LLMs to extract entities and relations, thereby generating a
knowledge graph (KG) efficiently. Based on the constructed KG, we introduce a
multi-path subgraph construction strategy that incorporates one-hop relations,
multi-hop relations, and importance-based relations, aiming to improve the
semantic relevance between the retrieved documents and the user query.
Subsequently, we designed a query-aware attention reward model that scores
subgraph triples based on their semantic relevance to the query. Then, we
select the highest score subgraph and enrich subgraph with additional triples
from other subgraphs that are highly semantically relevant to the query.
Finally, the entities, relations, and triples within the updated subgraph are
utilised to expand the original query, thereby enhancing its semantic
representation and improving the quality of LLMs' generation. We evaluate QMKGF
on the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA
dataset, our method achieves a ROUGE-1 score of 64.98\%, surpassing the
BGE-Rerank approach by 9.72 percentage points (from 55.26\% to 64.98\%).
Experimental results demonstrate the effectiveness and superiority of the QMKGF
approach.

</details>


### [315] [You Don't Bring Me Flowers: Mitigating Unwanted Recommendations Through Conformal Risk Control](https://arxiv.org/abs/2507.16829)
*Giovanni De Toni,Erasmo Purificato,Emilia Gómez,Bruno Lepri,Andrea Passerini,Cristian Consonni*

Main category: cs.IR

TL;DR: 提出了一种基于共形风险控制的方法，用于在个性化推荐中减少不希望的内容，该方法利用二元和隐式反馈，并能在实际数据上有效控制风险。


<details>
  <summary>Details</summary>
Motivation: 当前的推荐系统虽然能有效个性化内容，但也面临着传播不相关、不必要甚至有害推荐的批评，这不仅降低了用户满意度，还加剧了错误信息、激进主义和用户信任侵蚀等社会问题。

Method: 提出了一种直观的、模型无关的、分布自由的方法，该方法利用共形风险控制，通过利用对项目简单的二元反馈来保证有保证地限制个性化推荐中不希望出现的内容。此外，还利用隐式反馈解决了传统共形风险控制方法的局限性，例如推荐者可以提供更少推荐项目集的问题。

Result: 该方法在来自流行的在线视频共享平台的数据上进行了实验评估，结果表明该方法能够确保有效且可控地减少不希望的推荐，且付出的努力极少。

Conclusion: 该方法能有效且可控地减少不希望的推荐，且只需极少努力。

Abstract: Recommenders are significantly shaping online information consumption. While
effective at personalizing content, these systems increasingly face criticism
for propagating irrelevant, unwanted, and even harmful recommendations. Such
content degrades user satisfaction and contributes to significant societal
issues, including misinformation, radicalization, and erosion of user trust.
Although platforms offer mechanisms to mitigate exposure to undesired content,
these mechanisms are often insufficiently effective and slow to adapt to users'
feedback. This paper introduces an intuitive, model-agnostic, and
distribution-free method that uses conformal risk control to provably bound
unwanted content in personalized recommendations by leveraging simple binary
feedback on items. We also address a limitation of traditional conformal risk
control approaches, i.e., the fact that the recommender can provide a smaller
set of recommended items, by leveraging implicit feedback on consumed items to
expand the recommendation set while ensuring robust risk mitigation. Our
experimental evaluation on data coming from a popular online video-sharing
platform demonstrates that our approach ensures an effective and controllable
reduction of unwanted recommendations with minimal effort. The source code is
available here: https://github.com/geektoni/mitigating-harm-recsys.

</details>


### [316] [LLM4MEA: Data-free Model Extraction Attacks on Sequential Recommenders via Large Language Models](https://arxiv.org/abs/2507.16969)
*Shilong Zhao,Fei Sun,Kaike Zhang,Shaoling Jing,Du Su,Zhichao Shi,Zhiyi Yin,Huawei Shen,Xueqi Cheng*

Main category: cs.IR

TL;DR: LLM4MEA利用LLM作为排名者来生成数据，以更准确地提取推荐系统的模型，并提出了防御策略。


<details>
  <summary>Details</summary>
Motivation: 先前的黑盒模型提取攻击（MEAs）由于数据选择中的随机抽样，在暴露推荐系统漏洞方面效果不佳，这导致了合成数据和真实世界数据分布的不一致。为了克服这一限制，需要一种新的方法。

Method: 提出了一种名为LLM4MEA的新型模型提取方法，该方法利用大型语言模型（LLM）作为人类模拟的排名者来生成数据。通过LLM排名者与目标推荐系统的交互来生成数据。在每次交互中，LLM排名者会分析历史交互以了解用户行为，并从具有一致偏好的推荐中选择物品以扩展交互历史，用作MEA的训练数据。

Result: LLM4MEA在数据质量和攻击性能方面显著优于现有方法，将合成数据与真实世界数据的偏差减少了多达64.98%，并将MEA性能平均提高了44.82%。

Conclusion: LLM4MEA通过利用LLM作为类似人类的排名者来生成数据，在数据质量和攻击性能方面显著优于现有方法，将合成数据与真实世界数据的偏差减少了多达64.98%，并将MEA性能平均提高了44.82%。此外，还提出了一种有效的防御策略，并确定了可减轻MEA风险的关键超参数。

Abstract: Recent studies have demonstrated the vulnerability of sequential recommender
systems to Model Extraction Attacks (MEAs). MEAs collect responses from
recommender systems to replicate their functionality, enabling unauthorized
deployments and posing critical privacy and security risks. Black-box attacks
in prior MEAs are ineffective at exposing recommender system vulnerabilities
due to random sampling in data selection, which leads to misaligned synthetic
and real-world distributions. To overcome this limitation, we propose LLM4MEA,
a novel model extraction method that leverages Large Language Models (LLMs) as
human-like rankers to generate data. It generates data through interactions
between the LLM ranker and target recommender system. In each interaction, the
LLM ranker analyzes historical interactions to understand user behavior, and
selects items from recommendations with consistent preferences to extend the
interaction history, which serves as training data for MEA. Extensive
experiments demonstrate that LLM4MEA significantly outperforms existing
approaches in data quality and attack performance, reducing the divergence
between synthetic and real-world data by up to 64.98% and improving MEA
performance by 44.82% on average. From a defensive perspective, we propose a
simple yet effective defense strategy and identify key hyperparameters of
recommender systems that can mitigate the risk of MEAs.

</details>


### [317] [VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings](https://arxiv.org/abs/2507.17080)
*Ramin Giahi,Kehui Yao,Sriram Kollipara,Kai Zhao,Vahid Mirjalili,Jianpeng Xu,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: 为解决电商推荐中Vision-Language模型的对齐、文本模糊和领域不匹配问题，提出VL-CLIP框架，结合视觉基础和LLM增强文本，实现了CTR、ATC和GMV的显著提升，效果优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有vision-language模型（如CLIP）在电子商务推荐系统中存在三个关键挑战：1）弱对象级对齐，全局图像嵌入无法捕捉精细的产品属性；2）模糊的文本表示，产品描述缺乏上下文清晰度；3）领域不匹配，通用模型难以很好地泛化到电子商务特定数据。这些问题导致了次优的检索性能。

Method: 提出VL-CLIP框架，通过集成视觉基础（Visual Grounding）来精细化图像表示（定位关键产品），并利用基于LLM的代理生成增强的文本嵌入（消歧产品描述），以克服现有vision-language模型在电子商务推荐中的局限性（如弱对象级对齐、模糊文本表示和领域不匹配）。

Result: VL-CLIP框架在数千万商品上显著提高了检索准确性、多模态检索效果和推荐质量，具体表现为：点击率（CTR）提升18.6%，加购率（ATC）提升15.5%，商品交易总额（GMV）提升4.0%。实验结果还表明，VL-CLIP在精度和语义对齐方面优于CLIP、FashionCLIP和GCL等vision-language模型。

Conclusion: VL-CLIP框架通过结合视觉基础和LLM增强的文本表示，显著提高了电子商务推荐系统的检索准确性、多模态检索效果和推荐质量，并在实际应用中带来了CTR、ATC和GMV的提升，优于现有的CLIP、FashionCLIP和GCL等模型。

Abstract: Multimodal learning plays a critical role in e-commerce recommendation
platforms today, enabling accurate recommendations and product understanding.
However, existing vision-language models, such as CLIP, face key challenges in
e-commerce recommendation systems: 1) Weak object-level alignment, where global
image embeddings fail to capture fine-grained product attributes, leading to
suboptimal retrieval performance; 2) Ambiguous textual representations, where
product descriptions often lack contextual clarity, affecting cross-modal
matching; and 3) Domain mismatch, as generic vision-language models may not
generalize well to e-commerce-specific data. To address these limitations, we
propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating
Visual Grounding for fine-grained visual understanding and an LLM-based agent
for generating enriched text embeddings. Visual Grounding refines image
representations by localizing key products, while the LLM agent enhances
textual features by disambiguating product descriptions. Our approach
significantly improves retrieval accuracy, multimodal retrieval effectiveness,
and recommendation quality across tens of millions of items on one of the
largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by
15.5%, and GMV by 4.0%. Additional experimental results show that our framework
outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in
both precision and semantic alignment, demonstrating the potential of combining
object-aware visual grounding and LLM-enhanced text representation for robust
multimodal recommendations.

</details>


### [318] [Enhancing Transferability and Consistency in Cross-Domain Recommendations via Supervised Disentanglement](https://arxiv.org/abs/2507.17112)
*Yuhan Wang,Qing Xie,Zhifeng Bao,Mengzi Tang,Lin Li,Yongjian Liu*

Main category: cs.IR

TL;DR: DGCDR是一种新的跨域推荐方法，通过GNN增强的编码器-解码器框架和创新的解耦策略，解决了现有方法的局限性，在推荐任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的跨域推荐（CDR）方法虽然可以通过解耦表示学习来分离用户偏好中的域共享和域特定特征，但仍面临两大挑战：1. 预分离策略在提取协同信号之前就分离特征，破坏了域内交互并引入了噪声；2. 无监督解耦目标缺乏显式的特定任务指导，导致一致性和对齐性不佳。

Method: 提出了一种增强图神经网络（GNN）的编码器-解码器框架DGCDR。为了解决预分离策略导致的挑战，DGCDR首先应用GNN提取高阶协同信号，为解耦提供了一个更鲁棒的基础。然后，编码器动态地将特征解耦为域共享和域特定空间，在分离过程中保留了协同信息。为了解决无监督解耦目标缺乏显式任务指导的挑战，解码器引入了基于锚点的监督，利用分层特征关系来增强域内一致性和跨域对齐。

Result: DGCDR实现了最先进的性能，在关键指标上的提升高达11.59%。定性分析也验证了其优越的解耦质量和可迁移性。

Conclusion: DGCDR在现实世界数据集上的广泛实验表明，其在关键指标上的性能提升高达11.59%，达到了最先进的水平。定性分析进一步验证了其出色的解耦质量和可迁移性。

Abstract: Cross-domain recommendation (CDR) aims to alleviate the data sparsity by
transferring knowledge across domains. Disentangled representation learning
provides an effective solution to model complex user preferences by separating
intra-domain features (domain-shared and domain-specific features), thereby
enhancing robustness and interpretability. However, disentanglement-based CDR
methods employing generative modeling or GNNs with contrastive objectives face
two key challenges: (i) pre-separation strategies decouple features before
extracting collaborative signals, disrupting intra-domain interactions and
introducing noise; (ii) unsupervised disentanglement objectives lack explicit
task-specific guidance, resulting in limited consistency and suboptimal
alignment. To address these challenges, we propose DGCDR, a GNN-enhanced
encoder-decoder framework. To handle challenge (i), DGCDR first applies GNN to
extract high-order collaborative signals, providing enriched representations as
a robust foundation for disentanglement. The encoder then dynamically
disentangles features into domain-shared and -specific spaces, preserving
collaborative information during the separation process. To handle challenge
(ii), the decoder introduces an anchor-based supervision that leverages
hierarchical feature relationships to enhance intra-domain consistency and
cross-domain alignment. Extensive experiments on real-world datasets
demonstrate that DGCDR achieves state-of-the-art performance, with improvements
of up to 11.59% across key metrics. Qualitative analyses further validate its
superior disentanglement quality and transferability. Our source code and
datasets are available on GitHub for further comparison.

</details>


### [319] [R4ec: A Reasoning, Reflection, and Refinement Framework for Recommendation Systems](https://arxiv.org/abs/2507.17249)
*Hao Gu,Rui Zhong,Yu Xia,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: $R^{4}$ec 通过引入“慢思考”机制（推理、反思、优化）提升了 LLM 推荐系统的准确性和鲁棒性，并在实际应用中取得了显著成效。


<details>
  <summary>Details</summary>
Motivation: 现有利用大型语言模型（LLMs）进行推荐系统研究的方法主要依赖于基础的提示技术，这些技术类似于人类的“快思考”模式，容易在推理过程中出错，一个小的失误就可能导致错误的推断。为了解决这个问题，本研究旨在通过引入“慢思考”机制来提升推荐系统的鲁棒性和准确性。

Method: 提出了一种名为 $R^{4}$ec 的框架，该框架包含一个 actor 模型和一个 reflection 模型。Actor 模型负责进行推理，而 reflection 模型则评估 actor 模型的响应并提供反馈。通过迭代式的反思和优化过程，actor 模型能够不断改进其推理过程，最终生成更优的推荐结果，并将这些优化后的知识整合到推荐系统的骨干中以进行预测。

Result: 在 Amazon-Book 和 MovieLens-1M 数据集上的广泛实验证明了 $R^{4}$ec 的优越性。此外，该框架在大型在线广告平台上的部署带来了 2.2% 的收入增长。研究还探讨了 actor 模型和 reflection 模型的扩展性。

Conclusion: 该研究提出了一种名为 $R^{4}$ec 的框架，通过引入推理、反思和优化机制，将推荐系统升级为类似人类“慢思考”的系统，从而提高了推荐的准确性和效率。实验结果表明，$R^{4}$ec 在准确性和用户体验方面优于现有方法，并在实际在线广告平台部署后带来了收入的显著增长。

Abstract: Harnessing Large Language Models (LLMs) for recommendation systems has
emerged as a prominent avenue, drawing substantial research interest. However,
existing approaches primarily involve basic prompt techniques for knowledge
acquisition, which resemble System-1 thinking. This makes these methods highly
sensitive to errors in the reasoning path, where even a small mistake can lead
to an incorrect inference. To this end, in this paper, we propose $R^{4}$ec, a
reasoning, reflection and refinement framework that evolves the recommendation
system into a weak System-2 model. Specifically, we introduce two models: an
actor model that engages in reasoning, and a reflection model that judges these
responses and provides valuable feedback. Then the actor model will refine its
response based on the feedback, ultimately leading to improved responses. We
employ an iterative reflection and refinement process, enabling LLMs to
facilitate slow and deliberate System-2-like thinking. Ultimately, the final
refined knowledge will be incorporated into a recommendation backbone for
prediction. We conduct extensive experiments on Amazon-Book and MovieLens-1M
datasets to demonstrate the superiority of $R^{4}$ec. We also deploy $R^{4}$ec
on a large scale online advertising platform, showing 2.2\% increase of
revenue. Furthermore, we investigate the scaling properties of the actor model
and reflection model.

</details>


### [320] [Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems](https://arxiv.org/abs/2507.17290)
*Li Kang,Yuhan Zhao,Li Chen*

Main category: cs.IR

TL;DR: LLM在推荐系统惊奇度评估中表现优于传统指标，并且通过多LLM技术和辅助数据能进一步提升评估_致性，展现出成为准确且经济高效的评估工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估推荐系统中的惊奇度具有挑战性，现有算法依赖的代理指标与用户真实感知不符。受LLM在评估方法中日益增长的潜力启发，探索LLM是否能有效模拟人类用户进行惊奇度评估。

Method: 通过在源自电子商务和电影领域真实用户研究的两个数据集上进行元评估，重点关注LLM的准确性、辅助数据的影响以及多LLM技术的有效性。

Result: 即使是最简单的零样本LLM，其性能也与传统指标相当或更优。多LLM技术和辅助数据的引入能进一步提升与人类感知的_致性。LLM的最佳评估结果与用户研究结果相比，皮尔逊相关系数达到21.5%。

Conclusion: LLM 可作为有潜力的、准确且经济高效的评估者，为推荐系统中的惊奇度评估引入新范例。

Abstract: Serendipity plays a pivotal role in enhancing user satisfaction within
recommender systems, yet its evaluation poses significant challenges due to its
inherently subjective nature and conceptual ambiguity. Current algorithmic
approaches predominantly rely on proxy metrics for indirect assessment, often
failing to align with real user perceptions, thus creating a gap. With large
language models (LLMs) increasingly revolutionizing evaluation methodologies
across various human annotation tasks, we are inspired to explore a core
research proposition: Can LLMs effectively simulate human users for serendipity
evaluation? To address this question, we conduct a meta-evaluation on two
datasets derived from real user studies in the e-commerce and movie domains,
focusing on three key aspects: the accuracy of LLMs compared to conventional
proxy metrics, the influence of auxiliary data on LLM comprehension, and the
efficacy of recently popular multi-LLM techniques. Our findings indicate that
even the simplest zero-shot LLMs achieve parity with, or surpass, the
performance of conventional metrics. Furthermore, multi-LLM techniques and the
incorporation of auxiliary data further enhance alignment with human
perspectives. Based on our findings, the optimal evaluation by LLMs yields a
Pearson correlation coefficient of 21.5\% when compared to the results of the
user study. This research implies that LLMs may serve as potentially accurate
and cost-effective evaluators, introducing a new paradigm for serendipity
evaluation in recommender systems.

</details>


### [321] [EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations](https://arxiv.org/abs/2507.17323)
*Ruijie Yang,Yan Zhu,Peiyao Fu,Yizhe Zhang,Zhihua Wang,Quanlin Li,Pinghong Zhou,Xian Yang,Shuo Wang*

Main category: cs.IR

TL;DR: 本研究提出了 EndoFinder，一个利用多视图场景表示的在线息肉检索框架，用于可解释和可扩展的结直肠癌诊断。该框架通过自监督学习提取特征，并将息肉表示为三维场景，通过 Transformer 融合多视图信息，实现实时检索和提供可解释的诊断参考，在准确性和可解释性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在结直肠癌（CRC）息肉检测和诊断方面取得了进展，但它们通常需要大量的标注数据，并且输出结果缺乏可解释性。为了解决这些问题，本研究旨在开发一种可扩展且可解释的 CRC 诊断框架。

Method: 提出了一种名为 EndoFinder 的在线息肉检索框架。该框架首先开发了一个息肉感知图像编码器，结合了对比学习和重建任务，并以息肉分割掩码作为指导，实现了无需大规模标注数据的自监督学习。然后，将每个息肉视为一个三维“场景”，并引入了场景表示 Transformer，用于将息肉的多个视图融合到一个单一的潜在表示中。通过哈希层对该表示进行离散化，实现了与历史息肉病例数据库的实时检索，其中诊断信息作为可解释的参考。

Result: EndoFinder 在公开和新收集的息肉数据集上进行了再识别和病理分类评估。结果表明，EndoFinder 在准确性方面优于现有方法，同时提供透明的、基于检索的见解以支持临床决策。

Conclusion: EndoFinder 通过利用多视图场景表示，为可解释和可扩展的 CRC 诊断提供了一个在线息肉检索框架。它通过一个息肉感知图像编码器来提取鲁棒特征，然后通过场景表示 Transformer 融合息肉的多视图信息。该框架通过哈希层将表示离散化，实现了与历史息肉病例数据库的实时检索，并提供可解释的诊断信息。实验证明，EndoFinder 在准确性方面优于现有方法，并为临床决策提供透明的、基于检索的见解，为更有效的 AI 驱动的结肠镜检查工作流程提供了有前景的方向。

Abstract: Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,
underscoring the importance of timely polyp detection and diagnosis. While deep
learning models have improved optical-assisted diagnostics, they often demand
extensive labeled datasets and yield "black-box" outputs with limited
interpretability. In this paper, we propose EndoFinder, an online polyp
retrieval framework that leverages multi-view scene representations for
explainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image
Encoder by combining contrastive learning and a reconstruction task, guided by
polyp segmentation masks. This self-supervised approach captures robust
features without relying on large-scale annotated data. Next, we treat each
polyp as a three-dimensional "scene" and introduce a Scene Representation
Transformer, which fuses multiple views of the polyp into a single latent
representation. By discretizing this representation through a hashing layer,
EndoFinder enables real-time retrieval from a compiled database of historical
polyp cases, where diagnostic information serves as interpretable references
for new queries. We evaluate EndoFinder on both public and newly collected
polyp datasets for re-identification and pathology classification. Results show
that EndoFinder outperforms existing methods in accuracy while providing
transparent, retrieval-based insights for clinical decision-making. By
contributing a novel dataset and a scalable, explainable framework, our work
addresses key challenges in polyp diagnosis and offers a promising direction
for more efficient AI-driven colonoscopy workflows. The source code is
available at https://github.com/ku262/EndoFinder-Scene.

</details>


### [322] ["Beyond the past": Leveraging Audio and Human Memory for Sequential Music Recommendation](https://arxiv.org/abs/2507.17356)
*Viet-Tran Anh,Bruno Sguerra,Gabriel Meseguer-Brocal,Lea Briand,Manuel Moussallam*

Main category: cs.IR

TL;DR: 提出一个结合音频信息和ACT-R的推荐模型，以解决传统基于记忆的推荐模型难以推荐新曲目的问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于记忆的顺序推荐模型在推荐新曲目方面的局限性。

Method: 提出一个利用音频信息预测ACT-R激活并将其纳入推荐评分过程的模型。

Result: 使用专有数据证明了所提出模型的有效性，并公开了数据和源代码。

Conclusion: 该模型利用音频信息预测新曲目的ACT-R激活，并将其纳入推荐评分过程，有效弥补了基于记忆的模型在推荐新曲目方面的不足。

Abstract: On music streaming services, listening sessions are often composed of a
balance of familiar and new tracks. Recently, sequential recommender systems
have adopted cognitive-informed approaches, such as Adaptive Control of
Thought-Rational (ACT-R), to successfully improve the prediction of the most
relevant tracks for the next user session. However, one limitation of using a
model inspired by human memory (or the past), is that it struggles to recommend
new tracks that users have not previously listened to. To bridge this gap, here
we propose a model that leverages audio information to predict in advance the
ACT-R-like activation of new tracks and incorporates them into the
recommendation scoring process. We demonstrate the empirical effectiveness of
the proposed model using proprietary data, which we publicly release along with
the model's source code to foster future research in this field.

</details>


### [323] [Citation Recommendation using Deep Canonical Correlation Analysis](https://arxiv.org/abs/2507.17603)
*Conor McNamara,Effirul Ramlan*

Main category: cs.IR

TL;DR: 一种新的引文推荐算法，使用DCCA而非线性CCA来捕捉非线性关系，在DBLP数据集上提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有基于CCA的引文推荐方法在线性关系捕捉上的局限性，并更好地融合论文的多视图信息。

Method: 提出了一种新的引文推荐算法，采用深度典型相关分析（DCCA）来融合论文的文本和图结构表示，以捕捉它们之间复杂的非线性关系。

Result: 与最先进的基于CCA的方法相比，该算法在平均准确率（MAP@10）、精确率（Precision@10）和召回率（Recall@10）方面分别提高了11%、5%和7%，表明DCCA能够生成更具表现力的潜在表示。

Conclusion: 文章提出了一种新的引文推荐算法，利用深度典型相关分析（DCCA）捕捉文本和图结构表示之间复杂的非线性关系，在DBLP数据集上取得了显著的性能提升，优于现有的基于CCA的方法。

Abstract: Recent advances in citation recommendation have improved accuracy by
leveraging multi-view representation learning to integrate the various
modalities present in scholarly documents. However, effectively combining
multiple data views requires fusion techniques that can capture complementary
information while preserving the unique characteristics of each modality. We
propose a novel citation recommendation algorithm that improves upon linear
Canonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a
neural network extension capable of capturing complex, non-linear relationships
between distributed textual and graph-based representations of scientific
articles. Experiments on the large-scale DBLP (Digital Bibliography & Library
Project) citation network dataset demonstrate that our approach outperforms
state-of-the-art CCA-based methods, achieving relative improvements of over 11%
in Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These
gains reflect more relevant citation recommendations and enhanced ranking
quality, suggesting that DCCA's non-linear transformations yield more
expressive latent representations than CCA's linear projections.

</details>


### [324] [Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for Non-Overlapping Users](https://arxiv.org/abs/2507.17749)
*Weixin Chen,Yuhan Zhao,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 提出了一种为跨域推荐中的非重叠用户生成虚拟用户的方法，以解决现有方法造成的偏见问题，并取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的跨域推荐方法主要利用重叠用户将知识从源域转移到目标域，但存在一个关键的偏见：虽然重叠用户能显著提升推荐质量，但非重叠用户的受益却很小，甚至性能会下降。这种不公平性可能会侵蚀用户信任，并对业务参与度和收入产生负面影响。

Method: 提出了一种为非重叠目标域用户生成虚拟源域用户的方法，该方法利用双重注意力机制来区分重叠用户和非重叠用户之间的相似性，从而合成真实的虚拟用户嵌入。此外，还引入了一个限制器组件，以确保生成的虚拟用户与真实数据分布一致，同时保留每个用户的独特特征。该方法是模型无关的，可以无缝集成到任何跨域推荐模型中。

Result: 在三个公开数据集和五个跨域推荐基线上的综合实验表明，该方法能有效缓解跨域推荐中非重叠用户的偏见，且不损失整体准确性。

Conclusion: 实验证明该方法能有效缓解跨域推荐中非重叠用户的偏差，并且在整体准确性上没有损失。

Abstract: Cross-domain recommendation (CDR) methods predominantly leverage overlapping
users to transfer knowledge from a source domain to a target domain. However,
through empirical studies, we uncover a critical bias inherent in these
approaches: while overlapping users experience significant enhancements in
recommendation quality, non-overlapping users benefit minimally and even face
performance degradation. This unfairness may erode user trust, and,
consequently, negatively impact business engagement and revenue. To address
this issue, we propose a novel solution that generates virtual source-domain
users for non-overlapping target-domain users. Our method utilizes a dual
attention mechanism to discern similarities between overlapping and
non-overlapping users, thereby synthesizing realistic virtual user embeddings.
We further introduce a limiter component that ensures the generated virtual
users align with real-data distributions while preserving each user's unique
characteristics. Notably, our method is model-agnostic and can be seamlessly
integrated into any CDR model. Comprehensive experiments conducted on three
public datasets with five CDR baselines demonstrate that our method effectively
mitigates the CDR non-overlapping user bias, without loss of overall accuracy.
Our code is publicly available at https://github.com/WeixinChen98/VUG.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [325] [Optimal Pure Differentially Private Sparse Histograms in Near-Linear Deterministic Time](https://arxiv.org/abs/2507.17017)
*Florian Kerschbaum,Steven Lee,Hao Wu*

Main category: cs.DS

TL;DR: 该研究提出了一种新的纯差分私有稀疏直方图算法，其时间复杂度优于现有算法，解决了先前悬而未决的问题。


<details>
  <summary>Details</summary>
Motivation: 解决在$n$个参与者和大小为$d \gg n$的域上发布纯差分私有稀疏直方图的开放问题，打破二次障碍。

Method: 提出了一种新颖的私有项目覆盖技术，并带有目标长度填充，将近似差分私有基于稳定性的直方图算法转换为纯差分私有算法。

Result: 成功实现了最优估计误差，并将运行时间从$	ilde{O}(n^2)$提高到$O(n 

 d)$。

Conclusion: 本算法实现了最优的${}$估计误差，并在字随机访问存储器模型中以严格$O(n 

 d)$ 的时间运行，打破了之前$	ilde{O}(n^2)$的确定性时间界限，解决了二次障碍问题（Balcer and Vadhan, 2019）。

Abstract: We introduce an algorithm that releases a pure differentially private sparse
histogram over $n$ participants drawn from a domain of size $d \gg n$. Our
method attains the optimal $\ell_\infty$-estimation error and runs in strictly
$O(n \ln \ln d)$ time in the word-RAM model, thereby improving upon the
previous best known deterministic-time bound of $\tilde{O}(n^2)$ and resolving
the open problem of breaking this quadratic barrier (Balcer and Vadhan, 2019).
Central to our algorithm is a novel private item blanket technique with
target-length padding, which transforms the approximate differentially private
stability-based histogram algorithm into a pure differentially private one.

</details>


### [326] [Compatibility of Max and Sum Objectives for Committee Selection and $k$-Facility Location](https://arxiv.org/abs/2507.17063)
*Yue Han,Elliot Anshelevich*

Main category: cs.DS

TL;DR: 该研究针对设施选址和委员会选择问题，提出了一种能够同时优化多个目标（如客户成本的总和或最大值）的解决方案，证明了多目标兼顾的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于解决实际问题中，通常需要在多个相互冲突的目标之间进行权衡的情况。例如，在选择设施或委员会时，可能希望同时满足客户成本的最小化（无论是总成本还是最大成本）的要求。该研究旨在探索是否存在一种解决方案，能够同时在多个目标上都取得接近最优的表现，从而避免在不同目标之间做出不必要的牺牲。

Method: 该研究通过分析度量设施选址问题及其相关变体（如委员会选择问题），来探讨在任意度量空间中选择k个设施以服务客户C的场景。研究重点关注四种不同的目标函数，这些目标函数从客户和整体的角度出发，分别考虑了距离成本的“总和”和“最大值”。该研究的核心在于评估这些不同目标之间的兼容性，并证明存在一个解能够同时优化任意两个目标。

Result: 研究结果表明，在选择设施或代表委员会时，通常可以找到一个解决方案，该方案能同时在多个目标上都取得优异的表现。这意味着在实际应用中，不必为了优化一个目标而牺牲另一个目标，而是有可能实现多目标的兼顾。

Conclusion: 研究了度量设施选址问题（或等价地，委员会选择问题的变体），目标是在任意度量空间中选择 k 个设施来服务一组客户 C。同时考虑了四种不同的目标函数：客户 i 最小化其到所选设施的距离之和或最大值，以及整体目标考虑客户成本的总和或最大值。研究了这些目标之间的兼容性，并证明了存在同时接近上述任何两个目标的优化解。结果表明，在选择设施集或代表委员会时，通常可以形成一个在多个目标上都表现良好的解决方案，而无需牺牲一个目标来达成另一个。

Abstract: We study a version of the metric facility location problem (or, equivalently,
variants of the committee selection problem) in which we must choose $k$
facilities in an arbitrary metric space to serve some set of clients $C$. We
consider four different objectives, where each client $i\in C$ attempts to
minimize either the sum or the maximum of its distance to the chosen
facilities, and where the overall objective either considers the sum or the
maximum of the individual client costs. Rather than optimizing a single
objective at a time, we study how compatible these objectives are with each
other, and show the existence of solutions which are simultaneously
close-to-optimum for any pair of the above objectives. Our results show that
when choosing a set of facilities or a representative committee, it is often
possible to form a solution which is good for several objectives at the same
time, instead of sacrificing one desideratum to achieve another.

</details>


### [327] [Advancing Quantum State Preparation using LimTDD](https://arxiv.org/abs/2507.17170)
*Xin Hong,Aochu Dai,Chenjian Li,Sanjiang Li,Shenggang Ying,Mingsheng Ying*

Main category: cs.DS

TL;DR: 提出了一种基于LimTDDs的新型量子态制备（QSP）方法，比现有方法更高效，可扩展性更好。


<details>
  <summary>Details</summary>
Motivation: 为了在量子计算和量子信息处理中，尤其是在量子机器学习等算法的执行中，解决关键的量子态制备（QSP）任务，并针对不同数量的辅助量子比特（从无到有，直至足够多）提出一系列高效的QSP算法。

Method: 基于局部可逆映射张量决策图（LimTDDs）的新型决策图，这是一种结合了张量网络和决策图的紧凑态表示，用于降低量子电路复杂度。

Result: 实验证明，所提出的方法在处理大规模量子态时，无论是在运行时间还是门复杂度上，都显著优于现有方法，并具有更好的可扩展性。在某些情况下，还实现了指数级的性能提升。

Conclusion: 提出的量子态制备（QSP）算法在不同数量的辅助量子比特数下都展现出高效性，并且在运行时间和门复杂度方面显著优于现有方法，尤其在最佳情况下呈现指数级改进。

Abstract: Quantum state preparation (QSP) is a fundamental task in quantum computing
and quantum information processing. It is critical to the execution of many
quantum algorithms, including those in quantum machine learning. In this paper,
we propose a family of efficient QSP algorithms tailored to different numbers
of available ancilla qubits - ranging from no ancilla qubits, to a single
ancilla qubit, to a sufficiently large number of ancilla qubits. Our algorithms
are based on a novel decision diagram that is fundamentally different from the
approaches used in previous QSP algorithms. Specifically, our approach exploits
the power of Local Invertible Map Tensor Decision Diagrams (LimTDDs) - a highly
compact representation of quantum states that combines tensor networks and
decision diagrams to reduce quantum circuit complexity. Extensive experiments
demonstrate that our methods significantly outperform existing approaches and
exhibit better scalability for large-scale quantum states, both in terms of
runtime and gate complexity. Furthermore, our method shows exponential
improvement in best-case scenarios. This paper is an extended version of [1],
with three more algorithms proposed.

</details>


### [328] [RLZ-r and LZ-End-r: Enhancing Move-r](https://arxiv.org/abs/2507.17300)
*Patrick Dinklage,Johannes Fischer,Lukas Nalbach,Jan Zumbrink*

Main category: cs.DS

TL;DR: 本文提出使用压缩后缀数组（相对Lempel-Ziv和LZ-End）来优化r-index和Move-r索引，以加速字符串模式匹配中的定位查询。实验证明，这种方法能显著提升查询速度，尤其是在模式出现频率高时，并提供了空间与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的r-index和Move-r索引在回答定位查询时，需要为每个出现的模式评估函数$\Phi$，存在实际瓶颈。为了解决这个问题，本文旨在通过引入压缩后缀数组来增强索引，以实现更快的查询速度，尽管这会牺牲一些空间。

Method: 本文研究了在模式匹配的定位查询中，通过使用压缩后缀数组来增强r-index和Move-r索引。具体采用了两种压缩方案：相对Lempel-Ziv（RLZ）和LZ-End。通过实验评估了这两种方案在加速定位查询方面的性能。

Result: 实验结果表明，与不使用压缩后缀数组的原始索引相比，增强后的r-index和Move-r索引在定位查询方面有显著的速度提升，尤其是在模式出现次数较多的情况下。相对Lempel-Ziv方案在压缩率和访问速度之间取得了较好的平衡，而LZ-End方案则提供了更好的压缩率，但访问速度稍慢。

Conclusion: 通过实验证明，所提出的基于压缩后缀数组的r-index和Move-r索引在处理具有大量出现的模式时，可以显著加速定位查询。两种压缩方案（相对Lempel-Ziv和LZ-End）提供了索引大小与查询性能之间新的权衡选择。

Abstract: In pattern matching on strings, a locate query asks for an enumeration of all
the occurrences of a given pattern in a given text. The r-index [Gagie et al.,
2018] is a recently presented compressed self index that stores the text and
auxiliary information in compressed space. With some modifications, locate
queries can be answered in optimal time [Nishimoto & Tabei, 2021], which has
recently been proven relevant in practice in the form of Move-r [Bertram et
al., 2024]. However, there remains the practical bottleneck of evaluating
function $\Phi$ for every occurrence to report. This motivates enhancing the
index by a compressed representation of the suffix array featuring efficient
random access, trading off space for faster answering of locate queries
[Puglisi & Zhukova, 2021]. In this work, we build upon this idea considering
two suitable compression schemes: Relative Lempel-Ziv [Kuruppu et al., 2010],
improving the work by Puglisi and Zhukova, and LZ-End [Kreft & Navarro, 2010],
introducing a different trade-off where compression is better than for Relative
Lempel-Ziv at the cost of slower access times. We enhance both the r-index and
Move-r by the compressed suffix arrays and evaluate locate query performance in
an experiment. We show that locate queries can be sped up considerably in both
the r-index and Move-r, especially if the queried pattern has many occurrences.
The choice between two different compression schemes offers new trade-offs
regarding index size versus query performance.

</details>


### [329] [Residual Prophet Inequalities](https://arxiv.org/abs/2507.17391)
*Jose Correa,Sebastian Perez-Salazar,Dana Pizarro,Bruno Ziliotto*

Main category: cs.DS

TL;DR: This paper introduces the residual prophet inequality (RPI) problem, a variation of prophet inequality where the top k values are removed. It presents algorithms with competitive ratios for two models (FI and NI), achieving 1/(k+2) in the FI model and 1/(2k+2) in the NI model. For a specific case (i.i.d., k=1), it provides a near-optimal single-threshold strategy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study a variant of the classic prophet inequality problem, called residual prophet inequality (RPI), where the top k values are removed before the remaining n-k values are streamed to the gambler. This problem is relevant in scenarios where a portion of the best values are already allocated elsewhere.

Method: The paper proposes randomized algorithms for the RPI problem in both FI and NI models. The algorithm for the FI model is data-driven and uses the k+1 largest values from a single sample. For the special case of independent and identically distributed instances with k=1, a single-threshold algorithm is analyzed.

Result: In the FI model, a randomized algorithm with a competitive ratio of at least 1/(k+2) is presented, which is shown to be tight. In the NI model, a competitive ratio of 1/(2k+2) is guaranteed. For i.i.d. instances with k=1, a single-threshold algorithm achieves a competitive ratio of at least 0.4901, and the best possible ratio for such strategies is shown to be at most 0.5464.

Conclusion: The paper introduces the residual prophet inequality (RPI) problem and provides algorithms for two variants: the "finite" (FI) and "non-finite" (NI) models. In the FI model, a randomized algorithm achieves a tight competitive ratio of at least 1/(k+2). In the NI model, a similar algorithm guarantees a competitive ratio of 1/(2k+2). For independent and identically distributed instances with k=1, a single-threshold algorithm achieves a competitive ratio of at least 0.4901, with an upper bound of 0.5464.

Abstract: We introduce a variant of the classic prophet inequality, called
\emph{residual prophet inequality} (RPI). In the RPI problem, we consider a
finite sequence of $n$ nonnegative independent random values with known
distributions, and a known integer $0\leq k\leq n-1$. Before the gambler
observes the sequence, the top $k$ values are removed, whereas the remaining
$n-k$ values are streamed sequentially to the gambler. For example, one can
assume that the top $k$ values have already been allocated to a higher-priority
agent. Upon observing a value, the gambler must decide irrevocably whether to
accept or reject it, without the possibility of revisiting past values.
  We study two variants of RPI, according to whether the gambler learns online
of the identity of the variable that he sees (FI model) or not (NI model). Our
main result is a randomized algorithm in the FI model with \emph{competitive
ratio} of at least $1/(k+2)$, which we show is tight. Our algorithm is
data-driven and requires access only to the $k+1$ largest values of a single
sample from the $n$ input distributions. In the NI model, we provide a similar
algorithm that guarantees a competitive ratio of $1/(2k+2)$. We further analyze
independent and identically distributed instances when $k=1$. We build a
single-threshold algorithm with a competitive ratio of at least 0.4901, and
show that no single-threshold strategy can get a competitive ratio greater than
0.5464.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [330] [Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs](https://arxiv.org/abs/2507.17233)
*Marco Ciccalè,Daniel Jurjo-Rivas,Jose F. Morales,Pedro López-García,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 本研究提出了一种静态验证高阶（C）LP 程序及其高阶断言的新方法，通过将高阶谓词属性归约为一阶属性，并利用抽象解释技术进行处理，并在 Ciao 系统中进行了实现和评估。


<details>
  <summary>Details</summary>
Motivation: 在（C）LP 的背景下，高阶断言的运行时验证已得到关注，但编译时验证仍未得到充分探索。本研究旨在解决这一空白，探索高阶（C）LP 程序中高阶断言的编译时验证。

Method: 提出了一种新颖的方法，用于静态验证具有高阶断言的高阶（C）LP 程序。该方法通过谓词属性来描述高阶参数，这些属性利用了（Ciao）断言语言。通过精炼这些属性的语法和语义，并引入一个抽象标准来在编译时确定它们是否符合谓词属性，基于比较谓词属性与谓词断言的语义顺序关系。

Result: 报告了一个原型实现，并通过 Ciao 系统中的各种示例对其进行了评估，证明了该方法的有效性。

Conclusion: 该方法通过将高阶谓词属性归约为一阶属性，并利用基于抽象解释的静态分析器来处理它们，实现了高阶（C）LP 程序中高阶断言的静态验证。

Abstract: Higher-order constructs enable more expressive and concise code by allowing
procedures to be parameterized by other procedures. Assertions allow expressing
partial program specifications, which can be verified either at compile time
(statically) or run time (dynamically). In higher-order programs, assertions
can also describe higher-order arguments. While in the context of (C)LP,
run-time verification of higher-order assertions has received some attention,
compile-time verification remains relatively unexplored. We propose a novel
approach for statically verifying higher-order (C)LP programs with higher-order
assertions. Although we use the Ciao assertion language for illustration, our
approach is quite general and we believe is applicable to similar contexts.
Higher-order arguments are described using predicate properties -- a special
kind of property which exploits the (Ciao) assertion language. We refine the
syntax and semantics of these properties and introduce an abstract criterion to
determine conformance to a predicate property at compile time, based on a
semantic order relation comparing the predicate property with the predicate
assertions. We then show how to handle these properties using an abstract
interpretation-based static analyzer for programs with first-order assertions
by reducing predicate properties to first-order properties. Finally, we report
on a prototype implementation and evaluate it through various examples within
the Ciao system.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [331] [Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond](https://arxiv.org/abs/2507.17074)
*Sanzida Hoque,Abdullah Aydeger,Engin Zeydan,Madhusanka Liyanage*

Main category: cs.CR

TL;DR: 本研究在 5G 网络环境下对后量子密码学（PQC）算法进行了实际性能评估，发现在延迟敏感应用方面，ML-KEM 和 ML-DSA 表现最优，而 SPHINCS+ 和 HQC 的开销较大，不适用于对时间要求高的场景。


<details>
  <summary>Details</summary>
Motivation: 由于量子计算威胁经典公钥密码系统的安全性，需要过渡到后量子密码学（PQC），但 PQC 在实际无线通信环境中的性能尚待探索。

Method: 使用完整的 5G 仿真堆栈（Open5GS 和 UERANSIM）以及支持 PQC 的 TLS 1.3（通过 BoringSSL 和 liboqs），在现实网络条件下，对密钥封装机制和数字签名方案进行了测试。

Result: 在不同的加密配置和客户端负载下，基于握手延迟、CPU 和内存使用、带宽以及重传率对 PQC 算法进行了评估。

Conclusion: ML-KEM 和 ML-DSA 是 5G 网络中延迟敏感应用的最高效选择，而 SPHINCS+ 和 HQC 组合由于计算和传输开销较高，不适用于安全关键但对时间敏感的 5G 场景。

Abstract: The advent of quantum computing threatens the security of classical
public-key cryptographic systems, prompting the transition to post-quantum
cryptography (PQC). While PQC has been analyzed in theory, its performance in
practical wireless communication environments remains underexplored. This paper
presents a detailed implementation and performance evaluation of NIST-selected
PQC algorithms in user equipment (UE) to UE communications over 5G networks.
Using a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3
via BoringSSL and liboqs, we examine key encapsulation mechanisms and digital
signature schemes across realistic network conditions. We evaluate performance
based on handshake latency, CPU and memory usage, bandwidth, and retransmission
rates, under varying cryptographic configurations and client loads. Our
findings show that ML-KEM with ML-DSA offers the best efficiency for
latency-sensitive applications, while SPHINCS+ and HQC combinations incur
higher computational and transmission overheads, making them unsuitable for
security-critical but time-sensitive 5G scenarios.

</details>


### [332] [CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples](https://arxiv.org/abs/2507.16840)
*Weijia Yang,Tian Lan,Leyuan Liu,Wei Chen,Tianqing Zhu,Sheng Wen,Xiaosong Zhang*

Main category: cs.CR

TL;DR: CASPER是一种新的对比学习框架，可以有效且经济高效地检测智能合约中的庞氏骗局，即使在标记数据有限的情况下也能取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于深度学习的庞氏骗局检测方法通常依赖于全监督模型，而这种模型需要大量的标记数据。然而，这类数据通常很稀缺，阻碍了有效的模型训练。为了应对这一挑战，需要一种新的方法来提高在标记数据有限的情况下智能庞氏骗局检测的性能。

Method: CASPER（一种对比学习方法，用于检测具有更多负样本的智能庞氏骗局）利用对比学习技术，使用无标签数据集学习智能合约源代码的更有效表示，从而显著降低运营成本和系统复杂性。

Result: CASPER在XBlock数据集上进行了评估，在100%标记数据下，其F1分数比基线模型高出2.3%。更令人瞩目的是，在仅有25%标记数据的情况下，CASPER的F1分数比在相同实验条件下进行训练的基线模型高出近20%。

Conclusion: CASPER在XBlock数据集上进行了评估，在100%标记数据下，其F1分数比基线模型高出2.3%。更令人瞩目的是，在仅有25%标记数据的情况下，CASPER的F1分数比在相同实验条件下进行训练的基线模型高出近20%。这些结果凸显了CASPER在检测智能庞氏骗局方面的有效性和成本效益，为未来可扩展的欺诈检测解决方案铺平了道路。

Abstract: The rapid evolution of digital currency trading, fueled by the integration of
blockchain technology, has led to both innovation and the emergence of smart
Ponzi schemes. A smart Ponzi scheme is a fraudulent investment operation in
smart contract that uses funds from new investors to pay returns to earlier
investors. Traditional Ponzi scheme detection methods based on deep learning
typically rely on fully supervised models, which require large amounts of
labeled data. However, such data is often scarce, hindering effective model
training. To address this challenge, we propose a novel contrastive learning
framework, CASPER (Contrastive Approach for Smart Ponzi detectER with more
negative samples), designed to enhance smart Ponzi scheme detection in
blockchain transactions. By leveraging contrastive learning techniques, CASPER
can learn more effective representations of smart contract source code using
unlabeled datasets, significantly reducing both operational costs and system
complexity. We evaluate CASPER on the XBlock dataset, where it outperforms the
baseline by 2.3% in F1 score when trained with 100% labeled data. More
impressively, with only 25% labeled data, CASPER achieves an F1 score nearly
20% higher than the baseline under identical experimental conditions. These
results highlight CASPER's potential for effective and cost-efficient detection
of smart Ponzi schemes, paving the way for scalable fraud detection solutions
in the future.

</details>


### [333] [SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique Mapping](https://arxiv.org/abs/2507.16852)
*Álvaro Ruiz-Ródenas,Jaime Pujante Sáez,Daniel García-Algora,Mario Rodríguez Béjar,Jorge Blasco,José Luis Hernández-Ramos*

Main category: cs.CR

TL;DR: SynthCTI通过生成合成数据解决CTI分类中的数据不足和类别不平衡问题，显著提升模型性能，即使小模型也能超越未增强的大模型。


<details>
  <summary>Details</summary>
Motivation: 手动进行CTI挖掘中的威胁描述到MITRE ATT&CK技术的映射耗时耗力，且需要专业知识。现有的自动化方法面临高质量标记CTI数据稀疏和类别不平衡的挑战，而大多数近期研究侧重于模型架构而非数据局限性。

Method: SynthCTI是一个数据增强框架，旨在为代表性不足的MITRE ATT&CK技术生成高质量的合成CTI句子。该方法采用基于聚类的策略来提取训练数据的语义上下文，并指导语言模型（LLM）生成词汇多样且语义忠实的合成CTI句子。

Result: 在两个公开的CTI数据集CTI-to-MITRE和TRAM上，使用SynthCTI生成的数据可以持续提升模型的宏观F1分数。例如，ALBERT模型的宏观F1分数从0.35提升到0.52（相对提升48.6%），SecureBERT模型的宏观F1分数达到0.6558（从0.4412提升）。使用SynthCTI增强的小型模型性能优于未增强的大型模型。

Conclusion: SynthCTI通过生成高质量的合成CTI句子来解决CTI分类中的数据稀疏性和类别不平衡问题，显着提高了模型的性能，特别是对于样本量少的MITRE ATT&CK技术。通过使用聚类方法提取上下文并指导LLM生成多样化且忠实的合成数据，SynthCTI能够提升包括ALBERT和SecureBERT在内的各种模型性能。值得注意的是，经过SynthCTI增强的小型模型甚至优于未增强的大型模型，证明了数据生成方法在构建高效CTI分类系统中的价值。

Abstract: Cyber Threat Intelligence (CTI) mining involves extracting structured
insights from unstructured threat data, enabling organizations to understand
and respond to evolving adversarial behavior. A key task in CTI mining is
mapping threat descriptions to MITRE ATT\&CK techniques. However, this process
is often performed manually, requiring expert knowledge and substantial effort.
Automated approaches face two major challenges: the scarcity of high-quality
labeled CTI data and class imbalance, where many techniques have very few
examples. While domain-specific Large Language Models (LLMs) such as SecureBERT
have shown improved performance, most recent work focuses on model architecture
rather than addressing the data limitations. In this work, we present SynthCTI,
a data augmentation framework designed to generate high-quality synthetic CTI
sentences for underrepresented MITRE ATT\&CK techniques. Our method uses a
clustering-based strategy to extract semantic context from training data and
guide an LLM in producing synthetic CTI sentences that are lexically diverse
and semantically faithful. We evaluate SynthCTI on two publicly available CTI
datasets, CTI-to-MITRE and TRAM, using LLMs with different capacity.
Incorporating synthetic data leads to consistent macro-F1 improvements: for
example, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\%), and
SecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented
with SynthCTI outperform larger models trained without augmentation,
demonstrating the value of data generation methods for building efficient and
effective CTI classification systems.

</details>


### [334] [Building a robust OAuth token based API Security: A High level Overview](https://arxiv.org/abs/2507.16870)
*Senthilkumar Gopal*

Main category: cs.CR

TL;DR: 本文是关于如何构建安全的、可扩展的、基于令牌的API安全系统。


<details>
  <summary>Details</summary>
Motivation: 随着API的广泛应用，其安全挑战日益严峻，需要系统化、可扩展的解决方案来实现安全的身份验证和授权。

Method: 本文提出并讨论了构建基于令牌的API安全系统的基本组成部分、OAuth 2.0的集成、令牌架构的可扩展性、必要的密码学基础以及持久性策略。

Result: 通过遵循这些原则，开发者可以建立一个健壮的基线，同时保持定制特定域需求的灵活性，为应对不断变化的威胁环境做好准备。

Conclusion: 本文旨在为开发安全的、可扩展的、基于令牌的API安全系统提供基础知识，重点是OAuth 2.0、令牌架构、密码学和持久性策略，同时强调生命周期管理、范围定义、过期策略和撤销机制的最佳实践。

Abstract: APIs (Application Programming Interfaces) or Web Services are the
foundational building blocks that enable interconnected systems. However this
proliferation of APIs has also introduced security challenges that require
systematic and scalable solutions for secure authentication and authorization.
This paper presents the fundamentals necessary for building a such a
token-based API security system. It discusses the components necessary, the
integration of OAuth 2.0, extensibility of the token architectures, necessary
cryptographic foundations, and persistence strategies to ensure secure and
resilient operations. In addition to architectural concerns, the paper explores
best practices for token lifecycle management, scope definition, expiration
policies, and revocation mechanisms, all framed within a real-world scenario.
By adhering to these principles, developers can establish a robust baseline
while maintaining the flexibility to customize their domain-specific
requirements. The approach does not claim to cover all variations necessary for
diverse architectures but instead focuses on key principles essential for any
standard API token authentication system. Throughout, the paper emphasizes
balancing practical considerations with security imperatives and uses key
concepts such as the CIA triad, OAuth standards, secure token life cycle, and
practices for protecting sensitive user and application data. The intent is to
equip developers with the foundational knowledge necessary to build secure,
scalable token-based API security systems ready to handle the evolving threat
landscape.

</details>


### [335] [CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage](https://arxiv.org/abs/2507.16872)
*Na Li,Yansong Gao,Hongsheng Hu,Boyu Kuang,Anmin Fu*

Main category: cs.CR

TL;DR: CompLeak框架通过成员推断攻击评估模型压缩的隐私风险，发现压缩会引入隐私泄露，且多压缩模型会加剧泄露。


<details>
  <summary>Details</summary>
Motivation: 现有的模型压缩研究主要关注资源效率和模型性能的权衡，而忽视了压缩过程引入的隐私风险。

Method: 通过成员推断攻击（MIA）的视角，提出了CompLeak框架，该框架包含CompLeakNR、CompLeakSR和CompLeakMR三个变体，分别针对不同可用信息（单个压缩模型、原始模型和单个压缩模型、多个压缩模型）的情况进行隐私风险评估。

Result: 在七种不同的模型架构和六个基准数据集上进行的大量实验证明，CompLeak框架能够有效地评估模型压缩带来的隐私泄露风险，并且在不同压缩配置下，隐私泄露程度不同，多压缩模型的使用会显著增加隐私泄露。

Conclusion: 该研究首次提出了CompLeak框架，用于评估模型压缩（包括剪枝、量化和权重聚类）带来的隐私泄露风险，并证明了压缩配置会影响成员和非成员的区分度，以及多压缩模型可以加剧隐私泄露。

Abstract: Model compression is crucial for minimizing memory storage and accelerating
inference in deep learning (DL) models, including recent foundation models like
large language models (LLMs). Users can access different compressed model
versions according to their resources and budget. However, while existing
compression operations primarily focus on optimizing the trade-off between
resource efficiency and model performance, the privacy risks introduced by
compression remain overlooked and insufficiently understood.
  In this work, through the lens of membership inference attack (MIA), we
propose CompLeak, the first privacy risk evaluation framework examining three
widely used compression configurations that are pruning, quantization, and
weight clustering supported by the commercial model compression framework of
Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has
three variants, given available access to the number of compressed models and
original model. CompLeakNR starts by adopting existing MIA methods to attack a
single compressed model, and identifies that different compressed models
influence members and non-members differently. When the original model and one
compressed model are available, CompLeakSR leverages the compressed model as a
reference to the original model and uncovers more privacy by combining meta
information (e.g., confidence vector) from both models. When multiple
compressed models are available with/without accessing the original model,
CompLeakMR innovatively exploits privacy leakage info from multiple compressed
versions to substantially signify the overall privacy leakage. We conduct
extensive experiments on seven diverse model architectures (from ResNet to
foundation models of BERT and GPT-2), and six image and textual benchmark
datasets.

</details>


### [336] [Enabling Cyber Security Education through Digital Twins and Generative AI](https://arxiv.org/abs/2507.17518)
*Vita Santa Barletta,Vito Bavaro,Miriana Calvano,Antonio Curci,Antonio Piccinno,Davide Pio Posa*

Main category: cs.CR

TL;DR: 数字孪生与渗透测试工具（RTK）和大型语言模型（LLM）的结合，通过模拟现实网络环境，提高了网络安全教育和实践技能的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了研究将数字孪生与渗透测试工具和大型语言模型（LLM）集成，如何提高网络安全教育和操作准备能力。

Method: 本研究将数字孪生与渗透测试工具（特别是Red Team Knife）和大型语言模型（LLM）相结合，创建了一个模拟现实网络环境的框架，用于探索漏洞和防御策略。

Result: 初步结果表明，这种集成显著提高了网络安全培训的有效性和相关性，缩小了理论知识与实际应用之间的差距。

Conclusion: 该研究表明，数字孪生（DT）和大型语言模型（LLM）的结合可以革新网络安全教育，以满足不断变化的行业需求。

Abstract: Digital Twins (DTs) are gaining prominence in cybersecurity for their ability
to replicate complex IT (Information Technology), OT (Operational Technology),
and IoT (Internet of Things) infrastructures, allowing for real time
monitoring, threat analysis, and system simulation. This study investigates how
integrating DTs with penetration testing tools and Large Language Models (LLMs)
can enhance cybersecurity education and operational readiness. By simulating
realistic cyber environments, this approach offers a practical, interactive
framework for exploring vulnerabilities and defensive strategies. At the core
of this research is the Red Team Knife (RTK), a custom penetration testing
toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide
learners through key phases of cyberattacks, including reconnaissance,
exploitation, and response within a DT powered ecosystem. The incorporation of
Large Language Models (LLMs) further enriches the experience by providing
intelligent, real-time feedback, natural language threat explanations, and
adaptive learning support during training exercises. This combined DT LLM
framework is currently being piloted in academic settings to develop hands on
skills in vulnerability assessment, threat detection, and security operations.
Initial findings suggest that the integration significantly improves the
effectiveness and relevance of cybersecurity training, bridging the gap between
theoretical knowledge and real-world application. Ultimately, the research
demonstrates how DTs and LLMs together can transform cybersecurity education to
meet evolving industry demands.

</details>


### [337] [Revisiting Pre-trained Language Models for Vulnerability Detection](https://arxiv.org/abs/2507.16887)
*Youpeng Li,Weiliang Qi,Xuyu Wang,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: 本研究对17个预训练语言模型（PLMs）在代码漏洞检测（VD）任务中的表现进行了广泛评估，发现结合了代码语法和语义模式预训练任务的模型效果更佳。然而，这些模型在处理复杂依赖、代码扰动和长代码时仍存在局限性，并且上下文窗口的截断会导致标签错误。研究强调了全面评估和未来改进的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有针对漏洞检测（VD）的PLMs的实证研究在数据准备、评估设置和实验设置方面考虑不周，导致评估的准确性和全面性受到影响。因此，需要对PLMs在真实场景中的性能进行更全面的评估。

Method: 本研究通过构建新数据集，对17个PLMs（包括小型代码特定PLMs和大型PLMs）进行了广泛评估。评估方法包括微调和提示工程。研究比较了PLMs在不同训练和测试设置下的有效性和泛化能力，并分析了它们对代码归一化、抽象和语义保持转换的鲁棒性。

Result: 在代码漏洞检测任务中，结合了捕获代码语法和语义模式的预训练任务的PLMs优于通用PLMs以及仅在大型代码语料库上进行预训练或微调的模型。然而，这些模型在真实场景中仍面临挑战，例如检测复杂依赖关系的漏洞、处理代码归一化和抽象引入的扰动，以及识别语义保持的漏洞代码转换。此外，PLMs的有限上下文窗口导致的截断会引起不可忽略的标签错误。

Conclusion: 虽然预训练语言模型（PLMs）在代码相关任务中表现出有前景的结果，但它们在检测真实世界漏洞方面仍然面临重大挑战。本研究通过对17个PLMs进行广泛评估，比较了它们在不同设置下的性能，并分析了它们在处理代码归一化、抽象和语义保持转换等扰动时的鲁棒性。研究结果表明，在代码漏洞检测任务中，结合了捕获代码语法和语义模式的预训练任务的PLMs优于通用PLMs以及仅在大型代码语料库上进行预训练或微调的模型。然而，这些模型在真实场景中仍面临挑战，例如检测复杂依赖关系的漏洞、处理代码归一化和抽象引入的扰动，以及识别语义保持的漏洞代码转换。此外，PLMs的有限上下文窗口导致的截断会引起不可忽略的标签错误。本研究强调了在实际场景中进行全面模型性能评估的重要性，并为提高PLMs在现实漏洞检测应用中的有效性指出了未来方向。

Abstract: The rapid advancement of pre-trained language models (PLMs) has demonstrated
promising results for various code-related tasks. However, their effectiveness
in detecting real-world vulnerabilities remains a critical challenge. % for the
security community. While existing empirical studies evaluate PLMs for
vulnerability detection (VD), their inadequate consideration in data
preparation, evaluation setups, and experimental settings undermines the
accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,
an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and
large-scale PLMs using newly constructed datasets. Specifically, we compare the
performance of PLMs under both fine-tuning and prompt engineering, assess their
effectiveness and generalizability across various training and testing
settings, and analyze their robustness against code normalization, abstraction,
and semantic-preserving transformations.
  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks
designed to capture the syntactic and semantic patterns of code outperform both
general-purpose PLMs and those solely pre-trained or fine-tuned on large code
corpora. However, these models face notable challenges in real-world scenarios,
such as difficulties in detecting vulnerabilities with complex dependencies,
handling perturbations introduced by code normalization and abstraction, and
identifying semantic-preserving vulnerable code transformations. Also, the
truncation caused by the limited context windows of PLMs can lead to a
non-negligible amount of labeling errors. This study underscores the importance
of thorough evaluations of model performance in practical scenarios and
outlines future directions to help enhance the effectiveness of PLMs for
realistic VD applications.

</details>


### [338] [Evaluating Ensemble and Deep Learning Models for Static Malware Detection with Dimensionality Reduction Using the EMBER Dataset](https://arxiv.org/abs/2507.16952)
*Md Min-Ha-Zul Abedin,Tazqia Mehrub*

Main category: cs.CR

TL;DR: LightGBM and XGBoost are best for static malware detection. Choose dimensionality reduction methods carefully based on the model type.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of several machine learning algorithms for static malware detection using the EMBER dataset and to provide a benchmark for comparing classification models and preprocessing strategies.

Method: The study evaluates eight classification models (LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees, HistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet) using the EMBER dataset under three preprocessing settings (original feature space, PCA, and LDA). Models are assessed on accuracy, precision, recall, F1 score, and AUC. Exploratory data analysis (EDA) including mutual information ranking, PCA/t-SNE visualizations, and outlier detection (Isolation Forest, LOF) supports the analysis.

Result: Ensemble methods, particularly LightGBM and XGBoost, demonstrated the best overall performance with minimal sensitivity to PCA and consistent generalization. LDA improved KNN but reduced performance for boosting models. TabNet underperformed, likely due to architectural sensitivity to input structure. EDA confirmed the discriminatory capacity of key features in the EMBER dataset.

Conclusion: Boosting models remain the most reliable choice for high-dimensional static malware detection, and dimensionality reduction should be applied selectively based on model type.

Abstract: This study investigates the effectiveness of several machine learning
algorithms for static malware detection using the EMBER dataset, which contains
feature representations of Portable Executable (PE) files. We evaluate eight
classification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,
HistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three
preprocessing settings: original feature space, Principal Component Analysis
(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on
accuracy, precision, recall, F1 score, and AUC to examine both predictive
performance and robustness. Ensemble methods, especially LightGBM and XGBoost,
show the best overall performance across all configurations, with minimal
sensitivity to PCA and consistent generalization. LDA improves KNN performance
but significantly reduces accuracy for boosting models. TabNet, while promising
in theory, underperformed under feature reduction, likely due to architectural
sensitivity to input structure. The analysis is supported by detailed
exploratory data analysis (EDA), including mutual information ranking, PCA or
t-SNE visualizations, and outlier detection using Isolation Forest and Local
Outlier Factor (LOF), which confirm the discriminatory capacity of key features
in the EMBER dataset. The results suggest that boosting models remain the most
reliable choice for high-dimensional static malware detection, and that
dimensionality reduction should be applied selectively based on model type.
This work provides a benchmark for comparing classification models and
preprocessing strategies in malware detection tasks and contributes insights
that can guide future system development and real-world deployment.

</details>


### [339] [From Cracks to Crooks: YouTube as a Vector for Malware Distribution](https://arxiv.org/abs/2507.16996)
*Iman Vakilinia*

Main category: cs.CR

TL;DR: 网络犯罪分子利用YouTube推广恶意软件，利用欺骗性视频和多语言元数据规避检测。


<details>
  <summary>Details</summary>
Motivation: YouTube作为拥有数十亿用户和海量上传内容的平台，已成为网络犯罪分子的目标，他们利用该平台的开放性和可信度进行欺骗性宣传活动。

Method: 通过分析YouTube平台上的恶意活动，特别是网络犯罪分子如何利用欺骗性视频演示和元数据技术来传播恶意软件。

Result: 发现网络犯罪分子利用YouTube的多语言元数据功能开发了一种新的规避技术，以逃避自动化检测系统的检测。该技术正被越来越多地用于恶意视频中，以躲避检测和移除。

Conclusion: 网络犯罪分子正在利用YouTube的元数据功能来逃避检测，尤其是在推广免费软件或游戏作弊器的活动中。

Abstract: With billions of users and an immense volume of daily uploads, YouTube has
become an attractive target for cybercriminals aiming to leverage its vast
audience. The platform's openness and trustworthiness provide an ideal
environment for deceptive campaigns that can operate under the radar of
conventional security tools. This paper explores how cybercriminals exploit
YouTube to disseminate malware, focusing on campaigns that promote free
software or game cheats. It discusses deceptive video demonstrations and the
techniques behind malware delivery. Additionally, the paper presents a new
evasion technique that abuses YouTube's multilingual metadata capabilities to
circumvent automated detection systems. Findings indicate that this method is
increasingly being used in recent malicious videos to avoid detection and
removal.

</details>


### [340] [The Postman: A Journey of Ethical Hacking in PosteID/SPID Borderland](https://arxiv.org/abs/2507.17007)
*Gabriele Costa*

Main category: cs.CR

TL;DR: 对意大利公共数字身份系统 PosteID 的漏洞进行了评估，发现了一个关键的特权升级漏洞，并对分析和披露过程进行了介绍。


<details>
  <summary>Details</summary>
Motivation: 展示 PosteID 的漏洞评估活动，该活动发现了关键的特权升级漏洞，并最终进行了修补。

Method: 对 PosteID（意大利邮政 Poste Italiane 实施的意大利公共数字身份系统）进行了漏洞评估。

Result: 发现了一个关键的特权升级漏洞，并进行了修补。

Conclusion: 该分析和披露过程为道德黑客社区提供了宝贵的案例研究。

Abstract: This paper presents a vulnerability assessment activity that we carried out
on PosteID, the implementation of the Italian Public Digital Identity System
(SPID) by Poste Italiane. The activity led to the discovery of a critical
privilege escalation vulnerability, which was eventually patched. The overall
analysis and disclosure process represents a valuable case study for the
community of ethical hackers. In this work, we present both the technical steps
and the details of the disclosure process.

</details>


### [341] [Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs](https://arxiv.org/abs/2507.17010)
*H M Mohaimanul Islam,Huynh Q. N. Vo,Aditya Rane*

Main category: cs.CR

TL;DR: TrustDefender是一个结合了CNN和ZKP的两阶段框架，用于在XR环境中实时检测深度伪影，同时保护用户隐私，准确率达到95.3%。


<details>
  <summary>Details</summary>
Motivation: 为了应对深度伪影对信息完整性构成的威胁，同时满足XR平台计算能力的限制和敏感环境下的隐私要求。

Method: 提出了一种名为TrustDefender的两阶段框架：第一阶段使用轻量级卷积神经网络（CNN）实时检测扩展现实（XR）流中的深度伪影；第二阶段集成了一个简洁的零知识证明（ZKP）协议，在不泄露用户原始数据的情况下验证检测结果。

Result: TrustDefender在多个基准深度伪影数据集上实现了95.3%的检测准确率，并具有高效的证明生成能力，能够与高性能人工智能（AI）系统无缝集成。

Conclusion: TrustDefender通过结合卷积神经网络（CNN）和零知识证明（ZKP）协议，在计算效率和隐私保护方面取得了良好的平衡，为在沉浸式和注重隐私的应用中实现可靠的AI奠定了基础。

Abstract: In the era of synthetic media, deepfake manipulations pose a significant
threat to information integrity. To address this challenge, we propose
TrustDefender, a two-stage framework comprising (i) a lightweight convolutional
neural network (CNN) that detects deepfake imagery in real-time extended
reality (XR) streams, and (ii) an integrated succinct zero-knowledge proof
(ZKP) protocol that validates detection results without disclosing raw user
data. Our design addresses both the computational constraints of XR platforms
while adhering to the stringent privacy requirements in sensitive settings.
Experimental evaluations on multiple benchmark deepfake datasets demonstrate
that TrustDefender achieves 95.3% detection accuracy, coupled with efficient
proof generation underpinned by rigorous cryptography, ensuring seamless
integration with high-performance artificial intelligence (AI) systems. By
fusing advanced computer vision models with provable security mechanisms, our
work establishes a foundation for reliable AI in immersive and
privacy-sensitive applications.

</details>


### [342] [GATEBLEED: Exploiting On-Core Accelerator Power Gating for High Performance & Stealthy Attacks on AI](https://arxiv.org/abs/2507.17033)
*Joshua Kalyanapu,Farshad Dizani,Darsh Asher,Azam Ghanbari,Rosario Cammarota,Aydin Aysu,Samira Mirbagher Ajorpaz*

Main category: cs.CR

TL;DR: GATEBLEED是一种新的侧信道攻击，利用CPU电源门控，能够窃取AI模型中的敏感信息，并在多种AI库和模型上进行了有效攻击。


<details>
  <summary>Details</summary>
Motivation: 随着AI训练和推理功耗的增加，CPU集成了AI加速器，但这也带来了新的安全风险。研究动机是发现并量化这些加速器可能存在的侧信道攻击，特别是针对AI隐私的威胁。

Method: GATEBLEED通过利用CPU电源门控导致的定时延迟，在AI加速器（如Intel AMX）上执行矩阵乘法时，可以作为潜在的泄露点。研究者们识别了多个库中的潜在攻击点，并成功实施了端到端的微架构推理攻击。

Result: 研究发现了GATEBLEED侧信道，并在Transformer模型上实现了81%的成员推理准确率和0.89的精确率。对于基于CNN或Transformer的混合专家模型，专家选择的泄露准确率达到了100%。

Conclusion: GATEBLEED是一种利用CPU电源门控实现的高性能、隐蔽的侧信道攻击，可用于窃取AI模型中的敏感信息，包括成员推理和专家选择。

Abstract: As power consumption from AI training and inference continues to increase, AI
accelerators are being integrated directly into the CPU. Intel's Advanced
Matrix Extensions (AMX) is one such example, debuting on the 4th generation
Intel Xeon Scalable CPU. We discover a timing side and covert channel,
GATEBLEED, caused by the aggressive power gating utilized to keep the CPU
within operating limits. We show that the GATEBLEED side channel is a threat to
AI privacy as many ML models such as transformers and CNNs make critical
computationally-heavy decisions based on private values like confidence
thresholds and routing logits. Timing delays from selective powering down of
AMX components mean that each matrix multiplication is a potential leakage
point when executed on the AMX accelerator. Our research identifies over a
dozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,
TensorFlow, etc.), revealing that they can leak sensitive and private
information. GATEBLEED poses a risk for local and remote timing inference, even
under previous protective measures. GATEBLEED can be used as a high
performance, stealthy remote covert channel and a generic magnifier for timing
transmission channels, capable of bypassing traditional cache defenses to leak
arbitrary memory addresses and evading state of the art microarchitectural
attack detectors under realistic network conditions and system configurations
in which previous attacks fail. We implement an end-to-end microarchitectural
inference attack on a transformer model optimized with Intel AMX, achieving a
membership inference accuracy of 81% and a precision of 0.89. In a CNN-based or
transformer-based mixture-of-experts model optimized with Intel AMX, we leak
expert choice with 100% accuracy.

</details>


### [343] [SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure](https://arxiv.org/abs/2507.17064)
*Nafisa Anjum,Tasnuva Farheen*

Main category: cs.CR

TL;DR: 本研究全面分析了太空网络攻击向量和缓解措施，并提出了一个风险评分框架，以应对日益增长的网络威胁。


<details>
  <summary>Details</summary>
Motivation: 随着现代技术的发展，关键基础设施、通信和国家安全日益依赖于天基资产，这些资产以及数据中继系统和地面站等相关资产面临着严峻的网络攻击威胁。因此，需要强大的安全防御措施来确保数据完整性、维持安全操作以及保护太空和地面资产免受各种威胁。

Method: 本研究全面分析了包括地面、太空、卫星和卫星星座在内的各种可能的太空网络攻击向量，并评估了与太空基础设施相关的缓解措施的有效性，最后提出了一个风险评分框架。

Result: 本研究分析了各种可能的太空网络攻击向量，并评估了缓解措施的有效性，提出了一个风险评分框架，并确定了太空网络安全领域的研究挑战。

Conclusion: 本研究识别了网络安全措施在太空领域的发展和测试中的潜在研究挑战，并鼓励在太空领域采取强有力的网络安全措施。

Abstract: With the advent of modern technology, critical infrastructure,
communications, and national security depend increasingly on space-based
assets. These assets, along with associated assets like data relay systems and
ground stations, are, therefore, in serious danger of cyberattacks. Strong
security defenses are essential to ensure data integrity, maintain secure
operations, and protect assets in space and on the ground against various
threats. Previous research has found discrete vulnerabilities in space systems
and suggested specific solutions to address them. Such research has yielded
valuable insights, but lacks a thorough examination of space cyberattack
vectors and a rigorous assessment of the efficacy of mitigation techniques.
This study tackles this issue by taking a comprehensive approach to analyze the
range of possible space cyber-attack vectors, which include ground, space,
satellite, and satellite constellations. In order to address the particular
threats, the study also assesses the efficacy of mitigation measures that are
linked with space infrastructures and proposes a Risk Scoring Framework. Based
on the analysis, this paper identifies potential research challenges for
developing and testing cutting-edge technology solutions, encouraging robust
cybersecurity measures needed in space.

</details>


### [344] [A Privacy-Preserving Data Collection Method for Diversified Statistical Analysis](https://arxiv.org/abs/2507.17180)
*Hao Jiang,Quan Zhou,Dongdong Zhao,Shangshang Yang,Wenjian Luo,Xingyi Zhang*

Main category: cs.CR

TL;DR: 提出了一种名为RVNS的新型真实值负调查模型，用于在保护隐私的同时收集敏感信息的分布，解决了现有方法无法处理实值数据的问题，并得到实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据扰动的方法在保护隐私的同时，往往忽视了从分布角度衡量数据整体质量，难以满足多样化的统计分析需求。特别是负调查方法，虽然能保护隐私并收集敏感信息分布，但主要针对离散数据，无法处理实值数据分布。

Method: 提出了一种名为RVNS的新型真实值负调查模型，并通过优化问题来准确捕捉敏感信息的分布。该模型允许用户从与其真实敏感细节有偏差的范围内采样数据，而无需进行数据离散化。

Result: RVNS模型被证明符合差分隐私模型，具有良好的隐私保护能力。在合成和真实世界数据集上的广泛实验证实了该方法的有效性。

Conclusion: 该研究提出了首个用于真实值敏感信息收集的负调查模型RVNS，并为解决其优化问题提供了一种新颖的方法。理论分析表明RVNS符合差分隐私模型，实验证明了其有效性。

Abstract: Data perturbation-based privacy-preserving methods have been widely adopted
in various scenarios due to their efficiency and the elimination of the need
for a trusted third party. However, these methods primarily focus on individual
statistical indicators, neglecting the overall quality of the collected data
from a distributional perspective. Consequently, they often fall short of
meeting the diverse statistical analysis requirements encountered in practical
data analysis. As a promising sensitive data perturbation method, negative
survey methods is able to complete the task of collecting sensitive information
distribution while protecting personal privacy. Yet, existing negative survey
methods are primarily designed for discrete sensitive information and are
inadequate for real-valued data distributions. To bridge this gap, this paper
proposes a novel real-value negative survey model, termed RVNS, for the first
time in the field of real-value sensitive information collection. The RVNS
model exempts users from the necessity of discretizing their data and only
requires them to sample a set of data from a range that deviates from their
actual sensitive details, thereby preserving the privacy of their genuine
information. Moreover, to accurately capture the distribution of sensitive
information, an optimization problem is formulated, and a novel approach is
employed to solve it. Rigorous theoretical analysis demonstrates that the RVNS
model conforms to the differential privacy model, ensuring robust privacy
preservation. Comprehensive experiments conducted on both synthetic and
real-world datasets further validate the efficacy of the proposed method.

</details>


### [345] [Threshold-Protected Searchable Sharing: Privacy Preserving Aggregated-ANN Search for Collaborative RAG](https://arxiv.org/abs/2507.17199)
*Ruoyang Rykie Guo*

Main category: cs.CR

TL;DR: 本研究提出了一种名为SP-A$^2$NN的安全隐私聚合近似最近邻搜索方法，解决了LLM搜索服务中数据集成面临的隐私和兼容性挑战。该方法兼容HNSW索引，提高了搜索效率，并通过新颖的安全分析框架解决了AI发展中的泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 尽管数据集成是LLM驱动的搜索服务的重要趋势，但私人数据存储的局部约束以及与主流搜索技术（特别是HNSW索引）的兼容性问题阻碍了其发展。本研究旨在解决这些瓶颈，通过SP-A$^2$NN方法实现安全、注重隐私的数据集成，从而提高专业查询的响应相关性和质量，并使AI在提供服务时更加专业。

Method: 本研究开发了一种名为SP-A$^2$NN的安全隐私聚合近似最近邻搜索方法，该方法在阈值可搜索共享原语下兼容HNSW索引。研究构建并扩展了一个可共享的位图结构，以支持在共享数据上的搜索和动态插入，同时不破坏底层图拓扑。在理论方面，研究探索了一个新颖的安全分析框架，通过归纳法整合隐私分析，并构建了一个基于全新交互式博弈的泄漏猜测证明系统。

Result: 与朴素的（无向）图共享方法相比，该方法在以与HNSW相同的方式组织图时，将搜索复杂度从O(n$^2$)降低到O(n)。该方法能实现在共享数据上进行搜索和动态插入，同时不损害底层图拓扑。

Conclusion: 该研究提出了一种名为SP-A$^2$NN的安全隐私聚合近似最近邻搜索方法，该方法兼容HNSW索引，并能在共享数据上进行搜索和动态插入，同时保护数据隐私。理论上，研究提出了一个新颖的安全分析框架，通过归纳法进行隐私分析，并构建了一个基于交互式博弈的泄漏猜测证明系统，以解决AI发展中的关键安全挑战——泄漏分析。

Abstract: LLM-powered search services have driven data integration as a significant
trend. However, this trend's progress is fundamentally hindered, despite the
fact that combining individual knowledge can significantly improve the
relevance and quality of responses in specialized queries and make AI more
professional at providing services. Two key bottlenecks are private data
repositories' locality constraints and the need to maintain compatibility with
mainstream search techniques, particularly Hierarchical Navigable Small World
(HNSW) indexing for high-dimensional vector spaces. In this work, we develop a
secure and privacy-preserving aggregated approximate nearest neighbor search
(SP-A$^2$NN) with HNSW compatibility under a threshold-based searchable sharing
primitive. A sharable bitgraph structure is constructed and extended to support
searches and dynamical insertions over shared data without compromising the
underlying graph topology. The approach reduces the complexity of a search from
$O(n^2)$ to $O(n)$ compared to naive (undirected) graph-sharing approach when
organizing graphs in the identical HNSW manner.
  On the theoretical front, we explore a novel security analytical framework
that incorporates privacy analysis via reductions. The proposed
leakage-guessing proof system is built upon an entirely different interactive
game that is independent of existing coin-toss game design. Rather than being
purely theoretical, this system is rooted in existing proof systems but goes
beyond them to specifically address leakage concerns and standardize leakage
analysis -- one of the most critical security challenges with AI's rapid
development.

</details>


### [346] [Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs](https://arxiv.org/abs/2507.17259)
*Eyal German,Sagiv Antebi,Daniel Samira,Asaf Shabtai,Yuval Elovici*

Main category: cs.CR

TL;DR: 该研究提出了 Tab-MIA 数据集，用于评估 LLM 在处理表格数据时的隐私风险。研究发现，LLM 在记忆表格数据时会受到编码格式的影响，并且即使经过少量微调，也容易受到会员推断攻击，AUROC 分数接近 90%。


<details>
  <summary>Details</summary>
Motivation: LLM 正在被训练用于表格数据，但与非结构化文本不同，表格数据通常包含高度结构化和明确的个人身份信息（PII）。这带来了隐私风险，因为敏感记录可能被模型无意中保留并通过数据提取或会员推断攻击（MIAs）暴露。现有的 MIA 方法主要针对文本内容，但它们在应用于结构化数据时的有效性和威胁含义可能不同。

Method: 提出 Tab-MIA，一个用于评估 LLM 上表格数据 MIAs 的基准数据集，并展示了其用法。Tab-MIA 包含五个数据集，每种数据集都有六种不同的编码格式。使用 Tab-MIA，对在多种编码格式的表格数据上进行微调的 LLM 进行了最新的 MIA 方法的首次评估。

Result: LLM 会以因编码格式而异的方式记忆表格数据，使其容易受到会员推断攻击（MIAs）的提取。即使经过少量（低至三个 epoch）的微调，模型也表现出高度的脆弱性，在大多数情况下 AUROC 分数接近 90%。

Conclusion: LLMs 会以因编码格式而异的方式记忆表格数据，使其容易受到会员推断攻击（MIAs）的提取。即使经过少量（低至三个 epoch）的微调，模型也表现出高度的脆弱性，在大多数情况下 AUROC 分数接近 90%。

Abstract: Large language models (LLMs) are increasingly trained on tabular data, which,
unlike unstructured text, often contains personally identifiable information
(PII) in a highly structured and explicit format. As a result, privacy risks
arise, since sensitive records can be inadvertently retained by the model and
exposed through data extraction or membership inference attacks (MIAs). While
existing MIA methods primarily target textual content, their efficacy and
threat implications may differ when applied to structured data, due to its
limited content, diverse data types, unique value distributions, and
column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset
for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.
Tab-MIA comprises five data collections, each represented in six different
encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation
of state-of-the-art MIA methods on LLMs finetuned with tabular data across
multiple encoding formats. In the evaluation, we analyze the memorization
behavior of pretrained LLMs on structured data derived from Wikipedia tables.
Our findings show that LLMs memorize tabular data in ways that vary across
encoding formats, making them susceptible to extraction via MIAs. Even when
fine-tuned for as few as three epochs, models exhibit high vulnerability, with
AUROC scores approaching 90% in most cases. Tab-MIA enables systematic
evaluation of these risks and provides a foundation for developing
privacy-preserving methods for tabular data in LLMs.

</details>


### [347] [An Empirical Study on Virtual Reality Software Security Weaknesses](https://arxiv.org/abs/2507.17324)
*Yifan Xu,Jinfu Chen,Zhenyu Qi,Huashan Chen,Junyi Wang,Pengfei Hu,Feng Liu,Sen He*

Main category: cs.CR

TL;DR: 对GitHub上的VR项目进行安全漏洞分析，发现用户界面漏洞最多，开发工具风险较高，漏洞多在项目初期出现。


<details>
  <summary>Details</summary>
Motivation: 鉴于公共数据库中VR软件安全漏洞的可用性有限，本研究旨在系统性地收集和分析VR软件安全漏洞，以了解其普遍性、引入时间和移除方式。

Method: 通过分析GitHub上的334个VR项目中的1,681个软件安全漏洞，并构建了一个从GitHub提交数据收集VR软件安全漏洞的新颖框架，进行了实证研究。

Result: 研究发现，VR漏洞主要集中在用户界面方面，其次是资源相关漏洞。VR开发工具比VR应用程序带来更高的安全风险。此外，VR安全漏洞通常在VR软件开发初期引入。

Conclusion: VR软件安全漏洞研究表明，用户界面和资源相关漏洞最为普遍，开发工具的风险高于应用程序，且漏洞多在软件诞生之初引入。

Abstract: Virtual Reality (VR) has emerged as a transformative technology across
industries, yet its security weaknesses, including vulnerabilities, are
underinvestigated. This study investigates 334 VR projects hosted on GitHub,
examining 1,681 software security weaknesses to understand: what types of
weaknesses are prevalent in VR software; {\em when} and {\em how} weaknesses
are introduced; how long they have survived; and how they have been removed.
Due to the limited availability of VR software security weaknesses in public
databases (e.g., the National Vulnerability Database or NVD), we prepare the
{first systematic} dataset of VR software security weaknesses by introducing a
novel framework to collect such weaknesses from GitHub commit data. Our
empirical study on the dataset leads to useful insights, including: (i) VR
weaknesses are heavily skewed toward user interface weaknesses, followed by
resource-related weaknesses; (ii) VR development tools pose higher security
risks than VR applications; (iii) VR security weaknesses are often introduced
at the VR software birth time.

</details>


### [348] [A Zero-overhead Flow for Security Closure](https://arxiv.org/abs/2507.17385)
*Mohammad Eslami,Ashira Johara,Kyungbin Park,Samuel Pagliarini*

Main category: cs.CR

TL;DR: 提出了一种新的安全感知ASIC设计流程，可在不牺牲性能的情况下防御硬件木马和物理探测。


<details>
  <summary>Details</summary>
Motivation: 传统ASIC设计流程忽略了安全性评估，导致在保证性能的同时无法防御硬件木马和物理探测等安全威胁。

Method: 提出了一种安全感知型ASIC设计流程，并将其完全集成于商业工具中，能够同时满足安全性和性能要求。

Result: 实现了一种零开销的安全闭合流程，能够有效防御硬件木马和物理探测，且在ISPD'22基准电路测试中表现优异，同时开创性地实现了安全与性能的平衡。

Conclusion: 该方法在ISPD'22基准电路测试中取得了现有最好结果，同时对安全策略的开销极小。

Abstract: In the traditional Application-Specific Integrated Circuit (ASIC) design
flow, the concept of timing closure implies to reach convergence during
physical synthesis such that, under a given area and power budget, the design
works at the targeted frequency. However, security has been largely neglected
when evaluating the Quality of Results (QoR) from physical synthesis. In
general, commercial place & route tools do not understand security goals. In
this work, we propose a modified ASIC design flow that is security-aware and,
differently from prior research, does not degrade QoR for the sake of security
improvement. Therefore, we propose a first-of-its-kind zero-overhead flow for
security closure. Our flow is concerned with two distinct threat models: (i)
insertion of Hardware Trojans (HTs) and (ii) physical probing/fault injection.
Importantly, the flow is entirely executed within a commercial place & route
engine and is scalable. In several metrics, our security-aware flow achieves
the best-known results for the ISPD`22 set of benchmark circuits while
incurring negligible design overheads due to security-related strategies.
Finally, we open source the entire methodology (as a set of scripts) and also
share the protected circuits (as design databases) for the benefit of the
hardware security community.

</details>


### [349] [Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement](https://arxiv.org/abs/2507.17491)
*Nazatul H. Sultan,Xinlong Guan,Josef Pieprzyk,Wei Ni,Sharif Abuadbba,Hajime Suzuki*

Main category: cs.CR

TL;DR: 本文提出了一种改进的 5G 认证协议，解决了现有 5G-AKA 协议的安全和性能问题，通过无状态设计和完美前向保密性提高了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 5G 网络安全和用户认证至关重要，但现有的 5G-AKA 协议存在安全漏洞（易受主动攻击）和性能问题（依赖序列号同步，缺乏完美前向保密性）。

Method: 本文提出了一种增强型认证协议，首先通过引入无状态设计消除了对序列号的依赖，然后扩展该设计以增加完美前向保密性（PFS）。使用 ProVerif 对协议进行了严格的安全分析，并进行了原型实现和性能评估。

Result: 所提出的协议被证明在安全性上优于 5G-AKA，同时仅带来微小的计算开销，并成功解决了序列号同步和完美前向保密性问题。

Conclusion: 该论文提出的增强型认证协议通过引入无状态设计和完美前向保密性（PFS），解决了 5G-AKA 协议在安全性和性能方面的局限性。协议通过了 ProVerif 的严格安全分析，并进行了原型实现和性能评估，结果表明该协议在提供更强安全性的同时，仅带来微小的计算开销，是面向未来的实用解决方案。

Abstract: As 5G networks expand into critical infrastructure, secure and efficient user
authentication is more important than ever. The 5G-AKA protocol, standardized
by 3GPP in TS 33.501, is central to authentication in current 5G deployments.
It provides mutual authentication, user privacy, and key secrecy. However,
despite its adoption, 5G-AKA has known limitations in both security and
performance. While it focuses on protecting privacy against passive attackers,
recent studies show its vulnerabilities to active attacks. It also relies on a
sequence number mechanism to prevent replay attacks, requiring perfect
synchronization between the device and the core network. This stateful design
adds complexity, causes desynchronization, and incurs extra communication
overhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing
past communications if long-term keys are compromised-an increasing concern
amid sophisticated threats. This paper proposes an enhanced authentication
protocol that builds on 5G-AKA's design while addressing its shortcomings.
First, we introduce a stateless version that removes sequence number reliance,
reducing complexity while staying compatible with existing SIM cards and
infrastructure. We then extend this design to add PFS with minimal
cryptographic overhead. Both protocols are rigorously analyzed using ProVerif,
confirming their compliance with all major security requirements, including
resistance to passive and active attacks, as well as those defined by 3GPP and
academic studies. We also prototype both protocols and evaluate their
performance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the
proposed protocols offer stronger security with only minor computational
overhead, making them practical, future-ready solutions for 5G and beyond.

</details>


### [350] [Frequency Estimation of Correlated Multi-attribute Data under Local Differential Privacy](https://arxiv.org/abs/2507.17516)
*Shafizur Rahman Seeam,Ye Zheng,Yidan Hu*

Main category: cs.CR

TL;DR: Corr-RR 是一种新的 LDP 机制，通过利用属性相关性来提高数据效用，同时保持隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有 LDP 机制在处理多属性数据集时，要么跨属性分配隐私预算，要么独立处理属性，忽略了它们之间的自然相关性，导致噪声过大或预算分散，在高维情况下效用损失显著。

Method: Corr-RR 机制包括两个阶段：(1) 部分用户估计属性相关性；(2) 剩余用户扰动一个属性并利用学到的相关性推断其他属性。

Result: 实验证明 Corr-RR 在合成和真实数据集上持续优于最先进的 LDP 机制。

Conclusion: Corr-RR 优于现有 LDP 机制，尤其是在高维和强相关属性场景下，并具有理论保证。

Abstract: Large-scale data collection, from national censuses to IoT-enabled smart
homes, routinely gathers dozens of attributes per individual. These
multi-attribute datasets are vital for analytics but pose significant privacy
risks. Local Differential Privacy (LDP) is a powerful tool to protect user data
privacy by allowing users to locally perturb their records before releasing to
an untrusted data aggregator. However, existing LDP mechanisms either split the
privacy budget across all attributes or treat each attribute independently,
ignoring natural inter-attribute correlations. This leads to excessive noise or
fragmented budgets, resulting in significant utility loss, particularly in
high-dimensional settings.
  To overcome these limitations, we propose Correlated Randomized Response
(Corr-RR), a novel LDP mechanism that leverages correlations among attributes
to substantially improve utility while maintaining rigorous LDP guarantees.
Corr-RR allocates the full privacy budget to perturb a single, randomly
selected attribute and reconstructs the remaining attributes using estimated
interattribute dependencies, without incurring additional privacy cost. To
enable this, Corr-RR operates in two phases: (1) a subset of users apply
standard LDP mechanisms to estimate correlations, and (2) each remaining user
perturbs one attribute and infers the others using the learned correlations. We
theoretically prove that Corr-RR satisfies $\epsilon$-LDP, and extensive
experiments on synthetic and real-world datasets demonstrate that Corr-RR
consistently outperforms state-of-the-art LDP mechanisms, particularly in
scenarios with many attributes and strong inter-attribute correlations.

</details>


### [351] [Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses](https://arxiv.org/abs/2507.17655)
*Shams Shaikh,Trima P. Fernandes e Fizardo*

Main category: cs.CR

TL;DR: HSM和TPM在云环境中面临安全挑战，需要更先进的策略。


<details>
  <summary>Details</summary>
Motivation: 随着组织迁移到云端，加密密钥管理的安全性日益受到关注，而传统的HSM和TPM正面临云原生威胁的挑战。

Method: 分析了涉及HSM和TPM的重大安全故障，识别了常见的攻击向量，并探讨了机密计算、后量子密码学和去中心化密钥管理等替代方法。

Result: 研究结果表明，HSM和TPM在分布式环境中的有效性受到质疑，并强调了更具适应性和分层的架构在现代云安全中的必要性。

Conclusion: 云原生威胁对HSM和TPM的安全性构成了挑战。云环境中的系统性漏洞（如配置错误、API滥用、权限提升）可能导致攻击者访问敏感密钥。虽然HSM和TPM仍然重要，但现代云安全需要更具适应性和分层的架构。

Abstract: As organizations rapidly migrate to the cloud, the security of cryptographic
key management has become a growing concern. Hardware Security Modules (HSMs)
and Trusted Platform Modules (TPMs), traditionally seen as the gold standard
for securing encryption keys and digital trust, are increasingly challenged by
cloud-native threats. Real-world breaches have exposed weaknesses in cloud
deployments, including misconfigurations, API abuse, and privilege escalations,
allowing attackers to access sensitive key material and bypass protections.
These incidents reveal that while the hardware remains secure, the surrounding
cloud ecosystem introduces systemic vulnerabilities. This paper analyzes
notable security failures involving HSMs and TPMs, identifies common attack
vectors, and questions longstanding assumptions about their effectiveness in
distributed environments. We explore alternative approaches such as
confidential computing, post-quantum cryptography, and decentralized key
management. Our findings highlight that while HSMs and TPMs still play a role,
modern cloud security requires more adaptive, layered architectures. By
evaluating both current weaknesses and emerging models, this research equips
cloud architects and security engineers with strategies to reinforce
cryptographic trust in the evolving threat landscape.

</details>


### [352] [Quantifying the ROI of Cyber Threat Intelligence: A Data-Driven Approach](https://arxiv.org/abs/2507.17628)
*Matteo Strada*

Main category: cs.CR

TL;DR: CTI的价值很难量化，因为成功的防御不会留下痕迹。本研究提出了一种新的方法，通过考虑检测和响应时间以及其他因素来衡量CTI的投资回报率，并引入了TIEI指数来更好地评估其整体有效性。


<details>
  <summary>Details</summary>
Motivation: 解决网络威胁情报（CTI）估值中的持续性挑战，即由于负面证据问题，成功的威胁预防导致可观察到的财务影响最小，使得在传统的成本效益框架内难以证明CTI支出的合理性。

Method: 提出了一种数据驱动的方法，扩展了安全经济学中的既有模型（如Gordon-Loeb和FAIR模型），并引入了威胁情报有效性指数（TIEI）作为基于加权几何平均的复合指标，该指标会处罚在关键维度（质量、丰富度、集成度和操作影响）上的表现不佳，以捕捉限制整体性能的最薄弱环节的影响。

Result: 通过三个特定行业（金融、医疗保健和零售）的案例研究，以及基于MTTD、MTTR和对手滞留时间等经验指标的实施，证明了该框架的有效性，并提出了TIEI作为一种衡量CTI性能的方法。

Conclusion: 该研究提出了一种数据驱动的方法来量化网络威胁情报（CTI）的投资回报率（ROI），将其重新定义为风险缓解的可衡量贡献者。通过引入威胁情报有效性指数（TIEI）作为综合指标，并结合财务量化、对抗覆盖和业务赋能的定性评估，该混合模型将负面证据转化为可辩护的ROI解释，从而使CTI能够从一项支出转变为一项战略投资。

Abstract: The valuation of Cyber Threat Intelligence (CTI) remains a persistent
challenge due to the problem of negative evidence: successful threat prevention
results in non-events that generate minimal observable financial impact, making
CTI expenditures difficult to justify within traditional cost-benefit
frameworks. This study introduces a data-driven methodology for quantifying the
return on investment (ROI) of CTI, thereby reframing it as a measurable
contributor to risk mitigation. The proposed framework extends established
models in security economics, including the Gordon-Loeb and FAIR models, to
account for CTI's complex influence on both the probability of security
breaches and the severity of associated losses. The framework is
operationalized through empirically grounded performance indicators, such as
reductions in mean time to detect (MTTD), mean time to respond (MTTR), and
adversary dwell time, supported by three sector-specific case studies in
finance, healthcare, and retail. To address limitations in conventional linear
assessment methodologies, the Threat Intelligence Effectiveness Index (TIEI) is
introduced as a composite metric based on a weighted geometric mean. TIEI
penalizes underperformance across critical dimensions: quality, enrichment,
integration, and operational impact; thereby capturing bottleneck effect where
the least effective component limits overall performance. By integrating
financial quantification, adversarial coverage, and qualitative assessments of
business enablement, the proposed hybrid model converts negative evidence into
a justifiable ROI explanation. This approach offers a replicable means of
repositioning CTI from an expense to a strategic investment, enabling informed
decision-making and continuous optimization across diverse organizational
contexts.

</details>
